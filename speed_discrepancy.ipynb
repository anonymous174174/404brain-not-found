{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anonymous174174/404brain-not-found/blob/main/speed_discrepancy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/anonymous174174/404brain-not-found.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KKIfflRgeYjU",
        "outputId": "796d81c1-c619-44c5-cf79-49b7d965efee"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into '404brain-not-found'...\n",
            "remote: Enumerating objects: 386, done.\u001b[K\n",
            "remote: Counting objects: 100% (75/75), done.\u001b[K\n",
            "remote: Compressing objects: 100% (53/53), done.\u001b[K\n",
            "remote: Total 386 (delta 35), reused 55 (delta 17), pack-reused 311 (from 1)\u001b[K\n",
            "Receiving objects: 100% (386/386), 688.68 KiB | 10.43 MiB/s, done.\n",
            "Resolving deltas: 100% (240/240), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C6vZFT8zdk5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fnjnlzDZdlWv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/404brain-not-found"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "swc-lYANeeOb",
        "outputId": "79d7cba1-8881-426e-c724-41f19d4d2b19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/404brain-not-found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -e .\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qUBxPfHleqJ7",
        "outputId": "e8386d28-95f7-40ce-bb22-ce7baf7d3b19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Obtaining file:///content/404brain-not-found\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting torch>=2.7.1 (from neuronix==0.0.1)\n",
            "  Downloading torch-2.8.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (30 kB)\n",
            "Collecting rustworkx>=0.16.0 (from neuronix==0.0.1)\n",
            "  Downloading rustworkx-0.16.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy<3,>=1.16.0 in /usr/local/lib/python3.11/dist-packages (from rustworkx>=0.16.0->neuronix==0.0.1) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.7.1->neuronix==0.0.1) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.7.1->neuronix==0.0.1) (4.14.1)\n",
            "Collecting sympy>=1.13.3 (from torch>=2.7.1->neuronix==0.0.1)\n",
            "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.7.1->neuronix==0.0.1) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.7.1->neuronix==0.0.1) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.7.1->neuronix==0.0.1) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch>=2.7.1->neuronix==0.0.1)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.8.90 (from torch>=2.7.1->neuronix==0.0.1)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.8.90 (from torch>=2.7.1->neuronix==0.0.1)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.10.2.21 (from torch>=2.7.1->neuronix==0.0.1)\n",
            "  Downloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-cublas-cu12==12.8.4.1 (from torch>=2.7.1->neuronix==0.0.1)\n",
            "  Downloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cufft-cu12==11.3.3.83 (from torch>=2.7.1->neuronix==0.0.1)\n",
            "  Downloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.9.90 (from torch>=2.7.1->neuronix==0.0.1)\n",
            "  Downloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.7.3.90 (from torch>=2.7.1->neuronix==0.0.1)\n",
            "  Downloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.5.8.93 (from torch>=2.7.1->neuronix==0.0.1)\n",
            "  Downloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-cusparselt-cu12==0.7.1 (from torch>=2.7.1->neuronix==0.0.1)\n",
            "  Downloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
            "Collecting nvidia-nccl-cu12==2.27.3 (from torch>=2.7.1->neuronix==0.0.1)\n",
            "  Downloading nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.8.90 (from torch>=2.7.1->neuronix==0.0.1)\n",
            "  Downloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvjitlink-cu12==12.8.93 (from torch>=2.7.1->neuronix==0.0.1)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cufile-cu12==1.13.1.3 (from torch>=2.7.1->neuronix==0.0.1)\n",
            "  Downloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==3.4.0 (from torch>=2.7.1->neuronix==0.0.1)\n",
            "  Downloading triton-3.4.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.4.0->torch>=2.7.1->neuronix==0.0.1) (75.2.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch>=2.7.1->neuronix==0.0.1) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.7.1->neuronix==0.0.1) (3.0.2)\n",
            "Downloading rustworkx-0.16.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch-2.8.0-cp311-cp311-manylinux_2_28_x86_64.whl (888.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m888.1/888.1 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m594.3/594.3 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m86.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.0/88.0 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m954.8/954.8 kB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl (706.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m706.8/706.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m51.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.6/63.6 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.5/267.5 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.2/288.2 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl (287.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.2/287.2 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (322.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.4/322.4 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m46.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-3.4.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.5/155.5 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m73.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: neuronix\n",
            "  Building editable for neuronix (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for neuronix: filename=neuronix-0.0.1-0.editable-py3-none-any.whl size=4494 sha256=aebec398d5d9cf4d21e374cc27135346b645014e28a962603c479f58f0b9e5c8\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-xpgz206u/wheels/c4/39/f2/6d1f4739e56ddc2ac183ce44c7b967df2556cad956eca74e3a\n",
            "Successfully built neuronix\n",
            "Installing collected packages: nvidia-cusparselt-cu12, triton, sympy, rustworkx, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, neuronix\n",
            "  Attempting uninstall: nvidia-cusparselt-cu12\n",
            "    Found existing installation: nvidia-cusparselt-cu12 0.6.2\n",
            "    Uninstalling nvidia-cusparselt-cu12-0.6.2:\n",
            "      Successfully uninstalled nvidia-cusparselt-cu12-0.6.2\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.2.0\n",
            "    Uninstalling triton-3.2.0:\n",
            "      Successfully uninstalled triton-3.2.0\n",
            "  Attempting uninstall: sympy\n",
            "    Found existing installation: sympy 1.13.1\n",
            "    Uninstalling sympy-1.13.1:\n",
            "      Successfully uninstalled sympy-1.13.1\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.4.127\n",
            "    Uninstalling nvidia-nvtx-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.21.5\n",
            "    Uninstalling nvidia-nccl-cu12-2.21.5:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.21.5\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.6.0+cu124\n",
            "    Uninstalling torch-2.6.0+cu124:\n",
            "      Successfully uninstalled torch-2.6.0+cu124\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "fastai 2.7.19 requires torch<2.7,>=1.10, but you have torch 2.8.0 which is incompatible.\n",
            "torchvision 0.21.0+cu124 requires torch==2.6.0, but you have torch 2.8.0 which is incompatible.\n",
            "torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.8.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed neuronix-0.0.1 nvidia-cublas-cu12-12.8.4.1 nvidia-cuda-cupti-cu12-12.8.90 nvidia-cuda-nvrtc-cu12-12.8.93 nvidia-cuda-runtime-cu12-12.8.90 nvidia-cudnn-cu12-9.10.2.21 nvidia-cufft-cu12-11.3.3.83 nvidia-cufile-cu12-1.13.1.3 nvidia-curand-cu12-10.3.9.90 nvidia-cusolver-cu12-11.7.3.90 nvidia-cusparse-cu12-12.5.8.93 nvidia-cusparselt-cu12-0.7.1 nvidia-nccl-cu12-2.27.3 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvtx-cu12-12.8.90 rustworkx-0.16.0 sympy-1.14.0 torch-2.8.0 triton-3.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd .."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LbGUsm7ZfRwT",
        "outputId": "135c52db-af9b-4040-af1f-311a2cebad6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/404brain-not-found/examples/simple_cnn_model.py\n"
      ],
      "metadata": {
        "id": "hYbNdIwPgQuL",
        "outputId": "27d88e2c-2121-4e40-c170-4b24dffabd7d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Heads up the comparison between torch result and neuronix is with an rtol of 1e-4\n",
            "True\n",
            "NO STRONG REFERENCES FOUND\n",
            "W0806 18:11:12.332000 7911 torch/utils/cpp_extension.py:118] [0/0] No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n",
            "Neuronix implementation backward Takes 5.191701650619507 seconds\n",
            "Pytorch implementation backward Takes 0.3997359275817871 seconds\n",
            "ALL PARAMETERS ARE ON THE SAME DEVICE\n",
            "layer1.conv.weight: ✅ Same\n",
            "layer1.conv.bias: ✅ Same\n",
            "layer1.batchnorm.weight: ✅ Same\n",
            "layer1.batchnorm.bias: ✅ Same\n",
            "layer2.conv.weight: ✅ Same\n",
            "layer2.conv.bias: ✅ Same\n",
            "layer2.batchnorm.weight: ✅ Same\n",
            "layer2.batchnorm.bias: ✅ Same\n",
            "layer3.conv.weight: ✅ Same\n",
            "layer3.conv.bias: ✅ Same\n",
            "layer3.batchnorm.weight: ✅ Same\n",
            "layer3.batchnorm.bias: ✅ Same\n",
            "layer4.conv.weight: ✅ Same\n",
            "layer4.conv.bias: ✅ Same\n",
            "layer4.batchnorm.weight: ✅ Same\n",
            "layer4.batchnorm.bias: ✅ Same\n",
            "layer5.conv.weight: ✅ Same\n",
            "layer5.conv.bias: ✅ Same\n",
            "layer5.batchnorm.weight: ✅ Same\n",
            "layer5.batchnorm.bias: ✅ Same\n",
            "linear1.weight: ✅ Same\n",
            "linear1.bias: ✅ Same\n",
            "output.weight: ✅ Same\n",
            "output.bias: ✅ Same\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "device = \"cpu\"\n",
        "dtype = torch.float32"
      ],
      "metadata": {
        "id": "u8PjB8qQhtYu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Graph\n",
        "import rustworkx as rx\n",
        "import weakref\n",
        "class AutogradGraph:\n",
        "    \"\"\"\n",
        "    Manages the computation graph for automatic differentiation.\n",
        "    It uses a directed acyclic graph to track dependencies between tensors.\n",
        "    \"\"\"\n",
        "    __slots__ = ('graph', 'intermediate_tensors', '_check_cycles', '_auto_cleanup', '__weakref__')\n",
        "\n",
        "    def __init__(self, check_for_cycles=True, auto_cleanup=True):\n",
        "        self.graph = rx.PyDiGraph()\n",
        "        self.intermediate_tensors = {}\n",
        "        self._check_cycles = check_for_cycles\n",
        "        self._auto_cleanup = auto_cleanup\n",
        "\n",
        "    def __enter__(self):\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, exc_type, exc_value, traceback):\n",
        "        if self._check_cycles and self.check_cycle():\n",
        "            raise RuntimeError(\"Cycle detected in autograd graph on context exit.\")\n",
        "        if self._auto_cleanup:\n",
        "            self.intermediate_tensors.clear()\n",
        "            self.graph.clear()\n",
        "\n",
        "    def add_tensor_graph(self, tensor):\n",
        "        if not tensor._custom_requires_grad:\n",
        "            raise ValueError(\"Tensor with requires_grad=False cannot be added to the graph.\")\n",
        "        ref = weakref.proxy(tensor)\n",
        "        tensor_index = self.graph.add_node(ref)\n",
        "        tensor._node_id = tensor_index\n",
        "\n",
        "    def add_non_leaf_tensor_reference(self, tensor):\n",
        "        if not tensor._custom_requires_grad:\n",
        "            raise ValueError(\"Tensor must require grad.\")\n",
        "        if tensor._node_id in self.intermediate_tensors:\n",
        "            raise ValueError(\"Tensor reference already exists in intermediate tensors.\")\n",
        "        self.intermediate_tensors[tensor._node_id] = tensor\n",
        "\n",
        "    def add_edge(self, node_from, node_to, weight=None):\n",
        "        if not all(isinstance(n, int) for n in (node_from, node_to)):\n",
        "            raise TypeError(\"Node indices must be integers.\")\n",
        "        if not self.graph.has_node(node_from) or not self.graph.has_node(node_to):\n",
        "            raise ValueError(\"Nodes must exist before adding edge.\")\n",
        "        self.graph.add_edge(node_from, node_to, weight)\n",
        "\n",
        "    def check_cycle(self):\n",
        "        return not rx.is_directed_acyclic_graph(self.graph)\n",
        "\n",
        "    def reverse_toposort_from_tensor(self, tensor_index):\n",
        "        graph=self.graph\n",
        "        predecessors = list(rx.ancestors(graph, tensor_index))\n",
        "        predecessors.append(tensor_index)\n",
        "        sub_graph = graph.subgraph(predecessors)\n",
        "        return [sub_graph[i] for i in reversed(rx.topological_sort(sub_graph))]\n",
        "    # def alternative_reverse_toposort_from_tensor(self, tensor_index):\n",
        "    #     graph = self.graph\n",
        "    #     relevant_nodes = rx.ancestors(graph, tensor_index)\n",
        "    #     relevant_nodes.add(tensor_index)\n",
        "    #     full_topo = rx.topological_sort(graph)\n",
        "    #     relevant_topo = [graph[_node_id] for _node_id in reversed(full_topo) if _node_id in relevant_nodes]\n",
        "    #     return relevant_topo\n",
        "\n",
        "    def delete_node(self, node_index):\n",
        "        if not isinstance(node_index, int):\n",
        "            raise TypeError(\"Node index must be an integer.\")\n",
        "        if self.graph.has_node(node_index):\n",
        "             self.graph.remove_node(node_index)\n",
        "    def delete_edge(self, node_from, node_to):\n",
        "        if not self.graph.has_edge(node_from, node_to):\n",
        "            raise ValueError(\"Edge does not exist.\")\n",
        "        self.graph.remove_edge(node_from, node_to)\n",
        "\n",
        "    def del_non_leaf_tensor_reference(self, tensor_node_id):\n",
        "        self.intermediate_tensors.pop(tensor_node_id, None)\n",
        "\n",
        "    def delete_all_non_leaf_nodes(self):\n",
        "        # removes non leaf nodes from graph and clears the intermediate_tensors dict\n",
        "        self.graph.remove_nodes_from(list(self.intermediate_tensors.keys()))\n",
        "        for custom_tensor in self.intermediate_tensors.values():custom_tensor.clear()\n",
        "        self.intermediate_tensors.clear()\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"CustomAutogradGraph(nodes={self.graph.num_nodes()}, edges={self.graph.num_edges()})\""
      ],
      "metadata": {
        "cellView": "form",
        "id": "2lkeQWYWhEc-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Custom Tensor\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import weakref\n",
        "import numbers\n",
        "import math\n",
        "\n",
        "class CustomTensor:\n",
        "    \"\"\"\n",
        "    A custom tensor class that wraps a PyTorch tensor to enable a custom\n",
        "    autograd engine. It tracks operations to build a computation graph.\n",
        "    \"\"\"\n",
        "    __slots__ = ('tensor', '_node_id', '_custom_requires_grad', '_backward', 'graph', '__weakref__','_is_leaf')\n",
        "\n",
        "    def __new__(cls, data, *, _custom_requires_grad=False, device=device, dtype=dtype, graph=None, due_to_operation=False, is_leaf=False):\n",
        "        assert device is not None\n",
        "        assert dtype is not None\n",
        "        if isinstance(data, CustomTensor):\n",
        "            return data  # Don't rewrap\n",
        "        return super().__new__(cls)\n",
        "\n",
        "    def __init__(self, data, *, _custom_requires_grad=False, device=device, dtype=dtype, graph=None, due_to_operation=False, is_leaf=False):\n",
        "        if isinstance(data, CustomTensor):\n",
        "            return\n",
        "\n",
        "        self.tensor = data if due_to_operation else torch.as_tensor(data, dtype=dtype, device=device)\n",
        "        self.tensor.requires_grad_(False)\n",
        "        self._custom_requires_grad = _custom_requires_grad\n",
        "        self._node_id = None\n",
        "        self._backward = CustomTensor._empty_backward_hook\n",
        "        self.graph = None\n",
        "        self._is_leaf = is_leaf\n",
        "\n",
        "        if _custom_requires_grad:\n",
        "            self._init_graph(graph)\n",
        "\n",
        "    def _init_graph(self, graph):\n",
        "        if graph is None:\n",
        "            raise ValueError(\"Graph must be provided if requires_grad is True.\")\n",
        "        is_leaf=self._is_leaf\n",
        "        if is_leaf:\n",
        "            self.graph = weakref.proxy(graph)\n",
        "        else:\n",
        "            self.graph = graph # this line is only reached for tensors which are created by operations and graph passed is already a weakreference hence no need for wrapping\n",
        "        graph.add_tensor_graph(self)\n",
        "        if not is_leaf:\n",
        "            graph.add_non_leaf_tensor_reference(self)\n",
        "\n",
        "    def _clear_graph_references(self):\n",
        "        \"\"\"Safely nullifies graph-related attributes to make the tensor picklable.\"\"\"\n",
        "        self.graph = None\n",
        "        self._node_id = None\n",
        "        self._custom_requires_grad = False\n",
        "        self._backward = CustomTensor._empty_backward_hook\n",
        "        self.tensor.grad = None\n",
        "\n",
        "    def _add_to_graph(self,graph):\n",
        "        assert graph is not None\n",
        "        assert self._is_leaf\n",
        "        self._custom_requires_grad = True\n",
        "        self.graph = weakref.proxy(graph)\n",
        "        graph.add_tensor_graph(self)\n",
        "\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def _empty_backward_hook():\n",
        "      \"\"\"A picklable placeholder for the backward function.\"\"\"\n",
        "      return None\n",
        "\n",
        "    def clear(self):\n",
        "        \"\"\"A clearing function for intermediate tensors and saving the model\"\"\"\n",
        "        self.tensor.grad = None\n",
        "        self._custom_requires_grad = False\n",
        "        self._node_id = None\n",
        "        self._backward = CustomTensor._empty_backward_hook#lambda: None\n",
        "        self.graph = None\n",
        "\n",
        "    def del_tensor(self):\n",
        "        # Makes the custom Tensor object completely useless and makes it occupy as little memory as possible\n",
        "        self.tensor = None\n",
        "        self._custom_requires_grad = False\n",
        "        self._node_id = None\n",
        "        self._backward = CustomTensor._empty_backward_hook\n",
        "        self.graph = None\n",
        "        self._is_leaf = False\n",
        "\n",
        "    def _zero_grad(self):\n",
        "        \"\"\"Sets the gradient of the underlying tensor to zero.\"\"\"\n",
        "        if self.tensor.grad is None:\n",
        "            self.tensor.grad = torch.zeros_like(self.tensor)\n",
        "        else:\n",
        "            self.tensor.grad.zero_()\n",
        "\n",
        "    def zero_(self):\n",
        "        \"\"\"Sets the gradient of the underlying tensor to zero.\"\"\"\n",
        "        if self.tensor.grad is not None:\n",
        "            self.tensor.grad.zero_()\n",
        "\n",
        "    def to(self, device, dtype=None):\n",
        "        if dtype is None:\n",
        "            dtype = self.tensor.dtype\n",
        "        self.tensor = self.tensor.to(device, dtype)\n",
        "        return self\n",
        "\n",
        "\n",
        "    # --- Broadcasting Helper ---\n",
        "    @torch.compile\n",
        "    def _reduce_grad_for_broadcast(self, grad, target_shape):\n",
        "        \"\"\"Reduces a gradient to match the shape of a tensor that was broadcasted.\"\"\"\n",
        "        if grad.shape == target_shape:\n",
        "            return grad\n",
        "\n",
        "        # Add singleton dimensions to the front of target_shape to match grad's ndim\n",
        "        padded_target_shape = (1,) * (grad.ndim - len(target_shape)) + target_shape\n",
        "\n",
        "        # Identify dimensions that were broadcasted\n",
        "        sum_dims = [i for i, (grad_dim, target_dim) in enumerate(zip(grad.shape, padded_target_shape)) if target_dim == 1 and grad_dim > 1]\n",
        "\n",
        "        if sum_dims:\n",
        "            grad = grad.sum(dim=sum_dims, keepdim=True)\n",
        "\n",
        "        # Remove singleton dimensions to match the final target shape\n",
        "        return grad.reshape(target_shape)\n",
        "\n",
        "\n",
        "\n",
        "    def __add__(self, other):\n",
        "\n",
        "        if isinstance(other, numbers.Number):\n",
        "            return self._add_scalar(other)\n",
        "        elif isinstance(other, CustomTensor):\n",
        "            return self._add_tensor(other)\n",
        "        return NotImplemented\n",
        "    def __radd__(self,other):\n",
        "        return self + other\n",
        "    def __iadd__(self,other):\n",
        "        if isinstance(other, numbers.Number):\n",
        "            self.tensor.add_(other)\n",
        "        elif isinstance(other,CustomTensor):\n",
        "            self.tensor.add_(other.tensor)\n",
        "    def _add_scalar(self, scalar):\n",
        "        result_tensor = torch.add(self.tensor, scalar)\n",
        "        if not self._custom_requires_grad:\n",
        "            return CustomTensor(result_tensor,due_to_operation=True)\n",
        "        graph = self.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "        graph.add_edge(self._node_id, result._node_id)\n",
        "        self_ref = weakref.proxy(self)\n",
        "        result_ref = weakref.proxy(result)\n",
        "        def _backward():\n",
        "            if self_ref.tensor.grad is None: self_ref._zero_grad()\n",
        "            self_ref.tensor.grad.add_(result_ref.tensor.grad)\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "    def _add_tensor(self, other):\n",
        "        result_tensor = torch.add(self.tensor, other.tensor)\n",
        "        requires_grad = self._custom_requires_grad or other._custom_requires_grad\n",
        "        if not requires_grad:\n",
        "            return CustomTensor(result_tensor,due_to_operation=True)\n",
        "        graph = self.graph if self._custom_requires_grad else other.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "        self_ref = weakref.proxy(self)\n",
        "        other_ref = weakref.proxy(other)\n",
        "        if self._custom_requires_grad:\n",
        "            graph.add_edge(self._node_id, result._node_id)\n",
        "        if other._custom_requires_grad:\n",
        "            graph.add_edge(other._node_id, result._node_id)\n",
        "        result_ref = weakref.proxy(result)\n",
        "        def _backward():\n",
        "            if self_ref._custom_requires_grad:\n",
        "                if self_ref.tensor.grad is None: self_ref._zero_grad()\n",
        "                grad_for_self = self_ref._reduce_grad_for_broadcast(result_ref.tensor.grad, self_ref.tensor.shape)\n",
        "                self_ref.tensor.grad.add_(grad_for_self)\n",
        "            if other_ref._custom_requires_grad:\n",
        "                if other_ref.tensor.grad is None: other_ref._zero_grad()\n",
        "                grad_for_other = other_ref._reduce_grad_for_broadcast(result_ref.tensor.grad, other_ref.tensor.shape)\n",
        "                other_ref.tensor.grad.add_(grad_for_other)\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "\n",
        "    def __mul__(self, other):\n",
        "        if isinstance(other, numbers.Number):\n",
        "            return self._mul_scalar(other)\n",
        "        elif isinstance(other, CustomTensor):\n",
        "            return self._mul_tensor(other)\n",
        "        return NotImplemented\n",
        "    def __rmul__(self,other):\n",
        "        return self*other\n",
        "    def __imul__(self,other):\n",
        "        if isinstance(other, numbers.Number):\n",
        "            self.tensor.mul_(other)\n",
        "        elif isinstance(other,CustomTensor):\n",
        "            self.tensor.mul_(other.tensor)\n",
        "    def _mul_scalar(self, scalar):\n",
        "        result_tensor = torch.mul(self.tensor, scalar)\n",
        "        if not self._custom_requires_grad:\n",
        "            return CustomTensor(result_tensor,due_to_operation=True)\n",
        "        graph = self.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "        graph.add_edge(self._node_id, result._node_id)\n",
        "        self_ref = weakref.proxy(self)\n",
        "        result_ref = weakref.proxy(result)\n",
        "        def _backward():\n",
        "            if self_ref.tensor.grad is None:\n",
        "                self_ref._zero_grad()\n",
        "            self_ref.tensor.grad.add_(result_ref.tensor.grad * scalar)\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "    def _mul_tensor(self, other):\n",
        "        result_tensor = torch.mul(self.tensor, other.tensor)\n",
        "        requires_grad = self._custom_requires_grad or other._custom_requires_grad\n",
        "        if not requires_grad:\n",
        "            return CustomTensor(result_tensor,due_to_operation=True)\n",
        "        graph = self.graph if self._custom_requires_grad else other.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "        self_ref = weakref.proxy(self)\n",
        "        other_ref = weakref.proxy(other)\n",
        "        result_ref = weakref.proxy(result)\n",
        "        if self._custom_requires_grad:\n",
        "            graph.add_edge(self._node_id, result._node_id)\n",
        "        if other._custom_requires_grad:\n",
        "            graph.add_edge(other._node_id, result._node_id)\n",
        "        def _backward():\n",
        "            if self_ref._custom_requires_grad:\n",
        "                if self_ref.tensor.grad is None: self_ref._zero_grad()\n",
        "                grad_for_self = self_ref._reduce_grad_for_broadcast(result_ref.tensor.grad * other_ref.tensor, self_ref.tensor.shape)\n",
        "                self_ref.tensor.grad.add_(grad_for_self)\n",
        "            if other_ref._custom_requires_grad:\n",
        "                if other_ref.tensor.grad is None: other_ref._zero_grad()\n",
        "                grad_for_other = other_ref._reduce_grad_for_broadcast(result_ref.tensor.grad * self_ref.tensor, other_ref.tensor.shape)\n",
        "                other_ref.tensor.grad.add_(grad_for_other)\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "\n",
        "    def __sub__(self, other):\n",
        "        if isinstance(other, numbers.Number):\n",
        "            return self._sub_scalar(other)\n",
        "        elif isinstance(other, CustomTensor):\n",
        "            return self._sub_tensor(other)\n",
        "        return NotImplemented\n",
        "\n",
        "    def __rsub__(self, other):\n",
        "        if isinstance(other, numbers.Number):\n",
        "            return self._rsub_scalar(other)\n",
        "\n",
        "    def __isub__(self,other):\n",
        "        if isinstance(other, numbers.Number):\n",
        "            self.tensor.sub_(other)\n",
        "        elif isinstance(other,CustomTensor):\n",
        "            self.tensor.sub_(other.tensor)\n",
        "\n",
        "    def _rsub_scalar(self, scalar):\n",
        "        result_tensor = torch.sub(scalar, self.tensor)\n",
        "        if not self._custom_requires_grad:\n",
        "            return CustomTensor(result_tensor,due_to_operation=True)\n",
        "\n",
        "        graph = self.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "        graph.add_edge(self._node_id, result._node_id)\n",
        "\n",
        "        self_ref = weakref.proxy(self)\n",
        "        result_ref = weakref.proxy(result)\n",
        "        def _backward():\n",
        "            if self_ref.tensor.grad is None:\n",
        "                self_ref._zero_grad()\n",
        "            # Derivative of scalar - x is -1\n",
        "            self_ref.tensor.grad.sub_(result_ref.tensor.grad) # No broadcasting specific logic for scalar op\n",
        "\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "\n",
        "\n",
        "    def _sub_scalar(self, scalar):\n",
        "        result_tensor = torch.sub(self.tensor, scalar)\n",
        "        if not self._custom_requires_grad:\n",
        "            return CustomTensor(result_tensor,due_to_operation=True)\n",
        "\n",
        "        graph = self.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "        graph.add_edge(self._node_id, result._node_id)\n",
        "\n",
        "        self_ref = weakref.proxy(self)\n",
        "        result_ref = weakref.proxy(result)\n",
        "        def _backward():\n",
        "            if self_ref.tensor.grad is None:\n",
        "                self_ref._zero_grad()\n",
        "            self_ref.tensor.grad.add_(result_ref.tensor.grad) # No broadcasting specific logic for scalar op\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "\n",
        "    def _sub_tensor(self, other):\n",
        "        result_tensor = torch.sub(self.tensor, other.tensor)\n",
        "        requires_grad = self._custom_requires_grad or other._custom_requires_grad\n",
        "        if not requires_grad:\n",
        "            return CustomTensor(result_tensor,due_to_operation=True)\n",
        "\n",
        "        graph = self.graph if self._custom_requires_grad else other.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "\n",
        "        self_ref = weakref.proxy(self)\n",
        "        other_ref = weakref.proxy(other)\n",
        "        result_ref = weakref.proxy(result)\n",
        "\n",
        "        if self._custom_requires_grad:\n",
        "            graph.add_edge(self._node_id, result._node_id)\n",
        "        if other._custom_requires_grad:\n",
        "            graph.add_edge(other._node_id, result._node_id)\n",
        "\n",
        "        def _backward():\n",
        "            if self_ref._custom_requires_grad:\n",
        "                if self_ref.tensor.grad is None:\n",
        "                    self_ref._zero_grad()\n",
        "                grad_for_self = self_ref._reduce_grad_for_broadcast(result_ref.tensor.grad, self_ref.tensor.shape)\n",
        "                self_ref.tensor.grad.add_(grad_for_self)\n",
        "            if other_ref._custom_requires_grad:\n",
        "                if other_ref.tensor.grad is None:\n",
        "                    other_ref._zero_grad()\n",
        "                grad_for_other = other_ref._reduce_grad_for_broadcast(-result_ref.tensor.grad, other_ref.tensor.shape)\n",
        "                other_ref.tensor.grad.add_(grad_for_other)\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "\n",
        "    def __truediv__(self, other):\n",
        "        if isinstance(other, numbers.Number):\n",
        "            return self._div_scalar(other)\n",
        "        elif isinstance(other, CustomTensor):\n",
        "            return self._div_tensor(other)\n",
        "        return NotImplemented\n",
        "    def __itruediv__(self,other):\n",
        "        if isinstance(other, numbers.Number):\n",
        "            self.tensor.div_(other)\n",
        "        elif isinstance(other,CustomTensor):\n",
        "            self.tensor.div_(other.tensor)\n",
        "    def _div_scalar(self, scalar):\n",
        "        result_tensor = torch.div(self.tensor, scalar)\n",
        "        if not self._custom_requires_grad:\n",
        "            return CustomTensor(result_tensor,due_to_operation=True)\n",
        "\n",
        "        graph = self.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "        graph.add_edge(self._node_id, result._node_id)\n",
        "\n",
        "        self_ref = weakref.proxy(self)\n",
        "        result_ref = weakref.proxy(result)\n",
        "        def _backward():\n",
        "            if self_ref.tensor.grad is None:\n",
        "                self_ref._zero_grad()\n",
        "            self_ref.tensor.grad.add_(result_ref.tensor.grad / scalar)\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "\n",
        "    def _div_tensor(self,other):\n",
        "        result_tensor = torch.div(self.tensor, other.tensor)\n",
        "        requires_grad = self._custom_requires_grad or other._custom_requires_grad\n",
        "        if not requires_grad:\n",
        "            return CustomTensor(result_tensor,due_to_operation=True)\n",
        "\n",
        "        graph = self.graph if self._custom_requires_grad else other.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "\n",
        "        self_ref = weakref.proxy(self)\n",
        "        other_ref = weakref.proxy(other)\n",
        "        result_ref = weakref.proxy(result)\n",
        "\n",
        "        if self._custom_requires_grad:\n",
        "            graph.add_edge(self._node_id, result._node_id)\n",
        "        if other._custom_requires_grad:\n",
        "            graph.add_edge(other._node_id, result._node_id)\n",
        "\n",
        "        def _backward():\n",
        "            if self_ref._custom_requires_grad:\n",
        "                if self_ref.tensor.grad is None:\n",
        "                    self_ref._zero_grad()\n",
        "                grad_for_self = self_ref._reduce_grad_for_broadcast(result_ref.tensor.grad / other_ref.tensor, self_ref.tensor.shape)\n",
        "                self_ref.tensor.grad.add_(grad_for_self)\n",
        "            if other_ref._custom_requires_grad:\n",
        "                if other_ref.tensor.grad is None:\n",
        "                    other_ref._zero_grad()\n",
        "                grad_for_other = other_ref._reduce_grad_for_broadcast(-result_ref.tensor.grad * self_ref.tensor / other_ref.tensor.pow(2), other_ref.tensor.shape)\n",
        "                other_ref.tensor.grad.add_(grad_for_other)\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "\n",
        "    def pow(self, scalar):\n",
        "        result_tensor = torch.pow(self.tensor, scalar)\n",
        "        if not self._custom_requires_grad:\n",
        "            return CustomTensor(result_tensor,due_to_operation=True)\n",
        "\n",
        "        graph = self.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "        graph.add_edge(self._node_id, result._node_id)\n",
        "\n",
        "        self_ref = weakref.proxy(self)\n",
        "        result_ref = weakref.proxy(result)\n",
        "        def _backward():\n",
        "            if self_ref.tensor.grad is None:\n",
        "                self_ref._zero_grad()\n",
        "            grad_contrib = scalar * self_ref.tensor.pow(scalar - 1)\n",
        "            self_ref.tensor.grad.add_(result_ref.tensor.grad * grad_contrib)\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "    def __ipow__(self,other):\n",
        "        self.tensor.pow_(other)\n",
        "    def __pow__(self,other):\n",
        "      if isinstance(other, numbers.Number):\n",
        "          return self.pow(other)\n",
        "      return NotImplemented\n",
        "    def exp(self):\n",
        "        out = torch.exp(self.tensor)\n",
        "        if not self._custom_requires_grad:\n",
        "            return CustomTensor(out,due_to_operation=True)\n",
        "\n",
        "        graph = self.graph\n",
        "        result = CustomTensor(out, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "        graph.add_edge(self._node_id, result._node_id)\n",
        "        self_ref = weakref.proxy(self)\n",
        "        result_ref = weakref.proxy(result)\n",
        "        def _backward():\n",
        "            if self_ref.tensor.grad is None:\n",
        "                self_ref._zero_grad()\n",
        "            self_ref.tensor.grad.add_(result_ref.tensor.grad * out)\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "\n",
        "    def log(self):\n",
        "        out = torch.log(self.tensor)\n",
        "        if not self._custom_requires_grad:\n",
        "            return CustomTensor(out,due_to_operation=True)\n",
        "\n",
        "        graph = self.graph\n",
        "        result = CustomTensor(out, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "        graph.add_edge(self._node_id, result._node_id)\n",
        "        self_ref = weakref.proxy(self)\n",
        "        result_ref = weakref.proxy(result)\n",
        "        def _backward():\n",
        "            if self_ref.tensor.grad is None:\n",
        "                self_ref._zero_grad()\n",
        "            self_ref.tensor.grad.add_(result_ref.tensor.grad / self_ref.tensor)\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "\n",
        "    def sin(self):\n",
        "        out = torch.sin(self.tensor)\n",
        "        if not self._custom_requires_grad:\n",
        "            return CustomTensor(out,due_to_operation=True)\n",
        "\n",
        "        graph = self.graph\n",
        "        result = CustomTensor(out, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "        graph.add_edge(self._node_id, result._node_id)\n",
        "        self_ref = weakref.proxy(self)\n",
        "        result_ref = weakref.proxy(result)\n",
        "        def _backward():\n",
        "            if self_ref.tensor.grad is None:\n",
        "                self_ref._zero_grad()\n",
        "            self_ref.tensor.grad.add_(result_ref.tensor.grad * torch.cos(self_ref.tensor))\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "\n",
        "    def cos(self):\n",
        "        out = torch.cos(self.tensor)\n",
        "        if not self._custom_requires_grad:\n",
        "            return CustomTensor(out,due_to_operation=True)\n",
        "\n",
        "        graph = self.graph\n",
        "        result = CustomTensor(out, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "        graph.add_edge(self._node_id, result._node_id)\n",
        "        self_ref = weakref.proxy(self)\n",
        "        result_ref = weakref.proxy(result)\n",
        "        def _backward():\n",
        "            if self_ref.tensor.grad is None:\n",
        "                self_ref._zero_grad()\n",
        "            self_ref.tensor.grad.add_(-result_ref.tensor.grad*torch.sin(self_ref.tensor))\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "\n",
        "    def sqrt(self):\n",
        "        out = torch.sqrt(self.tensor)\n",
        "        if not self._custom_requires_grad:\n",
        "            return CustomTensor(out,due_to_operation=True)\n",
        "\n",
        "        graph = self.graph\n",
        "        result = CustomTensor(out, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "        graph.add_edge(self._node_id, result._node_id)\n",
        "        self_ref = weakref.proxy(self)\n",
        "        result_ref = weakref.proxy(result)\n",
        "        def _backward():\n",
        "            if self_ref.tensor.grad is None:\n",
        "                self_ref._zero_grad()\n",
        "            self_ref.tensor.grad.add_(result_ref.tensor.grad*0.5*self_ref.tensor.pow(-0.5))\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "    def __matmul__(self,other):\n",
        "        if isinstance(other, CustomTensor):\n",
        "            return self.matmul(other)\n",
        "        return NotImplemented\n",
        "    def matmul(self, other):\n",
        "        result_tensor = torch.matmul(self.tensor, other.tensor)\n",
        "        requires_grad = self._custom_requires_grad or other._custom_requires_grad\n",
        "        if not requires_grad:\n",
        "            return CustomTensor(result_tensor,due_to_operation=True)\n",
        "\n",
        "        graph = self.graph if self._custom_requires_grad else other.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "\n",
        "        self_ref = weakref.proxy(self)\n",
        "        other_ref = weakref.proxy(other)\n",
        "        result_ref = weakref.proxy(result)\n",
        "\n",
        "        if self._custom_requires_grad:\n",
        "            graph.add_edge(self._node_id, result._node_id)\n",
        "        if other._custom_requires_grad:\n",
        "            graph.add_edge(other._node_id, result._node_id)\n",
        "\n",
        "        def _backward():\n",
        "            if self_ref._custom_requires_grad:\n",
        "                if self_ref.tensor.grad is None: self_ref._zero_grad()\n",
        "                # Use robust broadcasting for matmul gradient\n",
        "                grad_for_self = torch.matmul(result_ref.tensor.grad, other_ref.tensor.transpose(-2, -1))\n",
        "                self_ref.tensor.grad.add_(self_ref._reduce_grad_for_broadcast(grad_for_self, self_ref.tensor.shape))\n",
        "            if other_ref._custom_requires_grad:\n",
        "                if other_ref.tensor.grad is None: other_ref._zero_grad()\n",
        "                grad_for_other = torch.matmul(self_ref.tensor.transpose(-2, -1), result_ref.tensor.grad)\n",
        "                other_ref.tensor.grad.add_(other_ref._reduce_grad_for_broadcast(grad_for_other, other_ref.tensor.shape))\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "    def dot(self, other):\n",
        "        # torch.dot only works for 1D tensors, or for higher-D tensors,\n",
        "        # it flattens them to 1D and then computes the dot product.\n",
        "        # This means the gradients will also be 1D, so no complex broadcasting\n",
        "        # reduction is needed on the output gradient itself.\n",
        "        # However, the input tensors themselves could have been results of broadcasting ops.\n",
        "        # For a truly general dot product, you'd use torch.matmul.\n",
        "        result_tensor = torch.dot(self.tensor.reshape(-1), other.tensor.reshape(-1))\n",
        "        requires_grad = self._custom_requires_grad or other._custom_requires_grad\n",
        "        if not requires_grad:\n",
        "            return CustomTensor(result_tensor,due_to_operation=True)\n",
        "\n",
        "        graph = self.graph if self._custom_requires_grad else other.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "\n",
        "        self_ref = weakref.proxy(self)\n",
        "        other_ref = weakref.proxy(other)\n",
        "        result_ref = weakref.proxy(result)\n",
        "\n",
        "        if self._custom_requires_grad:\n",
        "            graph.add_edge(self._node_id, result._node_id)\n",
        "        if other._custom_requires_grad:\n",
        "            graph.add_edge(other._node_id, result._node_id)\n",
        "\n",
        "        def _backward():\n",
        "            if self_ref._custom_requires_grad:\n",
        "                if self_ref.tensor.grad is None:\n",
        "                    self_ref._zero_grad()\n",
        "                # The grad from result_ref.tensor.grad will be a scalar.\n",
        "                # It needs to be multiplied by the other_ref.tensor (original shape)\n",
        "                # and then potentially re-shaped if original was >1D\n",
        "                grad_contrib = result_ref.tensor.grad * other_ref.tensor\n",
        "                self_ref.tensor.grad.add_(grad_contrib)\n",
        "            if other_ref._custom_requires_grad:\n",
        "                if other_ref.tensor.grad is None:\n",
        "                    other_ref._zero_grad()\n",
        "                grad_contrib = result_ref.tensor.grad * self_ref.tensor\n",
        "                other_ref.tensor.grad.add_(grad_contrib)\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "\n",
        "\n",
        "\n",
        "    # --- Unary Operations ---\n",
        "\n",
        "    def sum(self, dim=None, keepdim=False):\n",
        "        \"\"\"Computes the sum of elements along given dimensions.\"\"\"\n",
        "        result_tensor = self.tensor.sum(dim=dim, keepdim=keepdim)\n",
        "        if not self._custom_requires_grad:\n",
        "            return CustomTensor(result_tensor, due_to_operation=True)\n",
        "\n",
        "        graph = self.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "        graph.add_edge(self._node_id, result._node_id)\n",
        "\n",
        "        self_ref = weakref.proxy(self)\n",
        "        result_ref = weakref.proxy(result)\n",
        "\n",
        "        def _backward():\n",
        "            if self_ref.tensor.grad is None:\n",
        "                self_ref._zero_grad()\n",
        "\n",
        "            grad = result_ref.tensor.grad\n",
        "            # If keepdim was false, the summed dim was squeezed. We need to unsqueeze it back for broadcasting.\n",
        "            if not keepdim and dim is not None:\n",
        "                grad = grad.unsqueeze(dim)\n",
        "\n",
        "            self_ref.tensor.grad.add_(grad)\n",
        "\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "\n",
        "    def mean(self, dim=None, keepdim=False):\n",
        "        \"\"\"Computes the mean of elements along given dimensions.\"\"\"\n",
        "        result_tensor = self.tensor.mean(dim=dim, keepdim=keepdim)\n",
        "        if not self._custom_requires_grad:\n",
        "            return CustomTensor(result_tensor, due_to_operation=True)\n",
        "\n",
        "        graph = self.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "        graph.add_edge(self._node_id, result._node_id)\n",
        "\n",
        "        self_ref = weakref.proxy(self)\n",
        "        result_ref = weakref.proxy(result)\n",
        "\n",
        "        # Determine the number of elements that were averaged\n",
        "        if dim is None:\n",
        "            n = self.tensor.numel()\n",
        "        else:\n",
        "            n = self.tensor.shape[dim]\n",
        "\n",
        "        def _backward():\n",
        "            if self_ref.tensor.grad is None:\n",
        "                self_ref._zero_grad()\n",
        "\n",
        "            grad = result_ref.tensor.grad\n",
        "            if not keepdim and dim is not None:\n",
        "                grad = grad.unsqueeze(dim)\n",
        "\n",
        "            # Distribute gradient evenly\n",
        "            self_ref.tensor.grad.add_(grad / n)\n",
        "\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "\n",
        "    def reshape(self, *shape):\n",
        "        \"\"\"Reshapes the tensor to the given shape.\"\"\"\n",
        "        original_shape = self.shape\n",
        "        result_tensor = self.tensor.reshape(*shape)\n",
        "        if not self._custom_requires_grad:\n",
        "            return CustomTensor(result_tensor, due_to_operation=True)\n",
        "\n",
        "        graph = self.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "        graph.add_edge(self._node_id, result._node_id)\n",
        "\n",
        "        self_ref = weakref.proxy(self)\n",
        "        result_ref = weakref.proxy(result)\n",
        "\n",
        "        def _backward():\n",
        "            if self_ref.tensor.grad is None:\n",
        "                self_ref._zero_grad()\n",
        "            self_ref.tensor.grad.add_(result_ref.tensor.grad.reshape(original_shape))\n",
        "\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "\n",
        "    def transpose(self, dim0, dim1):\n",
        "        \"\"\"Transposes dimensions dim0 and dim1.\"\"\"\n",
        "        result_tensor = self.tensor.transpose(dim0, dim1)\n",
        "        if not self._custom_requires_grad:\n",
        "            return CustomTensor(result_tensor, due_to_operation=True)\n",
        "\n",
        "        graph = self.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "        graph.add_edge(self._node_id, result._node_id)\n",
        "\n",
        "        self_ref = weakref.proxy(self)\n",
        "        result_ref = weakref.proxy(result)\n",
        "\n",
        "        def _backward():\n",
        "            if self_ref.tensor.grad is None:\n",
        "                self_ref._zero_grad()\n",
        "            # The gradient operation for transpose is another transpose\n",
        "            self_ref.tensor.grad.add_(result_ref.tensor.grad.transpose(dim0, dim1))\n",
        "\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "\n",
        "    @property\n",
        "    def T(self):\n",
        "        \"\"\"Alias for transpose(-2, -1) for 2D or higher dimensional tensors.\"\"\"\n",
        "        if self.ndim < 2:\n",
        "            raise ValueError(\"`.T` is only supported on tensors with 2 or more dimensions.\")\n",
        "        return self.transpose(-2, -1)\n",
        "\n",
        "    def backward(self, weightage_tensor=1,retain_graph=False):\n",
        "        if not self._custom_requires_grad:\n",
        "            raise RuntimeError(\"Output tensor does not require grad.\")\n",
        "        if self.graph is None:\n",
        "            raise RuntimeError(\"Output tensor is not part of a graph.\")\n",
        "        graph = self.graph\n",
        "\n",
        "        # Initialize gradient for the output tensor\n",
        "        if isinstance(weightage_tensor, numbers.Number):\n",
        "            self.tensor.grad = torch.full_like(self.tensor, fill_value=weightage_tensor)\n",
        "        elif isinstance(weightage_tensor, torch.Tensor):\n",
        "            self.tensor.grad = weightage_tensor.to(self.tensor.device)#.clone()\n",
        "\n",
        "        nodes_to_process = graph.reverse_toposort_from_tensor(self._node_id)\n",
        "\n",
        "        for tensor_node in nodes_to_process:\n",
        "            tensor_node._backward()\n",
        "        if not retain_graph:\n",
        "            graph.delete_all_non_leaf_nodes()\n",
        "\n",
        "            #try:\n",
        "                # The node is a weakref.proxy, check if it's still alive\n",
        "                #if tensor_node.__class__ is weakref.ProxyType:\n",
        "            #        tensor_node._backward()\n",
        "            # except ReferenceError:\n",
        "            #     # The tensor object was garbage collected, skip.\n",
        "            #     print(\"dead reference node encountered\")\n",
        "            #     continue\n",
        "    # --- Properties and Dunder Methods ---\n",
        "    @property\n",
        "    def dtype(self): return self.tensor.dtype\n",
        "    @property\n",
        "    def ndim(self): return self.tensor.ndim\n",
        "    @property\n",
        "    def shape(self): return self.tensor.shape\n",
        "    @property\n",
        "    def grad(self): return self.tensor.grad\n",
        "    def __repr__(self):\n",
        "      return (\n",
        "          f\"{self.__class__.__name__}(\\n\"\n",
        "          f\"  memory_address   = {hex(id(self.tensor))},\\n\"\n",
        "          f\"  shape            = {self.tensor.shape},\\n\"\n",
        "          f\"  requires_grad    = {self._custom_requires_grad},\\n\"\n",
        "          f\"  node_id          = {self._node_id},\\n\"\n",
        "          f\"  is_leaf          = {self._is_leaf}\\n\"\n",
        "          f\")\"\n",
        "             )\n",
        "    # def __del__(self):\n",
        "    #     if self._node_id is not None and self._is_leaf:\n",
        "    #         try:\n",
        "    #             if self.graph: self.graph.delete_node(self._node_id)\n",
        "    #         except ReferenceError: # Graph might be gone first\n",
        "    #             pass"
      ],
      "metadata": {
        "cellView": "form",
        "id": "pEiEFpF0hitt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Modules\n",
        "import torch\n",
        "import math\n",
        "import weakref\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from collections import OrderedDict\n",
        "class Module:\n",
        "    \"\"\"\n",
        "    Base class for all neural network modules.\n",
        "    \"\"\"\n",
        "    device=device\n",
        "    dtype=dtype\n",
        "    __slots__ = ('_parameters', '_modules','_buffers', 'training')\n",
        "    def __init__(self):\n",
        "        self._parameters = OrderedDict()\n",
        "        self._modules = OrderedDict()\n",
        "        self._buffers = OrderedDict()\n",
        "        self.training = True #\n",
        "\n",
        "    def __setattr__(self, name, value):\n",
        "        if isinstance(value, CustomTensor):\n",
        "            #if value._custom_requires_grad:\n",
        "            self._parameters[name] = value\n",
        "        elif isinstance(value, Module):\n",
        "            self._modules[name] = value\n",
        "        # Handle buffers (non-parameter tensors like running_mean in BatchNorm)\n",
        "        elif isinstance(value, torch.Tensor):\n",
        "             self._buffers[name] = value\n",
        "        super().__setattr__(name, value)\n",
        "\n",
        "    def buffers(self):\n",
        "        \"\"\"Returns a list of all buffers in the module and its submodules.\"\"\"\n",
        "        buffs = list(self._buffers.values())\n",
        "        for module in self._modules.values():\n",
        "            buffs.extend(module.buffers())\n",
        "        return buffs\n",
        "    def parameters(self):\n",
        "        \"\"\"Returns a list of all parameters in the module and its submodules.\"\"\"\n",
        "        params = list(self._parameters.values())\n",
        "        for module in self._modules.values():\n",
        "            params.extend(module.parameters())\n",
        "        return params\n",
        "    # def modules(self):\n",
        "    #     \"\"\"Returns a list of all modules in the network.\"\"\"\n",
        "    #     modules_list = list(self._modules.values())\n",
        "    #     for module in self._modules.values():\n",
        "    #         modules_list.extend(module.modules())\n",
        "    #     return modules_list\n",
        "    def modules(self):\n",
        "        \"\"\"Returns an iterator over all submodules and the module in the network.\"\"\"\n",
        "        yield self\n",
        "        for module in self._modules.values():\n",
        "            yield from module.modules()\n",
        "\n",
        "    def zero_grad(self):\n",
        "        \"\"\"Sets gradients of all model parameters to zero.\"\"\"\n",
        "        for p in self.parameters():\n",
        "            p._zero_grad()\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"Sets the module and all its submodules to training mode.\"\"\"\n",
        "        self.training = True\n",
        "        for module in self._modules.values():\n",
        "            module.train()\n",
        "\n",
        "    def eval(self):\n",
        "        \"\"\"Sets the module and all its submodules to evaluation mode.\"\"\"\n",
        "        self.training = False\n",
        "        for module in self._modules.values():\n",
        "            module.eval()\n",
        "    def detach_graph(self):\n",
        "        for p in self.parameters():\n",
        "            p._clear_graph_references()\n",
        "        for module in self.modules():\n",
        "            if hasattr(module, 'graph'):\n",
        "                module.graph = None\n",
        "    def attach_graph(self,graph):\n",
        "        for p in self.parameters():\n",
        "            p._add_to_graph(graph)\n",
        "        for module in self.modules():\n",
        "            if hasattr(module, 'graph'):\n",
        "                module.graph = weakref.proxy(graph)\n",
        "    def prepare_for_saving(self):\n",
        "        for p in self.parameters():\n",
        "            p.clear()\n",
        "\n",
        "\n",
        "    def verify_all_graph_references_are_weak(self):\n",
        "        c=0\n",
        "        for p in self.parameters():\n",
        "            graph = getattr(p, \"graph\", None)\n",
        "            if graph is not None and not isinstance(graph, weakref.ProxyType):\n",
        "                print(f\"STRONG REFERENCE FOR GRAPH FOUND IN PARAMETER {p}\")\n",
        "                c+=1\n",
        "\n",
        "        for module in self.modules():\n",
        "            graph = getattr(module, \"graph\", None)\n",
        "            if graph is not None and not isinstance(graph, weakref.ProxyType):\n",
        "                print(f\"STRONG REFERENCE FOR GRAPH FOUND IN MODULE {module}\")\n",
        "                c+=1\n",
        "        if c==0:\n",
        "            print(\"NO STRONG REFERENCES FOUND\")\n",
        "\n",
        "\n",
        "    def verify_all_parameters_are_on_the_same_device(self,device):\n",
        "        c=0\n",
        "        device = torch.device(device)\n",
        "        for p in self.parameters():\n",
        "            if p.tensor.device != device:\n",
        "                print(f\"PARAMETER {p} IS NOT ON THE SAME DEVICE {device}\")\n",
        "                c+=1\n",
        "        for b in self.buffers():\n",
        "            if b.device != device:\n",
        "                print(f\"BUFFER {b} IS NOT ON THE SAME DEVICE {device}\")\n",
        "                c+=1\n",
        "        if c==0:\n",
        "            print(\"ALL PARAMETERS ARE ON THE SAME DEVICE\")\n",
        "\n",
        "\n",
        "    def __call__(self, *args, **kwargs):\n",
        "        return self.forward(*args, **kwargs)\n",
        "\n",
        "    def to(self, device, dtype=None):\n",
        "      \"\"\"Moves and/or casts the parameters and buffers.\"\"\"\n",
        "      # 1. Recursively call .to() on all sub-modules\n",
        "      for module in self._modules.values():\n",
        "          module.to(device, dtype)\n",
        "\n",
        "      # 2. Move the parameters of the current module\n",
        "      for param in self._parameters.values():\n",
        "          param.to(device, dtype)\n",
        "\n",
        "      # 3. Move the buffers of the current module\n",
        "      for name, buf in self._buffers.items():\n",
        "          # Create the new tensor on the correct device\n",
        "          new_buf = buf.to(device, dtype)\n",
        "          # tensor.to in pytorch creates a new tensor so we must update the references in both the _buffers dictionary and the batchnorm_nd.running_mean or any other var by the same buffer\n",
        "          # Reassign the buffer in the dictionary AND on the attribute itself\n",
        "          self._buffers[name] = new_buf\n",
        "          super().__setattr__(name, new_buf)\n",
        "\n",
        "      return self\n",
        "\n",
        "\n",
        "    def forward(self, *args, **kwargs):\n",
        "        raise NotImplementedError(\"Subclasses of Module must implement a forward method.\")\n",
        "\n",
        "class Linear(Module):\n",
        "    \"\"\"Applies a linear transformation to the incoming data: y = xA^T + b\n",
        "    types of activation relu,leaky_relu, gelu, sigmoid, tanh, silu,elu\"\"\"\n",
        "    __slots__ = ('in_features', 'out_features', 'graph', 'weight', 'bias','__weakref__')\n",
        "    _ACTIVATION_INIT = {\n",
        "        \"relu\": (\"kaiming_uniform_\", \"relu\"),\n",
        "        \"gelu\": (\"kaiming_uniform_\", \"relu\"),\n",
        "        \"silu\": (\"kaiming_uniform_\", \"relu\"),\n",
        "        \"elu\": (\"kaiming_uniform_\", \"relu\"),\n",
        "        \"gelu_approx\": (\"kaiming_uniform_\", \"relu\"),\n",
        "        \"leaky_relu\": (\"kaiming_uniform_\", \"leaky_relu\"),\n",
        "        \"sigmoid\": (\"xavier_uniform_\", 1.0),\n",
        "        \"tanh\": (\"xavier_uniform_\", 5/3)\n",
        "    }\n",
        "\n",
        "    def __new__(cls, in_features, out_features, bias=True, *, graph=None, activation=\"relu\"):\n",
        "        assert activation in cls._ACTIVATION_INIT\n",
        "        return super().__new__(cls)\n",
        "\n",
        "    def __init__(self, in_features, out_features, bias=True, *, graph=None, activation=\"relu\"):\n",
        "        super().__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.graph = weakref.proxy(graph) if graph is not None else None\n",
        "\n",
        "        # Initialize weight\n",
        "        self.weight = CustomTensor(torch.empty(out_features, in_features, device=Linear.device, dtype=Linear.dtype),\n",
        "                                 _custom_requires_grad=True if graph is not None else False, graph=graph if graph is not None else None, is_leaf=True)\n",
        "\n",
        "        init_method, init_param = self._ACTIVATION_INIT[activation]\n",
        "        if init_method == \"kaiming_uniform_\":\n",
        "            torch.nn.init.kaiming_uniform_(self.weight.tensor, nonlinearity=init_param)\n",
        "        else:  # xavier_uniform_\n",
        "            torch.nn.init.xavier_uniform_(self.weight.tensor, gain=init_param)\n",
        "\n",
        "        # Initialize bias\n",
        "        self.bias = CustomTensor(torch.zeros(out_features,device=Linear.device, dtype=Linear.dtype),\n",
        "                               _custom_requires_grad=True if graph is not None else False, graph=graph if graph is not None else None, is_leaf=True) if bias else None\n",
        "\n",
        "    def forward(self, input_tensor):\n",
        "        inp = input_tensor.tensor\n",
        "        is_1d = inp.ndim==1\n",
        "        if is_1d:\n",
        "            inp = inp.unsqueeze(0)\n",
        "        output = inp @ self.weight.tensor.transpose(-2, -1)\n",
        "        if self.bias is not None:\n",
        "            output.add_(self.bias.tensor)\n",
        "\n",
        "        if is_1d:\n",
        "            output = output.squeeze(0)\n",
        "        if not self.training:\n",
        "            return CustomTensor(output, due_to_operation=True)\n",
        "\n",
        "        # Training mode - setup gradient computation\n",
        "        result = CustomTensor(output, _custom_requires_grad=True, graph=self.graph,\n",
        "                            due_to_operation=True, is_leaf=False)\n",
        "\n",
        "        # Add edges to computation graph\n",
        "        if input_tensor._custom_requires_grad:\n",
        "            self.graph.add_edge(input_tensor._node_id, result._node_id)\n",
        "        if self.weight._custom_requires_grad:\n",
        "          self.graph.add_edge(self.weight._node_id, result._node_id)\n",
        "        if self.bias is not None:\n",
        "            if self.bias._custom_requires_grad:\n",
        "              self.graph.add_edge(self.bias._node_id, result._node_id)\n",
        "\n",
        "        # Create weak references for backward pass\n",
        "        refs = {\n",
        "            'weight': weakref.proxy(self.weight),\n",
        "            'input': weakref.proxy(input_tensor),\n",
        "            'result': weakref.proxy(result),\n",
        "            'bias': weakref.proxy(self.bias) if self.bias is not None else None,\n",
        "            'is_1d': is_1d\n",
        "        }\n",
        "\n",
        "        result._backward = self._create_backward(refs)\n",
        "        return result\n",
        "\n",
        "    def _create_backward(self, refs):\n",
        "        def _backward():\n",
        "            weight_ref, input_ref, result_ref, bias_ref, is_1d = refs['weight'], refs['input'], refs['result'], refs['bias'], refs['is_1d']\n",
        "            grad_output = result_ref.tensor.grad\n",
        "            inp = input_ref.tensor\n",
        "            if is_1d:\n",
        "                inp = inp.unsqueeze(0)\n",
        "                grad_output = grad_output.unsqueeze(0)\n",
        "\n",
        "            # Weight gradient\n",
        "            if weight_ref._custom_requires_grad:\n",
        "                if weight_ref.tensor.grad is None:\n",
        "                    weight_ref._zero_grad()\n",
        "                grad_w = torch.matmul(grad_output.transpose(-2, -1), inp)\n",
        "                weight_ref.tensor.grad.add_(weight_ref._reduce_grad_for_broadcast(grad_w, weight_ref.tensor.shape))\n",
        "\n",
        "            # Bias gradient\n",
        "            if bias_ref is not None and bias_ref._custom_requires_grad:\n",
        "                if bias_ref.tensor.grad is None:\n",
        "                    bias_ref._zero_grad()\n",
        "                grad_b = bias_ref._reduce_grad_for_broadcast(grad_output, bias_ref.tensor.shape)\n",
        "                bias_ref.tensor.grad.add_(grad_b)\n",
        "\n",
        "            # Input gradient\n",
        "            if input_ref._custom_requires_grad:\n",
        "                if input_ref.tensor.grad is None:\n",
        "                    input_ref._zero_grad()\n",
        "                grad_in = torch.matmul(grad_output, weight_ref.tensor)\n",
        "                if is_1d:\n",
        "                    grad_in = grad_in.squeeze(0)\n",
        "                input_ref.tensor.grad.add_(input_ref._reduce_grad_for_broadcast(grad_in, input_ref.tensor.shape))\n",
        "\n",
        "        return _backward\n",
        "\n",
        "class Conv2d(Module):\n",
        "    \"\"\"Applies a 2D convolution over an input signal composed of several input planes.\n",
        "    types of activation relu,leaky_relu, gelu, sigmoid, tanh, silu,elu\"\"\"\n",
        "    __slots__ = ('in_channels', 'out_channels', 'kernel_size', 'stride', 'dilation', 'padding', 'groups', 'graph', 'weight', 'bias','__weakref__')\n",
        "\n",
        "    # Lookup table for activation initialization\n",
        "    _ACTIVATION_INIT = {\n",
        "        \"relu\": (\"kaiming_uniform_\", \"relu\"),\n",
        "        \"gelu\": (\"kaiming_uniform_\", \"relu\"),\n",
        "        \"silu\": (\"kaiming_uniform_\", \"relu\"),\n",
        "        \"elu\": (\"kaiming_uniform_\", \"relu\"),\n",
        "        \"gelu_approx\": (\"kaiming_uniform_\", \"relu\"),\n",
        "        \"leaky_relu\": (\"kaiming_uniform_\", \"leaky_relu\"),\n",
        "        \"sigmoid\": (\"xavier_uniform_\", 1.0),\n",
        "        \"tanh\": (\"xavier_uniform_\", 5/3)\n",
        "    }\n",
        "\n",
        "    def __new__(cls, *,in_channels, out_channels, kernel_size, stride=1,dilation=1,groups=1,bias=True, padding=0, graph=None,activation=\"relu\"):\n",
        "        assert isinstance(kernel_size, int) or len(kernel_size) == 2\n",
        "        assert isinstance(stride, int) or len(stride) == 2\n",
        "        assert isinstance(dilation, int) or len(dilation) == 2\n",
        "        assert isinstance(padding, int) or len(padding) == 2\n",
        "        assert activation in cls._ACTIVATION_INIT\n",
        "        return super().__new__(cls)\n",
        "\n",
        "    def __init__(self, *,in_channels, out_channels, kernel_size, stride=1,dilation=1,groups=1,bias=True, padding=0, graph=None,activation=\"relu\"):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.kernel_size = (kernel_size, kernel_size) if isinstance(kernel_size, int) else kernel_size\n",
        "        self.stride = (stride, stride) if isinstance(stride, int) else stride\n",
        "        self.dilation = (dilation, dilation) if isinstance(dilation, int) else dilation\n",
        "        self.padding = (padding, padding) if isinstance(padding, int) else padding\n",
        "        self.groups = groups\n",
        "        self.graph = weakref.proxy(graph) if graph is not None else None\n",
        "\n",
        "        weight_shape = (out_channels, in_channels // groups, *self.kernel_size)\n",
        "        self.weight = CustomTensor(torch.empty(weight_shape,device=Conv2d.device,dtype=Conv2d.dtype), _custom_requires_grad=True if graph is not None else False, graph=graph if graph is not None else None, is_leaf=True)\n",
        "\n",
        "        # Use lookup table for initialization\n",
        "        init_method, init_param = self._ACTIVATION_INIT[activation]\n",
        "        if init_method == \"kaiming_uniform_\":\n",
        "            torch.nn.init.kaiming_uniform_(self.weight.tensor, nonlinearity=init_param)\n",
        "        else:  # xavier_uniform_\n",
        "            torch.nn.init.xavier_uniform_(self.weight.tensor, gain=init_param)\n",
        "\n",
        "        self.bias = CustomTensor(torch.zeros(out_channels,device=Conv2d.device,dtype=Conv2d.dtype), _custom_requires_grad=True if graph is not None else False, graph=graph if graph is not None else None, is_leaf=True) if bias else None\n",
        "\n",
        "    def forward(self, input_tensor):\n",
        "        output_tensor = F.conv2d(\n",
        "            input = input_tensor.tensor,\n",
        "            weight = self.weight.tensor,\n",
        "            bias = self.bias.tensor if self.bias else None,\n",
        "            stride = self.stride,\n",
        "            padding = self.padding,\n",
        "            groups=self.groups\n",
        "        )\n",
        "        if not self.training:\n",
        "            return CustomTensor(output_tensor, due_to_operation=True)\n",
        "\n",
        "        result = CustomTensor(output_tensor, _custom_requires_grad=True, graph=self.graph, due_to_operation=True, is_leaf=False)\n",
        "        if input_tensor._custom_requires_grad:\n",
        "          self.graph.add_edge(input_tensor._node_id, result._node_id)\n",
        "        if self.weight._custom_requires_grad:\n",
        "          self.graph.add_edge(self.weight._node_id, result._node_id)\n",
        "        if self.bias is not None:\n",
        "            if self.bias._custom_requires_grad:\n",
        "              self.graph.add_edge(self.bias._node_id, result._node_id)\n",
        "\n",
        "        # Create weak references for backward pass\n",
        "        refs = {\n",
        "            'input': weakref.proxy(input_tensor),\n",
        "            'weight': weakref.proxy(self.weight),\n",
        "            'bias': weakref.proxy(self.bias) if self.bias is not None else None,\n",
        "            'result': weakref.proxy(result)\n",
        "        }\n",
        "\n",
        "        result._backward = self._create_backward(refs)\n",
        "        return result\n",
        "\n",
        "    def _create_backward(self, refs):\n",
        "        def _backward():\n",
        "            input_ref, weight_ref, bias_ref, result_ref = refs['input'], refs['weight'], refs['bias'], refs['result']\n",
        "            grad_output = result_ref.tensor.grad\n",
        "\n",
        "            if bias_ref is not None:\n",
        "                if bias_ref._custom_requires_grad:\n",
        "                    if bias_ref.tensor.grad is None: bias_ref._zero_grad()\n",
        "                    bias_ref.tensor.grad.add_(grad_output.sum(dim=[0, 2, 3]))\n",
        "\n",
        "            if input_ref._custom_requires_grad:\n",
        "                if input_ref.tensor.grad is None: input_ref._zero_grad()\n",
        "                input_ref.tensor.grad.add_(\n",
        "                    self._calculate_gradient_input_tensor(input_ref.tensor,weight_ref.tensor,grad_output)\n",
        "                )\n",
        "\n",
        "            if weight_ref._custom_requires_grad:\n",
        "                if weight_ref.tensor.grad is None: weight_ref._zero_grad()\n",
        "                # tried vectorizing groups but failed hence using autograd for computing weight for efficiency (NOTE This is considered cheating)\n",
        "                weight_ref.tensor.grad.add_(\n",
        "                    torch.nn.grad.conv2d_weight(\n",
        "                    input=input_ref.tensor,\n",
        "                    weight_size=weight_ref.tensor.shape,\n",
        "                    grad_output=grad_output,\n",
        "                    stride=self.stride,\n",
        "                    padding=self.padding,\n",
        "                    dilation=self.dilation,\n",
        "                    groups=self.groups\n",
        "                    )\n",
        "                )\n",
        "        return _backward\n",
        "\n",
        "    @torch.compile\n",
        "    def _calculate_gradient_input_tensor(self, input_tensor,weight_tensor,grad_output):\n",
        "        h_in, w_in = input_tensor.shape[2], input_tensor.shape[3]\n",
        "        h_out, w_out = grad_output.shape[2], grad_output.shape[3]\n",
        "        stride = self.stride\n",
        "        padding = self.padding\n",
        "        kernel_size = self.kernel_size\n",
        "        dilation = self.dilation\n",
        "        # The formula relating input size to output size in a transposed convolution is:\n",
        "        # InputSize = (OutputSize - 1) * stride - 2 * padding + dilation * (kernel - 1) + output_padding + 1\n",
        "        # We rearrange this to solve for the required output_padding.\n",
        "        output_padding_h = h_in - ((h_out - 1) * stride[0] - 2 * padding[0] + dilation[0] * (kernel_size[0] - 1) + 1)\n",
        "        output_padding_w = w_in - ((w_out - 1) * stride[1] - 2 * padding[1] + dilation[1] * (kernel_size[1] - 1) + 1)\n",
        "        output_padding = (output_padding_h, output_padding_w)\n",
        "\n",
        "        grad_input = F.conv_transpose2d(\n",
        "            grad_output,\n",
        "            weight_tensor,\n",
        "            stride=stride,\n",
        "            padding=padding,\n",
        "            output_padding=output_padding,\n",
        "            dilation=dilation,\n",
        "            groups=self.groups\n",
        "        )\n",
        "        return grad_input\n",
        "\n",
        "    @torch.compile\n",
        "    def _calculate_gradient_weight_tensor_loop(self,input_tensor,grad_output):\n",
        "        #The gradient w.r.t. the weights is a convolution\n",
        "        # of the input (X) and the output gradient (grad_output).\n",
        "        # For grouped convolutions, we must perform this calculation for each group separately.\n",
        "        #O(b,co,oh,ow)=B(co)+ kh =0∑KH −1  kw =0∑KW −1  ci=(co/G)⋅(Cin/G)∑((co/G)+1)⋅(Cin/G)−1\n",
        "        #  Ipadded(b,ci,ih,iw)K(co ,ci ,kh ,kw ),\n",
        "        # where ih  = oh.sh+kh.dh, iw = ow.sw+kw.dw\n",
        "\n",
        "        # ∂L/∂K(ci′ ,co′ ,kh′ ,kw′ ) =b,oh,ow∑ G(b,co',oh,ow)\n",
        "        # Ipadded(b,ci', oh.sh + kh'.dh, ow.sw + kw'.dw)\n",
        "\n",
        "        # the original operation is a summation over kh and kw and the input image\n",
        "        # coordinates ih iw are sampled with dilation. (oh and ow for individual coordinates are constant)\n",
        "\n",
        "\n",
        "        # the equation for the gradient is a summation over oh and ow and the input image\n",
        "        # coordinates ih iw are sampled with stride.\n",
        "        # (kh and kw are constant for individual coordinates are constant)\n",
        "\n",
        "        # hence when calling conv2d we need to switch stride and dilation\n",
        "        # and also transpose the dimensions of batch and channel as for derivative with respect to weight the channels are fixed in the summation\n",
        "\n",
        "        in_channels = self.in_channels\n",
        "        groups = self.groups\n",
        "        out_channels = self.out_channels\n",
        "        in_channels_per_group = in_channels // groups\n",
        "        out_channels_per_group = out_channels // groups\n",
        "        grad_W_groups = []\n",
        "\n",
        "        for g in range(groups):\n",
        "            # Slice the input tensor to get the channels for the current group\n",
        "            start_in_ch = g * in_channels_per_group\n",
        "            end_in_ch = start_in_ch + in_channels_per_group\n",
        "            X_g = input_tensor[:, start_in_ch:end_in_ch, :, :]\n",
        "\n",
        "            # Slice the output gradient tensor to get the channels for the current group\n",
        "            start_out_ch = g * out_channels_per_group\n",
        "            end_out_ch = start_out_ch + out_channels_per_group\n",
        "            grad_output_g = grad_output[:, start_out_ch:end_out_ch, :, :]\n",
        "\n",
        "            # To calculate the weight gradient via a convolution, we must cleverly\n",
        "            # permute the input (X_g) and output gradient (grad_output_g) tensors.\n",
        "            # We treat X_g as the input and grad_output_g as the kernel.\n",
        "            # X_g: (N, Cin/g, H, W) -> permute -> (Cin/g, N, H, W)\n",
        "            # grad_output_g: (N, Cout/g, oH, oW) -> permute -> (Cout/g, N, oH, oW)\n",
        "            # The F.conv2d call then treats 'Cin/g' as the batch size and 'N' as the input channels.\n",
        "            # The stride and dilation parameters from the original convolution are swapped.\n",
        "            X_g_permuted = X_g.transpose(0, 1)\n",
        "            grad_output_g_permuted = grad_output_g.transpose(0, 1)\n",
        "\n",
        "            grad_W_g_permuted = F.conv2d(\n",
        "                X_g_permuted,\n",
        "                grad_output_g_permuted,\n",
        "                stride=self.dilation,\n",
        "                padding=self.padding,\n",
        "                dilation=self.stride,\n",
        "                groups=1 # The group calculation is handled by our loop, so this is a standard conv.\n",
        "            )\n",
        "\n",
        "            # The result has shape (Cin/g, Cout/g, kH, kW). We must permute it back to\n",
        "            # the standard weight layout of (Cout/g, Cin/g, kH, kW).\n",
        "            grad_W_g = grad_W_g_permuted.transpose(0, 1)\n",
        "            grad_W_groups.append(grad_W_g)\n",
        "\n",
        "        # Concatenate the gradients from all groups along the output channel dimension.\n",
        "        # The weight tensor for grouped convolutions is laid out by stacking the weights\n",
        "        # for each group, so we do the same for the gradient.\n",
        "        grad_weight = torch.cat(grad_W_groups, dim=0)\n",
        "        return grad_weight\n",
        "\n",
        "    # def _calculate_gradient_weight_tensor_cheating(self,input_tensor,grad_output):\n",
        "    #     return torch.nn.grad.conv2d_weight(\n",
        "    #     input=input_tensor,\n",
        "    #     weight_size=self.weight.tensor.shape,\n",
        "    #     grad_output=grad_output,\n",
        "    #     stride=self.stride,\n",
        "    #     padding=self.padding,\n",
        "    #     dilation=self.dilation,\n",
        "    #     groups=self.groups\n",
        "    #     )\n",
        "\n",
        "class BatchNorm_Nd(Module):\n",
        "    __slots__ = ('num_features', 'eps', 'momentum', 'graph', 'weight', 'bias', 'running_mean', 'running_var', '_channel_axis', '_shape_cache','__weakref__')\n",
        "    def __new__(cls, num_features, eps=1e-5, momentum=0.1, *, graph=None):\n",
        "        assert num_features > 0\n",
        "        return super().__new__(cls)\n",
        "\n",
        "    def __init__(self, num_features, eps=1e-5, momentum=0.1, *, graph=None):\n",
        "        super().__init__()\n",
        "        self.num_features = num_features\n",
        "        self.eps = eps\n",
        "        self.momentum = momentum\n",
        "        self.graph = weakref.proxy(graph) if graph is not None else None\n",
        "\n",
        "        self.weight = CustomTensor(torch.ones(num_features,device=BatchNorm_Nd.device,dtype=BatchNorm_Nd.dtype), _custom_requires_grad=True if graph is not None else False, graph=graph if graph is not None else None, is_leaf=True)\n",
        "        self.bias = CustomTensor(torch.zeros(num_features,device=BatchNorm_Nd.device,dtype=BatchNorm_Nd.dtype), _custom_requires_grad=True if graph is not None else False, graph=graph if graph is not None else None, is_leaf=True)\n",
        "\n",
        "        self.running_mean = torch.zeros(num_features,device=BatchNorm_Nd.device,dtype=BatchNorm_Nd.dtype)\n",
        "        self.running_var = torch.ones(num_features,device=BatchNorm_Nd.device,dtype=BatchNorm_Nd.dtype)\n",
        "\n",
        "        self._channel_axis = 1\n",
        "        self._shape_cache = {}\n",
        "\n",
        "    def _get_broadcast_shape(self, input_shape):\n",
        "        if input_shape not in self._shape_cache:\n",
        "            self._shape_cache[input_shape] = (1,) + (input_shape[1],) + (1,) * (len(input_shape) - 2)\n",
        "        return self._shape_cache[input_shape]\n",
        "\n",
        "    @torch.compile\n",
        "    def _compute_stats(self, x: torch.Tensor):\n",
        "        reduce_dims = tuple(i for i in range(x.dim()) if i != self._channel_axis)\n",
        "\n",
        "        mean = x.mean(dim=reduce_dims, keepdim=False)\n",
        "        var = x.var(dim=reduce_dims, keepdim=False, unbiased=False)\n",
        "\n",
        "        return mean, var\n",
        "\n",
        "    def _create_backward(self, input_tensor, result, torch_input_tensor, normalized,\n",
        "                        shape_to, weight_shaped, input_minus_mean, inv_std, total_elements):\n",
        "        \"\"\"Creates the _backward hook for result tensor\"\"\"\n",
        "        input_ref = weakref.proxy(input_tensor)\n",
        "        result_ref = weakref.proxy(result)\n",
        "        weight_ref = weakref.proxy(self.weight)\n",
        "        bias_ref = weakref.proxy(self.bias)\n",
        "\n",
        "        def _backward():\n",
        "            result_gradient = result_ref.tensor.grad\n",
        "            reduce_dims = tuple(i for i in range(input_ref.tensor.dim()) if i != self._channel_axis)\n",
        "            if bias_ref._custom_requires_grad:\n",
        "                if bias_ref.tensor.grad is None:\n",
        "                    bias_ref._zero_grad()\n",
        "                grad_bias = result_gradient.sum(dim=reduce_dims)\n",
        "                bias_ref.tensor.grad.add_(grad_bias.view(bias_ref.tensor.shape))\n",
        "\n",
        "            if weight_ref._custom_requires_grad:\n",
        "                if weight_ref.tensor.grad is None:\n",
        "                    weight_ref._zero_grad()\n",
        "                grad_weight = (result_gradient * normalized).sum(dim=reduce_dims)\n",
        "                weight_ref.tensor.grad.add_(grad_weight.view(weight_ref.tensor.shape))\n",
        "\n",
        "            if input_ref._custom_requires_grad:\n",
        "                if input_ref.tensor.grad is None:\n",
        "                    input_ref._zero_grad()\n",
        "                grad_input = self.batchnorm_gradient_for_input_tensor(\n",
        "                    result_gradient=result_gradient,\n",
        "                    input_tensor=torch_input_tensor,\n",
        "                    weight_shaped=weight_shaped,\n",
        "                    input_minus_mean=input_minus_mean,\n",
        "                    inv_std=inv_std,\n",
        "                    total_elements=total_elements\n",
        "                )\n",
        "                input_ref.tensor.grad.add_(grad_input)\n",
        "\n",
        "        return _backward\n",
        "\n",
        "    def forward(self, input_tensor):\n",
        "        torch_input_tensor = input_tensor.tensor\n",
        "        shape_to = self._get_broadcast_shape(torch_input_tensor.shape)\n",
        "\n",
        "        # Pre-compute shaped tensors once\n",
        "        weight_shaped = self.weight.tensor.view(shape_to)\n",
        "        bias_shaped = self.bias.tensor.view(shape_to)\n",
        "\n",
        "        if self.training:\n",
        "            batch_mean, batch_var = self._compute_stats(torch_input_tensor)\n",
        "            total_elements = torch_input_tensor.numel() // torch_input_tensor.shape[self._channel_axis]\n",
        "            unbiased_var = batch_var * total_elements / (total_elements - 1) if total_elements > 1 else batch_var\n",
        "\n",
        "            # Update running statistics in-place\n",
        "            self.running_mean.mul_(1-self.momentum).add_(batch_mean, alpha=self.momentum)\n",
        "            self.running_var.mul_(1-self.momentum).add_(unbiased_var, alpha=self.momentum)\n",
        "\n",
        "            mean, var = batch_mean, batch_var\n",
        "        else:\n",
        "            mean, var = self.running_mean, self.running_var\n",
        "            mean_shaped = mean.view(shape_to)\n",
        "            var_shaped = var.view(shape_to)\n",
        "            normalized = (torch_input_tensor - mean_shaped) / torch.sqrt(var_shaped + self.eps)\n",
        "            result = normalized * weight_shaped + bias_shaped\n",
        "            return CustomTensor(result, due_to_operation=True)\n",
        "\n",
        "        # Forward pass computation (training mode)\n",
        "        mean_shaped = mean.view(shape_to)\n",
        "        var_shaped = var.view(shape_to)\n",
        "\n",
        "        inv_std = torch.rsqrt(var_shaped + self.eps)\n",
        "        input_minus_mean = torch_input_tensor - mean_shaped\n",
        "        normalized = input_minus_mean * inv_std\n",
        "        output = normalized * weight_shaped + bias_shaped\n",
        "\n",
        "        result = CustomTensor(output, _custom_requires_grad=True, graph=self.graph,due_to_operation=True, is_leaf=False)\n",
        "\n",
        "        # Build computation graph\n",
        "        graph = self.graph\n",
        "        if input_tensor._custom_requires_grad:\n",
        "          graph.add_edge(input_tensor._node_id, result._node_id)\n",
        "        if self.weight._custom_requires_grad:\n",
        "          graph.add_edge(self.weight._node_id, result._node_id)\n",
        "        if self.bias._custom_requires_grad:\n",
        "          graph.add_edge(self.bias._node_id, result._node_id)\n",
        "\n",
        "        # Create and assign backward function\n",
        "        result._backward = self._create_backward(\n",
        "            input_tensor, result, torch_input_tensor, normalized,\n",
        "            shape_to, weight_shaped, input_minus_mean, inv_std, total_elements\n",
        "        )\n",
        "\n",
        "        return result\n",
        "\n",
        "    @torch.compile\n",
        "    def batchnorm_gradient_for_input_tensor(self, *, result_gradient, input_tensor, weight_shaped,\n",
        "                                          input_minus_mean, inv_std, total_elements):\n",
        "        reduce_dims = tuple(i for i in range(input_tensor.dim()) if i != self._channel_axis)\n",
        "\n",
        "        outer_term = weight_shaped * inv_std\n",
        "        term_1 = result_gradient\n",
        "        term_2 = (-1/total_elements) * result_gradient.sum(dim=reduce_dims, keepdim=True)\n",
        "        term3_sum_component = (input_minus_mean * result_gradient).sum(dim=reduce_dims, keepdim=True)\n",
        "        term3 = inv_std**2 * (-1/total_elements) * input_minus_mean * term3_sum_component\n",
        "        return outer_term * (term_1 + term_2 + term3)\n",
        "\n",
        "class MaxPool2d(Module):\n",
        "    __slots__ = ('kernel_size', 'stride', 'dilation', 'padding', 'graph','__weakref__')\n",
        "    def __new__(cls, *, kernel_size, stride=1, padding=0, dilation=1, graph=None):\n",
        "        assert isinstance(kernel_size, int) or len(kernel_size) == 2\n",
        "        assert isinstance(stride, int) or len(stride) == 2\n",
        "        assert isinstance(dilation, int) or len(dilation) == 2\n",
        "        assert isinstance(padding, int) or len(padding) == 2\n",
        "        return super().__new__(cls)\n",
        "\n",
        "    def __init__(self, *, kernel_size, stride=1, padding=0, dilation=1, graph=None):\n",
        "        super().__init__()\n",
        "        self.kernel_size = (kernel_size, kernel_size) if isinstance(kernel_size, int) else kernel_size\n",
        "        self.stride = (stride, stride) if isinstance(stride, int) else stride\n",
        "        self.dilation = (dilation, dilation) if isinstance(dilation, int) else dilation\n",
        "        self.padding = (padding, padding) if isinstance(padding, int) else padding\n",
        "        self.graph = weakref.proxy(graph) if graph is not None else None\n",
        "\n",
        "    def _create_backward(self, input_tensor, result, cached_indices):\n",
        "        \"\"\"Creates the _backward hook for result tensor\"\"\"\n",
        "        input_ref = weakref.proxy(input_tensor)\n",
        "        result_ref = weakref.proxy(result)\n",
        "\n",
        "        def _backward():\n",
        "            if input_ref._custom_requires_grad:\n",
        "              if input_ref.tensor.grad is None:\n",
        "                  input_ref._zero_grad()\n",
        "              grad_output = result_ref.tensor.grad\n",
        "              input = input_ref.tensor\n",
        "              grad_input = MaxPool2d._calculate_gradient_input_tensor(grad_output, cached_indices, input)\n",
        "              input_ref.tensor.grad.add_(grad_input)\n",
        "\n",
        "        return _backward\n",
        "\n",
        "    def forward(self, input_tensor):\n",
        "        kernel_size = self.kernel_size\n",
        "        stride = self.stride\n",
        "        padding = self.padding\n",
        "        dilation = self.dilation\n",
        "\n",
        "        output_tensor, max_indices = F.max_pool2d(\n",
        "            input=input_tensor.tensor,\n",
        "            kernel_size=kernel_size,\n",
        "            stride=stride,\n",
        "            padding=padding,\n",
        "            dilation=dilation,\n",
        "            return_indices=True\n",
        "        )\n",
        "\n",
        "        if not self.training:\n",
        "            return CustomTensor(output_tensor, due_to_operation=True)\n",
        "\n",
        "        graph = self.graph\n",
        "        result = CustomTensor(output_tensor, _custom_requires_grad=True, graph=graph,due_to_operation=True, is_leaf=False)\n",
        "        if input_tensor._custom_requires_grad:\n",
        "          graph.add_edge(input_tensor._node_id, result._node_id)\n",
        "\n",
        "\n",
        "        result._backward = self._create_backward(input_tensor, result, max_indices)\n",
        "\n",
        "        return result\n",
        "    @staticmethod\n",
        "    @torch.compile\n",
        "    def _calculate_gradient_input_tensor(grad_output, indices, input):\n",
        "      # grad_output: (N, C, H_out, W_out)\n",
        "      # indices:     (N, C, H_out, W_out)\n",
        "      N, C, H_out, W_out = grad_output.shape\n",
        "      # Initialize grad_input\n",
        "      grad_input = torch.zeros_like(input)\n",
        "      # Flatten spatial dims\n",
        "      grad_output_flat = grad_output.view(N, C, -1)\n",
        "      indices_flat = indices.view(N, C, -1)\n",
        "      grad_input_flat = grad_input.view(N, C, -1)\n",
        "      # Scatter gradients into appropriate positions\n",
        "      grad_input_flat.scatter_add_(2, indices_flat, grad_output_flat)\n",
        "      # Reshape back to input shape\n",
        "      grad_input = grad_input_flat.view(input.shape)\n",
        "      return grad_input\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"MaxPool2d(kernel_size={self.kernel_size}, stride={self.stride}, padding={self.padding})\"\n",
        "\n",
        "class AvgPool2d(Module):\n",
        "    __slots__ = ('kernel_size', 'stride', 'padding', 'graph','__weakref__')\n",
        "    def __new__(cls, *, kernel_size, stride=1, padding=0, graph=None):\n",
        "        assert isinstance(kernel_size, int) or len(kernel_size) == 2\n",
        "        assert isinstance(stride, int) or len(stride) == 2\n",
        "        assert isinstance(padding, int) or len(padding) == 2\n",
        "        return super().__new__(cls)\n",
        "\n",
        "    def __init__(self, *, kernel_size, stride=1, padding=0, graph=None):\n",
        "        super().__init__()\n",
        "        self.kernel_size = (kernel_size, kernel_size) if isinstance(kernel_size, int) else kernel_size\n",
        "        self.stride = (stride, stride) if isinstance(stride, int) else stride\n",
        "        self.padding = (padding, padding) if isinstance(padding, int) else padding\n",
        "        self.graph = weakref.proxy(graph) if graph is not None else None\n",
        "\n",
        "    def create_backward(self, input_tensor, result):\n",
        "        \"\"\"Creates the _backward hook for result tensor\"\"\"\n",
        "        input_ref = weakref.proxy(input_tensor)\n",
        "        result_ref = weakref.proxy(result)\n",
        "\n",
        "        def _backward():\n",
        "          if input_ref._custom_requires_grad:\n",
        "\n",
        "            if input_ref.tensor.grad is None:\n",
        "                input_ref._zero_grad()\n",
        "            grad_output = result_ref.tensor.grad\n",
        "            input = input_ref.tensor\n",
        "            grad_input = self._calculate_gradient_input_tensor(grad_output,input)\n",
        "            input_ref.tensor.grad.add_(grad_input)\n",
        "\n",
        "        return _backward\n",
        "\n",
        "    def forward(self, input_tensor):\n",
        "        kernel_size = self.kernel_size\n",
        "        stride = self.stride\n",
        "        padding = self.padding\n",
        "\n",
        "        output_tensor = F.avg_pool2d(\n",
        "            input=input_tensor.tensor,\n",
        "            kernel_size=kernel_size,\n",
        "            stride=stride,\n",
        "            padding=padding,\n",
        "            count_include_pad=True\n",
        "        )\n",
        "\n",
        "        if not self.training:\n",
        "            return CustomTensor(output_tensor, due_to_operation=True)\n",
        "\n",
        "        result = CustomTensor(output_tensor, _custom_requires_grad=True, graph=self.graph, due_to_operation=True, is_leaf=False)\n",
        "        graph = self.graph\n",
        "        if input_tensor._custom_requires_grad:\n",
        "          graph.add_edge(input_tensor._node_id, result._node_id)\n",
        "\n",
        "        result._backward = self.create_backward(input_tensor, result)\n",
        "\n",
        "        return result\n",
        "\n",
        "    @torch.compile\n",
        "    def _calculate_gradient_input_tensor(self,grad_output,input):\n",
        "\n",
        "            h_in, w_in = input.shape[2], input.shape[3]\n",
        "            h_out, w_out = grad_output.shape[2], grad_output.shape[3]\n",
        "            kernel_size=self.kernel_size\n",
        "            stride=self.stride\n",
        "            padding=self.padding\n",
        "\n",
        "            # The formula relating input size to output size in a transposed convolution is:\n",
        "            # InputSize = (OutputSize - 1) * stride - 2 * padding + dilation * (kernel - 1) + output_padding + 1\n",
        "            # We rearrange this to solve for the required output_padding.\n",
        "            output_padding_h = h_in - ((h_out - 1) * stride[0] - 2 * padding[0] +  (kernel_size[0] - 1) + 1)\n",
        "            output_padding_w = w_in - ((w_out - 1) * stride[1] - 2 * padding[1] +  (kernel_size[1] - 1) + 1)\n",
        "            output_padding = (output_padding_h, output_padding_w)\n",
        "            pool_size = kernel_size[0] * kernel_size[1]\n",
        "            grad_kernel = torch.ones(grad_output.shape[1], 1, kernel_size[0], kernel_size[1],device=grad_output.device,dtype=grad_output.dtype) / pool_size\n",
        "            grad_input = F.conv_transpose2d(\n",
        "                input= grad_output,\n",
        "                weight = grad_kernel,\n",
        "                stride = stride,\n",
        "                padding = padding,\n",
        "                output_padding=output_padding,\n",
        "                groups = input.shape[1]\n",
        "            )\n",
        "            return grad_input\n",
        "\n",
        "class ReLu(Module):\n",
        "    __slots__ = ('graph','__weakref__')\n",
        "    def __init__(self, *, graph=None):\n",
        "        super().__init__()\n",
        "        self.graph = weakref.proxy(graph) if graph is not None else None\n",
        "\n",
        "    def _create_backward(self, input_tensor, result):\n",
        "        \"\"\"Creates the _backward hook for result tensor\"\"\"\n",
        "        input_ref = weakref.proxy(input_tensor)\n",
        "        result_ref = weakref.proxy(result)\n",
        "\n",
        "        def _backward():\n",
        "          if input_ref._custom_requires_grad:\n",
        "            if input_ref.tensor.grad is None:\n",
        "                input_ref._zero_grad()\n",
        "            grad_output = result_ref.tensor.grad\n",
        "            grad_input = grad_output.clone()\n",
        "            grad_input[input_ref.tensor <= 0] = 0\n",
        "            input_ref.tensor.grad.add_(grad_input)\n",
        "\n",
        "        return _backward\n",
        "\n",
        "    def forward(self, input_tensor):\n",
        "        output_tensor = F.relu(input_tensor.tensor)\n",
        "        if not self.training:\n",
        "            return CustomTensor(output_tensor, due_to_operation=True)\n",
        "\n",
        "        result = CustomTensor(output_tensor, _custom_requires_grad=True, graph=self.graph,due_to_operation=True, is_leaf=False)\n",
        "        if input_tensor._custom_requires_grad:\n",
        "          self.graph.add_edge(input_tensor._node_id, result._node_id)\n",
        "        result._backward = self._create_backward(input_tensor, result)\n",
        "        return result\n",
        "\n",
        "class Leaky_ReLu(Module):\n",
        "    __slots__ = ('graph', 'negative_slope', '__weakref__')\n",
        "    def __new__(cls, *, negative_slope=0.01, graph=None):\n",
        "        assert negative_slope > 0\n",
        "        return super().__new__(cls)\n",
        "\n",
        "    def __init__(self, *, negative_slope=0.01, graph=None):\n",
        "        super().__init__()\n",
        "        self.graph = weakref.proxy(graph) if graph is not None else None\n",
        "        self.negative_slope = negative_slope\n",
        "\n",
        "    def _create_backward(self, input_tensor, result):\n",
        "        \"\"\"Creates the _backward hook for result tensor\"\"\"\n",
        "        input_ref = weakref.proxy(input_tensor)\n",
        "        result_ref = weakref.proxy(result)\n",
        "\n",
        "        def _backward():\n",
        "          if input_ref._custom_requires_grad:\n",
        "            if input_ref.tensor.grad is None:\n",
        "                input_ref._zero_grad()\n",
        "            grad_output = result_ref.tensor.grad\n",
        "            grad_input = grad_output.clone()\n",
        "            grad_input[input_ref.tensor <= 0] *= self.negative_slope\n",
        "            input_ref.tensor.grad.add_(grad_input)\n",
        "\n",
        "        return _backward\n",
        "\n",
        "    def forward(self, input_tensor):\n",
        "        output_tensor = F.leaky_relu(input_tensor.tensor, negative_slope=self.negative_slope)\n",
        "        if not self.training:\n",
        "            return CustomTensor(output_tensor, due_to_operation=True)\n",
        "\n",
        "        result = CustomTensor(output_tensor, _custom_requires_grad=True, graph=self.graph,due_to_operation=True, is_leaf=False)\n",
        "        if input_tensor._custom_requires_grad:\n",
        "          self.graph.add_edge(input_tensor._node_id, result._node_id)\n",
        "        result._backward = self._create_backward(input_tensor, result)\n",
        "        return result\n",
        "\n",
        "class Elu(Module):\n",
        "    __slots__ = ('graph', 'alpha', '__weakref__')\n",
        "    def __new__(cls, *, alpha=1.0, graph=None):\n",
        "        assert alpha > 0\n",
        "        return super().__new__(cls)\n",
        "\n",
        "    def __init__(self, *, alpha=1.0, graph=None):\n",
        "        super().__init__()\n",
        "        self.graph = weakref.proxy(graph) if graph is not None else None\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def _create_backward(self, input_tensor, result, output_tensor):\n",
        "        \"\"\"Creates the _backward hook for result tensor\"\"\"\n",
        "        input_ref = weakref.proxy(input_tensor)\n",
        "        result_ref = weakref.proxy(result)\n",
        "\n",
        "        def _backward():\n",
        "          if input_ref._custom_requires_grad:\n",
        "            if input_ref.tensor.grad is None:\n",
        "                input_ref._zero_grad()\n",
        "            grad_output = result_ref.tensor.grad\n",
        "            grad_input = grad_output.clone()\n",
        "            mask_neg = (input_ref.tensor.data <= 0)\n",
        "            grad_input[mask_neg] *= (self.alpha + output_tensor[mask_neg])\n",
        "            input_ref.tensor.grad.add_(grad_input)\n",
        "\n",
        "        return _backward\n",
        "\n",
        "    def forward(self, input_tensor):\n",
        "        output_tensor = F.elu(input_tensor.tensor, alpha=self.alpha)\n",
        "        if not self.training:\n",
        "            return CustomTensor(output_tensor, due_to_operation=True)\n",
        "\n",
        "        result = CustomTensor(output_tensor, _custom_requires_grad=True, graph=self.graph,due_to_operation=True, is_leaf=False)\n",
        "        if input_tensor._custom_requires_grad:\n",
        "          self.graph.add_edge(input_tensor._node_id, result._node_id)\n",
        "        result._backward = self._create_backward(input_tensor, result, output_tensor)\n",
        "        return result\n",
        "\n",
        "class GeLu(Module):\n",
        "    __slots__ = ('graph', 'approximate', '__weakref__')\n",
        "    def __new__(cls, *, approximate='none', graph=None):\n",
        "        assert approximate in {\"none\", \"tanh\"}\n",
        "        return super().__new__(cls)\n",
        "\n",
        "    def __init__(self, *, approximate='none', graph=None):\n",
        "        super().__init__()\n",
        "        self.graph = weakref.proxy(graph) if graph is not None else None\n",
        "        self.approximate = approximate\n",
        "\n",
        "    def _create_backward(self, input_tensor, result):\n",
        "        \"\"\"Creates the _backward hook for result tensor\"\"\"\n",
        "        input_ref = weakref.proxy(input_tensor)\n",
        "        result_ref = weakref.proxy(result)\n",
        "\n",
        "        def _backward():\n",
        "          if input_ref._custom_requires_grad:\n",
        "            if input_ref.tensor.grad is None:\n",
        "                input_ref._zero_grad()\n",
        "            grad_output = result_ref.tensor.grad\n",
        "            grad_input = GeLu.gelu_derivative(input_ref.tensor, grad_output, self.approximate)\n",
        "            input_ref.tensor.grad.add_(grad_input)\n",
        "\n",
        "        return _backward\n",
        "\n",
        "    def forward(self, input_tensor):\n",
        "        output_tensor = F.gelu(input_tensor.tensor, approximate=self.approximate)\n",
        "        if not self.training:\n",
        "            return CustomTensor(output_tensor, due_to_operation=True)\n",
        "\n",
        "        result = CustomTensor(output_tensor, _custom_requires_grad=True, graph=self.graph,due_to_operation=True, is_leaf=False)\n",
        "        if input_tensor._custom_requires_grad:\n",
        "          self.graph.add_edge(input_tensor._node_id, result._node_id)\n",
        "        result._backward = self._create_backward(input_tensor, result)\n",
        "        return result\n",
        "\n",
        "    @torch.compile\n",
        "    @staticmethod\n",
        "    def gelu_derivative(x: torch.Tensor, grad_output: torch.Tensor, approximate: str) -> torch.Tensor:\n",
        "        if approximate == \"none\":\n",
        "            sqrt_2_pi = 2.5066282749176025  # torch.tensor(2 * torch.pi).sqrt()\n",
        "            phi_x_cdf = 0.5 * (1 + torch.special.erf(x / 1.4142135381698608))  # torch.sqrt(torch.tensor(2.0))))\n",
        "            phi_x_pdf = torch.exp(-0.5 * x**2) / sqrt_2_pi\n",
        "            return (phi_x_cdf + x * phi_x_pdf) * grad_output\n",
        "        else:\n",
        "            sqrt_2_over_pi = 0.7978845238685608  # torch.tensor(2.0 / torch.pi).sqrt()\n",
        "            coeff_cubic = 0.044715\n",
        "            x2 = x.square()\n",
        "            inner = x + coeff_cubic * x2 * x\n",
        "            u = sqrt_2_over_pi * inner\n",
        "            tanh_u = torch.tanh(u)\n",
        "            poly = 1 + 3 * coeff_cubic * x2\n",
        "            return (0.5 * tanh_u + 0.5 * (1 - tanh_u.square()) * (sqrt_2_over_pi * poly * x) + 0.5) * grad_output\n",
        "\n",
        "class Sigmoid(Module):\n",
        "    __slots__ = ('graph', '__weakref__')\n",
        "    def __new__(cls, *, graph=None):\n",
        "        return super().__new__(cls)\n",
        "\n",
        "    def __init__(self, *, graph=None):\n",
        "        super().__init__()\n",
        "        self.graph = weakref.proxy(graph) if graph is not None else None\n",
        "\n",
        "    def _create_backward(self, input_tensor, result, output_tensor):\n",
        "        \"\"\"Creates the _backward hook for result tensor\"\"\"\n",
        "        input_ref = weakref.proxy(input_tensor)\n",
        "        result_ref = weakref.proxy(result)\n",
        "\n",
        "        def _backward():\n",
        "          if input_ref._custom_requires_grad:\n",
        "            if input_ref.tensor.grad is None:\n",
        "                input_ref._zero_grad()\n",
        "            grad_output = result_ref.tensor.grad\n",
        "            grad_input = grad_output * output_tensor * (1 - output_tensor)\n",
        "            input_ref.tensor.grad.add_(grad_input)\n",
        "\n",
        "        return _backward\n",
        "\n",
        "    def forward(self, input_tensor):\n",
        "        output_tensor = F.sigmoid(input_tensor.tensor)\n",
        "        if not self.training:\n",
        "            return CustomTensor(output_tensor, due_to_operation=True)\n",
        "\n",
        "        result = CustomTensor(output_tensor, _custom_requires_grad=True, graph=self.graph,due_to_operation=True, is_leaf=False)\n",
        "        if input_tensor._custom_requires_grad:\n",
        "          self.graph.add_edge(input_tensor._node_id, result._node_id)\n",
        "        result._backward = self._create_backward(input_tensor, result, output_tensor)\n",
        "        return result\n",
        "\n",
        "class Tanh(Module):\n",
        "    __slots__ = ('graph', '__weakref__')\n",
        "    def __new__(cls, *, graph=None):\n",
        "        return super().__new__(cls)\n",
        "\n",
        "    def __init__(self, *, graph=None):\n",
        "        super().__init__()\n",
        "        self.graph = weakref.proxy(graph) if graph is not None else None\n",
        "\n",
        "    def _create_backward(self, input_tensor, result, output_tensor):\n",
        "        \"\"\"Creates the _backward hook for result tensor\"\"\"\n",
        "        input_ref = weakref.proxy(input_tensor)\n",
        "        result_ref = weakref.proxy(result)\n",
        "\n",
        "        def _backward():\n",
        "          if input_ref._custom_requires_grad:\n",
        "            if input_ref.tensor.grad is None:\n",
        "                input_ref._zero_grad()\n",
        "            grad_output = result_ref.tensor.grad\n",
        "            grad_input = grad_output * (1 - output_tensor**2)\n",
        "            input_ref.tensor.grad.add_(grad_input)\n",
        "\n",
        "        return _backward\n",
        "\n",
        "    def forward(self, input_tensor):\n",
        "        output_tensor = F.tanh(input_tensor.tensor)\n",
        "        if not self.training:\n",
        "            return CustomTensor(output_tensor, due_to_operation=True)\n",
        "\n",
        "        result = CustomTensor(output_tensor, _custom_requires_grad=True, graph=self.graph,due_to_operation=True, is_leaf=False)\n",
        "        if input_tensor._custom_requires_grad:\n",
        "          self.graph.add_edge(input_tensor._node_id, result._node_id)\n",
        "        result._backward = self._create_backward(input_tensor, result, output_tensor)\n",
        "        return result\n",
        "\n",
        "class Silu(Module):\n",
        "    __slots__ = ('graph', '__weakref__')\n",
        "    def __new__(cls, *, graph=None):\n",
        "        return super().__new__(cls)\n",
        "\n",
        "    def __init__(self, *, graph=None):\n",
        "        super().__init__()\n",
        "        self.graph = weakref.proxy(graph) if graph is not None else None\n",
        "\n",
        "    def _create_backward(self, input_tensor, result, output_tensor):\n",
        "        \"\"\"Creates the _backward hook for result tensor\"\"\"\n",
        "        input_ref = weakref.proxy(input_tensor)\n",
        "        result_ref = weakref.proxy(result)\n",
        "\n",
        "        def _backward():\n",
        "          if input_ref._custom_requires_grad:\n",
        "            if input_ref.tensor.grad is None:\n",
        "                input_ref._zero_grad()\n",
        "            grad_output = result_ref.tensor.grad\n",
        "            s_input_tensor = output_tensor / input_ref.tensor\n",
        "            grad_input = grad_output * (s_input_tensor + output_tensor * (1 - s_input_tensor))\n",
        "            input_ref.tensor.grad.add_(grad_input)\n",
        "\n",
        "        return _backward\n",
        "\n",
        "    def forward(self, input_tensor):\n",
        "        output_tensor = F.silu(input_tensor.tensor)\n",
        "        if not self.training:\n",
        "            return CustomTensor(output_tensor, due_to_operation=True)\n",
        "\n",
        "        result = CustomTensor(output_tensor, _custom_requires_grad=True, graph=self.graph,due_to_operation=True, is_leaf=False)\n",
        "        if input_tensor._custom_requires_grad:\n",
        "          self.graph.add_edge(input_tensor._node_id, result._node_id)\n",
        "        result._backward = self._create_backward(input_tensor, result, output_tensor)\n",
        "        return result\n",
        "\n",
        "class Swish(Module):\n",
        "    # TODO: implement in future\n",
        "    __slots__ = ('graph', 'B', 'B_initial', '__weakref__')\n",
        "    def __new__(cls, *, B_initial=1.0, graph=None):\n",
        "        assert B_initial > 0\n",
        "        return super().__new__(cls)\n",
        "\n",
        "    def __init__(self, *, B_initial=1.0, graph=None):\n",
        "        super().__init__()\n",
        "        self.graph = weakref.proxy(graph) if graph is not None else None\n",
        "        self.B = CustomTensor([B_initial], _custom_requires_grad=True if graph is not None else False, graph=graph if graph is not None else None, is_leaf=True)\n",
        "        self.B_initial = B_initial\n",
        "\n",
        "    def _create_backward(self, input_tensor, result, output_tensor):\n",
        "        \"\"\"Creates the _backward hook for result tensor\"\"\"\n",
        "        input_ref = weakref.proxy(input_tensor)\n",
        "        result_ref = weakref.proxy(result)\n",
        "        B_ref = weakref.proxy(self.B)\n",
        "\n",
        "        def _backward():\n",
        "          input_requires_grad = input_ref._custom_requires_grad\n",
        "          B_requires_grad = B_ref._custom_requires_grad\n",
        "          if not input_requires_grad and not B_requires_grad:\n",
        "              return\n",
        "          grad_input, grad_B = self._calculate_gradients(\n",
        "              input_tensor=input_ref.tensor,\n",
        "              result=result_ref.tensor,\n",
        "              output_tensor=output_tensor,\n",
        "              B_tensor=B_ref.tensor\n",
        "          )\n",
        "          # grad_output = result_ref.tensor.grad\n",
        "          # sig_B_x = output_tensor / input_ref.tensor\n",
        "          # common = sig_B_x * (1 - sig_B_x) * grad_output\n",
        "\n",
        "          # grad_input = sig_B_x * grad_output + input_ref.tensor * B_ref.tensor * common\n",
        "          # grad_B = input_ref.tensor.square() * common\n",
        "\n",
        "          if input_requires_grad:\n",
        "              if input_ref.tensor.grad is None:\n",
        "                  input_ref._zero_grad()\n",
        "              input_ref.tensor.grad.add_(grad_input)\n",
        "          if B_requires_grad:\n",
        "              if B_ref.tensor.grad is None:\n",
        "                  B_ref._zero_grad()\n",
        "              B_ref.tensor.grad.add_(grad_B)\n",
        "\n",
        "        return _backward\n",
        "\n",
        "    def forward(self, input_tensor):\n",
        "        scale = self.B.tensor.item()\n",
        "        output_tensor = F.silu(scale * input_tensor.tensor) / scale\n",
        "        if not self.training:\n",
        "            return CustomTensor(output_tensor, due_to_operation=True)\n",
        "\n",
        "        result = CustomTensor(output_tensor, _custom_requires_grad=True, graph=self.graph,due_to_operation=True, is_leaf=False)\n",
        "        if input_tensor._custom_requires_grad:\n",
        "          self.graph.add_edge(input_tensor._node_id, result._node_id)\n",
        "        if self.B._custom_requires_grad:\n",
        "          self.graph.add_edge(self.B._node_id, result._node_id)\n",
        "        result._backward = self._create_backward(input_tensor, result, output_tensor)\n",
        "        return result\n",
        "\n",
        "    @torch.compile\n",
        "    def _calculate_gradients(self, input_tensor, result, output_tensor, B_tensor):\n",
        "        grad_output =result.grad\n",
        "        sig_B_x = output_tensor / input_tensor\n",
        "        common = sig_B_x * (1 - sig_B_x) * grad_output\n",
        "        grad_input = sig_B_x * grad_output + input_tensor * B_tensor * common\n",
        "        grad_B = input_tensor.square() * common\n",
        "        grad_B = grad_B.sum()\n",
        "        return grad_input, grad_B"
      ],
      "metadata": {
        "cellView": "form",
        "id": "rxgMxohIhbDP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Losses\n",
        "import weakref\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# TODO Lone MSE , MSE with softmax, MSE with sigmoid, cross entropy with softmax, binary cross entropy with sigmoid\n",
        "class MSE(Module):\n",
        "    __slots__ = ('graph','__weakref__')\n",
        "    def __init__(self, *, graph=None):\n",
        "        super().__init__()\n",
        "        self.graph = weakref.proxy(graph) if graph is not None else None\n",
        "\n",
        "    def forward(self, input_tensor, target_tensor, weight=None):\n",
        "        input_t = input_tensor.tensor\n",
        "        target_t = target_tensor.tensor\n",
        "\n",
        "        if weight is None:\n",
        "            loss = F.mse_loss(input_t, target_t, reduction='mean')\n",
        "        else:\n",
        "            weight_t = weight\n",
        "            squared_error = (input_t - target_t) ** 2\n",
        "\n",
        "            if weight_t.shape == input_t.shape:\n",
        "                # Per-pixel weight\n",
        "                weighted_error = weight_t * squared_error\n",
        "                loss = weighted_error.sum() / weight_t.sum()\n",
        "\n",
        "            elif weight_t.ndim == 1 and weight_t.shape[0] == input_t.shape[1]:\n",
        "                # Per-class weight\n",
        "                dims_to_add = [1] * (input_t.ndim - 2)\n",
        "                weight_t = weight_t.view(1, -1, *dims_to_add)\n",
        "                weighted_error = weight_t * squared_error\n",
        "                loss = weighted_error.sum() / weight_t.sum()\n",
        "\n",
        "            else:\n",
        "                raise ValueError(f\"Unsupported weight shape: {weight_t.shape}\")\n",
        "\n",
        "        if not self.training:\n",
        "            return CustomTensor(loss, due_to_operation=True)\n",
        "\n",
        "        result = CustomTensor(\n",
        "            loss,\n",
        "            _custom_requires_grad=True,\n",
        "            graph=self.graph,\n",
        "            due_to_operation=True,\n",
        "            is_leaf=False\n",
        "        )\n",
        "\n",
        "        if self.graph is not None:\n",
        "            self.graph.add_edge(input_tensor._node_id, result._node_id)\n",
        "            result._backward = self._create_backward(input_tensor, target_tensor, weight)\n",
        "\n",
        "        return result\n",
        "\n",
        "    def _create_backward(self, input_tensor, target_tensor, weight):\n",
        "        input_ref = weakref.proxy(input_tensor)\n",
        "        target_ref = weakref.proxy(target_tensor)\n",
        "        weight_ref = weight if weight is not None else None\n",
        "\n",
        "        def _backward():\n",
        "            if input_ref.tensor.grad is None:\n",
        "                input_ref._zero_grad()\n",
        "\n",
        "            grad_input = self._calculate_input_grad(\n",
        "                input_ref.tensor,\n",
        "                target_ref.tensor,\n",
        "                weight_ref\n",
        "            )\n",
        "            input_ref.tensor.grad.add_(grad_input)\n",
        "\n",
        "        return _backward\n",
        "\n",
        "    @torch.compile\n",
        "    def _calculate_input_grad(self, input_t, target_t, weight):\n",
        "        diff = input_t - target_t\n",
        "        if weight is None:\n",
        "            return (2 * diff) / input_t.numel()\n",
        "\n",
        "        if weight.shape == input_t.shape:\n",
        "            return (2 * weight * diff) / weight.sum()\n",
        "\n",
        "        elif weight.ndim == 1 and weight.shape[0] == input_t.shape[1]:\n",
        "            dims_to_add = [1] * (input_t.ndim - 2)\n",
        "            weight = weight.view(1, -1, *dims_to_add)\n",
        "            return (2 * weight * diff) / weight.sum()\n",
        "\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported weight shape in backward: {weight.shape}\")\n",
        "\n",
        "class CrossEntropyLoss(Module):\n",
        "    __slots__ = ('graph','__weakref__')\n",
        "    def __init__(self, *, graph=None):\n",
        "        super().__init__()\n",
        "        self.graph = weakref.proxy(graph) if graph is not None else None\n",
        "\n",
        "    def forward(self, input_tensor, target_tensor, weight= None):\n",
        "\n",
        "        output_tensor = F.cross_entropy(\n",
        "            input_tensor.tensor,\n",
        "            target_tensor.tensor,\n",
        "            reduction='mean',\n",
        "            weight=weight\n",
        "        )\n",
        "\n",
        "        if not self.training:\n",
        "            return CustomTensor(output_tensor, due_to_operation=True)\n",
        "\n",
        "        result = CustomTensor(\n",
        "            output_tensor,\n",
        "            _custom_requires_grad=True,\n",
        "            graph=self.graph,\n",
        "            due_to_operation=True,\n",
        "            is_leaf=False\n",
        "        )\n",
        "\n",
        "        self.graph.add_edge(input_tensor._node_id, result._node_id)\n",
        "        result._backward = self._create_backward(input_tensor, target_tensor, weight)\n",
        "        return result\n",
        "\n",
        "\n",
        "\n",
        "    def _create_backward(self, input_tensor, target_tensor,\n",
        "                        weight):\n",
        "        input_ref = weakref.proxy(input_tensor)\n",
        "        target_ref = weakref.proxy(target_tensor)\n",
        "        weight_ref = weight\n",
        "\n",
        "        def _backward():\n",
        "            if input_ref.tensor.grad is None:\n",
        "                input_ref._zero_grad()\n",
        "\n",
        "            grad_input = self._calculate_input_grad(\n",
        "                input_ref.tensor,\n",
        "                target_ref.tensor,\n",
        "                weight_ref\n",
        "            )\n",
        "            input_ref.tensor.grad.add_(grad_input)\n",
        "\n",
        "        return _backward\n",
        "\n",
        "    @torch.compile\n",
        "    def _calculate_input_grad(self, input_tensor, target_tensor,\n",
        "                             weight):\n",
        "        batch_size = input_tensor.size(0)\n",
        "        num_classes = input_tensor.size(1)\n",
        "\n",
        "        target_one_hot = F.one_hot(target_tensor, num_classes=num_classes).to(input_tensor.dtype)\n",
        "\n",
        "        softmax_probs = F.softmax(input_tensor, dim=1)\n",
        "\n",
        "        grad = softmax_probs - target_one_hot\n",
        "\n",
        "        if weight is not None:\n",
        "            sample_weights = weight[target_tensor].view(-1, 1)\n",
        "            grad = grad * sample_weights\n",
        "            normalizer = sample_weights.sum()\n",
        "        else:\n",
        "            normalizer = batch_size\n",
        "        grad = grad / normalizer\n",
        "        return grad\n",
        "\n",
        "class BCEWithLogitsLoss(Module):\n",
        "    __slots__ = ('graph','__weakref__')\n",
        "    def __init__(self, *, graph=None):\n",
        "\n",
        "        super().__init__()\n",
        "        self.graph = weakref.proxy(graph) if graph is not None else None\n",
        "\n",
        "    def forward(self, input_tensor, target_tensor, weight= None):\n",
        "        output_tensor = F.binary_cross_entropy_with_logits(\n",
        "            input_tensor.tensor,\n",
        "            target_tensor.tensor,\n",
        "            reduction='mean',\n",
        "            pos_weight=weight\n",
        "        )\n",
        "\n",
        "        if not self.training:\n",
        "            return CustomTensor(output_tensor, due_to_operation=True)\n",
        "\n",
        "\n",
        "        result = CustomTensor(\n",
        "            output_tensor,\n",
        "            _custom_requires_grad=True,\n",
        "            graph=self.graph,\n",
        "            due_to_operation=True,\n",
        "            is_leaf=False\n",
        "        )\n",
        "\n",
        "        if self.graph is not None:\n",
        "            self.graph.add_edge(input_tensor._node_id, result._node_id)\n",
        "            result._backward = self._create_backward(input_tensor, target_tensor, weight)\n",
        "\n",
        "        return result\n",
        "\n",
        "    def _create_backward(self, input_tensor, target_tensor, weight):\n",
        "\n",
        "        input_ref = weakref.proxy(input_tensor)\n",
        "        target_ref = weakref.proxy(target_tensor)\n",
        "        weight_ref = weight\n",
        "\n",
        "        def _backward():\n",
        "            if input_ref.tensor.grad is None:\n",
        "                input_ref._zero_grad()\n",
        "\n",
        "            grad_input = self._calculate_input_grad(\n",
        "                input_ref.tensor,\n",
        "                target_ref.tensor,\n",
        "                weight_ref\n",
        "            )\n",
        "\n",
        "\n",
        "            input_ref.tensor.grad.add_(grad_input)\n",
        "\n",
        "        return _backward\n",
        "\n",
        "    @torch.compile\n",
        "    def _calculate_input_grad(self, input_tensor, target_tensor, weight):\n",
        "        sigmoid_input = torch.sigmoid(input_tensor)\n",
        "\n",
        "        grad = (sigmoid_input - target_tensor) / input_tensor.numel()\n",
        "\n",
        "        if weight is not None:\n",
        "            # pos_weight affects the positive class term (where target == 1)\n",
        "            # The gradient becomes: (sigmoid - target) * weight / num_elements for positive targets\n",
        "            # For negative targets, it remains: sigmoid / num_elements\n",
        "            # This matches PyTorch's implementation of pos_weight in BCEWithLogitsLoss\n",
        "            weight_factor = torch.where(target_tensor == 1, weight, 1.0)\n",
        "            grad = grad * weight_factor\n",
        "\n",
        "        return grad"
      ],
      "metadata": {
        "cellView": "form",
        "id": "TE258c2-h2wJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Optimizers\n",
        "import torch\n",
        "\n",
        "class Optimizer:\n",
        "    __slots__ = ('param_groups', 'state')\n",
        "    def __init__(self, params, defaults):\n",
        "        self.param_groups = []\n",
        "        self.state = {}\n",
        "        param_list = list(params)\n",
        "\n",
        "        if not param_list:\n",
        "            raise ValueError(\"Optimizer got an empty parameter list.\")\n",
        "\n",
        "        param_group = {'params': param_list, **defaults}\n",
        "        self.param_groups.append(param_group)\n",
        "\n",
        "    def step(self):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def clear(self):\n",
        "        self.param_group = []\n",
        "        self.state.clear()\n",
        "\n",
        "    def zero_grad(self):\n",
        "        for group in self.param_groups:\n",
        "            for p in group['params']:\n",
        "                if p.tensor.grad is not None:\n",
        "                    p.tensor.grad.zero_()\n",
        "\n",
        "\n",
        "class SGD(Optimizer):\n",
        "    __slots__ = ()\n",
        "    def __new__(cls, params, lr, weight_decay=None):\n",
        "        assert lr > 0\n",
        "        assert weight_decay is None or weight_decay > 0\n",
        "        return super().__new__(cls)\n",
        "\n",
        "    def __init__(self, params, lr, weight_decay=None):\n",
        "        defaults = {'lr': lr, \"weight_decay\": weight_decay}\n",
        "        super().__init__(params, defaults)\n",
        "\n",
        "    def step(self):\n",
        "        for group in self.param_groups:\n",
        "            lr = group['lr']\n",
        "            weight_decay = group['weight_decay']\n",
        "\n",
        "            for p in group['params']:\n",
        "                t = p.tensor\n",
        "                grad = t.grad\n",
        "                if grad is None:\n",
        "                    continue\n",
        "\n",
        "                if weight_decay:\n",
        "                    grad = grad + t * weight_decay\n",
        "\n",
        "                t.add_(grad, alpha=-lr)\n",
        "\n",
        "\n",
        "class Momentum(Optimizer):\n",
        "    __slots__ = ()\n",
        "    def __new__(cls, params, lr, momentum=0.0, weight_decay=None):\n",
        "        assert lr > 0\n",
        "        assert momentum > 0\n",
        "        assert weight_decay is None or weight_decay > 0\n",
        "        return super().__new__(cls)\n",
        "\n",
        "    def __init__(self, params, lr, momentum=0.0, weight_decay=0.0):\n",
        "        defaults = {'lr': lr, 'momentum': momentum, 'weight_decay': weight_decay}\n",
        "        super().__init__(params, defaults)\n",
        "\n",
        "    def step(self):\n",
        "        state = self.state\n",
        "        for group in self.param_groups:\n",
        "            lr = group['lr']\n",
        "            momentum = group['momentum']\n",
        "            weight_decay = group['weight_decay']\n",
        "            for p in group['params']:\n",
        "                t = p.tensor\n",
        "                grad = t.grad\n",
        "                if grad is None:\n",
        "                    continue\n",
        "                if weight_decay:\n",
        "                    grad = grad + t * weight_decay\n",
        "\n",
        "                if p not in state:\n",
        "                    buf = torch.clone(grad)\n",
        "                    state[p] = {'momentum_buffer': buf}\n",
        "                else:\n",
        "                    buf = state[p]['momentum_buffer']\n",
        "                    buf.mul_(momentum).add_(grad)\n",
        "                grad = buf\n",
        "                t.add_(grad, alpha=-lr)\n",
        "\n",
        "\n",
        "class Nesterov(Optimizer):\n",
        "    __slots__ = ()\n",
        "    # This is a reformulated Nesterov not the original Nesterov\n",
        "    def __new__(cls, params, lr, momentum=0.0, weight_decay=None):\n",
        "        assert lr > 0\n",
        "        assert momentum > 0\n",
        "        assert weight_decay is None or weight_decay > 0\n",
        "        return super().__new__(cls)\n",
        "\n",
        "    def __init__(self, params, lr, momentum=0.0, weight_decay=None):\n",
        "        defaults = {'lr': lr, 'momentum': momentum, 'weight_decay': weight_decay}\n",
        "        super().__init__(params, defaults)\n",
        "\n",
        "    def step(self):\n",
        "        state = self.state\n",
        "        for group in self.param_groups:\n",
        "            lr = group['lr']\n",
        "            momentum = group['momentum']\n",
        "            weight_decay = group['weight_decay']\n",
        "\n",
        "            for p in group['params']:\n",
        "                t = p.tensor\n",
        "                grad = t.grad\n",
        "                if grad is None:\n",
        "                    continue\n",
        "\n",
        "                if weight_decay:\n",
        "                    grad = grad + t * weight_decay\n",
        "\n",
        "\n",
        "                if p not in state:\n",
        "                    buf = grad.clone()#.detach()\n",
        "                    state[p] = {'momentum_buffer': buf}\n",
        "                else:\n",
        "                    buf = state[p]['momentum_buffer']\n",
        "                    buf.mul_(momentum).add_(grad)\n",
        "\n",
        "                update_value = grad.add(buf, alpha=momentum)\n",
        "                t.add_(update_value, alpha=-lr)\n",
        "\n",
        "\n",
        "class AdamW(Optimizer):\n",
        "    __slots__ = ()\n",
        "    def __new__(cls, params, lr, betas=(0.9, 0.999), eps=1e-8, weight_decay=None):\n",
        "        assert lr >= 0.0\n",
        "        assert 0.0 <= betas[0] < 1.0\n",
        "        assert 0.0 <= betas[1] < 1.0\n",
        "        assert eps >= 0.0\n",
        "        assert weight_decay is None or weight_decay > 0.0\n",
        "        return super().__new__(cls)\n",
        "\n",
        "    def __init__(self, params, lr, betas=(0.9, 0.999), eps=1e-8, weight_decay=None):\n",
        "        defaults = {'lr': lr, 'betas': betas, 'eps': eps, 'weight_decay': weight_decay}\n",
        "        super().__init__(params, defaults)\n",
        "\n",
        "    def step(self):\n",
        "        for group in self.param_groups:\n",
        "            lr, (beta1, beta2), eps, weight_decay = (\n",
        "                group['lr'], group['betas'], group['eps'], group['weight_decay']\n",
        "            )\n",
        "\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "\n",
        "                grad = p.grad\n",
        "\n",
        "                if p not in self.state:\n",
        "                    self.state[p] = {\n",
        "                        'time_step': 0,\n",
        "                        'm': torch.zeros_like(p.tensor),\n",
        "                        'v': torch.zeros_like(p.tensor)\n",
        "                    }\n",
        "\n",
        "                state = self.state[p]\n",
        "                m, v = state['m'], state['v']\n",
        "\n",
        "                state['time_step'] += 1\n",
        "                t_step = state['time_step']\n",
        "\n",
        "                if weight_decay:\n",
        "                    p.tensor.mul_(1 - lr * weight_decay)\n",
        "\n",
        "                m.mul_(beta1).add_(grad, alpha=1 - beta1)\n",
        "                v.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)\n",
        "\n",
        "                m_corrected = m / (1 - beta1 ** t_step)\n",
        "                v_corrected = v / (1 - beta2 ** t_step)\n",
        "\n",
        "                update = m_corrected / (v_corrected.sqrt() + eps)\n",
        "                p.tensor.add_(update, alpha=-lr)\n",
        "\n",
        "\n",
        "class Lion(Optimizer):\n",
        "    \"\"\"Implements the Lion optimizer.\n",
        "\n",
        "    Based on the paper \"Symbolic Discovery of Optimization Algorithms\"\n",
        "    and reference implementation: https://github.com/lucidrains/lion-pytorch\n",
        "    \"\"\"\n",
        "    __slots__ = ()\n",
        "\n",
        "    def __new__(cls, params, lr=1e-4, betas=(0.9, 0.99), weight_decay=None):\n",
        "        assert lr > 0.\n",
        "        assert all([0. <= beta <= 1. for beta in betas])\n",
        "        assert weight_decay is None or weight_decay >= 0.\n",
        "        return super().__new__(cls)\n",
        "\n",
        "    def __init__(self, params, lr=1e-4, betas=(0.9, 0.99), weight_decay=None):\n",
        "        defaults = dict(\n",
        "            lr=lr,\n",
        "            betas=betas,\n",
        "            weight_decay=weight_decay\n",
        "        )\n",
        "        super().__init__(params, defaults)\n",
        "\n",
        "    def step(self):\n",
        "        for group in self.param_groups:\n",
        "            lr, wd, (beta1, beta2) = group['lr'], group['weight_decay'], group['betas']\n",
        "            state = self.state\n",
        "\n",
        "            for p_obj in group['params']:\n",
        "                p = p_obj.tensor\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "\n",
        "                grad = p.grad\n",
        "\n",
        "                if p_obj not in state:\n",
        "                    state[p_obj] = {\"exp_avg\": torch.zeros_like(p)}\n",
        "\n",
        "                exp_avg = state[p_obj]['exp_avg']\n",
        "\n",
        "                # decoupled weight decay\n",
        "                if wd:\n",
        "                    p.mul_(1. - lr * wd)\n",
        "\n",
        "                update = exp_avg.clone().mul_(beta1).add(grad, alpha=1. - beta1).sign_()\n",
        "                p.add_(update, alpha=-lr)\n",
        "                exp_avg.mul_(beta2).add_(grad, alpha=1. - beta2)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "krIF4dblh2zf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title the full Test\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "\n",
        "\n",
        "# Creating the model with pytorch\n",
        "print(\"Heads up the comparison between torch result and neuronix is with an rtol of 1e-4\")\n",
        "class ConvBNActMaxPool(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size_conv, stride_conv, padding_conv,\n",
        "                 kernel_size_pool, stride_pool, padding_pool):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(\n",
        "            in_channels=in_channels,\n",
        "            out_channels=out_channels,\n",
        "            kernel_size=kernel_size_conv,\n",
        "            stride=stride_conv,\n",
        "            padding=padding_conv,\n",
        "        )\n",
        "        self.batchnorm = nn.BatchNorm2d(num_features=out_channels, eps=1e-5, momentum=0.1)\n",
        "        self.activation = nn.GELU(approximate='tanh')  # PyTorch ≥ 1.13\n",
        "        self.maxpool = nn.MaxPool2d(\n",
        "            kernel_size=kernel_size_pool,\n",
        "            stride=stride_pool,\n",
        "            padding=padding_pool,\n",
        "            dilation=1\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.batchnorm(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.maxpool(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class CNN_Model_py(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Input shape: (3, 224, 224)\n",
        "        self.layer1 = ConvBNActMaxPool(3, 32, 3, 1, 0, 2, 2, 0)     # -> (32, 111, 111)\n",
        "        self.layer2 = ConvBNActMaxPool(32, 64, 3, 1, 0, 2, 2, 0)    # -> (64, 54, 54)\n",
        "        self.layer3 = ConvBNActMaxPool(64, 128, 3, 1, 0, 2, 2, 0)   # -> (128, 26, 26)\n",
        "        self.layer4 = ConvBNActMaxPool(128, 256, 5, 1, 0, 2, 2, 0)  # -> (256, 11, 11)\n",
        "        self.layer5 = ConvBNActMaxPool(256, 512, 7, 1, 0, 2, 2, 0)  # -> (512, 2, 2)\n",
        "\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear1 = nn.Linear(512 * 2 * 2, 512)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.output = nn.Linear(512, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        x = self.layer5(x)\n",
        "        x = self.flatten(x)       # -> (B, 2048)\n",
        "        x = self.linear1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.output(x)\n",
        "        return x\n",
        "\n",
        "# Creating the same model with neuronix\n",
        "class conv_batchnorm_activation_maxpool(Module):\n",
        "  def __init__(self, in_channels, out_channels, kernel_size_conv, stride_conv, padding_conv,kernel_size_pool,stride_pool,padding_pool, graph):\n",
        "    super().__init__()\n",
        "    self.conv = Conv2d(\n",
        "        in_channels = in_channels,\n",
        "        out_channels = out_channels,\n",
        "        kernel_size = kernel_size_conv,\n",
        "        stride = stride_conv,\n",
        "        padding = padding_conv,\n",
        "        graph = graph,\n",
        "        activation=\"gelu_approx\"\n",
        "    )\n",
        "    self.batchnorm = BatchNorm_Nd(\n",
        "        num_features=out_channels,\n",
        "        eps=1e-5,\n",
        "        momentum=0.1,\n",
        "        graph=graph)\n",
        "    self.activation = GeLu(\n",
        "        approximate='tanh',\n",
        "        graph=graph)\n",
        "    self.maxpool = MaxPool2d(\n",
        "        kernel_size=kernel_size_pool,#(2,2),\n",
        "        stride=stride_pool,#(2,2)\n",
        "        padding=padding_pool,#(0,0)\n",
        "        dilation=1,\n",
        "        graph=graph)\n",
        "  def forward(self, x):\n",
        "    x = self.conv(x)\n",
        "    x = self.batchnorm(x)\n",
        "    x = self.activation(x)\n",
        "    x = self.maxpool(x)\n",
        "    return x\n",
        "class CNN_Model(Module):\n",
        "  # configaration kernel size  [(3, 3), (3, 3), (3, 3), (5, 5), (7, 7)]\n",
        "  # output_channels [32,64,128,256,512]\n",
        "  # dense layer 512\n",
        "\n",
        "  # conv 224-3 +1 = 222\n",
        "  # max 222 -2/2 +1 = 111\n",
        "\n",
        "  # conv 111-3 +1 = 109\n",
        "  # max 109 -2/2 +1 = 54\n",
        "\n",
        "  # conv 54 -3 +1 = 52\n",
        "  # max 52 -2/2 +1 = 26\n",
        "\n",
        "  # conv 26 -5 +1 = 22\n",
        "  # max 22-2/2 +1 =11\n",
        "\n",
        "  # conv 11 -7 +1 = 5\n",
        "  # max = 5 -2/2 +1 = 2\n",
        "\n",
        "  # hence for linear layer the reshaped tensor is 512*2*2 = 2048\n",
        "\n",
        "  def __init__(self,graph):\n",
        "    super().__init__()\n",
        "    #in_channels, out_channels, kernel_size_conv, stride_conv, padding_conv,kernel_size_pool,stride_pool,padding_pool, graph\n",
        "    self.layer1 = conv_batchnorm_activation_maxpool(3,32, 3,1,0, 2,2,0, graph)\n",
        "    self.layer2 = conv_batchnorm_activation_maxpool(32,64, 3,1,0, 2,2,0, graph)\n",
        "    self.layer3 = conv_batchnorm_activation_maxpool(64,128, 3,1,0, 2,2,0, graph)\n",
        "    self.layer4 = conv_batchnorm_activation_maxpool(128,256, 5,1,0, 2,2,0, graph)\n",
        "    self.layer5 = conv_batchnorm_activation_maxpool(256,512, 7,1,0, 2,2,0, graph)\n",
        "    self.linear1 = Linear(\n",
        "        in_features=512*2*2,\n",
        "        out_features=512,\n",
        "        graph=graph,\n",
        "        activation=\"relu\"\n",
        "    )\n",
        "    self.ac1 = ReLu(graph=graph)\n",
        "    self.output = Linear(\n",
        "        in_features=512,\n",
        "        out_features=10,\n",
        "        graph=graph,\n",
        "        activation=\"relu\"\n",
        "    )\n",
        "  def forward(self, x):\n",
        "    x = self.layer1(x)\n",
        "    x = self.layer2(x)\n",
        "    x = self.layer3(x)\n",
        "    x = self.layer4(x)\n",
        "    x = self.layer5(x)\n",
        "    x = x.reshape((x.shape[0],-1))\n",
        "    x = self.linear1(x)\n",
        "    x = self.ac1(x)\n",
        "    x = self.output(x)\n",
        "    return x\n",
        "\n",
        "# Verifying Forward Pass with Torch\n",
        "cnn_model_pytorch = CNN_Model_py()\n",
        "cnn_model = CNN_Model(graph=None)\n",
        "with AutogradGraph() as graph, torch.inference_mode():\n",
        "  # copying all the weights of pytorch to our model\n",
        "  cnn_model_pytorch.layer1.conv.weight.data.copy_(\n",
        "    cnn_model._modules['layer1'].conv.weight.tensor\n",
        "  )\n",
        "  cnn_model_pytorch.layer1.conv.bias.data.copy_(\n",
        "      cnn_model._modules['layer1'].conv.bias.tensor\n",
        "  )\n",
        "  cnn_model_pytorch.layer1.batchnorm.weight.data.copy_(\n",
        "      cnn_model._modules['layer1'].batchnorm.weight.tensor\n",
        "  )\n",
        "  cnn_model_pytorch.layer1.batchnorm.bias.data.copy_(\n",
        "      cnn_model._modules['layer1'].batchnorm.bias.tensor\n",
        "  )\n",
        "  cnn_model_pytorch.layer1.batchnorm.running_mean.data.copy_(\n",
        "      cnn_model._modules['layer1'].batchnorm.running_mean\n",
        "  )\n",
        "  cnn_model_pytorch.layer1.batchnorm.running_var.data.copy_(\n",
        "      cnn_model._modules['layer1'].batchnorm.running_var\n",
        "  )\n",
        "\n",
        "  #### Layer 2 ####\n",
        "  cnn_model_pytorch.layer2.conv.weight.data.copy_(\n",
        "      cnn_model._modules['layer2'].conv.weight.tensor\n",
        "  )\n",
        "  cnn_model_pytorch.layer2.conv.bias.data.copy_(\n",
        "      cnn_model._modules['layer2'].conv.bias.tensor\n",
        "  )\n",
        "  cnn_model_pytorch.layer2.batchnorm.weight.data.copy_(\n",
        "      cnn_model._modules['layer2'].batchnorm.weight.tensor\n",
        "  )\n",
        "  cnn_model_pytorch.layer2.batchnorm.bias.data.copy_(\n",
        "      cnn_model._modules['layer2'].batchnorm.bias.tensor\n",
        "  )\n",
        "  cnn_model_pytorch.layer2.batchnorm.running_mean.data.copy_(\n",
        "      cnn_model._modules['layer2'].batchnorm.running_mean\n",
        "  )\n",
        "  cnn_model_pytorch.layer2.batchnorm.running_var.data.copy_(\n",
        "      cnn_model._modules['layer2'].batchnorm.running_var\n",
        "  )\n",
        "\n",
        "  #### Layer 3 ####\n",
        "  cnn_model_pytorch.layer3.conv.weight.data.copy_(\n",
        "      cnn_model._modules['layer3'].conv.weight.tensor\n",
        "  )\n",
        "  cnn_model_pytorch.layer3.conv.bias.data.copy_(\n",
        "      cnn_model._modules['layer3'].conv.bias.tensor\n",
        "  )\n",
        "  cnn_model_pytorch.layer3.batchnorm.weight.data.copy_(\n",
        "      cnn_model._modules['layer3'].batchnorm.weight.tensor\n",
        "  )\n",
        "  cnn_model_pytorch.layer3.batchnorm.bias.data.copy_(\n",
        "      cnn_model._modules['layer3'].batchnorm.bias.tensor\n",
        "  )\n",
        "  cnn_model_pytorch.layer3.batchnorm.running_mean.data.copy_(\n",
        "      cnn_model._modules['layer3'].batchnorm.running_mean\n",
        "  )\n",
        "  cnn_model_pytorch.layer3.batchnorm.running_var.data.copy_(\n",
        "      cnn_model._modules['layer3'].batchnorm.running_var\n",
        "  )\n",
        "\n",
        "  #### Layer 4 ####\n",
        "  cnn_model_pytorch.layer4.conv.weight.data.copy_(\n",
        "      cnn_model._modules['layer4'].conv.weight.tensor\n",
        "  )\n",
        "  cnn_model_pytorch.layer4.conv.bias.data.copy_(\n",
        "      cnn_model._modules['layer4'].conv.bias.tensor\n",
        "  )\n",
        "  cnn_model_pytorch.layer4.batchnorm.weight.data.copy_(\n",
        "      cnn_model._modules['layer4'].batchnorm.weight.tensor\n",
        "  )\n",
        "  cnn_model_pytorch.layer4.batchnorm.bias.data.copy_(\n",
        "      cnn_model._modules['layer4'].batchnorm.bias.tensor\n",
        "  )\n",
        "  cnn_model_pytorch.layer4.batchnorm.running_mean.data.copy_(\n",
        "      cnn_model._modules['layer4'].batchnorm.running_mean\n",
        "  )\n",
        "  cnn_model_pytorch.layer4.batchnorm.running_var.data.copy_(\n",
        "      cnn_model._modules['layer4'].batchnorm.running_var\n",
        "  )\n",
        "\n",
        "  #### Layer 5 ####\n",
        "  cnn_model_pytorch.layer5.conv.weight.data.copy_(\n",
        "      cnn_model._modules['layer5'].conv.weight.tensor\n",
        "  )\n",
        "  cnn_model_pytorch.layer5.conv.bias.data.copy_(\n",
        "      cnn_model._modules['layer5'].conv.bias.tensor\n",
        "  )\n",
        "  cnn_model_pytorch.layer5.batchnorm.weight.data.copy_(\n",
        "      cnn_model._modules['layer5'].batchnorm.weight.tensor\n",
        "  )\n",
        "  cnn_model_pytorch.layer5.batchnorm.bias.data.copy_(\n",
        "      cnn_model._modules['layer5'].batchnorm.bias.tensor\n",
        "  )\n",
        "  cnn_model_pytorch.layer5.batchnorm.running_mean.data.copy_(\n",
        "      cnn_model._modules['layer5'].batchnorm.running_mean\n",
        "  )\n",
        "  cnn_model_pytorch.layer5.batchnorm.running_var.data.copy_(\n",
        "      cnn_model._modules['layer5'].batchnorm.running_var\n",
        "  )\n",
        "  #### Linear 1 ####\n",
        "  cnn_model_pytorch.linear1.weight.data.copy_(\n",
        "      cnn_model._modules['linear1'].weight.tensor\n",
        "  )\n",
        "  cnn_model_pytorch.linear1.bias.data.copy_(\n",
        "      cnn_model._modules['linear1'].bias.tensor\n",
        "  )\n",
        "  #### Output ####\n",
        "  cnn_model_pytorch.output.weight.data.copy_(\n",
        "      cnn_model._modules['output'].weight.tensor\n",
        "  )\n",
        "  cnn_model_pytorch.output.bias.data.copy_(\n",
        "      cnn_model._modules['output'].bias.tensor\n",
        "  )\n",
        "\n",
        "\n",
        "\n",
        "  cnn_model.eval()\n",
        "  cnn_model_pytorch.eval()\n",
        "  sample_input_torch = torch.randn(3,3,224,224)\n",
        "  sample_input = CustomTensor(sample_input_torch.clone(), _custom_requires_grad=False, graph=None)\n",
        "  output = cnn_model(sample_input)\n",
        "  output_torch = cnn_model_pytorch(sample_input_torch)\n",
        "  print(torch.allclose(output.tensor,output_torch,rtol=1e-4))\n",
        "\n",
        "del cnn_model,cnn_model_pytorch,sample_input,sample_input_torch\n",
        "\n",
        "# Verifying Backward Pass with Torch\n",
        "cnn_model_pytorch = CNN_Model_py().to(device = device ,dtype = dtype)\n",
        "cnn_model = CNN_Model(graph=None).to(device = device ,dtype = dtype)\n",
        "# with AutogradGraph() as graph:\n",
        "#    dummy_tensor = CustomTensor(torch.rand(3,3,224,224),_custom_requires_grad = False,graph = None)\n",
        "#    cnn_model.attach_graph(graph=graph)\n",
        "#    o=cnn_model(dummy_tensor)\n",
        "#    l=o.sum()\n",
        "#    l.backward()\n",
        "# del l,o,dummy_tensor\n",
        "# cnn_model.detach_graph()\n",
        "# cnn_model.zero_grad()\n",
        "with AutogradGraph() as graph:\n",
        "\n",
        "\n",
        "  # copying all the weights of pytorch to our model\n",
        "  # assume cnn_model_pytorch (a trained torch CNN_Model_py) and\n",
        "  # cnn_model (your custom CNN_Model under AutogradGraph) are already instantiated\n",
        "  cnn_model.attach_graph(graph=graph)\n",
        "  cnn_model_pytorch.layer1.conv.weight.data.copy_(\n",
        "    cnn_model._modules['layer1'].conv.weight.tensor\n",
        "  )\n",
        "  cnn_model_pytorch.layer1.conv.bias.data.copy_(\n",
        "      cnn_model._modules['layer1'].conv.bias.tensor\n",
        "  )\n",
        "  cnn_model_pytorch.layer1.batchnorm.weight.data.copy_(\n",
        "      cnn_model._modules['layer1'].batchnorm.weight.tensor\n",
        "  )\n",
        "  cnn_model_pytorch.layer1.batchnorm.bias.data.copy_(\n",
        "      cnn_model._modules['layer1'].batchnorm.bias.tensor\n",
        "  )\n",
        "  cnn_model_pytorch.layer1.batchnorm.running_mean.data.copy_(\n",
        "      cnn_model._modules['layer1'].batchnorm.running_mean\n",
        "  )\n",
        "  cnn_model_pytorch.layer1.batchnorm.running_var.data.copy_(\n",
        "      cnn_model._modules['layer1'].batchnorm.running_var\n",
        "  )\n",
        "\n",
        "  #### Layer 2 ####\n",
        "  cnn_model_pytorch.layer2.conv.weight.data.copy_(\n",
        "      cnn_model._modules['layer2'].conv.weight.tensor\n",
        "  )\n",
        "  cnn_model_pytorch.layer2.conv.bias.data.copy_(\n",
        "      cnn_model._modules['layer2'].conv.bias.tensor\n",
        "  )\n",
        "  cnn_model_pytorch.layer2.batchnorm.weight.data.copy_(\n",
        "      cnn_model._modules['layer2'].batchnorm.weight.tensor\n",
        "  )\n",
        "  cnn_model_pytorch.layer2.batchnorm.bias.data.copy_(\n",
        "      cnn_model._modules['layer2'].batchnorm.bias.tensor\n",
        "  )\n",
        "  cnn_model_pytorch.layer2.batchnorm.running_mean.data.copy_(\n",
        "      cnn_model._modules['layer2'].batchnorm.running_mean\n",
        "  )\n",
        "  cnn_model_pytorch.layer2.batchnorm.running_var.data.copy_(\n",
        "      cnn_model._modules['layer2'].batchnorm.running_var\n",
        "  )\n",
        "\n",
        "  #### Layer 3 ####\n",
        "  cnn_model_pytorch.layer3.conv.weight.data.copy_(\n",
        "      cnn_model._modules['layer3'].conv.weight.tensor\n",
        "  )\n",
        "  cnn_model_pytorch.layer3.conv.bias.data.copy_(\n",
        "      cnn_model._modules['layer3'].conv.bias.tensor\n",
        "  )\n",
        "  cnn_model_pytorch.layer3.batchnorm.weight.data.copy_(\n",
        "      cnn_model._modules['layer3'].batchnorm.weight.tensor\n",
        "  )\n",
        "  cnn_model_pytorch.layer3.batchnorm.bias.data.copy_(\n",
        "      cnn_model._modules['layer3'].batchnorm.bias.tensor\n",
        "  )\n",
        "  cnn_model_pytorch.layer3.batchnorm.running_mean.data.copy_(\n",
        "      cnn_model._modules['layer3'].batchnorm.running_mean\n",
        "  )\n",
        "  cnn_model_pytorch.layer3.batchnorm.running_var.data.copy_(\n",
        "      cnn_model._modules['layer3'].batchnorm.running_var\n",
        "  )\n",
        "\n",
        "  #### Layer 4 ####\n",
        "  cnn_model_pytorch.layer4.conv.weight.data.copy_(\n",
        "      cnn_model._modules['layer4'].conv.weight.tensor\n",
        "  )\n",
        "  cnn_model_pytorch.layer4.conv.bias.data.copy_(\n",
        "      cnn_model._modules['layer4'].conv.bias.tensor\n",
        "  )\n",
        "  cnn_model_pytorch.layer4.batchnorm.weight.data.copy_(\n",
        "      cnn_model._modules['layer4'].batchnorm.weight.tensor\n",
        "  )\n",
        "  cnn_model_pytorch.layer4.batchnorm.bias.data.copy_(\n",
        "      cnn_model._modules['layer4'].batchnorm.bias.tensor\n",
        "  )\n",
        "  cnn_model_pytorch.layer4.batchnorm.running_mean.data.copy_(\n",
        "      cnn_model._modules['layer4'].batchnorm.running_mean\n",
        "  )\n",
        "  cnn_model_pytorch.layer4.batchnorm.running_var.data.copy_(\n",
        "      cnn_model._modules['layer4'].batchnorm.running_var\n",
        "  )\n",
        "\n",
        "  #### Layer 5 ####\n",
        "  cnn_model_pytorch.layer5.conv.weight.data.copy_(\n",
        "      cnn_model._modules['layer5'].conv.weight.tensor\n",
        "  )\n",
        "  cnn_model_pytorch.layer5.conv.bias.data.copy_(\n",
        "      cnn_model._modules['layer5'].conv.bias.tensor\n",
        "  )\n",
        "  cnn_model_pytorch.layer5.batchnorm.weight.data.copy_(\n",
        "      cnn_model._modules['layer5'].batchnorm.weight.tensor\n",
        "  )\n",
        "  cnn_model_pytorch.layer5.batchnorm.bias.data.copy_(\n",
        "      cnn_model._modules['layer5'].batchnorm.bias.tensor\n",
        "  )\n",
        "  cnn_model_pytorch.layer5.batchnorm.running_mean.data.copy_(\n",
        "      cnn_model._modules['layer5'].batchnorm.running_mean\n",
        "  )\n",
        "  cnn_model_pytorch.layer5.batchnorm.running_var.data.copy_(\n",
        "      cnn_model._modules['layer5'].batchnorm.running_var\n",
        "  )\n",
        "  #### Linear 1 ####\n",
        "  cnn_model_pytorch.linear1.weight.data.copy_(\n",
        "      cnn_model._modules['linear1'].weight.tensor\n",
        "  )\n",
        "  cnn_model_pytorch.linear1.bias.data.copy_(\n",
        "      cnn_model._modules['linear1'].bias.tensor\n",
        "  )\n",
        "  #### Output ####\n",
        "  cnn_model_pytorch.output.weight.data.copy_(\n",
        "      cnn_model._modules['output'].weight.tensor\n",
        "  )\n",
        "  cnn_model_pytorch.output.bias.data.copy_(\n",
        "      cnn_model._modules['output'].bias.tensor\n",
        "  )\n",
        "\n",
        "\n",
        "  cnn_model.verify_all_graph_references_are_weak()\n",
        "  cnn_model.train()\n",
        "  cnn_model_pytorch.train()\n",
        "\n",
        "  sample_input_torch = torch.randn(3,3,224,224)\n",
        "  sample_input = CustomTensor(sample_input_torch.clone(),_custom_requires_grad=True,graph=graph)\n",
        "  output = cnn_model(sample_input)\n",
        "  output_torch = cnn_model_pytorch(sample_input_torch)\n",
        "  loss = output.sum()\n",
        "  loss_torch = output_torch.sum()\n",
        "  st = time.time()\n",
        "  loss.backward()\n",
        "  et = time.time()\n",
        "  print(f\"Neuronix implementation backward Takes {et-st} seconds\")\n",
        "  st = time.time()\n",
        "  loss_torch.backward()\n",
        "  et = time.time()\n",
        "  print(f\"Pytorch implementation backward Takes {et-st} seconds\")\n",
        "\n",
        "cnn_model.verify_all_parameters_are_on_the_same_device(device)\n",
        "\n",
        "def compare_grads(param1, param2, name):\n",
        "    grad1 = param1.grad\n",
        "    grad2 = param2.grad\n",
        "    if grad1 is None or grad2 is None:\n",
        "        print(f\"{name}: One of the gradients is None.\")\n",
        "    else:\n",
        "        equal = torch.allclose(grad1, grad2, atol=1e-4)\n",
        "        print(f\"{name}: {'✅ Same' if equal else '❌ Different'}\")\n",
        "\n",
        "# Layer-wise comparisons\n",
        "for i in range(1, 6):\n",
        "    layer = f\"layer{i}\"\n",
        "    torch_layer = getattr(cnn_model_pytorch, layer)\n",
        "    custom_layer = cnn_model._modules[layer]\n",
        "\n",
        "    compare_grads(torch_layer.conv.weight, custom_layer.conv.weight.tensor, f\"{layer}.conv.weight\")\n",
        "    compare_grads(torch_layer.conv.bias, custom_layer.conv.bias.tensor, f\"{layer}.conv.bias\")\n",
        "    compare_grads(torch_layer.batchnorm.weight, custom_layer.batchnorm.weight.tensor, f\"{layer}.batchnorm.weight\")\n",
        "    compare_grads(torch_layer.batchnorm.bias, custom_layer.batchnorm.bias.tensor, f\"{layer}.batchnorm.bias\")\n",
        "\n",
        "# Linear 1\n",
        "compare_grads(cnn_model_pytorch.linear1.weight, cnn_model._modules['linear1'].weight.tensor, \"linear1.weight\")\n",
        "compare_grads(cnn_model_pytorch.linear1.bias, cnn_model._modules['linear1'].bias.tensor, \"linear1.bias\")\n",
        "\n",
        "# Output\n",
        "compare_grads(cnn_model_pytorch.output.weight, cnn_model._modules['output'].weight.tensor, \"output.weight\")\n",
        "compare_grads(cnn_model_pytorch.output.bias, cnn_model._modules['output'].bias.tensor, \"output.bias\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "bzHVpiYrh22r",
        "outputId": "ff56f24f-355e-4a92-e047-a3d25e5135e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Heads up the comparison between torch result and neuronix is with an rtol of 1e-4\n",
            "True\n",
            "NO STRONG REFERENCES FOUND\n",
            "Neuronix implementation backward Takes 0.5701744556427002 seconds\n",
            "Pytorch implementation backward Takes 0.39017772674560547 seconds\n",
            "ALL PARAMETERS ARE ON THE SAME DEVICE\n",
            "layer1.conv.weight: ✅ Same\n",
            "layer1.conv.bias: ✅ Same\n",
            "layer1.batchnorm.weight: ✅ Same\n",
            "layer1.batchnorm.bias: ✅ Same\n",
            "layer2.conv.weight: ✅ Same\n",
            "layer2.conv.bias: ✅ Same\n",
            "layer2.batchnorm.weight: ✅ Same\n",
            "layer2.batchnorm.bias: ✅ Same\n",
            "layer3.conv.weight: ✅ Same\n",
            "layer3.conv.bias: ✅ Same\n",
            "layer3.batchnorm.weight: ✅ Same\n",
            "layer3.batchnorm.bias: ✅ Same\n",
            "layer4.conv.weight: ✅ Same\n",
            "layer4.conv.bias: ✅ Same\n",
            "layer4.batchnorm.weight: ✅ Same\n",
            "layer4.batchnorm.bias: ✅ Same\n",
            "layer5.conv.weight: ✅ Same\n",
            "layer5.conv.bias: ✅ Same\n",
            "layer5.batchnorm.weight: ✅ Same\n",
            "layer5.batchnorm.bias: ✅ Same\n",
            "linear1.weight: ✅ Same\n",
            "linear1.bias: ✅ Same\n",
            "output.weight: ✅ Same\n",
            "output.bias: ✅ Same\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g1Kohk4Sh25M"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}