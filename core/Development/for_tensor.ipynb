{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6325da31",
   "metadata": {},
   "source": [
    "# old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd196e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class CustomTensor(torch.Tensor):\n",
    "    __slots__ = ('_is_leaf', '_node_id','_custom_requires_grad', '_backward')\n",
    "\n",
    "    def __new__(cls, data, *,_custom_requires_grad=False, is_leaf=False, graph=None,**kwargs):\n",
    "        # Construct the real tensor\n",
    "        if not isinstance(data, torch.Tensor):\n",
    "            data = torch.tensor(data, **kwargs)\n",
    "\n",
    "        instance = torch.Tensor._make_subclass(cls, data)\n",
    "        instance.requires_grad_(False)\n",
    "        instance._custom_requires_grad = _custom_requires_grad\n",
    "        instance._is_leaf = is_leaf\n",
    "        instance._node_id = None\n",
    "        instance._backward = lambda:None\n",
    "        if is_leaf and _custom_requires_grad and graph:\n",
    "            instance._node_id = graph.add_tensor_graph(instance)\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        return instance\n",
    "    \n",
    "    def _wrap(self,*,_custom_requires_grad,is_leaf):\n",
    "        self._custom_requires_grad = _custom_requires_grad\n",
    "        self._is_leaf = is_leaf\n",
    "        self._node_id = None\n",
    "        self._backward = lambda:None\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a2d8bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=CustomTensor([1,2,3],_custom_requires_grad=True,is_leaf=True)\n",
    "b=CustomTensor([1,2,3],_custom_requires_grad=True,is_leaf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8880311e",
   "metadata": {},
   "outputs": [],
   "source": [
    "c=a+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a94eeeb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.CustomTensor"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a338f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "c._wrap(_custom_requires_grad=True,is_leaf=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6887351f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c._is_leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2096e5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils._python_dispatch import TorchDispatchMode\n",
    "import weakref\n",
    "import rustworkx as rx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d33111e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutogradMode(TorchDispatchMode):\n",
    "    def __init__(self,check_for_cycles=True, auto_cleanup=True):\n",
    "        self.graph = rx.PyDiGraph()\n",
    "        self.intermediate_tensors = dict()\n",
    "        self._check_cycles = check_for_cycles\n",
    "        self._auto_cleanup = auto_cleanup\n",
    "\n",
    "    \n",
    "    def __enter__(self):\n",
    "        return super().__enter__()\n",
    "    \n",
    "    def __exit__(self, *args):\n",
    "        if self._check_cycles:\n",
    "            if not self.check_cycle():\n",
    "                raise RuntimeError(\"Cycle detected in autograd graph on context exit.\")\n",
    "        if self._auto_cleanup:\n",
    "            self.intermediate_tensors.clear()\n",
    "            self.graph.clear()  # Clears all nodes and edges\n",
    "        return super().__exit__(*args)\n",
    "    \n",
    "    def __torch_dispatch__(self, func, types, args=(), kwargs=None):\n",
    "        # 1) unwrap TrackingTensor â†’ raw torch.Tensor\n",
    "        if kwargs is None:\n",
    "            kwargs = {}\n",
    "        result = func(*args, **kwargs)\n",
    "        result._wrap(_custom_requires_grad=True,_is_leaf=False)\n",
    "        # TODO write poper LOGICAL FLOW here\n",
    "        return result\n",
    "    def add_tensor_graph(self, tensor):\n",
    "        requires_grad = tensor._custom_requires_grad\n",
    "        if not requires_grad:\n",
    "            raise ValueError(\"Tensor with require grad False cannot to be added to the graph.\")\n",
    "\n",
    "        tensor_index = self.graph.add_node(weakref.ref(tensor))\n",
    "        tensor._node_id  = tensor_index\n",
    "\n",
    "    def add_non_leaf_tensor_references(self, tensor):\n",
    "        requires_grad = tensor._custom_requires_grad\n",
    "        is_leaf = tensor._is_leaf\n",
    "        node_id = tensor._node_id\n",
    "\n",
    "        if not requires_grad or is_leaf:\n",
    "            raise ValueError(\"Tensor must be a non leaf tensor.\")\n",
    "\n",
    "        if node_id in self.intermediate_tensors:\n",
    "            raise ValueError(\"Tensor reference to persist in memory already exists.\")\n",
    "\n",
    "        self.intermediate_tensors[node_id] = tensor\n",
    "\n",
    "    def add_edge(self, node_from, node_to, weight=None):\n",
    "        if not isinstance(node_from, int) or not isinstance(node_to, int):\n",
    "            raise TypeError(\"Node indices must be integers.\")\n",
    "\n",
    "        graph = self.graph\n",
    "        if not graph.has_node(node_from) or not graph.has_node(node_to):\n",
    "            raise ValueError(\"Both nodes must exist in the graph before adding an edge.\")\n",
    "\n",
    "        graph.add_edge(node_from, node_to, weight)\n",
    "    \n",
    "    def check_cycle(self):\n",
    "        return rx.is_directed_acyclic_graph(self.graph)\n",
    "    \n",
    "    def reverse_toposort(self):\n",
    "       \n",
    "        if not self.check_cycle():\n",
    "            raise RuntimeError(\"Cannot perform topological sort on a graph with cycles.\")\n",
    "        graph = self.graph\n",
    "        node_indexes = rx.topological_sort(graph)\n",
    "        return [graph[node_index] for node_index in reversed(node_indexes)]\n",
    "    \n",
    "    def delete_node(self, node_index):\n",
    "        if not isinstance(node_index, int):\n",
    "            raise TypeError(\"Node index must be an integer.\")\n",
    "\n",
    "        graph = self.graph\n",
    "        if not graph.has_node(node_index):\n",
    "            raise ValueError(f\"Node index {node_index} does not exist in the graph.\")\n",
    "\n",
    "        graph.remove_node(node_index)\n",
    "\n",
    "    def delete_edge(self, node_from, node_to):\n",
    "        if not isinstance(node_from, int) or not isinstance(node_to, int):\n",
    "            raise TypeError(\"Node indices must be integers.\")\n",
    "\n",
    "        graph = self.graph\n",
    "        if not graph.has_edge(node_from, node_to):\n",
    "            raise ValueError(f\"Edge ({node_from}, {node_to}) does not exist in the graph.\")\n",
    "\n",
    "        graph.remove_edge(node_from, node_to)\n",
    "\n",
    "    def del_non_leaf_tensor_reference(self, tensor_node_id):\n",
    "        try:\n",
    "            del self.intermediate_tensors[tensor_node_id]\n",
    "        except KeyError:\n",
    "            raise KeyError(f\"No tensor reference found for node ID {tensor_node_id}\")\n",
    "\n",
    "    def __repr__(self):\n",
    "        graph = self.graph\n",
    "        return f\"CustomAutogradGraph(nodes={graph.num_nodes()}, edges={graph.num_edges()})\"\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf22ccac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.CustomTensor"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "type(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9ec70e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "gradmode=AutogradMode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c12f9c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3389d3c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomTensor([2, 4, 6])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29675338",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3e72bd85",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute '_wrap'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m gradmode \u001b[38;5;28;01mas\u001b[39;00m g:\n\u001b[1;32m----> 2\u001b[0m     a\u001b[38;5;241m=\u001b[39m\u001b[43mCustomTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m_custom_requires_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43mis_leaf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43mgraph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgraph\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m     b\u001b[38;5;241m=\u001b[39mCustomTensor([\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m3\u001b[39m],_custom_requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,is_leaf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,graph\u001b[38;5;241m=\u001b[39mg\u001b[38;5;241m.\u001b[39mgraph)\n\u001b[0;32m      4\u001b[0m     c\u001b[38;5;241m=\u001b[39ma\u001b[38;5;241m+\u001b[39mb\n",
      "Cell \u001b[1;32mIn[25], line 9\u001b[0m, in \u001b[0;36mCustomTensor.__new__\u001b[1;34m(cls, data, _custom_requires_grad, is_leaf, graph, **kwargs)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__new__\u001b[39m(\u001b[38;5;28mcls\u001b[39m, data, \u001b[38;5;241m*\u001b[39m,_custom_requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, is_leaf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m# Construct the real tensor\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m----> 9\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m     instance \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor\u001b[38;5;241m.\u001b[39m_make_subclass(\u001b[38;5;28mcls\u001b[39m, data)\n\u001b[0;32m     12\u001b[0m     instance\u001b[38;5;241m.\u001b[39mrequires_grad_(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[20], line 26\u001b[0m, in \u001b[0;36mAutogradMode.__torch_dispatch__\u001b[1;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m     25\u001b[0m result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m---> 26\u001b[0m \u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wrap\u001b[49m(_custom_requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,_is_leaf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# TODO write poper LOGICAL FLOW here\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Tensor' object has no attribute '_wrap'"
     ]
    }
   ],
   "source": [
    "with gradmode as g:\n",
    "    a=CustomTensor([1,2,3],_custom_requires_grad=True,is_leaf=True,graph=g.graph)\n",
    "    b=CustomTensor([1,2,3],_custom_requires_grad=True,is_leaf=True,graph=g.graph)\n",
    "    c=a+b\n",
    "    # c._wrap(_custom_requires_grad=True,is_leaf=False)\n",
    "    # c._is_leaf\n",
    "    # c._node_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1708af2",
   "metadata": {},
   "source": [
    "# New"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90e4866",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import weakref\n",
    "import numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfad73e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Operations:\n",
    "    @torch.jit.script\n",
    "    def add_tensor_and_scalar(tensor: torch.Tensor, scaler: float) -> torch.Tensor:\n",
    "        return tensor + scaler\n",
    "    \n",
    "    @torch.jit.script\n",
    "    def add_tensor_and_tensor(tensor1: torch.Tensor, tensor2: torch.Tensor) -> torch.Tensor:\n",
    "        return tensor1 + tensor2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3c2bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class CustomTensor:\n",
    "    __slots__ = ('tensor','_is_leaf', '_node_id','_custom_requires_grad', '_backward','graph')\n",
    "\n",
    "    def __init__(self, tensor, *,_custom_requires_grad=False, is_leaf=False,device=None,dtype=None,graph=None):\n",
    "        # Construct the real tensor\n",
    "        if not isinstance(tensor, torch.Tensor):\n",
    "            tensor = torch.tensor(tensor,requires_grad=False, dtype=dtype,device=device)\n",
    "        elif isinstance(tensor,CustomTensor):\n",
    "            return tensor\n",
    "        self.tensor = tensor\n",
    "        self._custom_requires_grad = _custom_requires_grad\n",
    "        self._is_leaf = is_leaf\n",
    "        self._node_id = None\n",
    "        self._backward = lambda:None\n",
    "        self.graph = None\n",
    "        if  _custom_requires_grad:\n",
    "            self.graph = weakref.ref(graph)\n",
    "            self._node_id = self.graph.add_tensor_graph(self)\n",
    "            if not is_leaf:\n",
    "                self.graph.add_non_leaf_tensor_references(self)\n",
    "    \n",
    "    def __add__(self,other):\n",
    "        \n",
    "        if isinstance(other, numbers.Number):\n",
    "            result = Operations.add_tensor_and_scalar(self.tensor,other)\n",
    "            requires_grad = self._custom_requires_grad\n",
    "            if requires_grad:\n",
    "                graph = self.graph()\n",
    "                result = CustomTensor(result,_custom_requires_grad=requires_grad,is_leaf=False,graph=graph)\n",
    "                graph.add_edge(self._node_id, result._node_id)\n",
    "                return result\n",
    "            else:\n",
    "                return CustomTensor(result,_custom_requires_grad=requires_grad,is_leaf=True)\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    # def __new__(cls, data, *,_custom_requires_grad=False, is_leaf=False, graph=None,**kwargs):\n",
    "    #     # Construct the real tensor\n",
    "    #     if not isinstance(data, torch.Tensor):\n",
    "    #         data = torch.tensor(data, **kwargs)\n",
    "\n",
    "    #     instance = torch.Tensor._make_subclass(cls, data)\n",
    "    #     instance.requires_grad_(False)\n",
    "    #     instance._custom_requires_grad = _custom_requires_grad\n",
    "    #     instance._is_leaf = is_leaf\n",
    "    #     instance._node_id = None\n",
    "    #     instance._backward = lambda:None\n",
    "    #     if is_leaf and _custom_requires_grad and graph:\n",
    "    #         instance._node_id = graph.add_tensor_graph(instance)\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "    #     return instance\n",
    "    \n",
    "    # def _wrap(self,*,_custom_requires_grad,is_leaf):\n",
    "    #     self._custom_requires_grad = _custom_requires_grad\n",
    "    #     self._is_leaf = is_leaf\n",
    "    #     self._node_id = None\n",
    "    #     self._backward = lambda:None\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2dc982dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9550f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Operations:\n",
    "    @torch.jit.script\n",
    "    def add_tensor_and_scalar(tensor: torch.Tensor, scaler: float) -> torch.Tensor:\n",
    "        return tensor + scaler\n",
    "    \n",
    "    @torch.jit.script\n",
    "    def add_tensor_and_tensor(tensor1: torch.Tensor, tensor2: torch.Tensor) -> torch.Tensor:\n",
    "        return tensor1 + tensor2\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fededed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "@torch.jit.script\n",
    "def add_tensor_and_scalar(tensor: torch.Tensor, scaler: float) -> torch.Tensor:\n",
    "    return tensor + scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58e6481e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3.5000, 4.5000, 5.5000, 6.5000, 7.5000])\n",
      "tensor([11., 12., 13., 14., 15.])\n",
      "tensor([[3., 4.],\n",
      "        [5., 6.]])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "add_tensor_and_scalar() Expected a value of type 'float' for argument 'scaler' but instead found type 'str'.\nPosition: 1\nValue: 'hello'\nDeclaration: add_tensor_and_scalar(Tensor tensor, float scaler) -> Tensor\nCast error details: Unable to cast Python instance of type <class 'str'> to C++ type '?' (#define PYBIND11_DETAILED_ERROR_MESSAGES or compile in debug mode for details)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 24\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Test with a non-numeric scalar\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 24\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43madd_tensor_and_scalar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhello\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28mprint\u001b[39m(e)  \u001b[38;5;66;03m# Expected output: TypeError: unsupported operand type(s) for +: 'Tensor' and 'str'\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: add_tensor_and_scalar() Expected a value of type 'float' for argument 'scaler' but instead found type 'str'.\nPosition: 1\nValue: 'hello'\nDeclaration: add_tensor_and_scalar(Tensor tensor, float scaler) -> Tensor\nCast error details: Unable to cast Python instance of type <class 'str'> to C++ type '?' (#define PYBIND11_DETAILED_ERROR_MESSAGES or compile in debug mode for details)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Create a tensor\n",
    "tensor = torch.tensor([1, 2, 3, 4, 5])\n",
    "\n",
    "# Add a scalar to the tensor\n",
    "result = add_tensor_and_scalar(tensor, 2.5)\n",
    "\n",
    "print(result)  # Expected output: tensor([3.5000, 4.5000, 5.5000, 6.5000, 7.5000])\n",
    "\n",
    "# Test with a different scalar\n",
    "result = add_tensor_and_scalar(tensor, 10)\n",
    "\n",
    "print(result)  # Expected output: tensor([11, 12, 13, 14, 15])\n",
    "\n",
    "# Test with a tensor of different shape\n",
    "tensor = torch.tensor([[1, 2], [3, 4]])\n",
    "result = add_tensor_and_scalar(tensor, 2)\n",
    "\n",
    "print(result)  # Expected output: tensor([[3, 4], [5, 6]])\n",
    "\n",
    "# Test with a non-numeric scalar\n",
    "try:\n",
    "    result = add_tensor_and_scalar(tensor, \"hello\")\n",
    "except TypeError as e:\n",
    "    print(e)  # Expected output: TypeError: unsupported operand type(s) for +: 'Tensor' and 'str'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
