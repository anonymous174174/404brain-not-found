{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# tensor defination and graph"
      ],
      "metadata": {
        "id": "VHu1t2yDejKY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rustworkx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z0MdM2ZoPnma",
        "outputId": "8c7e87ba-b1d2-412e-8c86-5d0af3a8e695"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: rustworkx in /usr/local/lib/python3.11/dist-packages (0.16.0)\n",
            "Requirement already satisfied: numpy<3,>=1.16.0 in /usr/local/lib/python3.11/dist-packages (from rustworkx) (2.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import weakref\n",
        "import numbers\n",
        "import rustworkx as rx\n",
        "import pytest\n",
        "\n",
        "class AutogradGraph:\n",
        "    __slots__ = ('graph', 'intermediate_tensors', '_check_cycles', '_auto_cleanup', '__weakref__')\n",
        "    def __init__(self, check_for_cycles=True, auto_cleanup=True):\n",
        "\n",
        "        self.graph = rx.PyDiGraph()\n",
        "        self.intermediate_tensors = {}\n",
        "        self._check_cycles = check_for_cycles\n",
        "        self._auto_cleanup = auto_cleanup\n",
        "\n",
        "    def __enter__(self):\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, exc_type, exc_value, traceback):\n",
        "        if self._check_cycles and self.check_cycle():\n",
        "            raise RuntimeError(\"Cycle detected in autograd graph on context exit.\")\n",
        "        if self._auto_cleanup:\n",
        "            self.intermediate_tensors.clear()\n",
        "            self.graph.clear()\n",
        "\n",
        "    def add_tensor_graph(self, tensor):\n",
        "        if not tensor._custom_requires_grad:\n",
        "            raise ValueError(\"Tensor with requires_grad=False cannot be added to the graph.\")\n",
        "\n",
        "\n",
        "        ref = weakref.proxy(tensor)\n",
        "        tensor_index = self.graph.add_node(ref)\n",
        "        tensor._node_id = tensor_index\n",
        "\n",
        "    def add_non_leaf_tensor_reference(self, tensor):\n",
        "        if not tensor._custom_requires_grad:\n",
        "            raise ValueError(\"Tensor must require grad.\")\n",
        "\n",
        "        if tensor._node_id in self.intermediate_tensors:\n",
        "            raise ValueError(\"Tensor reference already exists in intermediate tensors.\")\n",
        "\n",
        "        self.intermediate_tensors[tensor._node_id] = tensor\n",
        "\n",
        "\n",
        "    def add_edge(self, node_from, node_to, weight=None):\n",
        "        if not all(isinstance(n, int) for n in (node_from, node_to)):\n",
        "            raise TypeError(\"Node indices must be integers.\")\n",
        "        if not self.graph.has_node(node_from) or not self.graph.has_node(node_to):\n",
        "            raise ValueError(\"Nodes must exist before adding edge.\")\n",
        "        self.graph.add_edge(node_from, node_to, weight)\n",
        "\n",
        "    def check_cycle(self):\n",
        "        return not rx.is_directed_acyclic_graph(self.graph)\n",
        "\n",
        "    def reverse_toposort(self):\n",
        "        return [self.graph[n] for n in reversed(rx.topological_sort(self.graph))]\n",
        "\n",
        "    def reverse_toposort_from_tensor(self, tensor_index):\n",
        "        graph=self.graph\n",
        "        predecessors = list(rx.ancestors(graph, tensor_index))\n",
        "        predecessors.append(tensor_index)\n",
        "        sub_graph = graph.subgraph(predecessors)\n",
        "        return [sub_graph[i] for i in reversed(rx.topological_sort(sub_graph))]\n",
        "\n",
        "    # def alternative_reverse_toposort_from_tensor(self, tensor_index):\n",
        "    #     graph = self.graph\n",
        "    #     relevant_nodes = rx.ancestors(graph, tensor_index)\n",
        "    #     relevant_nodes.add(tensor_index)\n",
        "    #     full_topo = rx.topological_sort(graph)\n",
        "    #     relevant_topo = [graph[_node_id] for _node_id in reversed(full_topo) if _node_id in relevant_nodes]\n",
        "    #     return relevant_topo\n",
        "\n",
        "    def delete_node(self, node_index):\n",
        "        if not isinstance(node_index, int):\n",
        "            raise TypeError(\"Node index must be an integer.\")\n",
        "        if not self.graph.has_node(node_index):\n",
        "            raise ValueError(\"Node does not exist.\")\n",
        "        self.graph.remove_node(node_index)\n",
        "\n",
        "    def delete_edge(self, node_from, node_to):\n",
        "        if not self.graph.has_edge(node_from, node_to):\n",
        "            raise ValueError(\"Edge does not exist.\")\n",
        "        self.graph.remove_edge(node_from, node_to)\n",
        "\n",
        "    def del_non_leaf_tensor_reference(self, tensor_node_id):\n",
        "        self.intermediate_tensors.pop(tensor_node_id, None)\n",
        "\n",
        "    def delete_all_non_leaf_nodes(self):\n",
        "        # removes non leaf nodes from graph and clears the intermediate_tensors dict\n",
        "        self.graph.remove_nodes_from(list(self.intermediate_tensors.keys()))\n",
        "        self.intermediate_tensors.clear()\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"CustomAutogradGraph(nodes={self.graph.num_nodes()}, edges={self.graph.num_edges()})\"\n",
        "\n",
        "class CustomTensor:\n",
        "    __slots__ = ('tensor', '_node_id', '_custom_requires_grad', '_backward', 'graph', '__weakref__','_is_leaf')\n",
        "\n",
        "    def __new__(cls, data, *, _custom_requires_grad=False, device=None, dtype=None, graph=None, due_to_operation=False, is_leaf=False):\n",
        "        if isinstance(data, CustomTensor):\n",
        "            return data  # Don't rewrap\n",
        "        return super().__new__(cls)\n",
        "\n",
        "    def __init__(self, data, *, _custom_requires_grad=False, device=None, dtype=None, graph=None, due_to_operation=False, is_leaf=False):\n",
        "        if isinstance(data, CustomTensor):\n",
        "            return\n",
        "\n",
        "        self.tensor = data if due_to_operation else torch.as_tensor(data, dtype=dtype, device=device)\n",
        "        self.tensor.requires_grad_(False)\n",
        "        self._custom_requires_grad = _custom_requires_grad\n",
        "        self._node_id = None\n",
        "        self._backward = lambda: None\n",
        "        self.graph = None\n",
        "        self._is_leaf = is_leaf\n",
        "\n",
        "        if _custom_requires_grad:\n",
        "            self._init_graph(graph)\n",
        "\n",
        "    def _init_graph(self, graph):\n",
        "        if graph is None:\n",
        "            raise ValueError(\"Graph must be provided if requires_grad is True.\")\n",
        "        is_leaf=self._is_leaf\n",
        "        if is_leaf:\n",
        "          self.graph = weakref.proxy(graph)\n",
        "        else:\n",
        "          self.graph = graph # this line is only reached for tensors which are created by operations and graph passed is already a weakreference hence no need for wrapping\n",
        "        graph.add_tensor_graph(self)\n",
        "        if not is_leaf:\n",
        "            graph.add_non_leaf_tensor_reference(self)\n",
        "\n",
        "    def _zero_grad(self):\n",
        "        self.tensor.grad = torch.zeros_like(self.tensor)\n",
        "\n",
        "    def __add__(self, other):\n",
        "        if isinstance(other, numbers.Number):\n",
        "            return self._add_scalar(other)#, op=torch.add)#Operations.add_tensor_and_scalar)\n",
        "        elif isinstance(other, CustomTensor):\n",
        "            return self._add_tensor(other)#, op=torch.add)#Operations.add_tensor_and_tensor)\n",
        "        return NotImplemented\n",
        "\n",
        "    def _add_scalar(self, scalar):\n",
        "        result_tensor = torch.add(self.tensor, scalar)\n",
        "        if not self._custom_requires_grad:\n",
        "            return CustomTensor(result_tensor)\n",
        "\n",
        "        graph = self.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "        graph.add_edge(self._node_id, result._node_id)\n",
        "\n",
        "        self_ref = weakref.proxy(self)\n",
        "        result_ref = weakref.proxy(result)\n",
        "        def _backward():\n",
        "            if self_ref.tensor.grad is None:\n",
        "                self_ref._zero_grad()\n",
        "            # print(f\"Backward for scalar add: result_grad={result.tensor.grad}, self_grad_before={self_ref.tensor.grad}\") # Debugging\n",
        "            self_ref.tensor.grad.add_(result_ref.tensor.grad)\n",
        "            # print(f\"Backward for scalar add: self_grad_after={self_ref.tensor.grad}\") # Debugging\n",
        "\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "\n",
        "    def _add_tensor(self, other):\n",
        "        result_tensor = torch.add(self.tensor, other.tensor)\n",
        "        requires_grad = self._custom_requires_grad or other._custom_requires_grad\n",
        "\n",
        "        if not requires_grad:\n",
        "            return CustomTensor(result_tensor)\n",
        "\n",
        "        # Graph selection logic - assuming operations happen within a single graph context\n",
        "        graph = None\n",
        "        if self._custom_requires_grad:\n",
        "            graph = self.graph\n",
        "        elif other._custom_requires_grad:\n",
        "            graph = other.graph\n",
        "        else:\n",
        "            # This case should ideally not be reached if requires_grad is True\n",
        "            # and at least one operand has requires_grad\n",
        "            pass # Or raise an error if graph is truly missing\n",
        "\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "\n",
        "        self_ref = weakref.proxy(self)\n",
        "        other_ref = weakref.proxy(other)\n",
        "\n",
        "        if self._custom_requires_grad:\n",
        "            graph.add_edge(self._node_id, result._node_id)\n",
        "        if other._custom_requires_grad:\n",
        "            graph.add_edge(other._node_id, result._node_id)\n",
        "        result_ref = weakref.proxy(result)\n",
        "        def _backward():\n",
        "            # print(f\"Backward for tensor add: result_grad={result.tensor.grad}\") # Debugging\n",
        "            if self_ref._custom_requires_grad:\n",
        "                if self_ref.tensor.grad is None:\n",
        "                    self_ref._zero_grad()\n",
        "                self_ref.tensor.grad.add_(result_ref.tensor.grad)\n",
        "                # print(f\"  self_grad_after={self_ref.tensor.grad}\") # Debugging\n",
        "            if other_ref._custom_requires_grad:\n",
        "                if other_ref.tensor.grad is None:\n",
        "                    other_ref._zero_grad()\n",
        "                other_ref.tensor.grad.add_(result_ref.tensor.grad)\n",
        "                # print(f\"  other_grad_after={other_ref.tensor.grad}\") # Debugging\n",
        "\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "    def __mul__(self, other):\n",
        "        if isinstance(other, numbers.Number):\n",
        "            return self._mul_scalar(other)\n",
        "        elif isinstance(other, CustomTensor):\n",
        "            return self._mul_tensor(other)\n",
        "        return NotImplemented\n",
        "\n",
        "    def _mul_scalar(self, scalar):\n",
        "        result_tensor = torch.mul(self.tensor, scalar)\n",
        "        if not self._custom_requires_grad:\n",
        "            return CustomTensor(result_tensor)\n",
        "\n",
        "        graph = self.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "        graph.add_edge(self._node_id, result._node_id)\n",
        "\n",
        "        self_ref = weakref.proxy(self)\n",
        "        result_ref = weakref.proxy(result)\n",
        "        def _backward():\n",
        "            if self_ref.tensor.grad is None:\n",
        "                self_ref._zero_grad()\n",
        "            self_ref.tensor.grad.add_(result_ref.tensor.grad * scalar)\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "\n",
        "    def _mul_tensor(self, other):\n",
        "        result_tensor = torch.mul(self.tensor, other.tensor)\n",
        "        requires_grad = self._custom_requires_grad or other._custom_requires_grad\n",
        "        if not requires_grad:\n",
        "            return CustomTensor(result_tensor)\n",
        "\n",
        "        graph = self.graph if self._custom_requires_grad else other.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "\n",
        "        self_ref = weakref.proxy(self)\n",
        "        other_ref = weakref.proxy(other)\n",
        "        result_ref = weakref.proxy(result)\n",
        "\n",
        "        if self._custom_requires_grad:\n",
        "            graph.add_edge(self._node_id, result._node_id)\n",
        "        if other._custom_requires_grad:\n",
        "            graph.add_edge(other._node_id, result._node_id)\n",
        "\n",
        "        def _backward():\n",
        "            if self_ref._custom_requires_grad:\n",
        "                if self_ref.tensor.grad is None:\n",
        "                    self_ref._zero_grad()\n",
        "                self_ref.tensor.grad.add_(result_ref.tensor.grad * other_ref.tensor)\n",
        "            if other_ref._custom_requires_grad:\n",
        "                if other_ref.tensor.grad is None:\n",
        "                    other_ref._zero_grad()\n",
        "                other_ref.tensor.grad.add_(result_ref.tensor.grad * self_ref.tensor)\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "\n",
        "\n",
        "    def __sub__(self, other):\n",
        "        if isinstance(other, numbers.Number):\n",
        "            return self._sub_scalar(other)\n",
        "        elif isinstance(other, CustomTensor):\n",
        "            return self._sub_tensor(other)\n",
        "        return NotImplemented\n",
        "\n",
        "    def _sub_scalar(self, scalar):\n",
        "        result_tensor = torch.sub(self.tensor, scalar)\n",
        "        if not self._custom_requires_grad:\n",
        "            return CustomTensor(result_tensor)\n",
        "\n",
        "        graph = self.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "        graph.add_edge(self._node_id, result._node_id)\n",
        "\n",
        "        self_ref = weakref.proxy(self)\n",
        "        result_ref = weakref.proxy(result)\n",
        "        def _backward():\n",
        "            if self_ref.tensor.grad is None:\n",
        "                self_ref._zero_grad()\n",
        "            self_ref.tensor.grad.add_(result_ref.tensor.grad)\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "\n",
        "    def _sub_tensor(self, other):\n",
        "        result_tensor = torch.sub(self.tensor, other.tensor)\n",
        "        requires_grad = self._custom_requires_grad or other._custom_requires_grad\n",
        "        if not requires_grad:\n",
        "            return CustomTensor(result_tensor)\n",
        "\n",
        "        graph = self.graph if self._custom_requires_grad else other.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "\n",
        "        self_ref = weakref.proxy(self)\n",
        "        other_ref = weakref.proxy(other)\n",
        "        result_ref = weakref.proxy(result)\n",
        "\n",
        "        if self._custom_requires_grad:\n",
        "            graph.add_edge(self._node_id, result._node_id)\n",
        "        if other._custom_requires_grad:\n",
        "            graph.add_edge(other._node_id, result._node_id)\n",
        "\n",
        "        def _backward():\n",
        "            if self_ref._custom_requires_grad:\n",
        "                if self_ref.tensor.grad is None:\n",
        "                    self_ref._zero_grad()\n",
        "                self_ref.tensor.grad.add_(result_ref.tensor.grad)\n",
        "            if other_ref._custom_requires_grad:\n",
        "                if other_ref.tensor.grad is None:\n",
        "                    other_ref._zero_grad()\n",
        "                other_ref.tensor.grad.sub_(result_ref.tensor.grad)\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "\n",
        "    def __truediv__(self, scalar):\n",
        "        return self._div_scalar(scalar)\n",
        "\n",
        "    def _div_scalar(self, scalar):\n",
        "        result_tensor = torch.div(self.tensor, scalar)\n",
        "        if not self._custom_requires_grad:\n",
        "            return CustomTensor(result_tensor)\n",
        "\n",
        "        graph = self.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "        graph.add_edge(self._node_id, result._node_id)\n",
        "\n",
        "        self_ref = weakref.proxy(self)\n",
        "        result_ref = weakref.proxy(result)\n",
        "        def _backward():\n",
        "            if self_ref.tensor.grad is None:\n",
        "                self_ref._zero_grad()\n",
        "            self_ref.tensor.grad.add_(result_ref.tensor.grad / scalar)\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "\n",
        "\n",
        "    def pow(self, scalar):\n",
        "        result_tensor = torch.pow(self.tensor, scalar)\n",
        "        if not self._custom_requires_grad:\n",
        "            return CustomTensor(result_tensor)\n",
        "\n",
        "        graph = self.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "        graph.add_edge(self._node_id, result._node_id)\n",
        "\n",
        "        self_ref = weakref.proxy(self)\n",
        "        result_ref = weakref.proxy(result)\n",
        "        def _backward():\n",
        "            if self_ref.tensor.grad is None:\n",
        "                self_ref._zero_grad()\n",
        "            grad_contrib = scalar * self_ref.tensor.pow(scalar - 1)\n",
        "            self_ref.tensor.grad.add_(result_ref.tensor.grad * grad_contrib)\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "\n",
        "    def exp(self):\n",
        "        out = torch.exp(self.tensor)\n",
        "        return self._unary_op(out, lambda grad, out_tensor: grad * out_tensor)\n",
        "\n",
        "    def log(self):\n",
        "        out = torch.log(self.tensor)\n",
        "        return self._unary_op(out, lambda grad, input_tensor: grad / input_tensor)\n",
        "\n",
        "    def sin(self):\n",
        "        out = torch.sin(self.tensor)\n",
        "        return self._unary_op(out, lambda grad, input_tensor: grad * torch.cos(input_tensor))\n",
        "\n",
        "    def cos(self):\n",
        "        out = torch.cos(self.tensor)\n",
        "        return self._unary_op(out, lambda grad, input_tensor: -grad * torch.sin(input_tensor))\n",
        "\n",
        "    def sqrt(self):\n",
        "        out = torch.sqrt(self.tensor)\n",
        "        return self._unary_op(out, lambda grad, out_tensor: grad * 0.5 / out_tensor)\n",
        "\n",
        "    def _unary_op(self, result_tensor, backward_fn):\n",
        "        if not self._custom_requires_grad:\n",
        "            return CustomTensor(result_tensor)\n",
        "\n",
        "        graph = self.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "        graph.add_edge(self._node_id, result._node_id)\n",
        "\n",
        "        self_ref = weakref.proxy(self)\n",
        "        result_ref = weakref.proxy(result)\n",
        "        def _backward():\n",
        "            if self_ref.tensor.grad is None:\n",
        "                self_ref._zero_grad()\n",
        "            self_ref.tensor.grad.add_(backward_fn(result_ref.tensor.grad, self_ref.tensor))\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "\n",
        "    def matmul(self, other):\n",
        "        result_tensor = torch.matmul(self.tensor, other.tensor)\n",
        "        requires_grad = self._custom_requires_grad or other._custom_requires_grad\n",
        "        if not requires_grad:\n",
        "            return CustomTensor(result_tensor)\n",
        "\n",
        "        graph = self.graph if self._custom_requires_grad else other.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "\n",
        "        self_ref = weakref.proxy(self)\n",
        "        other_ref = weakref.proxy(other)\n",
        "        result_ref = weakref.proxy(result)\n",
        "\n",
        "        if self._custom_requires_grad:\n",
        "            graph.add_edge(self._node_id, result._node_id)\n",
        "        if other._custom_requires_grad:\n",
        "            graph.add_edge(other._node_id, result._node_id)\n",
        "\n",
        "        def _backward():\n",
        "            if self_ref._custom_requires_grad:\n",
        "                if self_ref.tensor.grad is None:\n",
        "                    self_ref._zero_grad()\n",
        "                self_ref.tensor.grad.add_(torch.matmul(result_ref.tensor.grad, other_ref.tensor.t()))\n",
        "            if other_ref._custom_requires_grad:\n",
        "                if other_ref.tensor.grad is None:\n",
        "                    other_ref._zero_grad()\n",
        "                other_ref.tensor.grad.add_(torch.matmul(self_ref.tensor.t(), result_ref.tensor.grad))\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "\n",
        "    def apply_mask(self, mask):\n",
        "        result_tensor = self.tensor * mask.tensor\n",
        "        requires_grad = self._custom_requires_grad or mask._custom_requires_grad\n",
        "        if not requires_grad:\n",
        "            return CustomTensor(result_tensor)\n",
        "\n",
        "        graph = self.graph if self._custom_requires_grad else mask.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "\n",
        "        self_ref = weakref.proxy(self)\n",
        "        mask_ref = weakref.proxy(mask)\n",
        "        result_ref = weakref.proxy(result)\n",
        "\n",
        "        if self._custom_requires_grad:\n",
        "            graph.add_edge(self._node_id, result._node_id)\n",
        "        if mask._custom_requires_grad:\n",
        "            graph.add_edge(mask._node_id, result._node_id)\n",
        "\n",
        "        def _backward():\n",
        "            if self_ref._custom_requires_grad:\n",
        "                if self_ref.tensor.grad is None:\n",
        "                    self_ref._zero_grad()\n",
        "                self_ref.tensor.grad.add_(result_ref.tensor.grad * mask_ref.tensor)\n",
        "            if mask_ref._custom_requires_grad:\n",
        "                if mask_ref.tensor.grad is None:\n",
        "                    mask_ref._zero_grad()\n",
        "                mask_ref.tensor.grad.add_(result_ref.tensor.grad * self_ref.tensor)\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "\n",
        "    def dot(self, other):\n",
        "        result_tensor = torch.dot(self.tensor, other.tensor)\n",
        "        requires_grad = self._custom_requires_grad or other._custom_requires_grad\n",
        "        if not requires_grad:\n",
        "            return CustomTensor(result_tensor)\n",
        "\n",
        "        graph = self.graph if self._custom_requires_grad else other.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "\n",
        "        self_ref = weakref.proxy(self)\n",
        "        other_ref = weakref.proxy(other)\n",
        "        result_ref = weakref.proxy(result)\n",
        "\n",
        "        if self._custom_requires_grad:\n",
        "            graph.add_edge(self._node_id, result._node_id)\n",
        "        if other._custom_requires_grad:\n",
        "            graph.add_edge(other._node_id, result._node_id)\n",
        "\n",
        "        def _backward():\n",
        "            if self_ref._custom_requires_grad:\n",
        "                if self_ref.tensor.grad is None:\n",
        "                    self_ref._zero_grad()\n",
        "                self_ref.tensor.grad.add_(result_ref.tensor.grad * other_ref.tensor)\n",
        "            if other_ref._custom_requires_grad:\n",
        "                if other_ref.tensor.grad is None:\n",
        "                    other_ref._zero_grad()\n",
        "                other_ref.tensor.grad.add_(result_ref.tensor.grad * self_ref.tensor)\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "    def backward(self,weightage_tensor=1):\n",
        "        if not self._custom_requires_grad:\n",
        "            raise RuntimeError(\"Output tensor does not require grad.\")\n",
        "        if self.graph is None:\n",
        "            raise RuntimeError(\"Output tensor is not part of a graph.\")\n",
        "        graph = self.graph\n",
        "\n",
        "        # Initialize gradient for the output tensor\n",
        "        if isinstance(weightage_tensor,numbers.Number):\n",
        "            self.tensor.grad = torch.full_like(self.tensor, fill_value=weightage_tensor)\n",
        "        elif isinstance(weightage_tensor,torch.Tensor):\n",
        "            self.tensor.grad = weightage_tensor.clone() # we don't want to modify the original tensor data\n",
        "\n",
        "        # Perform backward pass using topological sort\n",
        "\n",
        "        nodes_to_process = graph.reverse_toposort_from_tensor(self._node_id)\n",
        "\n",
        "        # Create a strong reference to intermediate tensors needed for backward pass\n",
        "        # This simulates how a real autograd engine would keep track of them\n",
        "        # The graph context's intermediate_tensors dict already serves this purpose.\n",
        "\n",
        "        for tensor_node in nodes_to_process:\n",
        "            # Check if the weak proxy is still valid (tensor is alive)\n",
        "            if tensor_node.__class__ is weakref.ProxyType and tensor_node.__repr__() is None:\n",
        "                # print(f\"Skipping dead proxy: {tensor_node}\") # Debugging\n",
        "                continue # Skip if the weak reference is dead\n",
        "\n",
        "            if tensor_node.tensor.grad is None and tensor_node is not self.tensor:\n",
        "                # This can happen if a tensor is part of the graph but its grad hasn't been set yet\n",
        "                # and it's not the root of the backward call. This typically means it's a leaf\n",
        "                # that wasn't used to compute the output or an intermediate that accumulated no grad.\n",
        "                # For simplicity in this test, we assume grads propagate.\n",
        "                # print(f\"Warning: Tensor node {tensor_node._node_id} has no grad before _backward call.\")\n",
        "                pass # A no-op for now. In a real system, you might want to handle this.\n",
        "\n",
        "            # Ensure that non-leaf tensors are still alive when their _backward is called\n",
        "            # The `intermediate_tensors` in `AutogradGraph` should keep them alive.\n",
        "            tensor_node._backward()\n",
        "\n",
        "        # Clean up intermediate tensors references after backward pass\n",
        "        # This would typically be handled by the graph context's exit, but\n",
        "        # if `_auto_cleanup` is False, you might need manual cleanup.\n",
        "        # Here, for testing GC, we'll let the context manager handle it.\n",
        "\n",
        "        def backward_dangerous(self,weightage_tensor=1):#ONLY USE IF CALLING BACKWARD ON THE LAST NODE ADDED TO THE GRAPH AND IF YOU WANT BACKWARD ON ALL\n",
        "          if not self._custom_requires_grad:\n",
        "              raise RuntimeError(\"Output tensor does not require grad.\")\n",
        "          if self.graph is None:\n",
        "              raise RuntimeError(\"Output tensor is not part of a graph.\")\n",
        "          graph = self.graph\n",
        "\n",
        "          # Initialize gradient for the output tensor\n",
        "          if isinstance(weightage_tensor,numbers.Number):\n",
        "              self.tensor.grad = torch.full_like(self.tensor, fill_value=weightage_tensor)\n",
        "          elif isinstance(weightage_tensor,torch.Tensor):\n",
        "              self.tensor.grad = weightage_tensor.clone() # we don't want to modify the original tensor data\n",
        "\n",
        "          # Perform backward pass using topological sort\n",
        "\n",
        "          nodes_to_process = graph.reverse_toposort(self._node_id)\n",
        "\n",
        "          # Create a strong reference to intermediate tensors needed for backward pass\n",
        "          # This simulates how a real autograd engine would keep track of them\n",
        "          # The graph context's intermediate_tensors dict already serves this purpose.\n",
        "\n",
        "          for tensor_node in nodes_to_process:\n",
        "              # Check if the weak proxy is still valid (tensor is alive)\n",
        "              if tensor_node.__class__ is weakref.ProxyType and tensor_node.__repr__() is None:\n",
        "                  # print(f\"Skipping dead proxy: {tensor_node}\") # Debugging\n",
        "                  continue # Skip if the weak reference is dead\n",
        "\n",
        "              if tensor_node.tensor.grad is None and tensor_node is not self.tensor:\n",
        "                  # This can happen if a tensor is part of the graph but its grad hasn't been set yet\n",
        "                  # and it's not the root of the backward call. This typically means it's a leaf\n",
        "                  # that wasn't used to compute the output or an intermediate that accumulated no grad.\n",
        "                  # For simplicity in this test, we assume grads propagate.\n",
        "                  # print(f\"Warning: Tensor node {tensor_node._node_id} has no grad before _backward call.\")\n",
        "                  pass # A no-op for now. In a real system, you might want to handle this.\n",
        "\n",
        "              # Ensure that non-leaf tensors are still alive when their _backward is called\n",
        "              # The `intermediate_tensors` in `AutogradGraph` should keep them alive.\n",
        "              tensor_node._backward()\n",
        "\n",
        "          # Clean up intermediate tensors references after backward pass\n",
        "          # This would typically be handled by the graph context's exit, but\n",
        "          # if `_auto_cleanup` is False, you might need manual cleanup.\n",
        "          # Here, for testing GC, we'll let the context manager handle it.\n",
        "\n",
        "\n",
        "\n",
        "    def __del__(self):\n",
        "      if self._node_id is not None and self._is_leaf and self.graph: #must remove leaf tensor from graph before it is deleted from memory\n",
        "        # self.graph.delete_node(self._node_id)\n",
        "        try:\n",
        "              # Check if graph is still alive before trying to delete\n",
        "              self.graph.delete_node(self._node_id)\n",
        "        except ReferenceError:\n",
        "              # Graph context has already been cleaned up, so do nothing.\n",
        "              pass\n",
        "      print(f\"Garbage Collector has decided that reference counts for {self._node_id} are zero so Goodbye!!\")\n",
        "\n"
      ],
      "metadata": {
        "id": "87J09aqWeiSp"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import weakref\n",
        "import numbers\n",
        "import rustworkx as rx\n",
        "import pytest\n",
        "\n",
        "# AutogradGraph class remains the same as you provided\n",
        "class AutogradGraph:\n",
        "    __slots__ = ('graph', 'intermediate_tensors', '_check_cycles', '_auto_cleanup', '__weakref__')\n",
        "    def __init__(self, check_for_cycles=True, auto_cleanup=True):\n",
        "        self.graph = rx.PyDiGraph()\n",
        "        self.intermediate_tensors = {}\n",
        "        self._check_cycles = check_for_cycles\n",
        "        self._auto_cleanup = auto_cleanup\n",
        "\n",
        "    def __enter__(self):\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, exc_type, exc_value, traceback):\n",
        "        if self._check_cycles and self.check_cycle():\n",
        "            raise RuntimeError(\"Cycle detected in autograd graph on context exit.\")\n",
        "        if self._auto_cleanup:\n",
        "            self.intermediate_tensors.clear()\n",
        "            self.graph.clear()\n",
        "\n",
        "    def add_tensor_graph(self, tensor):\n",
        "        if not tensor._custom_requires_grad:\n",
        "            raise ValueError(\"Tensor with requires_grad=False cannot be added to the graph.\")\n",
        "\n",
        "        ref = weakref.proxy(tensor)\n",
        "        tensor_index = self.graph.add_node(ref)\n",
        "        tensor._node_id = tensor_index\n",
        "\n",
        "    def add_non_leaf_tensor_reference(self, tensor):\n",
        "        if not tensor._custom_requires_grad:\n",
        "            raise ValueError(\"Tensor must require grad.\")\n",
        "        if tensor._node_id in self.intermediate_tensors:\n",
        "            raise ValueError(\"Tensor reference already exists in intermediate tensors.\")\n",
        "        self.intermediate_tensors[tensor._node_id] = tensor\n",
        "\n",
        "    def add_edge(self, node_from, node_to, weight=None):\n",
        "        if not all(isinstance(n, int) for n in (node_from, node_to)):\n",
        "            raise TypeError(\"Node indices must be integers.\")\n",
        "        if not self.graph.has_node(node_from) or not self.graph.has_node(node_to):\n",
        "            raise ValueError(\"Nodes must exist before adding edge.\")\n",
        "        self.graph.add_edge(node_from, node_to, weight)\n",
        "\n",
        "    def check_cycle(self):\n",
        "        return not rx.is_directed_acyclic_graph(self.graph)\n",
        "\n",
        "    def reverse_toposort(self):\n",
        "        return [self.graph[n] for n in reversed(rx.topological_sort(self.graph))]\n",
        "\n",
        "    def reverse_toposort_from_tensor(self, tensor_index):\n",
        "        graph=self.graph\n",
        "        predecessors = list(rx.ancestors(graph, tensor_index))\n",
        "        predecessors.append(tensor_index)\n",
        "        sub_graph = graph.subgraph(predecessors)\n",
        "        return [sub_graph[i] for i in reversed(rx.topological_sort(sub_graph))]\n",
        "\n",
        "    def delete_node(self, node_index):\n",
        "        if not isinstance(node_index, int):\n",
        "            raise TypeError(\"Node index must be an integer.\")\n",
        "        if not self.graph.has_node(node_index):\n",
        "            raise ValueError(\"Node does not exist.\")\n",
        "        self.graph.remove_node(node_index)\n",
        "\n",
        "    def delete_edge(self, node_from, node_to):\n",
        "        if not self.graph.has_edge(node_from, node_to):\n",
        "            raise ValueError(\"Edge does not exist.\")\n",
        "        self.graph.remove_edge(node_from, node_to)\n",
        "\n",
        "    def del_non_leaf_tensor_reference(self, tensor_node_id):\n",
        "        self.intermediate_tensors.pop(tensor_node_id, None)\n",
        "\n",
        "    def delete_all_non_leaf_nodes(self):\n",
        "        self.graph.remove_nodes_from(list(self.intermediate_tensors.keys()))\n",
        "        self.intermediate_tensors.clear()\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"CustomAutogradGraph(nodes={self.graph.num_nodes()}, edges={self.graph.num_edges()})\"\n",
        "\n",
        "class CustomTensor:\n",
        "    __slots__ = ('tensor', '_node_id', '_custom_requires_grad', '_backward', 'graph', '__weakref__','_is_leaf')\n",
        "\n",
        "    def __new__(cls, data, *, _custom_requires_grad=False, device=None, dtype=None, graph=None, due_to_operation=False, is_leaf=False):\n",
        "        if isinstance(data, CustomTensor):\n",
        "            return data\n",
        "        return super().__new__(cls)\n",
        "\n",
        "    def __init__(self, data, *, _custom_requires_grad=False, device=None, dtype=None, graph=None, due_to_operation=False, is_leaf=False):\n",
        "        if isinstance(data, CustomTensor):\n",
        "            return\n",
        "\n",
        "        self.tensor = data if due_to_operation else torch.as_tensor(data, dtype=dtype, device=device)\n",
        "        self.tensor.requires_grad_(False)\n",
        "        self._custom_requires_grad = _custom_requires_grad\n",
        "        self._node_id = None\n",
        "        self._backward = lambda: None\n",
        "        self.graph = None\n",
        "        self._is_leaf = is_leaf\n",
        "\n",
        "        if _custom_requires_grad:\n",
        "            self._init_graph(graph)\n",
        "\n",
        "    def _init_graph(self, graph):\n",
        "        if graph is None:\n",
        "            raise ValueError(\"Graph must be provided if requires_grad is True.\")\n",
        "        is_leaf=self._is_leaf\n",
        "        if is_leaf:\n",
        "          self.graph = weakref.proxy(graph)\n",
        "        else:\n",
        "          self.graph = graph\n",
        "        graph.add_tensor_graph(self)\n",
        "        if not is_leaf:\n",
        "            graph.add_non_leaf_tensor_reference(self)\n",
        "\n",
        "    def _zero_grad(self):\n",
        "        self.tensor.grad = torch.zeros_like(self.tensor)\n",
        "\n",
        "    def __add__(self, other):\n",
        "        if isinstance(other, numbers.Number):\n",
        "            return self._add_scalar(other)\n",
        "        elif isinstance(other, CustomTensor):\n",
        "            return self._add_tensor(other)\n",
        "        return NotImplemented\n",
        "\n",
        "    def _add_scalar(self, scalar):\n",
        "        result_tensor = torch.add(self.tensor, scalar)\n",
        "        if not self._custom_requires_grad:\n",
        "            return CustomTensor(result_tensor)\n",
        "\n",
        "        graph = self.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "        graph.add_edge(self._node_id, result._node_id)\n",
        "\n",
        "        self_ref = weakref.proxy(self)\n",
        "        result_ref = weakref.proxy(result)\n",
        "        def _backward():\n",
        "            if self_ref and result_ref:\n",
        "                if self_ref.tensor.grad is None:\n",
        "                    self_ref._zero_grad()\n",
        "                self_ref.tensor.grad.add_(result_ref.tensor.grad)\n",
        "\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "\n",
        "    def _add_tensor(self, other):\n",
        "        result_tensor = torch.add(self.tensor, other.tensor)\n",
        "        requires_grad = self._custom_requires_grad or other._custom_requires_grad\n",
        "        if not requires_grad:\n",
        "            return CustomTensor(result_tensor)\n",
        "\n",
        "        graph = self.graph if self._custom_requires_grad else other.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "\n",
        "        self_req_grad = self._custom_requires_grad\n",
        "        other_req_grad = other._custom_requires_grad\n",
        "        self_ref = weakref.proxy(self)\n",
        "        other_ref = weakref.proxy(other)\n",
        "        result_ref = weakref.proxy(result)\n",
        "\n",
        "        if self_req_grad:\n",
        "            graph.add_edge(self._node_id, result._node_id)\n",
        "        if other_req_grad:\n",
        "            graph.add_edge(other._node_id, result._node_id)\n",
        "\n",
        "        def _backward():\n",
        "            if result_ref:\n",
        "                if self_req_grad and self_ref:\n",
        "                    if self_ref.tensor.grad is None:\n",
        "                        self_ref._zero_grad()\n",
        "                    self_ref.tensor.grad.add_(result_ref.tensor.grad)\n",
        "                if other_req_grad and other_ref:\n",
        "                    if other_ref.tensor.grad is None:\n",
        "                        other_ref._zero_grad()\n",
        "                    other_ref.tensor.grad.add_(result_ref.tensor.grad)\n",
        "\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "\n",
        "    def __mul__(self, other):\n",
        "        if isinstance(other, numbers.Number):\n",
        "            return self._mul_scalar(other)\n",
        "        elif isinstance(other, CustomTensor):\n",
        "            return self._mul_tensor(other)\n",
        "        return NotImplemented\n",
        "\n",
        "    def _mul_scalar(self, scalar):\n",
        "        result_tensor = torch.mul(self.tensor, scalar)\n",
        "        if not self._custom_requires_grad:\n",
        "            return CustomTensor(result_tensor)\n",
        "\n",
        "        graph = self.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "        graph.add_edge(self._node_id, result._node_id)\n",
        "\n",
        "        self_ref = weakref.proxy(self)\n",
        "        result_ref = weakref.proxy(result)\n",
        "        def _backward():\n",
        "            if self_ref and result_ref:\n",
        "                if self_ref.tensor.grad is None:\n",
        "                    self_ref._zero_grad()\n",
        "                self_ref.tensor.grad.add_(result_ref.tensor.grad * scalar)\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "\n",
        "    def _mul_tensor(self, other):\n",
        "        result_tensor = torch.mul(self.tensor, other.tensor)\n",
        "        requires_grad = self._custom_requires_grad or other._custom_requires_grad\n",
        "        if not requires_grad:\n",
        "            return CustomTensor(result_tensor)\n",
        "\n",
        "        graph = self.graph if self._custom_requires_grad else other.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "\n",
        "        self_req_grad = self._custom_requires_grad\n",
        "        other_req_grad = other._custom_requires_grad\n",
        "        self_ref = weakref.proxy(self)\n",
        "        other_ref = weakref.proxy(other)\n",
        "        result_ref = weakref.proxy(result)\n",
        "\n",
        "        if self_req_grad:\n",
        "            graph.add_edge(self._node_id, result._node_id)\n",
        "        if other_req_grad:\n",
        "            graph.add_edge(other._node_id, result._node_id)\n",
        "\n",
        "        def _backward():\n",
        "            if result_ref:\n",
        "                if self_req_grad and self_ref:\n",
        "                    if self_ref.tensor.grad is None:\n",
        "                        self_ref._zero_grad()\n",
        "                    self_ref.tensor.grad.add_(result_ref.tensor.grad * other_ref.tensor)\n",
        "                if other_req_grad and other_ref:\n",
        "                    if other_ref.tensor.grad is None:\n",
        "                        other_ref._zero_grad()\n",
        "                    other_ref.tensor.grad.add_(result_ref.tensor.grad * self_ref.tensor)\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "\n",
        "    def __sub__(self, other):\n",
        "        if isinstance(other, numbers.Number):\n",
        "            return self._sub_scalar(other)\n",
        "        elif isinstance(other, CustomTensor):\n",
        "            return self._sub_tensor(other)\n",
        "        return NotImplemented\n",
        "\n",
        "    def _sub_scalar(self, scalar):\n",
        "        result_tensor = torch.sub(self.tensor, scalar)\n",
        "        if not self._custom_requires_grad:\n",
        "            return CustomTensor(result_tensor)\n",
        "\n",
        "        graph = self.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "        graph.add_edge(self._node_id, result._node_id)\n",
        "\n",
        "        self_ref = weakref.proxy(self)\n",
        "        result_ref = weakref.proxy(result)\n",
        "        def _backward():\n",
        "            if self_ref and result_ref:\n",
        "                if self_ref.tensor.grad is None:\n",
        "                    self_ref._zero_grad()\n",
        "                self_ref.tensor.grad.add_(result_ref.tensor.grad)\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "\n",
        "    def _sub_tensor(self, other):\n",
        "        result_tensor = torch.sub(self.tensor, other.tensor)\n",
        "        requires_grad = self._custom_requires_grad or other._custom_requires_grad\n",
        "        if not requires_grad:\n",
        "            return CustomTensor(result_tensor)\n",
        "\n",
        "        graph = self.graph if self._custom_requires_grad else other.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "\n",
        "        self_req_grad = self._custom_requires_grad\n",
        "        other_req_grad = other._custom_requires_grad\n",
        "        self_ref = weakref.proxy(self)\n",
        "        other_ref = weakref.proxy(other)\n",
        "        result_ref = weakref.proxy(result)\n",
        "\n",
        "        if self_req_grad:\n",
        "            graph.add_edge(self._node_id, result._node_id)\n",
        "        if other_req_grad:\n",
        "            graph.add_edge(other._node_id, result._node_id)\n",
        "\n",
        "        def _backward():\n",
        "            if result_ref:\n",
        "                if self_req_grad and self_ref:\n",
        "                    if self_ref.tensor.grad is None:\n",
        "                        self_ref._zero_grad()\n",
        "                    self_ref.tensor.grad.add_(result_ref.tensor.grad)\n",
        "                if other_req_grad and other_ref:\n",
        "                    if other_ref.tensor.grad is None:\n",
        "                        other_ref._zero_grad()\n",
        "                    other_ref.tensor.grad.sub_(result_ref.tensor.grad)\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "\n",
        "    def __truediv__(self, scalar):\n",
        "        return self._div_scalar(scalar)\n",
        "\n",
        "    def _div_scalar(self, scalar):\n",
        "        result_tensor = torch.div(self.tensor, scalar)\n",
        "        if not self._custom_requires_grad:\n",
        "            return CustomTensor(result_tensor)\n",
        "\n",
        "        graph = self.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "        graph.add_edge(self._node_id, result._node_id)\n",
        "\n",
        "        self_ref = weakref.proxy(self)\n",
        "        result_ref = weakref.proxy(result)\n",
        "        def _backward():\n",
        "            if self_ref and result_ref:\n",
        "                if self_ref.tensor.grad is None:\n",
        "                    self_ref._zero_grad()\n",
        "                self_ref.tensor.grad.add_(result_ref.tensor.grad / scalar)\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "\n",
        "    def pow(self, scalar):\n",
        "        result_tensor = torch.pow(self.tensor, scalar)\n",
        "        if not self._custom_requires_grad:\n",
        "            return CustomTensor(result_tensor)\n",
        "\n",
        "        graph = self.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "        graph.add_edge(self._node_id, result._node_id)\n",
        "\n",
        "        self_ref = weakref.proxy(self)\n",
        "        result_ref = weakref.proxy(result)\n",
        "        def _backward():\n",
        "            if self_ref and result_ref:\n",
        "                if self_ref.tensor.grad is None:\n",
        "                    self_ref._zero_grad()\n",
        "                grad_contrib = scalar * self_ref.tensor.pow(scalar - 1)\n",
        "                self_ref.tensor.grad.add_(result_ref.tensor.grad * grad_contrib)\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "\n",
        "    def exp(self):\n",
        "        out = torch.exp(self.tensor)\n",
        "        return self._unary_op(out, lambda grad, out_tensor: grad * out_tensor)\n",
        "\n",
        "    def log(self):\n",
        "        out = torch.log(self.tensor)\n",
        "        return self._unary_op(out, lambda grad, input_tensor: grad / input_tensor)\n",
        "\n",
        "    def sin(self):\n",
        "        out = torch.sin(self.tensor)\n",
        "        return self._unary_op(out, lambda grad, input_tensor: grad * torch.cos(input_tensor))\n",
        "\n",
        "    def cos(self):\n",
        "        out = torch.cos(self.tensor)\n",
        "        return self._unary_op(out, lambda grad, input_tensor: -grad * torch.sin(input_tensor))\n",
        "\n",
        "    def sqrt(self):\n",
        "        out = torch.sqrt(self.tensor)\n",
        "        return self._unary_op(out, lambda grad, out_tensor: grad * 0.5 / out_tensor)\n",
        "\n",
        "    def _unary_op(self, result_tensor, backward_fn):\n",
        "        if not self._custom_requires_grad:\n",
        "            return CustomTensor(result_tensor)\n",
        "\n",
        "        graph = self.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "        graph.add_edge(self._node_id, result._node_id)\n",
        "\n",
        "        self_ref = weakref.proxy(self)\n",
        "        result_ref = weakref.proxy(result)\n",
        "        def _backward():\n",
        "            if self_ref and result_ref:\n",
        "                if self_ref.tensor.grad is None:\n",
        "                    self_ref._zero_grad()\n",
        "                self_ref.tensor.grad.add_(backward_fn(result_ref.tensor.grad, self_ref.tensor))\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "\n",
        "    def matmul(self, other):\n",
        "        result_tensor = torch.matmul(self.tensor, other.tensor)\n",
        "        requires_grad = self._custom_requires_grad or other._custom_requires_grad\n",
        "        if not requires_grad:\n",
        "            return CustomTensor(result_tensor)\n",
        "\n",
        "        graph = self.graph if self._custom_requires_grad else other.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "\n",
        "        self_req_grad = self._custom_requires_grad\n",
        "        other_req_grad = other._custom_requires_grad\n",
        "        self_ref = weakref.proxy(self)\n",
        "        other_ref = weakref.proxy(other)\n",
        "        result_ref = weakref.proxy(result)\n",
        "\n",
        "        if self_req_grad:\n",
        "            graph.add_edge(self._node_id, result._node_id)\n",
        "        if other_req_grad:\n",
        "            graph.add_edge(other._node_id, result._node_id)\n",
        "\n",
        "        def _backward():\n",
        "            if result_ref:\n",
        "                if self_req_grad and self_ref:\n",
        "                    if self_ref.tensor.grad is None:\n",
        "                        self_ref._zero_grad()\n",
        "                    self_ref.tensor.grad.add_(torch.matmul(result_ref.tensor.grad, other_ref.tensor.t()))\n",
        "                if other_req_grad and other_ref:\n",
        "                    if other_ref.tensor.grad is None:\n",
        "                        other_ref._zero_grad()\n",
        "                    other_ref.tensor.grad.add_(torch.matmul(self_ref.tensor.t(), result_ref.tensor.grad))\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "\n",
        "    def apply_mask(self, mask):\n",
        "        result_tensor = self.tensor * mask.tensor\n",
        "        requires_grad = self._custom_requires_grad or mask._custom_requires_grad\n",
        "        if not requires_grad:\n",
        "            return CustomTensor(result_tensor)\n",
        "\n",
        "        graph = self.graph if self._custom_requires_grad else mask.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "\n",
        "        self_req_grad = self._custom_requires_grad\n",
        "        mask_req_grad = mask._custom_requires_grad\n",
        "        self_ref = weakref.proxy(self)\n",
        "        mask_ref = weakref.proxy(mask)\n",
        "        result_ref = weakref.proxy(result)\n",
        "\n",
        "        if self_req_grad:\n",
        "            graph.add_edge(self._node_id, result._node_id)\n",
        "        if mask_req_grad:\n",
        "            graph.add_edge(mask._node_id, result._node_id)\n",
        "\n",
        "        def _backward():\n",
        "            if result_ref:\n",
        "                if self_req_grad and self_ref:\n",
        "                    if self_ref.tensor.grad is None:\n",
        "                        self_ref._zero_grad()\n",
        "                    self_ref.tensor.grad.add_(result_ref.tensor.grad * mask_ref.tensor)\n",
        "                if mask_req_grad and mask_ref:\n",
        "                    if mask_ref.tensor.grad is None:\n",
        "                        mask_ref._zero_grad()\n",
        "                    mask_ref.tensor.grad.add_(result_ref.tensor.grad * self_ref.tensor)\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "\n",
        "    def dot(self, other):\n",
        "        result_tensor = torch.dot(self.tensor, other.tensor)\n",
        "        requires_grad = self._custom_requires_grad or other._custom_requires_grad\n",
        "        if not requires_grad:\n",
        "            return CustomTensor(result_tensor)\n",
        "\n",
        "        graph = self.graph if self._custom_requires_grad else other.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "\n",
        "        self_req_grad = self._custom_requires_grad\n",
        "        other_req_grad = other._custom_requires_grad\n",
        "        self_ref = weakref.proxy(self)\n",
        "        other_ref = weakref.proxy(other)\n",
        "        result_ref = weakref.proxy(result)\n",
        "\n",
        "        if self_req_grad:\n",
        "            graph.add_edge(self._node_id, result._node_id)\n",
        "        if other_req_grad:\n",
        "            graph.add_edge(other._node_id, result._node_id)\n",
        "\n",
        "        def _backward():\n",
        "            if result_ref:\n",
        "                if self_req_grad and self_ref:\n",
        "                    if self_ref.tensor.grad is None:\n",
        "                        self_ref._zero_grad()\n",
        "                    self_ref.tensor.grad.add_(result_ref.tensor.grad * other_ref.tensor)\n",
        "                if other_req_grad and other_ref:\n",
        "                    if other_ref.tensor.grad is None:\n",
        "                        other_ref._zero_grad()\n",
        "                    other_ref.tensor.grad.add_(result_ref.tensor.grad * self_ref.tensor)\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "\n",
        "    def backward(self, weightage_tensor=1):\n",
        "        if not self._custom_requires_grad:\n",
        "            raise RuntimeError(\"Output tensor does not require grad.\")\n",
        "        if self.graph is None:\n",
        "            raise RuntimeError(\"Output tensor is not part of a graph.\")\n",
        "        graph = self.graph\n",
        "\n",
        "        if isinstance(weightage_tensor, numbers.Number):\n",
        "            self.tensor.grad = torch.full_like(self.tensor, fill_value=weightage_tensor)\n",
        "        elif isinstance(weightage_tensor, torch.Tensor):\n",
        "            self.tensor.grad = weightage_tensor.clone()\n",
        "\n",
        "        nodes_to_process = graph.reverse_toposort_from_tensor(self._node_id)\n",
        "\n",
        "        for tensor_node in nodes_to_process:\n",
        "            try:\n",
        "                # This will raise a ReferenceError if the proxy is dead\n",
        "                if tensor_node.__class__ is weakref.ProxyType and tensor_node.__repr__() is None:\n",
        "                    continue\n",
        "                tensor_node._backward()\n",
        "            except ReferenceError:\n",
        "                # This catches dead weak references, allowing the process to continue\n",
        "                continue\n",
        "\n",
        "\n",
        "\n",
        "    def __del__(self):\n",
        "      if self._node_id is not None and self._is_leaf:\n",
        "          #self.graph.delete_node(self._node_id)\n",
        "          try:\n",
        "              # Check if graph is still alive before trying to delete\n",
        "              self.graph.delete_node(self._node_id)\n",
        "          except ReferenceError:\n",
        "              # Graph context has already been cleaned up, so do nothing.\n",
        "              pass\n",
        "      print(f\"Garbage Collector has decided that reference counts for node {self._node_id} are zero so Goodbye!!\")"
      ],
      "metadata": {
        "id": "4KZULXsEdEF3"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import gc\n",
        "class TestCustomAutogradSystem:\n",
        "\n",
        "    def test_basic_add_scalar_grad(self):\n",
        "        with AutogradGraph() as graph:\n",
        "            a = CustomTensor(torch.tensor([2.0, 3.0]), _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            b = a + 5.0 # (a + 5)\n",
        "            c = b + 10.0 # (a + 5 + 10)\n",
        "\n",
        "            # Manually run backward pass\n",
        "            c.backward(weightage_tensor=1)\n",
        "\n",
        "            # Expected gradients:\n",
        "            # dC/dA = 1.0 (for each element)\n",
        "            assert torch.allclose(a.tensor.grad, torch.tensor([1.0, 1.0]))\n",
        "            assert b.tensor.grad is not None\n",
        "            assert torch.allclose(b.tensor.grad, torch.tensor([1.0, 1.0])) # dC/dB = 1.0\n",
        "\n",
        "            # Verify graph structure\n",
        "            assert graph.graph.num_nodes() == 3\n",
        "            assert graph.graph.num_edges() == 2\n",
        "            assert graph.graph.has_edge(a._node_id, b._node_id)\n",
        "            assert graph.graph.has_edge(b._node_id, c._node_id)\n",
        "            assert graph.check_cycle() is False\n",
        "\n",
        "    def test_basic_add_tensor_grad(self):\n",
        "        with AutogradGraph() as graph:\n",
        "            a = CustomTensor(torch.tensor([2.0, 3.0]), _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            b = CustomTensor(torch.tensor([1.0, 2.0]), _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            c = a + b # (a + b)\n",
        "            d = c + 5.0 # (a + b + 5)\n",
        "\n",
        "            d.backward(weightage_tensor=1)\n",
        "\n",
        "            # Expected gradients:\n",
        "            # dD/dA = 1.0\n",
        "            # dD/dB = 1.0\n",
        "            assert torch.allclose(a.tensor.grad, torch.tensor([1.0, 1.0]))\n",
        "            assert torch.allclose(b.tensor.grad, torch.tensor([1.0, 1.0]))\n",
        "\n",
        "            # Verify graph structure\n",
        "            assert graph.graph.num_nodes() == 4\n",
        "            assert graph.graph.num_edges() == 3\n",
        "            assert graph.graph.has_edge(a._node_id, c._node_id)\n",
        "            assert graph.graph.has_edge(b._node_id, c._node_id)\n",
        "            assert graph.graph.has_edge(c._node_id, d._node_id)\n",
        "            assert graph.check_cycle() is False\n",
        "\n",
        "    def test_mixed_requires_grad_tensor_add(self):\n",
        "        with AutogradGraph() as graph:\n",
        "            a = CustomTensor(torch.tensor([2.0, 3.0]), _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            b = CustomTensor(torch.tensor([1.0, 2.0]), _custom_requires_grad=False) # Does not require grad\n",
        "            c = a + b # c should require grad, b's grad should be None\n",
        "\n",
        "            c.backward(weightage_tensor=1)\n",
        "\n",
        "            assert torch.allclose(a.tensor.grad, torch.tensor([1.0, 1.0]))\n",
        "            assert b.tensor.grad is None # b should not have a grad\n",
        "            assert c._custom_requires_grad is True\n",
        "\n",
        "            # Verify graph structure\n",
        "            assert graph.graph.num_nodes() == 2 # Only a and c in the graph\n",
        "            assert graph.graph.num_edges() == 1\n",
        "            assert graph.graph.has_node(a._node_id)\n",
        "            assert graph.graph.has_node(c._node_id)\n",
        "            assert graph.graph.has_edge(a._node_id, c._node_id)\n",
        "            #assert not graph.graph.has_node(b._node_id) # b should not be in graph\n",
        "\n",
        "    def test_no_requires_grad(self):\n",
        "        with AutogradGraph() as graph: # Graph created, but no tensors with requires_grad=True added\n",
        "            a = CustomTensor(torch.tensor([1.0]))\n",
        "            b = CustomTensor(torch.tensor([2.0]))\n",
        "            c = a + b\n",
        "            d = c + 3.0\n",
        "\n",
        "            assert not a._custom_requires_grad\n",
        "            assert not b._custom_requires_grad\n",
        "            assert not c._custom_requires_grad\n",
        "            assert not d._custom_requires_grad\n",
        "            assert graph.graph.num_nodes() == 0 # Graph should remain empty\n",
        "            assert graph.graph.num_edges() == 0\n",
        "\n",
        "            with pytest.raises(RuntimeError, match=\"Output tensor does not require grad.\"):\n",
        "                d.backward(weightage_tensor=1)\n",
        "\n",
        "    def test_autograd_graph_context_manager(self):\n",
        "        graph = None\n",
        "        with AutogradGraph(check_for_cycles=True, auto_cleanup=True) as g:\n",
        "            graph = g\n",
        "            a = CustomTensor(torch.tensor([1.0]), _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            b = a + 1.0\n",
        "            assert graph.graph.num_nodes() == 2\n",
        "            assert graph.graph.num_edges() == 1\n",
        "            assert len(graph.intermediate_tensors) == 1 # b should be in intermediate_tensors\n",
        "\n",
        "        # After exiting the context, graph should be empty\n",
        "        assert graph.graph.num_nodes() == 0\n",
        "        assert graph.graph.num_edges() == 0\n",
        "        assert len(graph.intermediate_tensors) == 0\n",
        "\n",
        "    def test_cycle_detection(self):\n",
        "      try:\n",
        "        with AutogradGraph(check_for_cycles=True, auto_cleanup=False) as graph: # auto_cleanup=False to inspect after error\n",
        "            a = CustomTensor(torch.tensor([1.0]), _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            b = CustomTensor(torch.tensor([2.0]), _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "\n",
        "            # Manually create a cycle (a -> b -> a)\n",
        "            graph.add_edge(a._node_id, b._node_id)\n",
        "            graph.add_edge(b._node_id, a._node_id)\n",
        "      except RuntimeError as e:\n",
        "        print(f\"Raised the error of cycle detected as {e}\")\n",
        "            # with pytest.raises(RuntimeError, match=\"Cycle detected in autograd graph on context exit.\"):\n",
        "            #     pass # The __exit__ method will be called here\n",
        "\n",
        "    def test_no_circular_references_non_leaf_tensors_die(self):\n",
        "          # This test relies on the garbage collector. It's a heuristic test\n",
        "        # as Python's GC timing is not strictly deterministic.\n",
        "        # However, with weakrefs, it should work for non-leaf tensors.\n",
        "\n",
        "      print(\"\\n--- Starting GC Test: No Circular References (Part 1) ---\")\n",
        "\n",
        "      graph_ref = None\n",
        "      output_tensor_weak_ref = None\n",
        "      node_id_d = -1 # To store node_id before d is deleted\n",
        "\n",
        "      # BLOCK 1: Create graph and tensors\n",
        "      with AutogradGraph(auto_cleanup=False) as graph: # Keep graph for inspection\n",
        "          graph_ref = weakref.ref(graph)\n",
        "          a = CustomTensor(torch.tensor([1.0]), _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "          b = a + 1.0 # Intermediate tensor\n",
        "          c = b + 2.0 # Intermediate tensor\n",
        "          d = c + 3.0 # Output tensor (also intermediate from graph's perspective)\n",
        "\n",
        "          # Store weak reference to 'd' BEFORE its strong reference is potentially removed\n",
        "          output_tensor_weak_ref = weakref.ref(d)\n",
        "          node_id_d = d._node_id # Store node_id while d is alive\n",
        "\n",
        "          print(f\"Initial: d object: {d}\")\n",
        "          print(f\"Initial: d._node_id: {node_id_d}\")\n",
        "          print(f\"Initial: graph.intermediate_tensors keys: {list(graph.intermediate_tensors.keys())}\")\n",
        "          # The ref count for `d` object itself will be high here because it's in `graph.intermediate_tensors`,\n",
        "          # and held by variable `d`, and by the temporary ref in `getrefcount`.\n",
        "          print(f\"Initial: refcount of d (via output_tensor_weak_ref.test_ref): {sys.getrefcount(output_tensor_weak_ref()) if output_tensor_weak_ref() else 'N/A'}\")\n",
        "          assert len(graph.intermediate_tensors) == 3 # b, c, d should be in intermediate_tensors\n",
        "\n",
        "      # BLOCK 2: After exiting context manager (auto_cleanup=False)\n",
        "      print(\"\\n--- After exiting 'with' block (auto_cleanup=False) ---\")\n",
        "      # The 'graph' variable still holds a strong reference to the AutogradGraph instance.\n",
        "      # graph_ref() should return the graph object.\n",
        "      assert graph_ref() is not None, \"Graph object should still be alive.\"\n",
        "      assert len(graph_ref().intermediate_tensors) == 3, \"Intermediate tensors should still be referenced by the graph.\"\n",
        "      print(f\"After 'with' block: d object (via weakref): {output_tensor_weak_ref()}\")\n",
        "      print(f\"After 'with' block: refcount of d (via output_tensor_weak_ref.test_ref): {sys.getrefcount(output_tensor_weak_ref())}\")\n",
        "\n",
        "      # BLOCK 3: Remove strong reference 'd' from local scope\n",
        "      print(\"\\n--- Deleting 'd' variable ---\")\n",
        "      del d # Remove the local strong reference to the CustomTensor object.\n",
        "      gc.collect() # Force garbage collection\n",
        "\n",
        "      # Now, output_tensor_weak_ref() *still* shouldn't be None because `graph_ref().intermediate_tensors`\n",
        "      # holds the strong reference.\n",
        "      print(f\"After del d + gc.collect(): d object (via weakref): {output_tensor_weak_ref()}\")\n",
        "      # We expect this to *not* be None yet, and to still show a refcount reflecting intermediate_tensors.\n",
        "      assert output_tensor_weak_ref() is not None, \"d should still be alive due to intermediate_tensors.\"\n",
        "      current_d_refcount_after_del_d = sys.getrefcount(output_tensor_weak_ref()) if output_tensor_weak_ref() else 'N/A'\n",
        "      print(f\"After del d + gc.collect(): refcount of d: {current_d_refcount_after_del_d}\")\n",
        "      # Expected refcount should be 2: one from intermediate_tensors, one from getrefcount()\n",
        "      assert current_d_refcount_after_del_d == 2, f\"Expected refcount 2, got {current_d_refcount_after_del_d}\"\n",
        "\n",
        "      # BLOCK 4: Remove strong reference from intermediate_tensors\n",
        "      print(f\"\\n--- Deleting strong reference from graph.intermediate_tensors for node {node_id_d} ---\")\n",
        "      graph_ref().del_non_leaf_tensor_reference(node_id_d) # THIS IS THE CRUCIAL STEP\n",
        "      print(f\"After del_non_leaf_tensor_reference: graph.intermediate_tensors keys: {list(graph_ref().intermediate_tensors.keys())}\")\n",
        "      #gc.collect() # Force garbage collection again\n",
        "\n",
        "      # Now, with the last strong reference gone, 'd' should be garbage collected.\n",
        "      print(f\"After del_non_leaf_tensor_reference + gc.collect(): d object (via weakref): {output_tensor_weak_ref()}\")\n",
        "      # This is where your original assertion was. It *should* pass now.\n",
        "      assert output_tensor_weak_ref() is None, \"Output tensor (non-leaf) should be garbage collected after its strong reference is deleted from intermediate_tensors.\"\n",
        "      print(\"Assertion Passed: Output tensor (d) was garbage collected.\")\n",
        "\n",
        "      # BLOCK 5: Verify other intermediate tensors are collected when graph is cleared\n",
        "      print(\"\\n--- Starting GC Test: All Intermediate Tensors ---\")\n",
        "      intermediate_tensors_wrefs = []\n",
        "      # Create a new graph and new tensors to avoid interference from previous block\n",
        "      with AutogradGraph(auto_cleanup=False) as graph_new:\n",
        "          a_new = CustomTensor(torch.tensor([1.0]), _custom_requires_grad=True, graph=graph_new, is_leaf=True)\n",
        "          b_new = a_new + 1.0 # Intermediate\n",
        "          c_new = b_new + 2.0 # Intermediate\n",
        "          d_new = c_new + 3.0 # Intermediate (output of a chain)\n",
        "\n",
        "          # Store weak references to the intermediate tensors\n",
        "          intermediate_tensors_wrefs.append(weakref.ref(b_new))\n",
        "          intermediate_tensors_wrefs.append(weakref.ref(c_new))\n",
        "          intermediate_tensors_wrefs.append(weakref.ref(d_new))\n",
        "\n",
        "          # Verify they are initially alive\n",
        "          assert all(wref() is not None for wref in intermediate_tensors_wrefs)\n",
        "          assert len(graph_new.intermediate_tensors) == 3\n",
        "\n",
        "      print(f\"After 'with' block (new graph): graph_new object: {graph_new}\")\n",
        "      assert graph_new is not None, \"New graph object should still be alive after 'with' block.\"\n",
        "      assert len(graph_new.intermediate_tensors) == 3, \"New graph intermediate_tensors should still hold refs.\"\n",
        "\n",
        "      # Manually clear the intermediate_tensors dictionary and remove graph reference\n",
        "      print(\"\\n--- Manually clearing graph.intermediate_tensors and deleting graph ---\")\n",
        "      graph_new.intermediate_tensors.clear()\n",
        "      del graph_new # Remove the strong reference to the graph itself\n",
        "      del b_new , c_new , d_new # deleting the local variable strong references\n",
        "      #gc.collect()\n",
        "\n",
        "      # Now, all non-leaf tensors should be garbage collected\n",
        "      for i, wref in enumerate(intermediate_tensors_wrefs):\n",
        "          print(f\"Intermediate tensor {i} (via weakref): {wref()}\")\n",
        "          assert wref() is None, f\"Intermediate tensor {i} should be garbage collected after graph context and intermediate_tensors are cleared.\"\n",
        "      print(\"Assertion Passed: All intermediate tensors were garbage collected.\")\n",
        "\n",
        "    def test_topological_sort_order(self):\n",
        "        with AutogradGraph() as graph:\n",
        "            t1 = CustomTensor(torch.tensor([1.0]), _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            t2 = CustomTensor(torch.tensor([2.0]), _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            t3 = t1 + t2\n",
        "            t4 = t3 + 5.0\n",
        "            t5 = t2 + 10.0 # Another branch\n",
        "            t6 = t4 + t5\n",
        "\n",
        "            # The topological sort should produce an order where dependencies come before their dependents.\n",
        "            # Reversed topological sort should produce an order where outputs come before their inputs.\n",
        "            # Example expected order: t6, t4, t5, t3, t2, t1 (or variations respecting dependencies)\n",
        "            sorted_tensors = graph.reverse_toposort()\n",
        "\n",
        "            # Check if dependencies are respected in reverse order\n",
        "            # If A -> B, then B should appear before A in reverse topological sort.\n",
        "            # t6 depends on t4, t5. So t6 should be before t4 and t5.\n",
        "            # t4 depends on t3. So t4 should be before t3.\n",
        "            # t5 depends on t2. So t5 should be before t2.\n",
        "            # t3 depends on t1, t2. So t3 should be before t1 and t2.\n",
        "\n",
        "            # Simple check: The first element should be t6 (the ultimate output).\n",
        "            assert sorted_tensors[0] is t6\n",
        "\n",
        "            # Check positions:\n",
        "            pos = {t: i for i, t in enumerate(sorted_tensors)}\n",
        "\n",
        "            assert pos[t6] < pos[t4]\n",
        "            assert pos[t6] < pos[t5]\n",
        "            assert pos[t4] < pos[t3]\n",
        "            assert pos[t5] < pos[t2]\n",
        "            assert pos[t3] < pos[t1]\n",
        "            assert pos[t3] < pos[t2] # t3 also depends on t2\n",
        "\n",
        "            # Additional check: t2 is a dependency for both t3 and t5.\n",
        "            # In reverse topo sort, t3 and t5 must appear before t2.\n",
        "            assert pos[t3] < pos[t2]\n",
        "            assert pos[t5] < pos[t2]\n",
        "\n",
        "            # t1 is only a dependency for t3.\n",
        "            assert pos[t3] < pos[t1]\n",
        "\n",
        "            # Check if all 6 tensors are in the sorted list\n",
        "            assert len(sorted_tensors) == 6\n",
        "            assert set(sorted_tensors) == {t1, t2, t3, t4, t5, t6}"
      ],
      "metadata": {
        "id": "e-C1iyCUeiVY"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import numbers\n",
        "import weakref\n",
        "import rustworkx as rx\n",
        "from typing import Optional, Any\n",
        "\n",
        "\n",
        "class AutogradTester:\n",
        "    \"\"\"Test suite to verify custom autograd against PyTorch's autograd\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.passed_tests = 0\n",
        "        self.failed_tests = 0\n",
        "        self.tolerance = 1e-6\n",
        "\n",
        "    def assert_tensors_close(self, custom_tensor, pytorch_tensor, test_name, check_grad=True):\n",
        "        \"\"\"Compare custom tensor with PyTorch tensor\"\"\"\n",
        "        try:\n",
        "            # Check values\n",
        "            np.testing.assert_allclose(\n",
        "                custom_tensor.tensor.detach().numpy(),\n",
        "                pytorch_tensor.detach().numpy(),\n",
        "                rtol=self.tolerance,\n",
        "                atol=self.tolerance\n",
        "            )\n",
        "\n",
        "            # Check gradients if requested\n",
        "            if check_grad and pytorch_tensor.grad is not None:\n",
        "                if custom_tensor.tensor.grad is None:\n",
        "                    raise AssertionError(f\"Custom tensor has no gradient in {test_name}\")\n",
        "\n",
        "                np.testing.assert_allclose(\n",
        "                    custom_tensor.tensor.grad.detach().numpy(),\n",
        "                    pytorch_tensor.grad.detach().numpy(),\n",
        "                    rtol=self.tolerance,\n",
        "                    atol=self.tolerance\n",
        "                )\n",
        "\n",
        "            print(f\" {test_name}\")\n",
        "            self.passed_tests += 1\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\" {test_name}: {str(e)}\")\n",
        "            self.failed_tests += 1\n",
        "\n",
        "    def test_basic_operations(self):\n",
        "        \"\"\"Test basic arithmetic operations\"\"\"\n",
        "        print(\"\\n=== Testing Basic Operations ===\")\n",
        "\n",
        "        with AutogradGraph() as graph:\n",
        "            # Test scalar addition\n",
        "            x_custom = CustomTensor([2.0, 3.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = x_custom + 5.0\n",
        "            y_custom.backward(torch.ones_like(y_custom.tensor))\n",
        "\n",
        "            x_pytorch = torch.tensor([2.0, 3.0], requires_grad=True)\n",
        "            y_pytorch = x_pytorch + 5.0\n",
        "            y_pytorch.backward(torch.ones_like(y_pytorch))\n",
        "\n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Scalar Addition\")\n",
        "            self.assert_tensors_close(y_custom, y_pytorch, \"Scalar Addition Result\", check_grad=False)\n",
        "\n",
        "        with AutogradGraph() as graph:\n",
        "            # Test tensor addition\n",
        "            x_custom = CustomTensor([1.0, 2.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = CustomTensor([3.0, 4.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            z_custom = x_custom + y_custom\n",
        "            z_custom.backward(torch.ones_like(z_custom.tensor))\n",
        "\n",
        "            x_pytorch = torch.tensor([1.0, 2.0], requires_grad=True)\n",
        "            y_pytorch = torch.tensor([3.0, 4.0], requires_grad=True)\n",
        "            z_pytorch = x_pytorch + y_pytorch\n",
        "            z_pytorch.backward(torch.ones_like(z_pytorch))\n",
        "\n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Tensor Addition - x\")\n",
        "            self.assert_tensors_close(y_custom, y_pytorch, \"Tensor Addition - y\")\n",
        "\n",
        "    def test_multiplication(self):\n",
        "        \"\"\"Test multiplication operations\"\"\"\n",
        "        print(\"\\n=== Testing Multiplication ===\")\n",
        "\n",
        "        with AutogradGraph() as graph:\n",
        "            # Test scalar multiplication\n",
        "            x_custom = CustomTensor([2.0, 3.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = x_custom * 4.0\n",
        "            y_custom.backward(torch.ones_like(y_custom.tensor))\n",
        "\n",
        "            x_pytorch = torch.tensor([2.0, 3.0], requires_grad=True)\n",
        "            y_pytorch = x_pytorch * 4.0\n",
        "            y_pytorch.backward(torch.ones_like(y_pytorch))\n",
        "\n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Scalar Multiplication\")\n",
        "\n",
        "        with AutogradGraph() as graph:\n",
        "            # Test tensor multiplication\n",
        "            x_custom = CustomTensor([2.0, 3.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = CustomTensor([4.0, 5.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            z_custom = x_custom * y_custom\n",
        "            z_custom.backward(torch.ones_like(z_custom.tensor))\n",
        "\n",
        "            x_pytorch = torch.tensor([2.0, 3.0], requires_grad=True)\n",
        "            y_pytorch = torch.tensor([4.0, 5.0], requires_grad=True)\n",
        "            z_pytorch = x_pytorch * y_pytorch\n",
        "            z_pytorch.backward(torch.ones_like(z_pytorch))\n",
        "\n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Tensor Multiplication - x\")\n",
        "            self.assert_tensors_close(y_custom, y_pytorch, \"Tensor Multiplication - y\")\n",
        "\n",
        "    def test_subtraction_division(self):\n",
        "        \"\"\"Test subtraction and division\"\"\"\n",
        "        print(\"\\n=== Testing Subtraction and Division ===\")\n",
        "\n",
        "        with AutogradGraph() as graph:\n",
        "            # Test subtraction\n",
        "            x_custom = CustomTensor([5.0, 6.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = x_custom - 2.0\n",
        "            y_custom.backward(torch.ones_like(y_custom.tensor))\n",
        "\n",
        "            x_pytorch = torch.tensor([5.0, 6.0], requires_grad=True)\n",
        "            y_pytorch = x_pytorch - 2.0\n",
        "            y_pytorch.backward(torch.ones_like(y_pytorch))\n",
        "\n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Scalar Subtraction\")\n",
        "\n",
        "        with AutogradGraph() as graph:\n",
        "            # Test division\n",
        "            x_custom = CustomTensor([8.0, 12.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = x_custom / 4.0\n",
        "            y_custom.backward(torch.ones_like(y_custom.tensor))\n",
        "\n",
        "            x_pytorch = torch.tensor([8.0, 12.0], requires_grad=True)\n",
        "            y_pytorch = x_pytorch / 4.0\n",
        "            y_pytorch.backward(torch.ones_like(y_pytorch))\n",
        "\n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Scalar Division\")\n",
        "\n",
        "    def test_power_function(self):\n",
        "        \"\"\"Test power operation\"\"\"\n",
        "        print(\"\\n=== Testing Power Function ===\")\n",
        "\n",
        "        with AutogradGraph() as graph:\n",
        "            x_custom = CustomTensor([2.0, 3.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = x_custom.pow(3.0)\n",
        "            y_custom.backward(torch.ones_like(y_custom.tensor))\n",
        "\n",
        "            x_pytorch = torch.tensor([2.0, 3.0], requires_grad=True)\n",
        "            y_pytorch = torch.pow(x_pytorch, 3.0)\n",
        "            y_pytorch.backward(torch.ones_like(y_pytorch))\n",
        "\n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Power Function\")\n",
        "\n",
        "    def test_unary_functions(self):\n",
        "        \"\"\"Test unary mathematical functions\"\"\"\n",
        "        print(\"\\n=== Testing Unary Functions ===\")\n",
        "\n",
        "        # Test exp\n",
        "        with AutogradGraph() as graph:\n",
        "            x_custom = CustomTensor([1.0, 2.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = x_custom.exp()\n",
        "            y_custom.backward(torch.ones_like(y_custom.tensor))\n",
        "\n",
        "            x_pytorch = torch.tensor([1.0, 2.0], requires_grad=True)\n",
        "            y_pytorch = torch.exp(x_pytorch)\n",
        "            y_pytorch.backward(torch.ones_like(y_pytorch))\n",
        "\n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Exponential Function\")\n",
        "\n",
        "        # Test log\n",
        "        with AutogradGraph() as graph:\n",
        "            x_custom = CustomTensor([1.0, 2.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = x_custom.log()\n",
        "            y_custom.backward(torch.ones_like(y_custom.tensor))\n",
        "\n",
        "            x_pytorch = torch.tensor([1.0, 2.0], requires_grad=True)\n",
        "            y_pytorch = torch.log(x_pytorch)\n",
        "            y_pytorch.backward(torch.ones_like(y_pytorch))\n",
        "\n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Logarithm Function\")\n",
        "\n",
        "        # Test sin\n",
        "        with AutogradGraph() as graph:\n",
        "            x_custom = CustomTensor([0.5, 1.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = x_custom.sin()\n",
        "            y_custom.backward(torch.ones_like(y_custom.tensor))\n",
        "\n",
        "            x_pytorch = torch.tensor([0.5, 1.0], requires_grad=True)\n",
        "            y_pytorch = torch.sin(x_pytorch)\n",
        "            y_pytorch.backward(torch.ones_like(y_pytorch))\n",
        "\n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Sine Function\")\n",
        "\n",
        "        # Test sqrt\n",
        "        with AutogradGraph() as graph:\n",
        "            x_custom = CustomTensor([4.0, 9.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = x_custom.sqrt()\n",
        "            y_custom.backward(torch.ones_like(y_custom.tensor))\n",
        "\n",
        "            x_pytorch = torch.tensor([4.0, 9.0], requires_grad=True)\n",
        "            y_pytorch = torch.sqrt(x_pytorch)\n",
        "            y_pytorch.backward(torch.ones_like(y_pytorch))\n",
        "\n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Square Root Function\")\n",
        "\n",
        "    def test_matrix_operations(self):\n",
        "        \"\"\"Test matrix operations\"\"\"\n",
        "        print(\"\\n=== Testing Matrix Operations ===\")\n",
        "\n",
        "        # Test matrix multiplication\n",
        "        with AutogradGraph() as graph:\n",
        "            x_custom = CustomTensor([[1.0, 2.0], [3.0, 4.0]], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = CustomTensor([[5.0, 6.0], [7.0, 8.0]], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            z_custom = x_custom.matmul(y_custom)\n",
        "            z_custom.backward(torch.ones_like(z_custom.tensor))\n",
        "\n",
        "            x_pytorch = torch.tensor([[1.0, 2.0], [3.0, 4.0]], requires_grad=True)\n",
        "            y_pytorch = torch.tensor([[5.0, 6.0], [7.0, 8.0]], requires_grad=True)\n",
        "            z_pytorch = torch.matmul(x_pytorch, y_pytorch)\n",
        "            z_pytorch.backward(torch.ones_like(z_pytorch))\n",
        "\n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Matrix Multiplication - x\")\n",
        "            self.assert_tensors_close(y_custom, y_pytorch, \"Matrix Multiplication - y\")\n",
        "\n",
        "        # Test dot product\n",
        "        with AutogradGraph() as graph:\n",
        "            x_custom = CustomTensor([1.0, 2.0, 3.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = CustomTensor([4.0, 5.0, 6.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            z_custom = x_custom.dot(y_custom)\n",
        "            z_custom.backward()\n",
        "\n",
        "            x_pytorch = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
        "            y_pytorch = torch.tensor([4.0, 5.0, 6.0], requires_grad=True)\n",
        "            z_pytorch = torch.dot(x_pytorch, y_pytorch)\n",
        "            z_pytorch.backward()\n",
        "\n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Dot Product - x\")\n",
        "            self.assert_tensors_close(y_custom, y_pytorch, \"Dot Product - y\")\n",
        "\n",
        "    def test_complex_chain(self):\n",
        "        \"\"\"Test complex computational chains\"\"\"\n",
        "        print(\"\\n=== Testing Complex Chains ===\")\n",
        "\n",
        "        with AutogradGraph() as graph:\n",
        "            # Test: z = (x + y) * (x - y) + x^2\n",
        "            x_custom = CustomTensor([3.0, 4.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = CustomTensor([1.0, 2.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "\n",
        "            sum_custom = x_custom + y_custom\n",
        "            diff_custom = x_custom - y_custom\n",
        "            prod_custom = sum_custom * diff_custom\n",
        "            x_squared_custom = x_custom.pow(2.0)\n",
        "            z_custom = prod_custom + x_squared_custom\n",
        "\n",
        "            z_custom.backward(torch.ones_like(z_custom.tensor))\n",
        "\n",
        "            x_pytorch = torch.tensor([3.0, 4.0], requires_grad=True)\n",
        "            y_pytorch = torch.tensor([1.0, 2.0], requires_grad=True)\n",
        "\n",
        "            sum_pytorch = x_pytorch + y_pytorch\n",
        "            diff_pytorch = x_pytorch - y_pytorch\n",
        "            prod_pytorch = sum_pytorch * diff_pytorch\n",
        "            x_squared_pytorch = torch.pow(x_pytorch, 2.0)\n",
        "            z_pytorch = prod_pytorch + x_squared_pytorch\n",
        "\n",
        "            z_pytorch.backward(torch.ones_like(z_pytorch))\n",
        "\n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Complex Chain - x\")\n",
        "            self.assert_tensors_close(y_custom, y_pytorch, \"Complex Chain - y\")\n",
        "\n",
        "    def test_mixed_operations(self):\n",
        "        \"\"\"Test mixing operations with and without gradients\"\"\"\n",
        "        print(\"\\n=== Testing Mixed Operations ===\")\n",
        "\n",
        "        with AutogradGraph() as graph:\n",
        "            # One tensor requires grad, other doesn't\n",
        "            x_custom = CustomTensor([2.0, 3.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = CustomTensor([4.0, 5.0])  # No grad\n",
        "            z_custom = x_custom * y_custom\n",
        "            z_custom.backward(torch.ones_like(z_custom.tensor))\n",
        "\n",
        "            x_pytorch = torch.tensor([2.0, 3.0], requires_grad=True)\n",
        "            y_pytorch = torch.tensor([4.0, 5.0])  # No grad\n",
        "            z_pytorch = x_pytorch * y_pytorch\n",
        "            z_pytorch.backward(torch.ones_like(z_pytorch))\n",
        "\n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Mixed Operations - x\")\n",
        "\n",
        "    def run_all_tests(self):\n",
        "        \"\"\"Run all tests\"\"\"\n",
        "        print(\"Running Custom Autograd Correctness Tests\")\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "        self.test_basic_operations()\n",
        "        self.test_multiplication()\n",
        "        self.test_subtraction_division()\n",
        "        self.test_power_function()\n",
        "        self.test_unary_functions()\n",
        "        self.test_matrix_operations()\n",
        "        self.test_complex_chain()\n",
        "        self.test_mixed_operations()\n",
        "\n",
        "        print(f\"\\n\" + \"=\" * 50)\n",
        "        print(f\"Test Results: {self.passed_tests} passed, {self.failed_tests} failed\")\n",
        "\n",
        "        if self.failed_tests == 0:\n",
        "            print(\" All tests passed! Your autograd implementation is correct.\")\n",
        "        else:\n",
        "            print(\" Some tests failed. Check the implementation.\")\n",
        "\n",
        "        return self.failed_tests == 0\n",
        "\n"
      ],
      "metadata": {
        "id": "OIZ8vtXZP4aI"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing"
      ],
      "metadata": {
        "id": "JHRsXXgQP1Wl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time"
      ],
      "metadata": {
        "id": "KUwZonZrXjtC"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import gc\n",
        "with AutogradGraph() as graph:\n",
        "  with torch.no_grad():\n",
        "    x=CustomTensor([1,2,3],_custom_requires_grad=True,is_leaf=True,graph=graph)\n",
        "    y=CustomTensor([4,5,6],_custom_requires_grad=True,is_leaf=True,graph=graph)\n",
        "    z=x+y\n",
        "    x=5+5\n",
        "    y=5+5\n",
        "    #z=None\n",
        "    #gc.collect()\n",
        "  # z.backward(torch.ones_like(z.tensor))\n",
        "\n",
        "# time.sleep(2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rs9Zf9-WVB5O",
        "outputId": "bc0ab2bd-6bcd-43c3-b160-aca8bd1f2aad"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Garbage Collector has decided that reference counts for node 0 are zero so Goodbye!!\n",
            "Garbage Collector has decided that reference counts for node 1 are zero so Goodbye!!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "import weakref\n",
        "\n",
        "with AutogradGraph() as graph:\n",
        "    with torch.no_grad():\n",
        "        x = CustomTensor([1,2,3], _custom_requires_grad=True, is_leaf=True, graph=graph)\n",
        "        y = CustomTensor([4,5,6], _custom_requires_grad=True, is_leaf=True, graph=graph)\n",
        "\n",
        "        # Create weak references to track them\n",
        "        x_weak = weakref.ref(x)\n",
        "        y_weak = weakref.ref(y)\n",
        "\n",
        "        z = x + y\n",
        "        x = 5 + 5\n",
        "        y = 5 + 5\n",
        "\n",
        "        print(f\"Before gc.collect(): x alive: {x_weak() is not None}, y alive: {y_weak() is not None}\")\n",
        "        gc.collect()\n",
        "        print(f\"After gc.collect(): x alive: {x_weak() is not None}, y alive: {y_weak() is not None}\")"
      ],
      "metadata": {
        "id": "eCI1rLvobHTe",
        "outputId": "16b85673-06c7-41db-bd30-8c6b25c857ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before gc.collect(): x alive: True, y alive: True\n",
            "After gc.collect(): x alive: True, y alive: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "firJxmtRbSzM",
        "outputId": "a56e7672-90fa-4932-ecda-20d9ebbda8ca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "z.tensor"
      ],
      "metadata": {
        "id": "OE0F_P3LX9aV",
        "outputId": "001e8dba-ae09-470c-d108-4ab0c17e3498",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([5, 7, 9])"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys"
      ],
      "metadata": {
        "id": "ce6EKyCmWW8V"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sys.getrefcount(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YVSjaC8iWUsc",
        "outputId": "3c02159c-d989-460a-cd07-0dc8e24fcf2d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del x,y"
      ],
      "metadata": {
        "id": "1hc2XlWlV9ja"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k=TestCustomAutogradSystem()\n",
        "k.test_basic_add_scalar_grad()\n",
        "k.test_basic_add_tensor_grad()\n",
        "k.test_mixed_requires_grad_tensor_add()\n",
        "k.test_no_requires_grad()\n",
        "k.test_autograd_graph_context_manager()\n",
        "k.test_cycle_detection()\n",
        "k.test_no_circular_references_non_leaf_tensors_die()\n",
        "k.test_cycle_detection()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZQjc8BCeiXf",
        "outputId": "6b0bf19b-85dd-4671-9f32-62bdba78c747"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Garbage Collector has decided that reference counts for node 0 are zero so Goodbye!!\n",
            "Garbage Collector has decided that reference counts for node 1 are zero so Goodbye!!\n",
            "Garbage Collector has decided that reference counts for node 2 are zero so Goodbye!!\n",
            "Garbage Collector has decided that reference counts for node 0 are zero so Goodbye!!\n",
            "Garbage Collector has decided that reference counts for node 1 are zero so Goodbye!!\n",
            "Garbage Collector has decided that reference counts for node 2 are zero so Goodbye!!\n",
            "Garbage Collector has decided that reference counts for node 3 are zero so Goodbye!!\n",
            "Garbage Collector has decided that reference counts for node 0 are zero so Goodbye!!\n",
            "Garbage Collector has decided that reference counts for node None are zero so Goodbye!!\n",
            "Garbage Collector has decided that reference counts for node 1 are zero so Goodbye!!\n",
            "Garbage Collector has decided that reference counts for node None are zero so Goodbye!!\n",
            "Garbage Collector has decided that reference counts for node None are zero so Goodbye!!\n",
            "Garbage Collector has decided that reference counts for node None are zero so Goodbye!!\n",
            "Garbage Collector has decided that reference counts for node None are zero so Goodbye!!\n",
            "Garbage Collector has decided that reference counts for node 0 are zero so Goodbye!!\n",
            "Garbage Collector has decided that reference counts for node 1 are zero so Goodbye!!\n",
            "Raised the error of cycle detected as Cycle detected in autograd graph on context exit.\n",
            "Garbage Collector has decided that reference counts for node 0 are zero so Goodbye!!\n",
            "Garbage Collector has decided that reference counts for node 1 are zero so Goodbye!!\n",
            "\n",
            "--- Starting GC Test: No Circular References (Part 1) ---\n",
            "Initial: d object: <__main__.CustomTensor object at 0x7ab8268e3ac0>\n",
            "Initial: d._node_id: 3\n",
            "Initial: graph.intermediate_tensors keys: [1, 2, 3]\n",
            "Initial: refcount of d (via output_tensor_weak_ref.test_ref): 3\n",
            "\n",
            "--- After exiting 'with' block (auto_cleanup=False) ---\n",
            "After 'with' block: d object (via weakref): <__main__.CustomTensor object at 0x7ab8268e3ac0>\n",
            "After 'with' block: refcount of d (via output_tensor_weak_ref.test_ref): 3\n",
            "\n",
            "--- Deleting 'd' variable ---\n",
            "After del d + gc.collect(): d object (via weakref): <__main__.CustomTensor object at 0x7ab8268e3ac0>\n",
            "After del d + gc.collect(): refcount of d: 2\n",
            "\n",
            "--- Deleting strong reference from graph.intermediate_tensors for node 3 ---\n",
            "Garbage Collector has decided that reference counts for node 3 are zero so Goodbye!!\n",
            "After del_non_leaf_tensor_reference: graph.intermediate_tensors keys: [1, 2]\n",
            "After del_non_leaf_tensor_reference + gc.collect(): d object (via weakref): None\n",
            "Assertion Passed: Output tensor (d) was garbage collected.\n",
            "\n",
            "--- Starting GC Test: All Intermediate Tensors ---\n",
            "After 'with' block (new graph): graph_new object: CustomAutogradGraph(nodes=4, edges=3)\n",
            "\n",
            "--- Manually clearing graph.intermediate_tensors and deleting graph ---\n",
            "Garbage Collector has decided that reference counts for node 1 are zero so Goodbye!!\n",
            "Garbage Collector has decided that reference counts for node 2 are zero so Goodbye!!\n",
            "Garbage Collector has decided that reference counts for node 3 are zero so Goodbye!!\n",
            "Intermediate tensor 0 (via weakref): None\n",
            "Intermediate tensor 1 (via weakref): None\n",
            "Intermediate tensor 2 (via weakref): None\n",
            "Assertion Passed: All intermediate tensors were garbage collected.\n",
            "Garbage Collector has decided that reference counts for node 0 are zero so Goodbye!!\n",
            "Garbage Collector has decided that reference counts for node 1 are zero so Goodbye!!\n",
            "Garbage Collector has decided that reference counts for node 2 are zero so Goodbye!!\n",
            "Garbage Collector has decided that reference counts for node 0 are zero so Goodbye!!\n",
            "Raised the error of cycle detected as Cycle detected in autograd graph on context exit.\n",
            "Garbage Collector has decided that reference counts for node 0 are zero so Goodbye!!\n",
            "Garbage Collector has decided that reference counts for node 1 are zero so Goodbye!!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Usage example:\n",
        "if __name__ == \"__main__\":\n",
        "    # Insert your AutogradGraph and CustomTensor classes here\n",
        "    # Then run the tests\n",
        "\n",
        "    tester = AutogradTester()\n",
        "    success = tester.run_all_tests()\n",
        "\n",
        "    # Additional manual verification\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"Manual Verification Example:\")\n",
        "\n",
        "    with AutogradGraph() as graph:\n",
        "        x = CustomTensor([1.0, 2.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "        y = CustomTensor([3.0, 4.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "\n",
        "        # Compute z = x^2 + 2*x*y + y^2 = (x + y)^2\n",
        "        z = x.pow(2.0) + (x * y * 2.0) + y.pow(2.0)\n",
        "        print(f\"z = {z.tensor}\")\n",
        "\n",
        "        # Backward pass\n",
        "        z.backward(torch.ones_like(z.tensor))\n",
        "\n",
        "        print(f\"dz/dx = {x.tensor.grad}\")  # Should be 2*(x + y)\n",
        "        print(f\"dz/dy = {y.tensor.grad}\")  # Should be 2*(x + y)\n",
        "\n",
        "        # Expected: dz/dx = dz/dy = 2*(x + y) = 2*[4, 6] = [8, 12]\n",
        "        expected_grad = 2 * (x.tensor + y.tensor)\n",
        "        print(f\"Expected gradient: {expected_grad}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65Z7gu56U2UZ",
        "outputId": "28bdfb2d-ed79-4339-948c-82d3cbde2215"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Custom Autograd Correctness Tests\n",
            "==================================================\n",
            "\n",
            "=== Testing Basic Operations ===\n",
            " Scalar Addition\n",
            " Scalar Addition Result\n",
            "Garbage Collector has decided that reference counts for node 0 are zero so Goodbye!!\n",
            "Garbage Collector has decided that reference counts for node 1 are zero so Goodbye!!\n",
            " Tensor Addition - x\n",
            " Tensor Addition - y\n",
            "Garbage Collector has decided that reference counts for node 0 are zero so Goodbye!!\n",
            "Garbage Collector has decided that reference counts for node 1 are zero so Goodbye!!\n",
            "Garbage Collector has decided that reference counts for node 2 are zero so Goodbye!!\n",
            "\n",
            "=== Testing Multiplication ===\n",
            " Scalar Multiplication\n",
            "Garbage Collector has decided that reference counts for node 0 are zero so Goodbye!!\n",
            "Garbage Collector has decided that reference counts for node 1 are zero so Goodbye!!\n",
            " Tensor Multiplication - x\n",
            " Tensor Multiplication - y\n",
            "Garbage Collector has decided that reference counts for node 0 are zero so Goodbye!!\n",
            "Garbage Collector has decided that reference counts for node 1 are zero so Goodbye!!\n",
            "Garbage Collector has decided that reference counts for node 2 are zero so Goodbye!!\n",
            "\n",
            "=== Testing Subtraction and Division ===\n",
            " Scalar Subtraction\n",
            "Garbage Collector has decided that reference counts for node 0 are zero so Goodbye!!\n",
            "Garbage Collector has decided that reference counts for node 1 are zero so Goodbye!!\n",
            " Scalar Division\n",
            "Garbage Collector has decided that reference counts for node 0 are zero so Goodbye!!\n",
            "Garbage Collector has decided that reference counts for node 1 are zero so Goodbye!!\n",
            "\n",
            "=== Testing Power Function ===\n",
            " Power Function\n",
            "Garbage Collector has decided that reference counts for node 0 are zero so Goodbye!!\n",
            "Garbage Collector has decided that reference counts for node 1 are zero so Goodbye!!\n",
            "\n",
            "=== Testing Unary Functions ===\n",
            " Exponential Function: \n",
            "Not equal to tolerance rtol=1e-06, atol=1e-06\n",
            "\n",
            "Mismatched elements: 2 / 2 (100%)\n",
            "Max absolute difference among violations: 5.389056\n",
            "Max relative difference among violations: 0.72932947\n",
            " ACTUAL: array([1., 2.], dtype=float32)\n",
            " DESIRED: array([2.718282, 7.389056], dtype=float32)\n",
            "Garbage Collector has decided that reference counts for node 0 are zero so Goodbye!!\n",
            "Garbage Collector has decided that reference counts for node 1 are zero so Goodbye!!\n",
            " Logarithm Function\n",
            "Garbage Collector has decided that reference counts for node 0 are zero so Goodbye!!\n",
            "Garbage Collector has decided that reference counts for node 1 are zero so Goodbye!!\n",
            " Sine Function\n",
            "Garbage Collector has decided that reference counts for node 0 are zero so Goodbye!!\n",
            "Garbage Collector has decided that reference counts for node 1 are zero so Goodbye!!\n",
            " Square Root Function: \n",
            "Not equal to tolerance rtol=1e-06, atol=1e-06\n",
            "\n",
            "Mismatched elements: 2 / 2 (100%)\n",
            "Max absolute difference among violations: 0.125\n",
            "Max relative difference among violations: 0.6666667\n",
            " ACTUAL: array([0.125   , 0.055556], dtype=float32)\n",
            " DESIRED: array([0.25    , 0.166667], dtype=float32)\n",
            "Garbage Collector has decided that reference counts for node 0 are zero so Goodbye!!\n",
            "Garbage Collector has decided that reference counts for node 1 are zero so Goodbye!!\n",
            "\n",
            "=== Testing Matrix Operations ===\n",
            " Matrix Multiplication - x\n",
            " Matrix Multiplication - y\n",
            "Garbage Collector has decided that reference counts for node 0 are zero so Goodbye!!\n",
            "Garbage Collector has decided that reference counts for node 1 are zero so Goodbye!!\n",
            "Garbage Collector has decided that reference counts for node 2 are zero so Goodbye!!\n",
            " Dot Product - x\n",
            " Dot Product - y\n",
            "Garbage Collector has decided that reference counts for node 0 are zero so Goodbye!!\n",
            "Garbage Collector has decided that reference counts for node 1 are zero so Goodbye!!\n",
            "Garbage Collector has decided that reference counts for node 2 are zero so Goodbye!!\n",
            "\n",
            "=== Testing Complex Chains ===\n",
            " Complex Chain - x\n",
            " Complex Chain - y\n",
            "Garbage Collector has decided that reference counts for node 0 are zero so Goodbye!!\n",
            "Garbage Collector has decided that reference counts for node 1 are zero so Goodbye!!\n",
            "Garbage Collector has decided that reference counts for node 2 are zero so Goodbye!!\n",
            "Garbage Collector has decided that reference counts for node 3 are zero so Goodbye!!\n",
            "Garbage Collector has decided that reference counts for node 4 are zero so Goodbye!!\n",
            "Garbage Collector has decided that reference counts for node 5 are zero so Goodbye!!\n",
            "Garbage Collector has decided that reference counts for node 6 are zero so Goodbye!!\n",
            "\n",
            "=== Testing Mixed Operations ===\n",
            " Mixed Operations - x\n",
            "Garbage Collector has decided that reference counts for node 0 are zero so Goodbye!!\n",
            "Garbage Collector has decided that reference counts for node None are zero so Goodbye!!\n",
            "Garbage Collector has decided that reference counts for node 1 are zero so Goodbye!!\n",
            "\n",
            "==================================================\n",
            "Test Results: 19 passed, 2 failed\n",
            " Some tests failed. Check the implementation.\n",
            "\n",
            "==================================================\n",
            "Manual Verification Example:\n",
            "z = tensor([16., 36.])\n",
            "dz/dx = tensor([ 8., 12.])\n",
            "dz/dy = tensor([ 8., 12.])\n",
            "Expected gradient: tensor([ 8., 12.])\n",
            "Garbage Collector has decided that reference counts for node 2 are zero so Goodbye!!\n",
            "Garbage Collector has decided that reference counts for node 3 are zero so Goodbye!!\n",
            "Garbage Collector has decided that reference counts for node 4 are zero so Goodbye!!\n",
            "Garbage Collector has decided that reference counts for node 5 are zero so Goodbye!!\n",
            "Garbage Collector has decided that reference counts for node 6 are zero so Goodbye!!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "imWxgz1YeibH"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor(1.0, requires_grad=True)\n",
        "y = torch.tensor(2.0, requires_grad=True)\n",
        "\n",
        "z = x + y       # z = x + y\n",
        "x = z * y       # x is reassigned (detached from old x)\n",
        "e = z + x + y   # e depends on new x (which is z*y)\n",
        "e.backward(retain_graph=True)    # Computes gradients for y (old x is gone)\n",
        "print(y.grad)\n",
        "e.backward()    # Accumulates gradients again (for y only)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LW0rXhH4eidu",
        "outputId": "e5b7d2e8-7cf7-4d79-cad1-018af20a1de0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(7.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y.grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R4SZxqe2j9l0",
        "outputId": "c01d9ad1-2355-4646-82cc-1ef341fb6e00"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(14.)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del x,y"
      ],
      "metadata": {
        "id": "O89VtsCLlCKp"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor(1.0, requires_grad=True)\n",
        "y = torch.tensor(2.0, requires_grad=True)\n",
        "\n",
        "for _ in range(2):\n",
        "    z = x + y       # z = x + y\n",
        "    x = z * y       # x is reassigned (detached)\n",
        "    e = z + x + y   # e depends on new x\n",
        "    e.backward(retain_graph=True)    # Gradients computed per iteration"
      ],
      "metadata": {
        "id": "hLw_lWSZkAeF"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y.grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1Cxa6LNkDb1",
        "outputId": "8088ee11-7c0b-4daf-c6bf-3a691d93666a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(34.)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Case 1: Multiple e.backward() calls\n",
        "x = torch.tensor(1.0, requires_grad=True)\n",
        "y = torch.tensor(2.0, requires_grad=True)\n",
        "\n",
        "z = x + y\n",
        "x_reassigned = z * y # This creates a new tensor, x now points to it\n",
        "e = z + x_reassigned + y\n",
        "\n",
        "e.backward(retain_graph=True)\n",
        "print(f\"Case 1 - First backward, y.grad: {y.grad.item()}\") # Should be 7.0\n",
        "e.backward()\n",
        "print(f\"Case 1 - Second backward, y.grad: {y.grad.item()}\") # Should be 14.0\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZtayD4V9lF9o",
        "outputId": "9e080519-7c94-4fef-ca80-f22cdde43afc"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Case 1 - First backward, y.grad: 7.0\n",
            "Case 1 - Second backward, y.grad: 14.0\n",
            "\n",
            "Loop Iteration 1\n",
            "y.grad after iteration 1: 7.0\n",
            "\n",
            "Loop Iteration 2\n",
            "y.grad after iteration 2: 34.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchviz\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P-t28Mdlus34",
        "outputId": "4678e7e1-c8d7-4d9a-f236-0c45ea8a0f61"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchviz\n",
            "  Downloading torchviz-0.0.3-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from torchviz) (2.6.0+cu124)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.11/dist-packages (from torchviz) (0.21)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->torchviz)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->torchviz)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->torchviz)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->torchviz)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->torchviz)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->torchviz)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->torchviz)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->torchviz)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->torchviz)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->torchviz)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->torchviz) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->torchviz) (3.0.2)\n",
            "Downloading torchviz-0.0.3-py3-none-any.whl (5.7 kB)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m61.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m58.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torchviz\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 torchviz-0.0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchviz import make_dot\n",
        "import torch"
      ],
      "metadata": {
        "id": "dx-fKu21uKSX"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del x,y"
      ],
      "metadata": {
        "id": "8LZe55EQu70P"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Reset gradients for comparison\n",
        "x = torch.tensor(1.0, requires_grad=True)\n",
        "y = torch.tensor(2.0, requires_grad=True)\n",
        "\n",
        "# Case 2: In a loop\n",
        "for i in range(2):\n",
        "    print(f\"\\nLoop Iteration {i+1}\")\n",
        "    # z depends on the *current* x and y\n",
        "    z = x + y\n",
        "    # x is reassigned to a *new* tensor resulting from z*y\n",
        "    x = z * y\n",
        "    e = z + x + y\n",
        "    print(f\"x: {x.item()}, y: {y.item()}, z: {z.item()}, e: {e.item()}\")\n",
        "    e.backward(retain_graph=True) # Retain graph for all but the last iteration if needed\n",
        "    make_dot(e, params={'x': x, 'y': y}).render(f\"autograd_graph_iter_{i}\", format=\"png\")\n",
        "    print(f\"y.grad after iteration {i+1}: {y.grad.item()}\")\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JqDSOpRjuHYO",
        "outputId": "f121fd5d-1cd3-401d-de80-34a74f6bc8a9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loop Iteration 1\n",
            "x: 6.0, y: 2.0, z: 3.0, e: 11.0\n",
            "y.grad after iteration 1: 7.0\n",
            "\n",
            "Loop Iteration 2\n",
            "x: 16.0, y: 2.0, z: 8.0, e: 26.0\n",
            "y.grad after iteration 2: 34.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor(1.0, requires_grad=True)\n",
        "y = torch.tensor(2.0, requires_grad=True)\n",
        "t1 =x+y\n",
        "t2=t1*y\n",
        "t3=t2+y\n",
        "t4= t3*y\n",
        "t5 = t4+t3\n",
        "t6 = t5+y\n",
        "e=t6\n",
        "e.backward()"
      ],
      "metadata": {
        "id": "XA2jmsOZut_R"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_nodes(node):\n",
        "    count = 0\n",
        "    if node is not None:\n",
        "        count += 1\n",
        "        for next_node, _ in node.next_functions:\n",
        "            count += count_nodes(next_node)\n",
        "    return count"
      ],
      "metadata": {
        "id": "7-xVvSspADxM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y.grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sXZNdO8P5XZF",
        "outputId": "dfc65696-e16a-49ed-a6bf-2073122e1494"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(27.)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4ps0jB5B7hvH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor(1.0, requires_grad=True)\n",
        "y = torch.tensor(2.0, requires_grad=True)\n",
        "z1=x+y\n",
        "x1 = z1*y\n",
        "e1 = z1+y+x1\n",
        "z2=x1+y\n",
        "x2 = z2*y\n",
        "e2=z2+x2+y\n",
        "print(e2.item())\n",
        "e2.backward()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dp7VRNJr5YCK",
        "outputId": "421347a6-7b9c-4a9f-a05a-1a19640a4b32"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "26.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "make_dot(e2, params={'x': x, 'y': y})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 675
        },
        "id": "92rWjw-R9Y8Y",
        "outputId": "0bcbfe55-e1f4-4722-fd63-b1266455269c"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.43.0 (0)\n -->\n<!-- Title: %3 Pages: 1 -->\n<svg width=\"266pt\" height=\"490pt\"\n viewBox=\"0.00 0.00 266.00 490.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 486)\">\n<title>%3</title>\n<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-486 262,-486 262,4 -4,4\"/>\n<!-- 139605310900368 -->\n<g id=\"node1\" class=\"node\">\n<title>139605310900368</title>\n<polygon fill=\"#caff70\" stroke=\"black\" points=\"240.5,-31 186.5,-31 186.5,0 240.5,0 240.5,-31\"/>\n<text text-anchor=\"middle\" x=\"213.5\" y=\"-7\" font-family=\"monospace\" font-size=\"10.00\"> ()</text>\n</g>\n<!-- 139605323122288 -->\n<g id=\"node2\" class=\"node\">\n<title>139605323122288</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"258,-86 169,-86 169,-67 258,-67 258,-86\"/>\n<text text-anchor=\"middle\" x=\"213.5\" y=\"-74\" font-family=\"monospace\" font-size=\"10.00\">AddBackward0</text>\n</g>\n<!-- 139605323122288&#45;&gt;139605310900368 -->\n<g id=\"edge15\" class=\"edge\">\n<title>139605323122288&#45;&gt;139605310900368</title>\n<path fill=\"none\" stroke=\"black\" d=\"M213.5,-66.79C213.5,-60.07 213.5,-50.4 213.5,-41.34\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"217,-41.19 213.5,-31.19 210,-41.19 217,-41.19\"/>\n</g>\n<!-- 139605323129488 -->\n<g id=\"node3\" class=\"node\">\n<title>139605323129488</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"222,-141 133,-141 133,-122 222,-122 222,-141\"/>\n<text text-anchor=\"middle\" x=\"177.5\" y=\"-129\" font-family=\"monospace\" font-size=\"10.00\">AddBackward0</text>\n</g>\n<!-- 139605323129488&#45;&gt;139605323122288 -->\n<g id=\"edge1\" class=\"edge\">\n<title>139605323129488&#45;&gt;139605323122288</title>\n<path fill=\"none\" stroke=\"black\" d=\"M183.44,-121.75C188.48,-114.34 195.84,-103.5 202.01,-94.41\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"204.94,-96.33 207.67,-86.09 199.15,-92.39 204.94,-96.33\"/>\n</g>\n<!-- 139605323124544 -->\n<g id=\"node4\" class=\"node\">\n<title>139605323124544</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"166,-251 77,-251 77,-232 166,-232 166,-251\"/>\n<text text-anchor=\"middle\" x=\"121.5\" y=\"-239\" font-family=\"monospace\" font-size=\"10.00\">AddBackward0</text>\n</g>\n<!-- 139605323124544&#45;&gt;139605323129488 -->\n<g id=\"edge2\" class=\"edge\">\n<title>139605323124544&#45;&gt;139605323129488</title>\n<path fill=\"none\" stroke=\"black\" d=\"M119.81,-231.74C117.78,-218.96 115.68,-194.8 124.5,-177 130.66,-164.55 142.07,-154.27 152.73,-146.74\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"154.93,-149.48 161.37,-141.07 151.09,-143.63 154.93,-149.48\"/>\n</g>\n<!-- 139605323136784 -->\n<g id=\"node11\" class=\"node\">\n<title>139605323136784</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"222,-196 133,-196 133,-177 222,-177 222,-196\"/>\n<text text-anchor=\"middle\" x=\"177.5\" y=\"-184\" font-family=\"monospace\" font-size=\"10.00\">MulBackward0</text>\n</g>\n<!-- 139605323124544&#45;&gt;139605323136784 -->\n<g id=\"edge12\" class=\"edge\">\n<title>139605323124544&#45;&gt;139605323136784</title>\n<path fill=\"none\" stroke=\"black\" d=\"M130.5,-231.98C138.69,-224.23 151.01,-212.58 160.97,-203.14\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"163.48,-205.59 168.34,-196.17 158.67,-200.5 163.48,-205.59\"/>\n</g>\n<!-- 139605323126032 -->\n<g id=\"node5\" class=\"node\">\n<title>139605323126032</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"137,-306 48,-306 48,-287 137,-287 137,-306\"/>\n<text text-anchor=\"middle\" x=\"92.5\" y=\"-294\" font-family=\"monospace\" font-size=\"10.00\">MulBackward0</text>\n</g>\n<!-- 139605323126032&#45;&gt;139605323124544 -->\n<g id=\"edge3\" class=\"edge\">\n<title>139605323126032&#45;&gt;139605323124544</title>\n<path fill=\"none\" stroke=\"black\" d=\"M97.29,-286.75C101.26,-279.49 107.03,-268.95 111.93,-259.98\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"115.07,-261.54 116.8,-251.09 108.93,-258.18 115.07,-261.54\"/>\n</g>\n<!-- 139605323135728 -->\n<g id=\"node6\" class=\"node\">\n<title>139605323135728</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"99,-361 10,-361 10,-342 99,-342 99,-361\"/>\n<text text-anchor=\"middle\" x=\"54.5\" y=\"-349\" font-family=\"monospace\" font-size=\"10.00\">AddBackward0</text>\n</g>\n<!-- 139605323135728&#45;&gt;139605323126032 -->\n<g id=\"edge4\" class=\"edge\">\n<title>139605323135728&#45;&gt;139605323126032</title>\n<path fill=\"none\" stroke=\"black\" d=\"M60.77,-341.75C66.09,-334.34 73.86,-323.5 80.38,-314.41\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"83.36,-316.26 86.34,-306.09 77.67,-312.18 83.36,-316.26\"/>\n</g>\n<!-- 139605323126752 -->\n<g id=\"node7\" class=\"node\">\n<title>139605323126752</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"101,-416 0,-416 0,-397 101,-397 101,-416\"/>\n<text text-anchor=\"middle\" x=\"50.5\" y=\"-404\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n</g>\n<!-- 139605323126752&#45;&gt;139605323135728 -->\n<g id=\"edge5\" class=\"edge\">\n<title>139605323126752&#45;&gt;139605323135728</title>\n<path fill=\"none\" stroke=\"black\" d=\"M51.16,-396.75C51.68,-389.8 52.44,-379.85 53.09,-371.13\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"56.59,-371.32 53.85,-361.09 49.61,-370.8 56.59,-371.32\"/>\n</g>\n<!-- 139605299770096 -->\n<g id=\"node8\" class=\"node\">\n<title>139605299770096</title>\n<polygon fill=\"lightblue\" stroke=\"black\" points=\"77.5,-482 23.5,-482 23.5,-452 77.5,-452 77.5,-482\"/>\n<text text-anchor=\"middle\" x=\"50.5\" y=\"-470\" font-family=\"monospace\" font-size=\"10.00\">x</text>\n<text text-anchor=\"middle\" x=\"50.5\" y=\"-459\" font-family=\"monospace\" font-size=\"10.00\"> ()</text>\n</g>\n<!-- 139605299770096&#45;&gt;139605323126752 -->\n<g id=\"edge6\" class=\"edge\">\n<title>139605299770096&#45;&gt;139605323126752</title>\n<path fill=\"none\" stroke=\"black\" d=\"M50.5,-451.84C50.5,-444.21 50.5,-434.7 50.5,-426.45\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"54,-426.27 50.5,-416.27 47,-426.27 54,-426.27\"/>\n</g>\n<!-- 139605323121424 -->\n<g id=\"node9\" class=\"node\">\n<title>139605323121424</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"220,-416 119,-416 119,-397 220,-397 220,-416\"/>\n<text text-anchor=\"middle\" x=\"169.5\" y=\"-404\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n</g>\n<!-- 139605323121424&#45;&gt;139605323122288 -->\n<g id=\"edge14\" class=\"edge\">\n<title>139605323121424&#45;&gt;139605323122288</title>\n<path fill=\"none\" stroke=\"black\" d=\"M182.51,-396.81C205.41,-380.35 250.5,-342.27 250.5,-297.5 250.5,-297.5 250.5,-297.5 250.5,-185.5 250.5,-152.12 234.48,-115.91 223.52,-95.04\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"226.51,-93.21 218.64,-86.12 220.37,-96.57 226.51,-93.21\"/>\n</g>\n<!-- 139605323121424&#45;&gt;139605323124544 -->\n<g id=\"edge10\" class=\"edge\">\n<title>139605323121424&#45;&gt;139605323124544</title>\n<path fill=\"none\" stroke=\"black\" d=\"M168.64,-396.93C166.46,-377.1 159.86,-326.67 145.5,-287 142.1,-277.61 136.99,-267.78 132.35,-259.75\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"135.24,-257.77 127.08,-251.01 129.24,-261.38 135.24,-257.77\"/>\n</g>\n<!-- 139605323121424&#45;&gt;139605323126032 -->\n<g id=\"edge9\" class=\"edge\">\n<title>139605323121424&#45;&gt;139605323126032</title>\n<path fill=\"none\" stroke=\"black\" d=\"M163.2,-396.66C150.39,-378.69 121.12,-337.64 104.54,-314.38\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"107.33,-312.27 98.67,-306.16 101.63,-316.33 107.33,-312.27\"/>\n</g>\n<!-- 139605323121424&#45;&gt;139605323135728 -->\n<g id=\"edge7\" class=\"edge\">\n<title>139605323121424&#45;&gt;139605323135728</title>\n<path fill=\"none\" stroke=\"black\" d=\"M151.02,-396.98C132.52,-388.46 103.82,-375.23 82.52,-365.41\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"83.86,-362.18 73.31,-361.17 80.93,-368.53 83.86,-362.18\"/>\n</g>\n<!-- 139605323121424&#45;&gt;139605323136784 -->\n<g id=\"edge13\" class=\"edge\">\n<title>139605323121424&#45;&gt;139605323136784</title>\n<path fill=\"none\" stroke=\"black\" d=\"M171.02,-396.67C172.48,-387.71 174.59,-373.44 175.5,-361 179.6,-304.81 178.77,-238.12 178.03,-206.26\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"181.53,-206.09 177.78,-196.18 174.53,-206.27 181.53,-206.09\"/>\n</g>\n<!-- 139605299577680 -->\n<g id=\"node10\" class=\"node\">\n<title>139605299577680</title>\n<polygon fill=\"lightblue\" stroke=\"black\" points=\"196.5,-482 142.5,-482 142.5,-452 196.5,-452 196.5,-482\"/>\n<text text-anchor=\"middle\" x=\"169.5\" y=\"-470\" font-family=\"monospace\" font-size=\"10.00\">y</text>\n<text text-anchor=\"middle\" x=\"169.5\" y=\"-459\" font-family=\"monospace\" font-size=\"10.00\"> ()</text>\n</g>\n<!-- 139605299577680&#45;&gt;139605323121424 -->\n<g id=\"edge8\" class=\"edge\">\n<title>139605299577680&#45;&gt;139605323121424</title>\n<path fill=\"none\" stroke=\"black\" d=\"M169.5,-451.84C169.5,-444.21 169.5,-434.7 169.5,-426.45\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"173,-426.27 169.5,-416.27 166,-426.27 173,-426.27\"/>\n</g>\n<!-- 139605323136784&#45;&gt;139605323129488 -->\n<g id=\"edge11\" class=\"edge\">\n<title>139605323136784&#45;&gt;139605323129488</title>\n<path fill=\"none\" stroke=\"black\" d=\"M177.5,-176.75C177.5,-169.8 177.5,-159.85 177.5,-151.13\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"181,-151.09 177.5,-141.09 174,-151.09 181,-151.09\"/>\n</g>\n</g>\n</svg>\n",
            "text/plain": [
              "<graphviz.graphs.Digraph at 0x7ef864510550>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y.grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uIu-n3ON5vIX",
        "outputId": "fa280073-d988-4b06-f515-6bc9b7e708a7"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(27.)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AOlcGJUR5vvW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aa28ec4f"
      },
      "source": [
        "%reset -f"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Reset gradients for comparison\n",
        "x = torch.tensor(1.0, requires_grad=True)\n",
        "y = torch.tensor(2.0, requires_grad=True)\n",
        "\n",
        "# Case 2: In a loop\n",
        "for i in range(2):\n",
        "    print(f\"\\nLoop Iteration {i+1}\")\n",
        "    # z depends on the *current* x and y\n",
        "    z = x + y\n",
        "    # x is reassigned to a *new* tensor resulting from z*y\n",
        "    x = z * y\n",
        "    e = z + x + y\n",
        "    print(f\"x: {x.item()}, y: {y.item()}, z: {z.item()}, e: {e.item()}\")\n",
        "    #e.backward(retain_graph=True) # Retain graph for all but the last iteration if needed\n",
        "    #make_dot(e, params={'x': x, 'y': y}).render(f\"autograd_graph_iter_{i}\", format=\"png\")\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ViHSdKNH8ad2",
        "outputId": "109f5301-2ec6-4931-d674-50c46cc78ef8"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loop Iteration 1\n",
            "x: 6.0, y: 2.0, z: 3.0, e: 11.0\n",
            "\n",
            "Loop Iteration 2\n",
            "x: 16.0, y: 2.0, z: 8.0, e: 26.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if False:\n",
        "  print(\"478920\")"
      ],
      "metadata": {
        "id": "E4ctsaSYNZyM"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "e.backward()"
      ],
      "metadata": {
        "id": "jpUs0TwX8cRo"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchviz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VNgJnqZQDcCC",
        "outputId": "ae934d2b-2526-44d4-f6fa-b3e114ef8e9f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchviz\n",
            "  Downloading torchviz-0.0.3-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from torchviz) (2.6.0+cu124)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.11/dist-packages (from torchviz) (0.21)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->torchviz)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->torchviz)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->torchviz)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->torchviz)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->torchviz)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->torchviz)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->torchviz)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->torchviz)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->torchviz)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->torchviz)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->torchviz) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->torchviz) (3.0.2)\n",
            "Downloading torchviz-0.0.3-py3-none-any.whl (5.7 kB)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m88.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m75.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m40.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m669.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m83.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torchviz\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 torchviz-0.0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%reset -f\n",
        "import torch\n",
        "import torch.autograd.graph as ag\n",
        "from torchviz import make_dot"
      ],
      "metadata": {
        "id": "zc1BeRM8BDjZ"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ag.save_on_cpu(True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-cYSBoZyBB3-",
        "outputId": "24be449c-5f25-489f-c60d-c8bd9c7d50c4"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.autograd.graph.save_on_cpu at 0x7ef8559fccd0>"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor(1.0, requires_grad=True)\n",
        "y = torch.tensor(2.0, requires_grad=True)\n",
        "z1=x+y\n",
        "x1 = z1*y\n",
        "e1 = z1+y+x1\n",
        "z2=x1+y\n",
        "x2 = z2*y\n",
        "e2=z2+x2+y"
      ],
      "metadata": {
        "id": "t8SvlgGAEXt9"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "x = torch.tensor(1.0, requires_grad=True)\n",
        "y = torch.tensor(2.0, requires_grad=True)\n",
        "t1 =x+y\n",
        "t2=t1*y\n",
        "t3=t2+y\n",
        "t4= t3*y\n",
        "t5 = t4+t3\n",
        "t6 = t5+y\n",
        "e=t6\n",
        "# e.backward()"
      ],
      "metadata": {
        "id": "GaIkxkZyAdFi"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_nodes(node):\n",
        "    count = 0\n",
        "    if node is not None:\n",
        "        count += 1\n",
        "        for next_node, _ in node.next_functions:\n",
        "            count += count_nodes(next_node)\n",
        "    return count"
      ],
      "metadata": {
        "id": "j2O0juHr8lkE"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count_nodes(e2.grad_fn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FDi9WHEnAfPi",
        "outputId": "7d9fded0-5311-4790-9f7a-79bb3e82a797"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "19"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "make_dot(e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 677
        },
        "id": "-jKS2tszDHTg",
        "outputId": "7413eeda-0062-4ab0-a021-1f09b1d7cd68"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.43.0 (0)\n -->\n<!-- Title: %3 Pages: 1 -->\n<svg width=\"266pt\" height=\"491pt\"\n viewBox=\"0.00 0.00 266.00 491.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 487)\">\n<title>%3</title>\n<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-487 262,-487 262,4 -4,4\"/>\n<!-- 136215381822832 -->\n<g id=\"node1\" class=\"node\">\n<title>136215381822832</title>\n<polygon fill=\"#caff70\" stroke=\"black\" points=\"240.5,-31 186.5,-31 186.5,0 240.5,0 240.5,-31\"/>\n<text text-anchor=\"middle\" x=\"213.5\" y=\"-7\" font-family=\"monospace\" font-size=\"10.00\"> ()</text>\n</g>\n<!-- 136215381967200 -->\n<g id=\"node2\" class=\"node\">\n<title>136215381967200</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"258,-86 169,-86 169,-67 258,-67 258,-86\"/>\n<text text-anchor=\"middle\" x=\"213.5\" y=\"-74\" font-family=\"monospace\" font-size=\"10.00\">AddBackward0</text>\n</g>\n<!-- 136215381967200&#45;&gt;136215381822832 -->\n<g id=\"edge15\" class=\"edge\">\n<title>136215381967200&#45;&gt;136215381822832</title>\n<path fill=\"none\" stroke=\"black\" d=\"M213.5,-66.79C213.5,-60.07 213.5,-50.4 213.5,-41.34\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"217,-41.19 213.5,-31.19 210,-41.19 217,-41.19\"/>\n</g>\n<!-- 136215381967008 -->\n<g id=\"node3\" class=\"node\">\n<title>136215381967008</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"222,-141 133,-141 133,-122 222,-122 222,-141\"/>\n<text text-anchor=\"middle\" x=\"177.5\" y=\"-129\" font-family=\"monospace\" font-size=\"10.00\">AddBackward0</text>\n</g>\n<!-- 136215381967008&#45;&gt;136215381967200 -->\n<g id=\"edge1\" class=\"edge\">\n<title>136215381967008&#45;&gt;136215381967200</title>\n<path fill=\"none\" stroke=\"black\" d=\"M183.44,-121.75C188.48,-114.34 195.84,-103.5 202.01,-94.41\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"204.94,-96.33 207.67,-86.09 199.15,-92.39 204.94,-96.33\"/>\n</g>\n<!-- 136215381967296 -->\n<g id=\"node4\" class=\"node\">\n<title>136215381967296</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"222,-196 133,-196 133,-177 222,-177 222,-196\"/>\n<text text-anchor=\"middle\" x=\"177.5\" y=\"-184\" font-family=\"monospace\" font-size=\"10.00\">MulBackward0</text>\n</g>\n<!-- 136215381967296&#45;&gt;136215381967008 -->\n<g id=\"edge2\" class=\"edge\">\n<title>136215381967296&#45;&gt;136215381967008</title>\n<path fill=\"none\" stroke=\"black\" d=\"M177.5,-176.75C177.5,-169.8 177.5,-159.85 177.5,-151.13\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"181,-151.09 177.5,-141.09 174,-151.09 181,-151.09\"/>\n</g>\n<!-- 136215381967056 -->\n<g id=\"node5\" class=\"node\">\n<title>136215381967056</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"166,-251 77,-251 77,-232 166,-232 166,-251\"/>\n<text text-anchor=\"middle\" x=\"121.5\" y=\"-239\" font-family=\"monospace\" font-size=\"10.00\">AddBackward0</text>\n</g>\n<!-- 136215381967056&#45;&gt;136215381967008 -->\n<g id=\"edge13\" class=\"edge\">\n<title>136215381967056&#45;&gt;136215381967008</title>\n<path fill=\"none\" stroke=\"black\" d=\"M119.81,-231.74C117.78,-218.96 115.68,-194.8 124.5,-177 130.66,-164.55 142.07,-154.27 152.73,-146.74\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"154.93,-149.48 161.37,-141.07 151.09,-143.63 154.93,-149.48\"/>\n</g>\n<!-- 136215381967056&#45;&gt;136215381967296 -->\n<g id=\"edge3\" class=\"edge\">\n<title>136215381967056&#45;&gt;136215381967296</title>\n<path fill=\"none\" stroke=\"black\" d=\"M130.5,-231.98C138.69,-224.23 151.01,-212.58 160.97,-203.14\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"163.48,-205.59 168.34,-196.17 158.67,-200.5 163.48,-205.59\"/>\n</g>\n<!-- 136215381972384 -->\n<g id=\"node6\" class=\"node\">\n<title>136215381972384</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"137,-306 48,-306 48,-287 137,-287 137,-306\"/>\n<text text-anchor=\"middle\" x=\"92.5\" y=\"-294\" font-family=\"monospace\" font-size=\"10.00\">MulBackward0</text>\n</g>\n<!-- 136215381972384&#45;&gt;136215381967056 -->\n<g id=\"edge4\" class=\"edge\">\n<title>136215381972384&#45;&gt;136215381967056</title>\n<path fill=\"none\" stroke=\"black\" d=\"M97.29,-286.75C101.26,-279.49 107.03,-268.95 111.93,-259.98\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"115.07,-261.54 116.8,-251.09 108.93,-258.18 115.07,-261.54\"/>\n</g>\n<!-- 136215381979200 -->\n<g id=\"node7\" class=\"node\">\n<title>136215381979200</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"99,-361 10,-361 10,-342 99,-342 99,-361\"/>\n<text text-anchor=\"middle\" x=\"54.5\" y=\"-349\" font-family=\"monospace\" font-size=\"10.00\">AddBackward0</text>\n</g>\n<!-- 136215381979200&#45;&gt;136215381972384 -->\n<g id=\"edge5\" class=\"edge\">\n<title>136215381979200&#45;&gt;136215381972384</title>\n<path fill=\"none\" stroke=\"black\" d=\"M60.77,-341.75C66.09,-334.34 73.86,-323.5 80.38,-314.41\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"83.36,-316.26 86.34,-306.09 77.67,-312.18 83.36,-316.26\"/>\n</g>\n<!-- 136215381982464 -->\n<g id=\"node8\" class=\"node\">\n<title>136215381982464</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"101,-416 0,-416 0,-397 101,-397 101,-416\"/>\n<text text-anchor=\"middle\" x=\"50.5\" y=\"-404\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n</g>\n<!-- 136215381982464&#45;&gt;136215381979200 -->\n<g id=\"edge6\" class=\"edge\">\n<title>136215381982464&#45;&gt;136215381979200</title>\n<path fill=\"none\" stroke=\"black\" d=\"M51.16,-396.75C51.68,-389.8 52.44,-379.85 53.09,-371.13\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"56.59,-371.32 53.85,-361.09 49.61,-370.8 56.59,-371.32\"/>\n</g>\n<!-- 136215381822256 -->\n<g id=\"node9\" class=\"node\">\n<title>136215381822256</title>\n<polygon fill=\"lightblue\" stroke=\"black\" points=\"77.5,-483 23.5,-483 23.5,-452 77.5,-452 77.5,-483\"/>\n<text text-anchor=\"middle\" x=\"50.5\" y=\"-459\" font-family=\"monospace\" font-size=\"10.00\"> ()</text>\n</g>\n<!-- 136215381822256&#45;&gt;136215381982464 -->\n<g id=\"edge7\" class=\"edge\">\n<title>136215381822256&#45;&gt;136215381982464</title>\n<path fill=\"none\" stroke=\"black\" d=\"M50.5,-451.92C50.5,-444.22 50.5,-434.69 50.5,-426.43\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"54,-426.25 50.5,-416.25 47,-426.25 54,-426.25\"/>\n</g>\n<!-- 136215381966912 -->\n<g id=\"node10\" class=\"node\">\n<title>136215381966912</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"220,-416 119,-416 119,-397 220,-397 220,-416\"/>\n<text text-anchor=\"middle\" x=\"169.5\" y=\"-404\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n</g>\n<!-- 136215381966912&#45;&gt;136215381967200 -->\n<g id=\"edge14\" class=\"edge\">\n<title>136215381966912&#45;&gt;136215381967200</title>\n<path fill=\"none\" stroke=\"black\" d=\"M182.51,-396.81C205.41,-380.35 250.5,-342.27 250.5,-297.5 250.5,-297.5 250.5,-297.5 250.5,-185.5 250.5,-152.12 234.48,-115.91 223.52,-95.04\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"226.51,-93.21 218.64,-86.12 220.37,-96.57 226.51,-93.21\"/>\n</g>\n<!-- 136215381966912&#45;&gt;136215381967296 -->\n<g id=\"edge12\" class=\"edge\">\n<title>136215381966912&#45;&gt;136215381967296</title>\n<path fill=\"none\" stroke=\"black\" d=\"M171.02,-396.67C172.48,-387.71 174.59,-373.44 175.5,-361 179.6,-304.81 178.77,-238.12 178.03,-206.26\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"181.53,-206.09 177.78,-196.18 174.53,-206.27 181.53,-206.09\"/>\n</g>\n<!-- 136215381966912&#45;&gt;136215381967056 -->\n<g id=\"edge11\" class=\"edge\">\n<title>136215381966912&#45;&gt;136215381967056</title>\n<path fill=\"none\" stroke=\"black\" d=\"M168.64,-396.93C166.46,-377.1 159.86,-326.67 145.5,-287 142.1,-277.61 136.99,-267.78 132.35,-259.75\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"135.24,-257.77 127.08,-251.01 129.24,-261.38 135.24,-257.77\"/>\n</g>\n<!-- 136215381966912&#45;&gt;136215381972384 -->\n<g id=\"edge10\" class=\"edge\">\n<title>136215381966912&#45;&gt;136215381972384</title>\n<path fill=\"none\" stroke=\"black\" d=\"M163.2,-396.66C150.39,-378.69 121.12,-337.64 104.54,-314.38\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"107.33,-312.27 98.67,-306.16 101.63,-316.33 107.33,-312.27\"/>\n</g>\n<!-- 136215381966912&#45;&gt;136215381979200 -->\n<g id=\"edge8\" class=\"edge\">\n<title>136215381966912&#45;&gt;136215381979200</title>\n<path fill=\"none\" stroke=\"black\" d=\"M151.02,-396.98C132.52,-388.46 103.82,-375.23 82.52,-365.41\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"83.86,-362.18 73.31,-361.17 80.93,-368.53 83.86,-362.18\"/>\n</g>\n<!-- 136215381822160 -->\n<g id=\"node11\" class=\"node\">\n<title>136215381822160</title>\n<polygon fill=\"lightblue\" stroke=\"black\" points=\"196.5,-483 142.5,-483 142.5,-452 196.5,-452 196.5,-483\"/>\n<text text-anchor=\"middle\" x=\"169.5\" y=\"-459\" font-family=\"monospace\" font-size=\"10.00\"> ()</text>\n</g>\n<!-- 136215381822160&#45;&gt;136215381966912 -->\n<g id=\"edge9\" class=\"edge\">\n<title>136215381822160&#45;&gt;136215381966912</title>\n<path fill=\"none\" stroke=\"black\" d=\"M169.5,-451.92C169.5,-444.22 169.5,-434.69 169.5,-426.43\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"173,-426.25 169.5,-416.25 166,-426.25 173,-426.25\"/>\n</g>\n</g>\n</svg>\n",
            "text/plain": [
              "<graphviz.graphs.Digraph at 0x7be31ced7490>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ag.save_on_cpu(False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-zsDN5jMBGvf",
        "outputId": "d76bcbeb-e22d-4531-d946-666bcf4c2e6f"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.autograd.graph.save_on_cpu at 0x7ef855a0c490>"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.autograd.graph as ag\n",
        "\n",
        "# Enable graph debugging\n",
        "ag.save_on_cpu(True) # This saves data needed for visualization on CPU\n",
        "\n",
        "x = torch.tensor(1.0, requires_grad=True)\n",
        "y = torch.tensor(2.0, requires_grad=True)\n",
        "\n",
        "z1 = x + y\n",
        "x1 = z1 * y\n",
        "e1 = z1 + y + x1 # Let's stop here for a simpler graph to visualize.\n",
        "\n",
        "# You can access the grad_fn of the final tensor\n",
        "print(e1.grad_fn)\n",
        "\n",
        "# To visualize the graph (requires graphviz installed and dot in PATH)\n",
        "# dot_graph = ag.make_dot(e1, params={'x': x, 'y': y})\n",
        "# dot_graph.render(\"computation_graph\", view=True) # Saves as PDF and opens\n",
        "\n",
        "# You can also iterate through the graph (less straightforward for simple count)\n",
        "def count_nodes(node):\n",
        "    count = 0\n",
        "    if node is not None:\n",
        "        count += 1\n",
        "        for next_node, _ in node.next_functions:\n",
        "            count += count_nodes(next_node)\n",
        "    return count\n",
        "\n",
        "# This counts the grad_fn nodes, not the tensors directly\n",
        "print(f\"Number of grad_fn nodes: {count_nodes(e1.grad_fn)}\")\n",
        "\n",
        "ag.save_on_cpu(False) # Disable debugging after use"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KgCRpLcW8m21",
        "outputId": "c62b3b6d-68fe-4a3d-dffd-b9c2350ea5fb"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<AddBackward0 object at 0x7ef86452f0a0>\n",
            "Number of grad_fn nodes: 11\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.autograd.graph.save_on_cpu at 0x7ef865e14590>"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "x = torch.tensor(1.0, requires_grad=True)\n",
        "y = x * 2\n",
        "y.backward()\n",
        "grad = x.grad  # grad now references the gradient tensor\n",
        "del x          # x is deallocated, but grad still holds a reference\n",
        "print(grad)    # Outputs tensor(2.0), as grad persists"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2f8-atJTHxbU",
        "outputId": "d7b051ef-15b7-4a40-e0a7-867873fe60d1"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(2.)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}