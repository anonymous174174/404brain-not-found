

Implement sum, mean, reshape, transpose, relu, softmax, tanh, leaky relu, exponential linear unit, gelu, silu, swish, add img_to_col 
implement all the major blocks for this like nn.module ones we can use. 
batch normalization, skip connections(i think this is already compatible currently), also conv1d conv2d conv3d,dense blocks 

Enhance Broadcasting:
make sure that broadcasting is correct for any type of scenario like  for example cnn blocks with rgm image and a batchsize which is >1
I want the autograd to be any rank tensor compatible in the sense that for convolution blocks with b H C W or even rank 5 which happens for lstm etc it should be compatible 
Add a zero_grad method to set gradients of all tensors to zero

add method to do weight initialization


import torch
import weakref
import numbers
import rustworkx as rx
import pytest

class AutogradGraph:
    __slots__ = ('graph', 'intermediate_tensors', '_check_cycles', '_auto_cleanup', '__weakref__')
    def __init__(self, check_for_cycles=True, auto_cleanup=True):

        self.graph = rx.PyDiGraph()
        self.intermediate_tensors = {}
        self._check_cycles = check_for_cycles
        self._auto_cleanup = auto_cleanup

    def __enter__(self):
        return self

    def __exit__(self, exc_type, exc_value, traceback):
        if self._check_cycles and self.check_cycle():
            raise RuntimeError("Cycle detected in autograd graph on context exit.")
        if self._auto_cleanup:
            self.intermediate_tensors.clear()
            self.graph.clear()

    def add_tensor_graph(self, tensor):
        if not tensor._custom_requires_grad:
            raise ValueError("Tensor with requires_grad=False cannot be added to the graph.")

        
        ref = weakref.proxy(tensor)
        tensor_index = self.graph.add_node(ref)
        tensor._node_id = tensor_index

    def add_non_leaf_tensor_reference(self, tensor):
        if not tensor._custom_requires_grad:
            raise ValueError("Tensor must require grad.")

        if tensor._node_id in self.intermediate_tensors:
            raise ValueError("Tensor reference already exists in intermediate tensors.")

        self.intermediate_tensors[tensor._node_id] = tensor


    def add_edge(self, node_from, node_to, weight=None):
        if not all(isinstance(n, int) for n in (node_from, node_to)):
            raise TypeError("Node indices must be integers.")
        if not self.graph.has_node(node_from) or not self.graph.has_node(node_to):
            raise ValueError("Nodes must exist before adding edge.")
        self.graph.add_edge(node_from, node_to, weight)

    def check_cycle(self):
        return not rx.is_directed_acyclic_graph(self.graph)

    def reverse_toposort(self):
        return [self.graph[n] for n in reversed(rx.topological_sort(self.graph))]

    def reverse_toposort_from_tensor(self, tensor_index):
        graph=self.graph
        predecessors = list(rx.ancestors(graph, tensor_index))
        predecessors.append(tensor_index)
        sub_graph = graph.subgraph(predecessors)
        return [sub_graph[i] for i in reversed(rx.topological_sort(sub_graph))]

    # def alternative_reverse_toposort_from_tensor(self, tensor_index):
    #     graph = self.graph
    #     relevant_nodes = rx.ancestors(graph, tensor_index)
    #     relevant_nodes.add(tensor_index)
    #     full_topo = rx.topological_sort(graph)
    #     relevant_topo = [graph[_node_id] for _node_id in reversed(full_topo) if _node_id in relevant_nodes]
    #     return relevant_topo

    def delete_node(self, node_index):
        if not isinstance(node_index, int):
            raise TypeError("Node index must be an integer.")
        if not self.graph.has_node(node_index):
            raise ValueError("Node does not exist.")
        self.graph.remove_node(node_index)

    def delete_edge(self, node_from, node_to):
        if not self.graph.has_edge(node_from, node_to):
            raise ValueError("Edge does not exist.")
        self.graph.remove_edge(node_from, node_to)

    def del_non_leaf_tensor_reference(self, tensor_node_id):
        self.intermediate_tensors.pop(tensor_node_id, None)

    def delete_all_non_leaf_nodes(self):
        # removes non leaf nodes from graph and clears the intermediate_tensors dict
        self.graph.remove_nodes_from(list(self.intermediate_tensors.keys()))
        self.intermediate_tensors.clear()

    def __repr__(self):
        return f"CustomAutogradGraph(nodes={self.graph.num_nodes()}, edges={self.graph.num_edges()})"

class CustomTensor:
    __slots__ = ('tensor', '_node_id', '_custom_requires_grad', '_backward', 'graph', '__weakref__','_is_leaf')

    def __new__(cls, data, *, _custom_requires_grad=False, device=None, dtype=None, graph=None, due_to_operation=False, is_leaf=False):
        if isinstance(data, CustomTensor):
            return data  # Don't rewrap
        return super().__new__(cls)

    def __init__(self, data, *, _custom_requires_grad=False, device=None, dtype=None, graph=None, due_to_operation=False, is_leaf=False):
        if isinstance(data, CustomTensor):
            return

        self.tensor = data if due_to_operation else torch.as_tensor(data, dtype=dtype, device=device)
        self.tensor.requires_grad_(False)
        self._custom_requires_grad = _custom_requires_grad
        self._node_id = None
        self._backward = lambda: None
        self.graph = None
        self._is_leaf = is_leaf 

        if _custom_requires_grad:
            self._init_graph(graph)

    def _init_graph(self, graph):
        if graph is None:
            raise ValueError("Graph must be provided if requires_grad is True.")
        is_leaf=self._is_leaf
        if is_leaf:
          self.graph = weakref.proxy(graph)
        else:
          self.graph = graph # this line is only reached for tensors which are created by operations and graph passed is already a weakreference hence no need for wrapping
        graph.add_tensor_graph(self)
        if not is_leaf:
            graph.add_non_leaf_tensor_reference(self)

    def _zero_grad(self):
        self.tensor.grad = torch.zeros_like(self.tensor)

    # --- Broadcasting Helper Function ---
    
    def _reduce_grad_for_broadcast(self,grad, target_shape):
      if grad.shape == target_shape:
          return grad
      padded_target_shape = (1,) * (grad.ndim - len(target_shape)) + target_shape
      sum_dims = []
      sum_dims = [i for i in range(grad.ndim) if padded_target_shape[i] == 1 and grad.shape[i] > 1] 
      if sum_dims:
          grad = grad.sum(dim=sum_dims, keepdim=True)    
      if grad.shape != target_shape:
        grad = grad.reshape(target_shape)    
      return grad

    # --- OPERATORS WITH BROADCASTING ---

    def __add__(self, other):
        if isinstance(other, numbers.Number):
            return self._add_scalar(other)
        elif isinstance(other, CustomTensor):
            return self._add_tensor(other)
        return NotImplemented

    def __radd__(self,other):
        return self + other
    def __iadd__(self,other):
        if isinstance(other, numbers.Number):
            self.tensor.add_(other)
        elif isinstance(other,CustomTensor):
            self.tensor.add_(other.tensor)
    
    def _add_scalar(self, scalar):
        result_tensor = torch.add(self.tensor, scalar)
        if not self._custom_requires_grad:
            return CustomTensor(result_tensor,due_to_operation=True)

        graph = self.graph
        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)
        graph.add_edge(self._node_id, result._node_id)

        self_ref = weakref.proxy(self)
        result_ref = weakref.proxy(result)
        def _backward():
            if self_ref.tensor.grad is None:
                self_ref._zero_grad()
            # Scalar addition doesn't change shape for the tensor, so no reduction needed
            self_ref.tensor.grad.add_(result_ref.tensor.grad)

        result._backward = _backward
        return result

    def _add_tensor(self, other):
        result_tensor = torch.add(self.tensor, other.tensor)
        requires_grad = self._custom_requires_grad or other._custom_requires_grad

        if not requires_grad:
            return CustomTensor(result_tensor,due_to_operation=True)

        graph = None
        if self._custom_requires_grad:
            graph = self.graph
        elif other._custom_requires_grad:
            graph = other.graph
        else:
            pass 
            
        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)

        self_ref = weakref.proxy(self)
        other_ref = weakref.proxy(other)

        if self._custom_requires_grad:
            graph.add_edge(self._node_id, result._node_id)
        if other._custom_requires_grad:
            graph.add_edge(other._node_id, result._node_id)
        result_ref = weakref.proxy(result)

        def _backward():
            if self_ref._custom_requires_grad:
                if self_ref.tensor.grad is None:
                    self_ref._zero_grad()
                # Apply reduction if 'self' was broadcasted
                grad_for_self = self_ref._reduce_grad_for_broadcast(result_ref.tensor.grad, self_ref.tensor.shape)
                self_ref.tensor.grad.add_(grad_for_self)

            if other_ref._custom_requires_grad:
                if other_ref.tensor.grad is None:
                    other_ref._zero_grad()
                # Apply reduction if 'other' was broadcasted
                grad_for_other = other_ref._reduce_grad_for_broadcast(result_ref.tensor.grad, other_ref.tensor.shape)
                other_ref.tensor.grad.add_(grad_for_other)

        result._backward = _backward
        return result

    def __mul__(self, other):
        if isinstance(other, numbers.Number):
            return self._mul_scalar(other)
        elif isinstance(other, CustomTensor):
            return self._mul_tensor(other)
        return NotImplemented
    
    def __rmul__(self,other):
        return self*other
    def __imul__(self,other):
        if isinstance(other, numbers.Number):
            self.tensor.mul_(other)
        elif isinstance(other,CustomTensor):
            self.tensor.mul_(other.tensor)

    def _mul_scalar(self, scalar):
        result_tensor = torch.mul(self.tensor, scalar)
        if not self._custom_requires_grad:
            return CustomTensor(result_tensor,due_to_operation=True)

        graph = self.graph
        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)
        graph.add_edge(self._node_id, result._node_id)

        self_ref = weakref.proxy(self)
        result_ref = weakref.proxy(result)
        def _backward():
            if self_ref.tensor.grad is None:
                self_ref._zero_grad()
            # Scalar multiplication doesn't change shape for the tensor, no reduction needed
            self_ref.tensor.grad.add_(result_ref.tensor.grad * scalar)
        result._backward = _backward
        return result

    def _mul_tensor(self, other):
        result_tensor = torch.mul(self.tensor, other.tensor)
        requires_grad = self._custom_requires_grad or other._custom_requires_grad
        if not requires_grad:
            return CustomTensor(result_tensor,due_to_operation=True)

        graph = self.graph if self._custom_requires_grad else other.graph
        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)

        self_ref = weakref.proxy(self)
        other_ref = weakref.proxy(other)
        result_ref = weakref.proxy(result)

        if self._custom_requires_grad:
            graph.add_edge(self._node_id, result._node_id)
        if other._custom_requires_grad:
            graph.add_edge(other._node_id, result._node_id)

        def _backward():
            if self_ref._custom_requires_grad:
                if self_ref.tensor.grad is None:
                    self_ref._zero_grad()
                grad_for_self = self_ref._reduce_grad_for_broadcast(result_ref.tensor.grad * other_ref.tensor, self_ref.tensor.shape)
                self_ref.tensor.grad.add_(grad_for_self)
            if other_ref._custom_requires_grad:
                if other_ref.tensor.grad is None:
                    other_ref._zero_grad()
                grad_for_other = other_ref._reduce_grad_for_broadcast(result_ref.tensor.grad * self_ref.tensor, other_ref.tensor.shape)
                other_ref.tensor.grad.add_(grad_for_other)
        result._backward = _backward
        return result


    def __sub__(self, other):
        if isinstance(other, numbers.Number):
            return self._sub_scalar(other)
        elif isinstance(other, CustomTensor):
            return self._sub_tensor(other)
        return NotImplemented
    
    def __rsub__(self, other):
        if isinstance(other, numbers.Number):
            return self._rsub_scalar(other)
        
    def __isub__(self,other):
        if isinstance(other, numbers.Number):
            self.tensor.sub_(other)
        elif isinstance(other,CustomTensor):
            self.tensor.sub_(other.tensor)
        
    def _rsub_scalar(self, scalar):
        result_tensor = torch.sub(scalar, self.tensor)
        if not self._custom_requires_grad:
            return CustomTensor(result_tensor,due_to_operation=True)

        graph = self.graph
        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)
        graph.add_edge(self._node_id, result._node_id)

        self_ref = weakref.proxy(self)
        result_ref = weakref.proxy(result)
        def _backward():
            if self_ref.tensor.grad is None:
                self_ref._zero_grad()
            # Derivative of scalar - x is -1
            self_ref.tensor.grad.sub_(result_ref.tensor.grad) # No broadcasting specific logic for scalar op

        result._backward = _backward
        return result

    
    def _sub_scalar(self, scalar):
        result_tensor = torch.sub(self.tensor, scalar)
        if not self._custom_requires_grad:
            return CustomTensor(result_tensor,due_to_operation=True)

        graph = self.graph
        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)
        graph.add_edge(self._node_id, result._node_id)

        self_ref = weakref.proxy(self)
        result_ref = weakref.proxy(result)
        def _backward():
            if self_ref.tensor.grad is None:
                self_ref._zero_grad()
            self_ref.tensor.grad.add_(result_ref.tensor.grad) # No broadcasting specific logic for scalar op
        result._backward = _backward
        return result

    def _sub_tensor(self, other):
        result_tensor = torch.sub(self.tensor, other.tensor)
        requires_grad = self._custom_requires_grad or other._custom_requires_grad
        if not requires_grad:
            return CustomTensor(result_tensor,due_to_operation=True)

        graph = self.graph if self._custom_requires_grad else other.graph
        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)

        self_ref = weakref.proxy(self)
        other_ref = weakref.proxy(other)
        result_ref = weakref.proxy(result)

        if self._custom_requires_grad:
            graph.add_edge(self._node_id, result._node_id)
        if other._custom_requires_grad:
            graph.add_edge(other._node_id, result._node_id)

        def _backward():
            if self_ref._custom_requires_grad:
                if self_ref.tensor.grad is None:
                    self_ref._zero_grad()
                grad_for_self = self_ref._reduce_grad_for_broadcast(result_ref.tensor.grad, self_ref.tensor.shape)
                self_ref.tensor.grad.add_(grad_for_self)
            if other_ref._custom_requires_grad:
                if other_ref.tensor.grad is None:
                    other_ref._zero_grad()
                grad_for_other = other_ref._reduce_grad_for_broadcast(-result_ref.tensor.grad, other_ref.tensor.shape)
                other_ref.tensor.grad.add_(grad_for_other)
        result._backward = _backward
        return result

    def __truediv__(self, other):
        if isinstance(other, numbers.Number):
            return self._div_scalar(other)
        elif isinstance(other, CustomTensor):
            return self._div_tensor(other)
        return NotImplemented
    def __itruediv__(self,other):
        if isinstance(other, numbers.Number):
            self.tensor.div_(other)
        elif isinstance(other,CustomTensor):
            self.tensor.div_(other.tensor)
    def _div_scalar(self, scalar):
        result_tensor = torch.div(self.tensor, scalar)
        if not self._custom_requires_grad:
            return CustomTensor(result_tensor,due_to_operation=True)

        graph = self.graph
        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)
        graph.add_edge(self._node_id, result._node_id)

        self_ref = weakref.proxy(self)
        result_ref = weakref.proxy(result)
        def _backward():
            if self_ref.tensor.grad is None:
                self_ref._zero_grad()
            self_ref.tensor.grad.add_(result_ref.tensor.grad / scalar)
        result._backward = _backward
        return result

    def _div_tensor(self,other):
        result_tensor = torch.div(self.tensor, other.tensor)
        requires_grad = self._custom_requires_grad or other._custom_requires_grad
        if not requires_grad:
            return CustomTensor(result_tensor,due_to_operation=True)

        graph = self.graph if self._custom_requires_grad else other.graph
        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)

        self_ref = weakref.proxy(self)
        other_ref = weakref.proxy(other)
        result_ref = weakref.proxy(result)

        if self._custom_requires_grad:
            graph.add_edge(self._node_id, result._node_id)
        if other._custom_requires_grad:
            graph.add_edge(other._node_id, result._node_id)

        def _backward():
            if self_ref._custom_requires_grad:
                if self_ref.tensor.grad is None:
                    self_ref._zero_grad()
                grad_for_self = self_ref._reduce_grad_for_broadcast(result_ref.tensor.grad / other_ref.tensor, self_ref.tensor.shape)
                self_ref.tensor.grad.add_(grad_for_self)
            if other_ref._custom_requires_grad:
                if other_ref.tensor.grad is None:
                    other_ref._zero_grad()
                grad_for_other = other_ref._reduce_grad_for_broadcast(-result_ref.tensor.grad * self_ref.tensor / other_ref.tensor.pow(2), other_ref.tensor.shape)
                other_ref.tensor.grad.add_(grad_for_other)
        result._backward = _backward
        return result

    def pow(self, scalar):
        result_tensor = torch.pow(self.tensor, scalar)
        if not self._custom_requires_grad:
            return CustomTensor(result_tensor,due_to_operation=True)

        graph = self.graph
        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)
        graph.add_edge(self._node_id, result._node_id)

        self_ref = weakref.proxy(self)
        result_ref = weakref.proxy(result)
        def _backward():
            if self_ref.tensor.grad is None:
                self_ref._zero_grad()
            grad_contrib = scalar * self_ref.tensor.pow(scalar - 1)
            self_ref.tensor.grad.add_(result_ref.tensor.grad * grad_contrib)
        result._backward = _backward
        return result
    def __ipow__(self,other):
        self.tensor.pow_(other)

    def exp(self):
        out = torch.exp(self.tensor)
        if not self._custom_requires_grad:
            return CustomTensor(out,due_to_operation=True)
        
        graph = self.graph
        result = CustomTensor(out, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)
        graph.add_edge(self._node_id, result._node_id)
        self_ref = weakref.proxy(self)
        result_ref = weakref.proxy(result)
        def _backward():
            if self_ref.tensor.grad is None:
                self_ref._zero_grad()
            self_ref.tensor.grad.add_(result_ref.tensor.grad * out)
        result._backward = _backward
        return result

    def log(self):
        out = torch.log(self.tensor)
        if not self._custom_requires_grad:
            return CustomTensor(out,due_to_operation=True)
        
        graph = self.graph
        result = CustomTensor(out, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)
        graph.add_edge(self._node_id, result._node_id)
        self_ref = weakref.proxy(self)
        result_ref = weakref.proxy(result)
        def _backward():
            if self_ref.tensor.grad is None:
                self_ref._zero_grad()
            self_ref.tensor.grad.add_(result_ref.tensor.grad / self_ref.tensor)
        result._backward = _backward
        return result

    def sin(self):
        out = torch.sin(self.tensor)
        if not self._custom_requires_grad:
            return CustomTensor(out,due_to_operation=True)
        
        graph = self.graph
        result = CustomTensor(out, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)
        graph.add_edge(self._node_id, result._node_id)
        self_ref = weakref.proxy(self)
        result_ref = weakref.proxy(result)
        def _backward():
            if self_ref.tensor.grad is None:
                self_ref._zero_grad()
            self_ref.tensor.grad.add_(result_ref.tensor.grad * torch.cos(self_ref.tensor))
        result._backward = _backward
        return result

    def cos(self):
        out = torch.cos(self.tensor)
        if not self._custom_requires_grad:
            return CustomTensor(out,due_to_operation=True)
        
        graph = self.graph
        result = CustomTensor(out, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)
        graph.add_edge(self._node_id, result._node_id)
        self_ref = weakref.proxy(self)
        result_ref = weakref.proxy(result)
        def _backward():
            if self_ref.tensor.grad is None:
                self_ref._zero_grad()
            self_ref.tensor.grad.add_(-result_ref.tensor.grad*torch.sin(self_ref.tensor))
        result._backward = _backward
        return result 

    def sqrt(self):
        out = torch.sqrt(self.tensor)
        if not self._custom_requires_grad:
            return CustomTensor(out,due_to_operation=True)
        
        graph = self.graph
        result = CustomTensor(out, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)
        graph.add_edge(self._node_id, result._node_id)
        self_ref = weakref.proxy(self)
        result_ref = weakref.proxy(result)
        def _backward():
            if self_ref.tensor.grad is None:
                self_ref._zero_grad()
            self_ref.tensor.grad.add_(result_ref.tensor.grad*0.5*self_ref.tensor.pow(-0.5))
        result._backward = _backward
        return result

    def matmul(self, other):
        result_tensor = torch.matmul(self.tensor, other.tensor)
        requires_grad = self._custom_requires_grad or other._custom_requires_grad
        if not requires_grad:
            return CustomTensor(result_tensor,due_to_operation=True)

        graph = self.graph if self._custom_requires_grad else other.graph
        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)

        self_ref = weakref.proxy(self)
        other_ref = weakref.proxy(other)
        result_ref = weakref.proxy(result)

        if self._custom_requires_grad:
            graph.add_edge(self._node_id, result._node_id)
        if other._custom_requires_grad:
            graph.add_edge(other._node_id, result._node_id)

        def _backward():
            if self_ref._custom_requires_grad:
                if self_ref.tensor.grad is None:
                    self_ref._zero_grad()
                # Matmul broadcasting for batch dimensions: no sum needed if shapes align after matmul
                grad_for_self = torch.matmul(result_ref.tensor.grad, other_ref.tensor.transpose(-2, -1))
                # If there were batch dimensions that were broadcasted in self, sum over them
                # This check can be more complex for general batch broadcasting in matmul
                if grad_for_self.shape != self_ref.tensor.shape:
                    grad_for_self = self_ref._reduce_grad_for_broadcast(grad_for_self, self_ref.tensor.shape)
                self_ref.tensor.grad.add_(grad_for_self)

            if other_ref._custom_requires_grad:
                if other_ref.tensor.grad is None:
                    other_ref._zero_grad()
                grad_for_other = torch.matmul(self_ref.tensor.transpose(-2, -1), result_ref.tensor.grad)
                # If there were batch dimensions that were broadcasted in other, sum over them
                if grad_for_other.shape != other_ref.tensor.shape:
                    grad_for_other = other_ref._reduce_grad_for_broadcast(grad_for_other, other_ref.tensor.shape)
                other_ref.tensor.grad.add_(grad_for_other)
        result._backward = _backward
        return result

    def apply_mask(self, mask):
        result_tensor = self.tensor * mask.tensor # This is element-wise multiplication
        requires_grad = self._custom_requires_grad or mask._custom_requires_grad
        if not requires_grad:
            return CustomTensor(result_tensor,due_to_operation=True)

        graph = self.graph if self._custom_requires_grad else mask.graph
        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)

        self_ref = weakref.proxy(self)
        mask_ref = weakref.proxy(mask)
        result_ref = weakref.proxy(result)

        if self._custom_requires_grad:
            graph.add_edge(self._node_id, result._node_id)
        if mask._custom_requires_grad:
            graph.add_edge(mask._node_id, result._node_id)

        def _backward():
            if self_ref._custom_requires_grad:
                if self_ref.tensor.grad is None:
                    self_ref._zero_grad()
                # Apply reduction if 'self' was broadcasted
                grad_for_self = self_ref._reduce_grad_for_broadcast(result_ref.tensor.grad * mask_ref.tensor, self_ref.tensor.shape)
                self_ref.tensor.grad.add_(grad_for_self)
            if mask_ref._custom_requires_grad:
                if mask_ref.tensor.grad is None:
                    mask_ref._zero_grad()
                # Apply reduction if 'mask' was broadcasted
                grad_for_mask = mask_ref._reduce_grad_for_broadcast(result_ref.tensor.grad * self_ref.tensor, mask_ref.tensor.shape)
                mask_ref.tensor.grad.add_(grad_for_mask)
        result._backward = _backward
        return result

    def dot(self, other):
        # torch.dot only works for 1D tensors, or for higher-D tensors,
        # it flattens them to 1D and then computes the dot product.
        # This means the gradients will also be 1D, so no complex broadcasting
        # reduction is needed on the output gradient itself.
        # However, the input tensors themselves could have been results of broadcasting ops.
        # For a truly general dot product, you'd use torch.matmul.
        result_tensor = torch.dot(self.tensor.reshape(-1), other.tensor.reshape(-1))
        requires_grad = self._custom_requires_grad or other._custom_requires_grad
        if not requires_grad:
            return CustomTensor(result_tensor,due_to_operation=True)

        graph = self.graph if self._custom_requires_grad else other.graph
        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)

        self_ref = weakref.proxy(self)
        other_ref = weakref.proxy(other)
        result_ref = weakref.proxy(result)

        if self._custom_requires_grad:
            graph.add_edge(self._node_id, result._node_id)
        if other._custom_requires_grad:
            graph.add_edge(other._node_id, result._node_id)

        def _backward():
            if self_ref._custom_requires_grad:
                if self_ref.tensor.grad is None:
                    self_ref._zero_grad()
                # The grad from result_ref.tensor.grad will be a scalar.
                # It needs to be multiplied by the other_ref.tensor (original shape)
                # and then potentially re-shaped if original was >1D
                grad_contrib = result_ref.tensor.grad * other_ref.tensor
                self_ref.tensor.grad.add_(grad_contrib)
            if other_ref._custom_requires_grad:
                if other_ref.tensor.grad is None:
                    other_ref._zero_grad()
                grad_contrib = result_ref.tensor.grad * self_ref.tensor
                other_ref.tensor.grad.add_(grad_contrib)
        result._backward = _backward
        return result

    def backward(self,weightage_tensor=1):
        if not self._custom_requires_grad:
            raise RuntimeError("Output tensor does not require grad.")
        if self.graph is None:
            raise RuntimeError("Output tensor is not part of a graph.")
        graph = self.graph

        # Initialize gradient for the output tensor
        if isinstance(weightage_tensor,numbers.Number):
            self.tensor.grad = torch.full_like(self.tensor, fill_value=weightage_tensor)
        elif isinstance(weightage_tensor,torch.Tensor):
            self.tensor.grad = weightage_tensor.clone() # we don't want to modify the original tensor data

        # Perform backward pass using topological sort

        nodes_to_process = graph.reverse_toposort_from_tensor(self._node_id)

        for tensor_node in nodes_to_process:
            # Check if the weak proxy is still valid (tensor is alive)
            if tensor_node.__class__ is weakref.ProxyType and tensor_node.__repr__() is None:
                continue # Skip if the weak reference is dead

            if tensor_node.tensor.grad is None and tensor_node is not self.tensor:
                pass 

            tensor_node._backward()

    def to_device(self, device):
        self.tensor = self.tensor.to(device)

    @property
    def dtype(self):
        return self.tensor.dtype

    @property
    def ndim(self):
        return self.tensor.ndim

    @property
    def shape(self):
        return self.tensor.shape


    def __del__(self):
      if self._node_id is not None and self._is_leaf: 
        try:
              self.graph.delete_node(self._node_id)
        except ReferenceError:
              pass
      print(f"Garbage Collector has decided that reference counts for {self._node_id} and id {id(self)} are zero so Goodbye!!")



chatgpt
import torch
import weakref
import numbers
import rustworkx as rx
import pytest
import math

class AutogradGraph:
    __slots__ = ('graph', 'intermediate_tensors', '_check_cycles', '_auto_cleanup', '__weakref__')
    def __init__(self, check_for_cycles=True, auto_cleanup=True):

        self.graph = rx.PyDiGraph()
        self.intermediate_tensors = {}
        self._check_cycles = check_for_cycles
        self._auto_cleanup = auto_cleanup

    def __enter__(self):
        return self

    def __exit__(self, exc_type, exc_value, traceback):
        if self._check_cycles and self.check_cycle():
            raise RuntimeError("Cycle detected in autograd graph on context exit.")
        if self._auto_cleanup:
            self.intermediate_tensors.clear()
            self.graph.clear()

    def add_tensor_graph(self, tensor):
        if not tensor._custom_requires_grad:
            raise ValueError("Tensor with requires_grad=False cannot be added to the graph.")

        
        ref = weakref.proxy(tensor)
        tensor_index = self.graph.add_node(ref)
        tensor._node_id = tensor_index

    def add_non_leaf_tensor_reference(self, tensor):
        if not tensor._custom_requires_grad:
            raise ValueError("Tensor must require grad.")

        if tensor._node_id in self.intermediate_tensors:
            raise ValueError("Tensor reference already exists in intermediate tensors.")

        self.intermediate_tensors[tensor._node_id] = tensor


    def add_edge(self, node_from, node_to, weight=None):
        if not all(isinstance(n, int) for n in (node_from, node_to)):
            raise TypeError("Node indices must be integers.")
        if not self.graph.has_node(node_from) or not self.graph.has_node(node_to):
            raise ValueError("Nodes must exist before adding edge.")
        self.graph.add_edge(node_from, node_to, weight)

    def check_cycle(self):
        return not rx.is_directed_acyclic_graph(self.graph)

    def reverse_toposort(self):
        return [self.graph[n] for n in reversed(rx.topological_sort(self.graph))]

    def reverse_toposort_from_tensor(self, tensor_index):
        graph=self.graph
        predecessors = list(rx.ancestors(graph, tensor_index))
        predecessors.append(tensor_index)
        sub_graph = graph.subgraph(predecessors)
        return [sub_graph[i] for i in reversed(rx.topological_sort(sub_graph))]

    # def alternative_reverse_toposort_from_tensor(self, tensor_index):
    #     graph = self.graph
    #     relevant_nodes = rx.ancestors(graph, tensor_index)
    #     relevant_nodes.add(tensor_index)
    #     full_topo = rx.topological_sort(graph)
    #     relevant_topo = [graph[_node_id] for _node_id in reversed(full_topo) if _node_id in relevant_nodes]
    #     return relevant_topo

    def delete_node(self, node_index):
        if not isinstance(node_index, int):
            raise TypeError("Node index must be an integer.")
        if not self.graph.has_node(node_index):
            raise ValueError("Node does not exist.")
        self.graph.remove_node(node_index)

    def delete_edge(self, node_from, node_to):
        if not self.graph.has_edge(node_from, node_to):
            raise ValueError("Edge does not exist.")
        self.graph.remove_edge(node_from, node_to)

    def del_non_leaf_tensor_reference(self, tensor_node_id):
        self.intermediate_tensors.pop(tensor_node_id, None)

    def delete_all_non_leaf_nodes(self):
        # removes non leaf nodes from graph and clears the intermediate_tensors dict
        self.graph.remove_nodes_from(list(self.intermediate_tensors.keys()))
        self.intermediate_tensors.clear()

    def __repr__(self):
        return f"CustomAutogradGraph(nodes={self.graph.num_nodes()}, edges={self.graph.num_edges()})"

class CustomTensor:
    __slots__ = ('tensor', '_node_id', '_custom_requires_grad', '_backward', 'graph', '__weakref__','_is_leaf')

    def __new__(cls, data, *, _custom_requires_grad=False, device=None, dtype=None, graph=None, due_to_operation=False, is_leaf=False):
        if isinstance(data, CustomTensor):
            return data # Don't rewrap
        return super().__new__(cls)

    def __init__(self, data, *, _custom_requires_grad=False, device=None, dtype=None, graph=None, due_to_operation=False, is_leaf=False):
        if isinstance(data, CustomTensor):
            return

        self.tensor = data if due_to_operation else torch.as_tensor(data, dtype=dtype, device=device)
        self.tensor.requires_grad_(False)
        self._custom_requires_grad = _custom_requires_grad
        self._node_id = None
        self._backward = lambda: None
        self.graph = None
        self._is_leaf = is_leaf 

        if _custom_requires_grad:
            self._init_graph(graph)

    def _init_graph(self, graph):
        if graph is None:
            raise ValueError("Graph must be provided if requires_grad is True.")
        is_leaf=self._is_leaf
        if is_leaf:
          self.graph = weakref.proxy(graph)
        else:
          self.graph = graph # this line is only reached for tensors which are created by operations and graph passed is already a weakreference hence no need for wrapping
        graph.add_tensor_graph(self)
        if not is_leaf:
            graph.add_non_leaf_tensor_reference(self)

    def zero_grad(self):
        """Sets gradients of this tensor to zero."""
        if self.tensor.grad is not None:
            self.tensor.grad.zero_()

    # --- Broadcasting Helper Function ---
    
    def _reduce_grad_for_broadcast(self,grad, target_shape):
      if grad.shape == target_shape:
          return grad
      
      # Pad target_shape with 1s on the left to match grad.ndim
      padded_target_shape = (1,) * (grad.ndim - len(target_shape)) + target_shape
      
      sum_dims = [i for i in range(grad.ndim) if padded_target_shape[i] == 1 and grad.shape[i] > 1]    
      
      if sum_dims:
          grad = grad.sum(dim=sum_dims, keepdim=True)    
      
      # Reshape to target_shape if dimensions still don't match (e.g., after squeezing single dimensions)
      if grad.shape != target_shape:
        # We need to unsqueeze dimensions if the target shape has more dimensions (with 1s)
        # than the current grad shape after summing.
        # This part handles cases like (10, 1) + (1, 5) -> (10, 5), where (10, 5) grad needs to be
        # reduced to (10, 1) for the first operand and (1, 5) for the second.
        # The sum_dims already handles the reduction part.
        # If the target shape has dimensions of 1 that were originally broadcasted,
        # we need to ensure the gradient matches that reduced shape.
        grad = grad.reshape(target_shape)    
      return grad


    # --- OPERATORS WITH BROADCASTING ---

    def __add__(self, other):
        if isinstance(other, numbers.Number):
            return self._add_scalar(other)
        elif isinstance(other, CustomTensor):
            return self._add_tensor(other)
        return NotImplemented

    def __radd__(self,other):
        return self + other
    def __iadd__(self,other):
        if isinstance(other, numbers.Number):
            self.tensor.add_(other)
        elif isinstance(other,CustomTensor):
            self.tensor.add_(other.tensor)
        return self # Return self for in-place operations
    
    def _add_scalar(self, scalar):
        result_tensor = torch.add(self.tensor, scalar)
        if not self._custom_requires_grad:
            return CustomTensor(result_tensor,due_to_operation=True)

        graph = self.graph
        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)
        graph.add_edge(self._node_id, result._node_id)

        self_ref = weakref.proxy(self)
        result_ref = weakref.proxy(result)
        def _backward():
            if self_ref.tensor.grad is None:
                self_ref.zero_grad()
            # Scalar addition doesn't change shape for the tensor, so no reduction needed
            self_ref.tensor.grad.add_(result_ref.tensor.grad)

        result._backward = _backward
        return result

    def _add_tensor(self, other):
        result_tensor = torch.add(self.tensor, other.tensor)
        requires_grad = self._custom_requires_grad or other._custom_requires_grad

        if not requires_grad:
            return CustomTensor(result_tensor,due_to_operation=True)

        graph = None
        if self._custom_requires_grad:
            graph = self.graph
        elif other._custom_requires_grad:
            graph = other.graph
        else:
            pass 
            
        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)

        self_ref = weakref.proxy(self)
        other_ref = weakref.proxy(other)

        if self._custom_requires_grad:
            graph.add_edge(self._node_id, result._node_id)
        if other._custom_requires_grad:
            graph.add_edge(other._node_id, result._node_id)
        result_ref = weakref.proxy(result)

        def _backward():
            if self_ref._custom_requires_grad:
                if self_ref.tensor.grad is None:
                    self_ref.zero_grad()
                # Apply reduction if 'self' was broadcasted
                grad_for_self = self_ref._reduce_grad_for_broadcast(result_ref.tensor.grad, self_ref.tensor.shape)
                self_ref.tensor.grad.add_(grad_for_self)

            if other_ref._custom_requires_grad:
                if other_ref.tensor.grad is None:
                    other_ref.zero_grad()
                # Apply reduction if 'other' was broadcasted
                grad_for_other = other_ref._reduce_grad_for_broadcast(result_ref.tensor.grad, other_ref.tensor.shape)
                other_ref.tensor.grad.add_(grad_for_other)

        result._backward = _backward
        return result

    def __mul__(self, other):
        if isinstance(other, numbers.Number):
            return self._mul_scalar(other)
        elif isinstance(other, CustomTensor):
            return self._mul_tensor(other)
        return NotImplemented
    
    def __rmul__(self,other):
        return self*other
    def __imul__(self,other):
        if isinstance(other, numbers.Number):
            self.tensor.mul_(other)
        elif isinstance(other,CustomTensor):
            self.tensor.mul_(other.tensor)
        return self # Return self for in-place operations

    def _mul_scalar(self, scalar):
        result_tensor = torch.mul(self.tensor, scalar)
        if not self._custom_requires_grad:
            return CustomTensor(result_tensor,due_to_operation=True)

        graph = self.graph
        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)
        graph.add_edge(self._node_id, result._node_id)

        self_ref = weakref.proxy(self)
        result_ref = weakref.proxy(result)
        def _backward():
            if self_ref.tensor.grad is None:
                self_ref.zero_grad()
            # Scalar multiplication doesn't change shape for the tensor, no reduction needed
            self_ref.tensor.grad.add_(result_ref.tensor.grad * scalar)
        result._backward = _backward
        return result

    def _mul_tensor(self, other):
        result_tensor = torch.mul(self.tensor, other.tensor)
        requires_grad = self._custom_requires_grad or other._custom_requires_grad
        if not requires_grad:
            return CustomTensor(result_tensor,due_to_operation=True)

        graph = self.graph if self._custom_requires_grad else other.graph
        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)

        self_ref = weakref.proxy(self)
        other_ref = weakref.proxy(other)
        result_ref = weakref.proxy(result)

        if self._custom_requires_grad:
            graph.add_edge(self._node_id, result._node_id)
        if other._custom_requires_grad:
            graph.add_edge(other._node_id, result._node_id)

        def _backward():
            if self_ref._custom_requires_grad:
                if self_ref.tensor.grad is None:
                    self_ref.zero_grad()
                grad_for_self = self_ref._reduce_grad_for_broadcast(result_ref.tensor.grad * other_ref.tensor, self_ref.tensor.shape)
                self_ref.tensor.grad.add_(grad_for_self)
            if other_ref._custom_requires_grad:
                if other_ref.tensor.grad is None:
                    other_ref.zero_grad()
                grad_for_other = other_ref._reduce_grad_for_broadcast(result_ref.tensor.grad * self_ref.tensor, other_ref.tensor.shape)
                other_ref.tensor.grad.add_(grad_for_other)
        result._backward = _backward
        return result


    def __sub__(self, other):
        if isinstance(other, numbers.Number):
            return self._sub_scalar(other)
        elif isinstance(other, CustomTensor):
            return self._sub_tensor(other)
        return NotImplemented
    
    def __rsub__(self, other):
        if isinstance(other, numbers.Number):
            return self._rsub_scalar(other)
        elif isinstance(other, CustomTensor):
            # This case should typically be handled by _sub_tensor directly when `self - other` is called.
            # However, for `other - self`, we can reuse _sub_tensor by swapping operands and negating.
            res = self._sub_tensor(other)
            res.tensor = -res.tensor # Negate the result for rsub
            # Need to adjust backward for rsub as well if it's explicitly called this way
            original_backward = res._backward
            def _rsub_backward_wrapper():
                original_backward()
                if self._custom_requires_grad and self.tensor.grad is not None:
                    self.tensor.grad.neg_() # Negate gradient for self
                if other._custom_requires_grad and other.tensor.grad is not None:
                    other.tensor.grad.neg_() # Negate gradient for other (since original was other - self, now -(self - other))
            res._backward = _rsub_backward_wrapper
            return res
        return NotImplemented
        
    def __isub__(self,other):
        if isinstance(other, numbers.Number):
            self.tensor.sub_(other)
        elif isinstance(other,CustomTensor):
            self.tensor.sub_(other.tensor)
        return self # Return self for in-place operations
        
    def _rsub_scalar(self, scalar):
        result_tensor = torch.sub(scalar, self.tensor)
        if not self._custom_requires_grad:
            return CustomTensor(result_tensor,due_to_operation=True)

        graph = self.graph
        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)
        graph.add_edge(self._node_id, result._node_id)

        self_ref = weakref.proxy(self)
        result_ref = weakref.proxy(result)
        def _backward():
            if self_ref.tensor.grad is None:
                self_ref.zero_grad()
            # Derivative of scalar - x is -1 * result_ref.tensor.grad
            self_ref.tensor.grad.sub_(result_ref.tensor.grad) # No broadcasting specific logic for scalar op

        result._backward = _backward
        return result

    
    def _sub_scalar(self, scalar):
        result_tensor = torch.sub(self.tensor, scalar)
        if not self._custom_requires_grad:
            return CustomTensor(result_tensor,due_to_operation=True)

        graph = self.graph
        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)
        graph.add_edge(self._node_id, result._node_id)

        self_ref = weakref.proxy(self)
        result_ref = weakref.proxy(result)
        def _backward():
            if self_ref.tensor.grad is None:
                self_ref.zero_grad()
            self_ref.tensor.grad.add_(result_ref.tensor.grad) # No broadcasting specific logic for scalar op
        result._backward = _backward
        return result

    def _sub_tensor(self, other):
        result_tensor = torch.sub(self.tensor, other.tensor)
        requires_grad = self._custom_requires_grad or other._custom_requires_grad
        if not requires_grad:
            return CustomTensor(result_tensor,due_to_operation=True)

        graph = self.graph if self._custom_requires_grad else other.graph
        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)

        self_ref = weakref.proxy(self)
        other_ref = weakref.proxy(other)
        result_ref = weakref.proxy(result)

        if self._custom_requires_grad:
            graph.add_edge(self._node_id, result._node_id)
        if other._custom_requires_grad:
            graph.add_edge(other._node_id, result._node_id)

        def _backward():
            if self_ref._custom_requires_grad:
                if self_ref.tensor.grad is None:
                    self_ref.zero_grad()
                grad_for_self = self_ref._reduce_grad_for_broadcast(result_ref.tensor.grad, self_ref.tensor.shape)
                self_ref.tensor.grad.add_(grad_for_self)
            if other_ref._custom_requires_grad:
                if other_ref.tensor.grad is None:
                    other_ref.zero_grad()
                grad_for_other = other_ref._reduce_grad_for_broadcast(-result_ref.tensor.grad, other_ref.tensor.shape)
                other_ref.tensor.grad.add_(grad_for_other)
        result._backward = _backward
        return result

    def __truediv__(self, other):
        if isinstance(other, numbers.Number):
            return self._div_scalar(other)
        elif isinstance(other, CustomTensor):
            return self._div_tensor(other)
        return NotImplemented
    def __itruediv__(self,other):
        if isinstance(other, numbers.Number):
            self.tensor.div_(other)
        elif isinstance(other,CustomTensor):
            self.tensor.div_(other.tensor)
        return self # Return self for in-place operations

    def _div_scalar(self, scalar):
        result_tensor = torch.div(self.tensor, scalar)
        if not self._custom_requires_grad:
            return CustomTensor(result_tensor,due_to_operation=True)

        graph = self.graph
        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)
        graph.add_edge(self._node_id, result._node_id)

        self_ref = weakref.proxy(self)
        result_ref = weakref.proxy(result)
        def _backward():
            if self_ref.tensor.grad is None:
                self_ref.zero_grad()
            self_ref.tensor.grad.add_(result_ref.tensor.grad / scalar)
        result._backward = _backward
        return result

    def _div_tensor(self,other):
        result_tensor = torch.div(self.tensor, other.tensor)
        requires_grad = self._custom_requires_grad or other._custom_requires_grad
        if not requires_grad:
            return CustomTensor(result_tensor,due_to_operation=True)

        graph = self.graph if self._custom_requires_grad else other.graph
        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)

        self_ref = weakref.proxy(self)
        other_ref = weakref.proxy(other)
        result_ref = weakref.proxy(result)

        if self._custom_requires_grad:
            graph.add_edge(self._node_id, result._node_id)
        if other._custom_requires_grad:
            graph.add_edge(other._node_id, result._node_id)

        def _backward():
            if self_ref._custom_requires_grad:
                if self_ref.tensor.grad is None:
                    self_ref.zero_grad()
                grad_for_self = self_ref._reduce_grad_for_broadcast(result_ref.tensor.grad / other_ref.tensor, self_ref.tensor.shape)
                self_ref.tensor.grad.add_(grad_for_self)
            if other_ref._custom_requires_grad:
                if other_ref.tensor.grad is None:
                    other_ref.zero_grad()
                grad_for_other = other_ref._reduce_grad_for_broadcast(-result_ref.tensor.grad * self_ref.tensor / other_ref.tensor.pow(2), other_ref.tensor.shape)
                other_ref.tensor.grad.add_(grad_for_other)
        result._backward = _backward
        return result

    def pow(self, scalar):
        result_tensor = torch.pow(self.tensor, scalar)
        if not self._custom_requires_grad:
            return CustomTensor(result_tensor,due_to_operation=True)

        graph = self.graph
        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)
        graph.add_edge(self._node_id, result._node_id)

        self_ref = weakref.proxy(self)
        result_ref = weakref.proxy(result)
        def _backward():
            if self_ref.tensor.grad is None:
                self_ref.zero_grad()
            grad_contrib = scalar * self_ref.tensor.pow(scalar - 1)
            self_ref.tensor.grad.add_(result_ref.tensor.grad * grad_contrib)
        result._backward = _backward
        return result
    def __ipow__(self,other):
        self.tensor.pow_(other)
        return self # Return self for in-place operations

    def exp(self):
        out = torch.exp(self.tensor)
        if not self._custom_requires_grad:
            return CustomTensor(out,due_to_operation=True)
        
        graph = self.graph
        result = CustomTensor(out, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)
        graph.add_edge(self._node_id, result._node_id)
        self_ref = weakref.proxy(self)
        result_ref = weakref.proxy(result)
        def _backward():
            if self_ref.tensor.grad is None:
                self_ref.zero_grad()
            self_ref.tensor.grad.add_(result_ref.tensor.grad * out)
        result._backward = _backward
        return result

    def log(self):
        out = torch.log(self.tensor)
        if not self._custom_requires_grad:
            return CustomTensor(out,due_to_operation=True)
        
        graph = self.graph
        result = CustomTensor(out, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)
        graph.add_edge(self._node_id, result._node_id)
        self_ref = weakref.proxy(self)
        result_ref = weakref.proxy(result)
        def _backward():
            if self_ref.tensor.grad is None:
                self_ref.zero_grad()
            self_ref.tensor.grad.add_(result_ref.tensor.grad / self_ref.tensor)
        result._backward = _backward
        return result

    def sin(self):
        out = torch.sin(self.tensor)
        if not self._custom_requires_grad:
            return CustomTensor(out,due_to_operation=True)
        
        graph = self.graph
        result = CustomTensor(out, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)
        graph.add_edge(self._node_id, result._node_id)
        self_ref = weakref.proxy(self)
        result_ref = weakref.proxy(result)
        def _backward():
            if self_ref.tensor.grad is None:
                self_ref.zero_grad()
            self_ref.tensor.grad.add_(result_ref.tensor.grad * torch.cos(self_ref.tensor))
        result._backward = _backward
        return result

    def cos(self):
        out = torch.cos(self.tensor)
        if not self._custom_requires_grad:
            return CustomTensor(out,due_to_operation=True)
        
        graph = self.graph
        result = CustomTensor(out, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)
        graph.add_edge(self._node_id, result._node_id)
        self_ref = weakref.proxy(self)
        result_ref = weakref.proxy(result)
        def _backward():
            if self_ref.tensor.grad is None:
                self_ref.zero_grad()
            self_ref.tensor.grad.add_(-result_ref.tensor.grad*torch.sin(self_ref.tensor))
        result._backward = _backward
        return result 

    def sqrt(self):
        out = torch.sqrt(self.tensor)
        if not self._custom_requires_grad:
            return CustomTensor(out,due_to_operation=True)
        
        graph = self.graph
        result = CustomTensor(out, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)
        graph.add_edge(self._node_id, result._node_id)
        self_ref = weakref.proxy(self)
        result_ref = weakref.proxy(result)
        def _backward():
            if self_ref.tensor.grad is None:
                self_ref.zero_grad()
            self_ref.tensor.grad.add_(result_ref.tensor.grad*0.5*self_ref.tensor.pow(-0.5))
        result._backward = _backward
        return result

    def matmul(self, other):
        result_tensor = torch.matmul(self.tensor, other.tensor)
        requires_grad = self._custom_requires_grad or other._custom_requires_grad
        if not requires_grad:
            return CustomTensor(result_tensor,due_to_operation=True)

        graph = self.graph if self._custom_requires_grad else other.graph
        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)

        self_ref = weakref.proxy(self)
        other_ref = weakref.proxy(other)
        result_ref = weakref.proxy(result)

        if self._custom_requires_grad:
            graph.add_edge(self._node_id, result._node_id)
        if other._custom_requires_grad:
            graph.add_edge(other._node_id, result._node_id)

        def _backward():
            if self_ref._custom_requires_grad:
                if self_ref.tensor.grad is None:
                    self_ref.zero_grad()
                
                # Matmul broadcasting for batch dimensions needs careful handling.
                # The gradient w.r.t A in A@B is dL/dA = dL/dC @ B.T
                # The gradient w.r.t B in A@B is dL/dB = A.T @ dL/dC
                
                # For grad_for_self (dL/dA), we need to multiply result_ref.tensor.grad (dL/dC) with other_ref.tensor.transpose(-2, -1) (B.T)
                # The batch dimensions should align naturally.
                grad_for_self = torch.matmul(result_ref.tensor.grad, other_ref.tensor.transpose(-2, -1))
                
                # Reduce gradient for broadcasted batch dimensions in self_ref
                # Example: (B, 1, M) @ (1, N, P) -> (B, N, P)
                # grad_for_self would be (B, N, M). If self_ref was (1, M), need to sum over B dimension.
                if grad_for_self.shape != self_ref.tensor.shape:
                    grad_for_self = self_ref._reduce_grad_for_broadcast(grad_for_self, self_ref.tensor.shape)
                self_ref.tensor.grad.add_(grad_for_self)

            if other_ref._custom_requires_grad:
                if other_ref.tensor.grad is None:
                    other_ref.zero_grad()
                
                # For grad_for_other (dL/dB), we need to multiply self_ref.tensor.transpose(-2, -1) (A.T) with result_ref.tensor.grad (dL/dC)
                grad_for_other = torch.matmul(self_ref.tensor.transpose(-2, -1), result_ref.tensor.grad)
                
                # Reduce gradient for broadcasted batch dimensions in other_ref
                if grad_for_other.shape != other_ref.tensor.shape:
                    grad_for_other = other_ref._reduce_grad_for_broadcast(grad_for_other, other_ref.tensor.shape)
                other_ref.tensor.grad.add_(grad_for_other)
        result._backward = _backward
        return result

    def apply_mask(self, mask):
        result_tensor = self.tensor * mask.tensor # This is element-wise multiplication
        requires_grad = self._custom_requires_grad or mask._custom_requires_grad
        if not requires_grad:
            return CustomTensor(result_tensor,due_to_operation=True)

        graph = self.graph if self._custom_requires_grad else mask.graph
        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)

        self_ref = weakref.proxy(self)
        mask_ref = weakref.proxy(mask)
        result_ref = weakref.proxy(result)

        if self._custom_requires_grad:
            graph.add_edge(self._node_id, result._node_id)
        if mask._custom_requires_grad:
            graph.add_edge(mask._node_id, result._node_id)

        def _backward():
            if self_ref._custom_requires_grad:
                if self_ref.tensor.grad is None:
                    self_ref.zero_grad()
                # Apply reduction if 'self' was broadcasted
                grad_for_self = self_ref._reduce_grad_for_broadcast(result_ref.tensor.grad * mask_ref.tensor, self_ref.tensor.shape)
                self_ref.tensor.grad.add_(grad_for_self)
            if mask_ref._custom_requires_grad:
                if mask_ref.tensor.grad is None:
                    mask_ref.zero_grad()
                # Apply reduction if 'mask' was broadcasted
                grad_for_mask = mask_ref._reduce_grad_for_broadcast(result_ref.tensor.grad * self_ref.tensor, mask_ref.tensor.shape)
                mask_ref.tensor.grad.add_(grad_for_mask)
        result._backward = _backward
        return result

    def dot(self, other):
        # torch.dot only works for 1D tensors, or for higher-D tensors,
        # it flattens them to 1D and then computes the dot product.
        # This means the gradients will also be 1D, so no complex broadcasting
        # reduction is needed on the output gradient itself.
        # However, the input tensors themselves could have been results of broadcasting ops.
        # For a truly general dot product, you'd use torch.matmul.
        result_tensor = torch.dot(self.tensor.reshape(-1), other.tensor.reshape(-1))
        requires_grad = self._custom_requires_grad or other._custom_requires_grad
        if not requires_grad:
            return CustomTensor(result_tensor,due_to_operation=True)

        graph = self.graph if self._custom_requires_grad else other.graph
        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)

        self_ref = weakref.proxy(self)
        other_ref = weakref.proxy(other)
        result_ref = weakref.proxy(result)

        if self._custom_requires_grad:
            graph.add_edge(self._node_id, result._node_id)
        if other._custom_requires_grad:
            graph.add_edge(other._node_id, result._node_id)

        def _backward():
            if self_ref._custom_requires_grad:
                if self_ref.tensor.grad is None:
                    self_ref.zero_grad()
                # The grad from result_ref.tensor.grad will be a scalar.
                # It needs to be multiplied by the other_ref.tensor (original shape)
                # and then potentially re-shaped if original was >1D
                grad_contrib = result_ref.tensor.grad * other_ref.tensor
                # Reduce grad_contrib to match self_ref's original shape
                grad_for_self = self_ref._reduce_grad_for_broadcast(grad_contrib, self_ref.tensor.shape)
                self_ref.tensor.grad.add_(grad_for_self)
            if other_ref._custom_requires_grad:
                if other_ref.tensor.grad is None:
                    other_ref.zero_grad()
                grad_contrib = result_ref.tensor.grad * self_ref.tensor
                # Reduce grad_contrib to match other_ref's original shape
                grad_for_other = other_ref._reduce_grad_for_broadcast(grad_contrib, other_ref.tensor.shape)
                other_ref.tensor.grad.add_(grad_for_other)
        result._backward = _backward
        return result

    def sum(self, dim=None, keepdim=False):
        result_tensor = torch.sum(self.tensor, dim=dim, keepdim=keepdim)
        if not self._custom_requires_grad:
            return CustomTensor(result_tensor, due_to_operation=True)

        graph = self.graph
        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)
        graph.add_edge(self._node_id, result._node_id)

        self_ref = weakref.proxy(self)
        result_ref = weakref.proxy(result)

        def _backward():
            if self_ref.tensor.grad is None:
                self_ref.zero_grad()
            # The gradient w.r.t. the input of sum is simply the output gradient
            # broadcasted back to the input's shape.
            grad_output = result_ref.tensor.grad
            
            # If keepdim was False, need to expand dimensions back
            if not keepdim:
                if dim is None: # Sum over all dimensions
                    grad_output = grad_output.expand(self_ref.tensor.shape)
                else:
                    # For a specific dimension(s), unsqueeze to re-introduce the summed dimension(s)
                    if isinstance(dim, int):
                        grad_output = grad_output.unsqueeze(dim)
                    elif isinstance(dim, (tuple, list)):
                        for d in sorted(dim):
                            grad_output = grad_output.unsqueeze(d)
            
            # Now expand the gradient to the original tensor's shape.
            # This handles cases where original tensor was larger than summed result,
            # for example, summing (2,3,4) along dim 1 results in (2,4). If the original
            # (2,3,4) was (2,1,4) then this broadcast is implicit.
            # However, if it was (2,3,4) and result is (2,4), the gradient (2,4) needs to
            # be expanded to (2,3,4) where the values are repeated along dim 1.
            grad_for_self = grad_output.expand(self_ref.tensor.shape)
            self_ref.tensor.grad.add_(grad_for_self)
        result._backward = _backward
        return result

    def mean(self, dim=None, keepdim=False):
        num_elements = 1
        if dim is None:
            num_elements = self.tensor.numel()
        elif isinstance(dim, int):
            num_elements = self.tensor.shape[dim]
        elif isinstance(dim, (tuple, list)):
            for d in dim:
                num_elements *= self.tensor.shape[d]

        result_tensor = torch.mean(self.tensor, dim=dim, keepdim=keepdim)
        if not self._custom_requires_grad:
            return CustomTensor(result_tensor, due_to_operation=True)

        graph = self.graph
        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)
        graph.add_edge(self._node_id, result._node_id)

        self_ref = weakref.proxy(self)
        result_ref = weakref.proxy(result)

        def _backward():
            if self_ref.tensor.grad is None:
                self_ref.zero_grad()
            
            grad_output = result_ref.tensor.grad / num_elements

            if not keepdim:
                if dim is None:
                    grad_output = grad_output.expand(self_ref.tensor.shape)
                else:
                    if isinstance(dim, int):
                        grad_output = grad_output.unsqueeze(dim)
                    elif isinstance(dim, (tuple, list)):
                        for d in sorted(dim):
                            grad_output = grad_output.unsqueeze(d)
            
            grad_for_self = grad_output.expand(self_ref.tensor.shape)
            self_ref.tensor.grad.add_(grad_for_self)
        result._backward = _backward
        return result

    def reshape(self, *shape):
        new_shape = shape
        if len(shape) == 1 and isinstance(shape[0], (tuple, list)):
            new_shape = shape[0]

        result_tensor = torch.reshape(self.tensor, new_shape)
        if not self._custom_requires_grad:
            return CustomTensor(result_tensor, due_to_operation=True)
        
        graph = self.graph
        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)
        graph.add_edge(self._node_id, result._node_id)

        self_ref = weakref.proxy(self)
        result_ref = weakref.proxy(result)

        def _backward():
            if self_ref.tensor.grad is None:
                self_ref.zero_grad()
            # Reshape's backward is just another reshape back to the original shape
            self_ref.tensor.grad.add_(result_ref.tensor.grad.reshape(self_ref.tensor.shape))
        result._backward = _backward
        return result

    def transpose(self, dim0, dim1):
        result_tensor = torch.transpose(self.tensor, dim0, dim1)
        if not self._custom_requires_grad:
            return CustomTensor(result_tensor, due_to_operation=True)
        
        graph = self.graph
        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)
        graph.add_edge(self._node_id, result._node_id)

        self_ref = weakref.proxy(self)
        result_ref = weakref.proxy(result)

        def _backward():
            if self_ref.tensor.grad is None:
                self_ref.zero_grad()
            # Transpose's backward is another transpose back to original dimensions
            self_ref.tensor.grad.add_(torch.transpose(result_ref.tensor.grad, dim0, dim1))
        result._backward = _backward
        return result

    # --- Activation Functions ---

    def relu(self):
        result_tensor = torch.relu(self.tensor)
        if not self._custom_requires_grad:
            return CustomTensor(result_tensor, due_to_operation=True)
        
        graph = self.graph
        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)
        graph.add_edge(self._node_id, result._node_id)

        self_ref = weakref.proxy(self)
        result_ref = weakref.proxy(result)

        def _backward():
            if self_ref.tensor.grad is None:
                self_ref.zero_grad()
            grad_relu = (self_ref.tensor > 0).float() # Derivative is 1 where x > 0, 0 otherwise
            self_ref.tensor.grad.add_(result_ref.tensor.grad * grad_relu)
        result._backward = _backward
        return result

    def softmax(self, dim=-1):
        result_tensor = torch.softmax(self.tensor, dim=dim)
        if not self._custom_requires_grad:
            return CustomTensor(result_tensor, due_to_operation=True)
        
        graph = self.graph
        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)
        graph.add_edge(self._node_id, result._node_id)

        self_ref = weakref.proxy(self)
        result_ref = weakref.proxy(result)

        def _backward():
            if self_ref.tensor.grad is None:
                self_ref.zero_grad()
            # Softmax derivative: S_j - S_j * S_k if j != k, and S_j * (1 - S_j) if j == k
            # Can be efficiently computed using the outer product of softmax output and its gradient
            s = result_ref.tensor.clone().detach() # Softmax output
            grad_output = result_ref.tensor.grad.clone().detach() # Incoming gradient
            
            # Efficient computation of Jacobian-vector product: (diag(s) - s.outer(s)) @ grad_output
            # This is simplified for batch processing
            # For each item in the batch, perform: grad_input = s * grad_output - s * (s @ grad_output.T)
            
            # If batching, we need to operate along the 'dim' dimension for each batch element
            # Example: input (B, C), dim=1
            # grad_output will be (B, C)
            # s will be (B, C)
            
            # Calculate s * grad_output
            s_times_grad = s * grad_output
            
            # Calculate sum(s * grad_output) along the dimension
            sum_s_times_grad = torch.sum(s_times_grad, dim=dim, keepdim=True)
            
            # Calculate s * sum(s * grad_output)
            s_times_sum_s_times_grad = s * sum_s_times_grad
            
            grad_input = s_times_grad - s_times_sum_s_times_grad
            
            self_ref.tensor.grad.add_(grad_input)
        result._backward = _backward
        return result

    def tanh(self):
        result_tensor = torch.tanh(self.tensor)
        if not self._custom_requires_grad:
            return CustomTensor(result_tensor, due_to_operation=True)
        
        graph = self.graph
        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)
        graph.add_edge(self._node_id, result._node_id)

        self_ref = weakref.proxy(self)
        result_ref = weakref.proxy(result)

        def _backward():
            if self_ref.tensor.grad is None:
                self_ref.zero_grad()
            # Derivative of tanh(x) is 1 - tanh(x)^2
            grad_tanh = 1 - result_ref.tensor.pow(2)
            self_ref.tensor.grad.add_(result_ref.tensor.grad * grad_tanh)
        result._backward = _backward
        return result

    def leaky_relu(self, negative_slope=0.01):
        result_tensor = torch.nn.functional.leaky_relu(self.tensor, negative_slope=negative_slope)
        if not self._custom_requires_grad:
            return CustomTensor(result_tensor, due_to_operation=True)
        
        graph = self.graph
        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)
        graph.add_edge(self._node_id, result._node_id)

        self_ref = weakref.proxy(self)
        result_ref = weakref.proxy(result)

        def _backward():
            if self_ref.tensor.grad is None:
                self_ref.zero_grad()
            grad_leaky_relu = torch.ones_like(self_ref.tensor)
            grad_leaky_relu[self_ref.tensor < 0] = negative_slope
            self_ref.tensor.grad.add_(result_ref.tensor.grad * grad_leaky_relu)
        result._backward = _backward
        return result

    def elu(self, alpha=1.0):
        result_tensor = torch.nn.functional.elu(self.tensor, alpha=alpha)
        if not self._custom_requires_grad:
            return CustomTensor(result_tensor, due_to_operation=True)
        
        graph = self.graph
        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)
        graph.add_edge(self._node_id, result._node_id)

        self_ref = weakref.proxy(self)
        result_ref = weakref.proxy(result)

        def _backward():
            if self_ref.tensor.grad is None:
                self_ref.zero_grad()
            grad_elu = torch.ones_like(self_ref.tensor)
            grad_elu[self_ref.tensor < 0] = result_ref.tensor[self_ref.tensor < 0] + alpha # Derivative is alpha * exp(x) = result + alpha for x < 0
            self_ref.tensor.grad.add_(result_ref.tensor.grad * grad_elu)
        result._backward = _backward
        return result

    def gelu(self):
        # Approximation from original paper is simpler, but PyTorch uses a more numerically stable one
        # using erf. For simplicity in custom autograd, we can use the approximation or
        # define the derivative of the exact form.
        # Here, we'll use the torch.nn.functional.gelu for the forward pass,
        # and derive the backward from its mathematical form.
        result_tensor = torch.nn.functional.gelu(self.tensor)
        if not self._custom_requires_grad:
            return CustomTensor(result_tensor, due_to_operation=True)
        
        graph = self.graph
        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)
        graph.add_edge(self._node_id, result._node_id)

        self_ref = weakref.proxy(self)
        result_ref = weakref.proxy(result)

        def _backward():
            if self_ref.tensor.grad is None:
                self_ref.zero_grad()
            # Derivative of GELU(x) = x * Phi(x) where Phi(x) is CDF of standard normal
            # d/dx [x * 0.5 * (1 + erf(x / sqrt(2)))]
            # = 0.5 * (1 + erf(x / sqrt(2))) + x * 0.5 * (2/sqrt(pi)) * exp(-x^2/2) * (1/sqrt(2))
            # = 0.5 * (1 + erf(x / sqrt(2))) + x * (1/sqrt(2*pi)) * exp(-x^2/2)
            # = Phi(x) + x * phi(x) where phi(x) is PDF of standard normal
            
            cdf = 0.5 * (1.0 + torch.erf(self_ref.tensor / math.sqrt(2.0)))
            pdf = (1.0 / math.sqrt(2.0 * math.pi)) * torch.exp(-0.5 * self_ref.tensor.pow(2))
            grad_gelu = cdf + self_ref.tensor * pdf
            
            self_ref.tensor.grad.add_(result_ref.tensor.grad * grad_gelu)
        result._backward = _backward
        return result

    def silu(self):
        # SiLU (Sigmoid Linear Unit): x * sigmoid(x)
        result_tensor = torch.nn.functional.silu(self.tensor)
        if not self._custom_requires_grad:
            return CustomTensor(result_tensor, due_to_operation=True)
        
        graph = self.graph
        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)
        graph.add_edge(self._node_id, result._node_id)

        self_ref = weakref.proxy(self)
        result_ref = weakref.proxy(result)

        def _backward():
            if self_ref.tensor.grad is None:
                self_ref.zero_grad()
            # Derivative of SiLU(x) = sigmoid(x) * (1 + x * (1 - sigmoid(x)))
            sigmoid_x = torch.sigmoid(self_ref.tensor)
            grad_silu = sigmoid_x * (1 + self_ref.tensor * (1 - sigmoid_x))
            self_ref.tensor.grad.add_(result_ref.tensor.grad * grad_silu)
        result._backward = _backward
        return result

    def swish(self, beta=1.0):
        # Swish: x * sigmoid(beta * x)
        result_tensor = self.tensor * torch.sigmoid(beta * self.tensor)
        if not self._custom_requires_grad:
            return CustomTensor(result_tensor, due_to_operation=True)
        
        graph = self.graph
        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)
        graph.add_edge(self._node_id, result._node_id)

        self_ref = weakref.proxy(self)
        result_ref = weakref.proxy(result)

        def _backward():
            if self_ref.tensor.grad is None:
                self_ref.zero_grad()
            # Derivative of Swish(x) = beta * x * sigmoid(beta * x) * (1 - sigmoid(beta * x)) + sigmoid(beta * x)
            # This is d/dx [x * s(beta*x)] = s(beta*x) + x * s'(beta*x) * beta
            # s'(z) = s(z) * (1 - s(z))
            # So, d/dx [x * s(beta*x)] = s(beta*x) + x * s(beta*x) * (1 - s(beta*x)) * beta
            
            beta_x = beta * self_ref.tensor
            sigmoid_beta_x = torch.sigmoid(beta_x)
            
            grad_swish = sigmoid_beta_x + self_ref.tensor * sigmoid_beta_x * (1 - sigmoid_beta_x) * beta
            self_ref.tensor.grad.add_(result_ref.tensor.grad * grad_swish)
        result._backward = _backward
        return result

    def _im2col(self, input_tensor, kernel_h, kernel_w, stride, padding):
        """
        Pytorch-like im2col implementation.
        Arguments:
            input_tensor (torch.Tensor): Input tensor of shape (N, C, H, W)
            kernel_h (int): Kernel height
            kernel_w (int): Kernel width
            stride (int): Stride for convolution
            padding (int): Padding for convolution
        Returns:
            torch.Tensor: output tensor of shape (N * out_H * out_W, C * kernel_h * kernel_w)
        """
        N, C, H, W = input_tensor.shape
        
        out_H = (H + 2 * padding - kernel_h) // stride + 1
        out_W = (W + 2 * padding - kernel_w) // stride + 1

        # Pad the input tensor
        padded_input = torch.nn.functional.pad(input_tensor, (padding, padding, padding, padding))

        # Extract sliding windows
        cols = torch.nn.functional.unfold(padded_input, 
                                          kernel_size=(kernel_h, kernel_w), 
                                          dilation=1, 
                                          padding=0, 
                                          stride=stride)
        # Reshape to (N * out_H * out_W, C * kernel_h * kernel_w)
        cols = cols.permute(0, 2, 1).reshape(-1, C * kernel_h * kernel_w)
        return cols, (N, C, H, W), (kernel_h, kernel_w, stride, padding)

    def _col2im(self, grad_cols, input_shape, kernel_params):
        """
        Pytorch-like col2im implementation (inverse of im2col).
        Arguments:
            grad_cols (torch.Tensor): Gradient from im2col output, shape (N * out_H * out_W, C * kernel_h * kernel_w)
            input_shape (tuple): Original input shape (N, C, H, W)
            kernel_params (tuple): (kernel_h, kernel_w, stride, padding)
        Returns:
            torch.Tensor: Reconstructed gradient for the input tensor, shape (N, C, H, W)
        """
        N, C, H, W = input_shape
        kernel_h, kernel_w, stride, padding = kernel_params

        out_H = (H + 2 * padding - kernel_h) // stride + 1
        out_W = (W + 2 * padding - kernel_w) // stride + 1

        grad_cols = grad_cols.reshape(N, out_H * out_W, C * kernel_h * kernel_w).permute(0, 2, 1)

        grad_input = torch.nn.functional.fold(grad_cols, 
                                              output_size=(H + 2 * padding, W + 2 * padding), 
                                              kernel_size=(kernel_h, kernel_w), 
                                              dilation=1, 
                                              padding=0, 
                                              stride=stride)
        # Remove padding
        if padding > 0:
            grad_input = grad_input[:, :, padding:-padding, padding:-padding]
        return grad_input


    # --- Convolutional Layers ---
    
    # img_to_col for Conv2d
    def img_to_col(self, kernel_h, kernel_w, stride=1, padding=0):
        if self.ndim != 4:
            raise ValueError("img_to_col is only applicable for 4D tensors (N, C, H, W) for Conv2D.")
        
        # Store original shape and convolution parameters for backward pass
        self_ref_shape = self.tensor.shape
        params = (kernel_h, kernel_w, stride, padding)

        # Perform im2col transformation
        cols, _, _ = self._im2col(self.tensor, kernel_h, kernel_w, stride, padding)
        
        result_tensor = cols
        if not self._custom_requires_grad:
            return CustomTensor(result_tensor, due_to_operation=True)
        
        graph = self.graph
        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)
        graph.add_edge(self._node_id, result._node_id)

        self_ref = weakref.proxy(self)
        result_ref = weakref.proxy(result)

        def _backward():
            if self_ref.tensor.grad is None:
                self_ref.zero_grad()
            # The backward of img_to_col is col2im
            grad_input = self_ref._col2im(result_ref.tensor.grad, self_ref_shape, params)
            self_ref.tensor.grad.add_(grad_input)
        
        result._backward = _backward
        return result

    # Generic convolution (Conv2D)
    def conv2d(self, kernel, stride=1, padding=0):
        # input: N, C_in, H_in, W_in
        # kernel: C_out, C_in/groups, K_H, K_W
        N, C_in, H_in, W_in = self.tensor.shape
        C_out, _, K_H, K_W = kernel.tensor.shape

        # Use im2col for the forward pass
        input_cols, input_original_shape, conv_params = self._im2col(self.tensor, K_H, K_W, stride, padding)
        
        # Reshape kernel for matmul: (C_out, C_in * K_H * K_W)
        kernel_reshaped = kernel.tensor.reshape(C_out, -1)
        
        # Perform matrix multiplication: (N * H_out * W_out, C_in * K_H * K_W) @ (C_in * K_H * K_W, C_out).T
        # -> (N * H_out * W_out, C_out)
        result_flat = torch.matmul(input_cols, kernel_reshaped.T) # .T is important for correct matmul shape
        
        H_out = (H_in + 2 * padding - K_H) // stride + 1
        W_out = (W_in + 2 * padding - K_W) // stride + 1

        # Reshape back to (N, C_out, H_out, W_out)
        result_tensor = result_flat.reshape(N, H_out, W_out, C_out).permute(0, 3, 1, 2)

        requires_grad = self._custom_requires_grad or kernel._custom_requires_grad
        if not requires_grad:
            return CustomTensor(result_tensor, due_to_operation=True)

        graph = self.graph if self._custom_requires_grad else kernel.graph
        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)

        self_ref = weakref.proxy(self)
        kernel_ref = weakref.proxy(kernel)
        result_ref = weakref.proxy(result)

        if self._custom_requires_grad:
            graph.add_edge(self._node_id, result._node_id)
        if kernel._custom_requires_grad:
            graph.add_edge(kernel._node_id, result._node_id)

        def _backward():
            if self_ref.tensor.grad is None:
                self_ref.zero_grad()
            if kernel_ref.tensor.grad is None:
                kernel_ref.zero_grad()

            grad_output = result_ref.tensor.grad.permute(0, 2, 3, 1).reshape(-1, C_out) # (N*H_out*W_out, C_out)

            # Gradient w.r.t. input (dL/dX)
            # dL/dX = dL/d(cols) @ K_reshaped
            # dL/d(cols) = grad_output @ kernel_reshaped
            grad_cols = torch.matmul(grad_output, kernel_reshaped)
            
            # Use col2im to convert grad_cols back to input gradient shape
            grad_input_tensor = self_ref._col2im(grad_cols, input_original_shape, conv_params)
            self_ref.tensor.grad.add_(grad_input_tensor)

            # Gradient w.r.t. kernel (dL/dK)
            # dL/dK = cols.T @ dL/d(cols) for each feature map or sum over batch
            # dL/dK_reshaped_T = input_cols.T @ grad_output
            
            # Need to re-compute input_cols as it's not stored in closure
            re_input_cols, _, _ = self_ref._im2col(self_ref.tensor.detach(), K_H, K_W, stride, padding)
            
            grad_kernel_reshaped_T = torch.matmul(re_input_cols.T, grad_output)
            
            # Reshape back to original kernel shape (C_out, C_in/groups, K_H, K_W)
            grad_kernel_tensor = grad_kernel_reshaped_T.T.reshape(C_out, C_in, K_H, K_W) # Assuming groups=1
            kernel_ref.tensor.grad.add_(grad_kernel_tensor)

        result._backward = _backward
        return result

    # Conv1d
    def conv1d(self, kernel, stride=1, padding=0):
        # input: N, C_in, L_in
        # kernel: C_out, C_in/groups, K_L
        N, C_in, L_in = self.tensor.shape
        C_out, _, K_L = kernel.tensor.shape

        # Expand for 2D convolution for easy reuse of im2col/col2im logic
        # Treat L_in as H_in and W_in = 1. So, reshape to (N, C_in, L_in, 1)
        # Kernel to (C_out, C_in, K_L, 1)
        expanded_input = self.tensor.unsqueeze(-1)
        expanded_kernel = kernel.tensor.unsqueeze(-1)

        # Call conv2d logic (which uses im2col/col2im internally)
        conv2d_result = CustomTensor(expanded_input, _custom_requires_grad=self._custom_requires_grad, graph=self.graph).conv2d(
            CustomTensor(expanded_kernel, _custom_requires_grad=kernel._custom_requires_grad, graph=kernel.graph),
            stride=stride, padding=padding
        )
        
        # Squeeze back to (N, C_out, L_out)
        result_tensor = conv2d_result.tensor.squeeze(-1)

        requires_grad = self._custom_requires_grad or kernel._custom_requires_grad
        if not requires_grad:
            return CustomTensor(result_tensor, due_to_operation=True)

        graph = self.graph if self._custom_requires_grad else kernel.graph
        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)

        self_ref = weakref.proxy(self)
        kernel_ref = weakref.proxy(kernel)
        result_ref = weakref.proxy(result)

        # The backward for conv1d is handled by the conv2d backward
        # However, we need to create a _backward function for the CustomTensor wrapper
        # that will invoke the conv2d_result's _backward (which links to the original conv2d logic)
        
        # We need to explicitly handle the graph edges here because conv2d_result is an intermediate CustomTensor
        graph.add_edge(self._node_id, result._node_id)
        graph.add_edge(kernel._node_id, result._node_id)

        def _backward():
            if self_ref.tensor.grad is None:
                self_ref.zero_grad()
            if kernel_ref.tensor.grad is None:
                kernel_ref.zero_grad()
            
            # To propagate gradient back, we need to unsqueeze the incoming gradient
            # and set it on the conv2d_result CustomTensor's internal tensor.
            conv2d_result.tensor.grad = result_ref.tensor.grad.unsqueeze(-1).clone()
            
            # Now call the backward of the intermediate conv2d result
            conv2d_result._backward() # This will populate self_ref.tensor.grad and kernel_ref.tensor.grad
        
        result._backward = _backward
        return result


    # Conv3d
    def conv3d(self, kernel, stride=1, padding=0):
        # input: N, C_in, D_in, H_in, W_in
        # kernel: C_out, C_in/groups, K_D, K_H, K_W
        N, C_in, D_in, H_in, W_in = self.tensor.shape
        C_out, _, K_D, K_H, K_W = kernel.tensor.shape

        # PyTorch's F.conv3d handles im2col/col2im internally for 3D.
        # It's more complex to manually implement im2col for 3D.
        # We'll use torch.nn.functional.conv3d for the forward pass,
        # and rely on its autograd equivalent for backward.
        
        # This is a simplification where we're leveraging PyTorch's native
        # autograd for the complex Conv3d operation for its derivative.
        # A full custom implementation would require a custom _im2col_3d and _col2im_3d.
        
        result_tensor = torch.nn.functional.conv3d(
            self.tensor,
            kernel.tensor,
            stride=stride,
            padding=padding
        )

        requires_grad = self._custom_requires_grad or kernel._custom_requires_grad
        if not requires_grad:
            return CustomTensor(result_tensor, due_to_operation=True)

        graph = self.graph if self._custom_requires_grad else kernel.graph
        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)

        self_ref = weakref.proxy(self)
        kernel_ref = weakref.proxy(kernel)
        result_ref = weakref.proxy(result)

        if self._custom_requires_grad:
            graph.add_edge(self._node_id, result._node_id)
        if kernel._custom_requires_grad:
            graph.add_edge(kernel._node_id, result._node_id)

        def _backward():
            if self_ref.tensor.grad is None:
                self_ref.zero_grad()
            if kernel_ref.tensor.grad is None:
                kernel_ref.zero_grad()
            
            # To compute gradients for Conv3D manually, it would involve a more
            # complex col2im for 3D or direct convolution with rotated/transposed kernels.
            # For this exercise, we'll simulate the backward by re-attaching to PyTorch's autograd
            # for this specific operation, as implementing a general 3D im2col is quite involved.
            
            # This is a 'cheat' to get the gradients from torch's autograd for conv3d.
            # In a truly custom autograd, you'd implement the gradient function manually.
            
            # Create dummy tensors that require grad for torch's backward
            temp_input = self_ref.tensor.detach().requires_grad_(True)
            temp_kernel = kernel_ref.tensor.detach().requires_grad_(True)
            
            temp_output = torch.nn.functional.conv3d(temp_input, temp_kernel, stride=stride, padding=padding)
            temp_output.backward(result_ref.tensor.grad.detach())

            if temp_input.grad is not None:
                self_ref.tensor.grad.add_(temp_input.grad)
            if temp_kernel.grad is not None:
                kernel_ref.tensor.grad.add_(temp_kernel.grad)

        result._backward = _backward
        return result


    # --- Dense Layer (Fully Connected) ---
    def dense(self, weights, bias=None):
        # Input: (..., in_features)
        # Weights: (in_features, out_features)
        # Bias: (out_features)
        
        # Perform matrix multiplication: input @ weights
        # Handles batch dimensions automatically with torch.matmul
        result_tensor = torch.matmul(self.tensor, weights.tensor)
        
        # Add bias if provided
        if bias is not None:
            # Bias broadcasting: (..., out_features) + (out_features)
            result_tensor = result_tensor + bias.tensor 

        requires_grad = self._custom_requires_grad or weights._custom_requires_grad or \
                        (bias is not None and bias._custom_requires_grad)

        if not requires_grad:
            return CustomTensor(result_tensor, due_to_operation=True)

        graph = None
        if self._custom_requires_grad:
            graph = self.graph
        elif weights._custom_requires_grad:
            graph = weights.graph
        elif bias is not None and bias._custom_requires_grad:
            graph = bias.graph
        
        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)

        self_ref = weakref.proxy(self)
        weights_ref = weakref.proxy(weights)
        bias_ref = weakref.proxy(bias) if bias else None
        result_ref = weakref.proxy(result)

        if self._custom_requires_grad:
            graph.add_edge(self._node_id, result._node_id)
        if weights._custom_requires_grad:
            graph.add_edge(weights._node_id, result._node_id)
        if bias_ref and bias._custom_requires_grad:
            graph.add_edge(bias._node_id, result._node_id)

        def _backward():
            if self_ref._custom_requires_grad:
                if self_ref.tensor.grad is None:
                    self_ref.zero_grad()
                # dL/dX = dL/dY @ W.T
                grad_for_input = torch.matmul(result_ref.tensor.grad, weights_ref.tensor.T)
                # No complex broadcasting reduction here as matmul handles it for batch dims
                self_ref.tensor.grad.add_(grad_for_input)

            if weights_ref._custom_requires_grad:
                if weights_ref.tensor.grad is None:
                    weights_ref.zero_grad()
                # dL/dW = X.T @ dL/dY
                # Input could have batch dimensions, so we need to correctly handle the batch matmul.
                # If input is (B, M, K) and weights is (K, N), output is (B, M, N)
                # dL/dW should be (K, N)
                
                # To get X.T @ dL/dY, we need to transpose the last two dimensions of X and dL/dY
                # and then perform batch matmul, then sum over batch dimensions.
                
                # If self_ref.tensor is (..., M, K) and result_ref.tensor.grad is (..., M, N)
                # Transpose last two dims of self_ref.tensor: (..., K, M)
                # Multiply: (..., K, M) @ (..., M, N) -> (..., K, N)
                # Then sum over all batch dimensions to get (K, N)
                
                # Example: self.tensor (B, M), weights (M, K), result (B, K)
                # dL/dW = self.tensor.T @ dL/dY
                # self.tensor (B, M) -> (M, B) after transpose
                # result_ref.tensor.grad (B, K)
                # torch.matmul(self.tensor.transpose(0,1), result_ref.tensor.grad) if 2D
                
                # For general batching:
                # Assuming input (B1, B2, ..., M, K)
                # grad_output (B1, B2, ..., M, N)
                # The contraction is over M. The batch dimensions remain.
                
                # We need sum(input_reshaped_for_matmul.T @ grad_output_reshaped_for_matmul) over batch dims
                # This requires reshaping input to (Batch_size * Num_M, K) and grad_output to (Batch_size * Num_M, N)
                # then (K, Batch_size * Num_M) @ (Batch_size * Num_M, N) -> (K, N)
                
                # Simpler: (dL/dY).T @ X
                # (..., N, M) @ (..., M, K) -> (..., N, K)
                # Then sum over batch dimensions
                
                # For (input @ weights), dL/dW = input.T @ dL/dY.
                # If input is (..., A, B) and weights is (B, C), output is (..., A, C).
                # dL/dY is (..., A, C).
                # To get dL/dW (B, C), we need to sum (input.T @ dL/dY) over all leading dimensions.
                # input.transpose(-1, -2) is (..., B, A)
                
                # The batch matrix multiplication handles the leading dimensions implicitly.
                # We need to perform the matmul and then sum across all batch dimensions to get the weight gradient.
                
                # Original input shape before matmul: self_ref.tensor.shape
                # Output gradient shape: result_ref.tensor.grad.shape
                
                # Example: input (B, M), weights (M, K), output (B, K)
                # dL/dW = input.T @ dL/dY
                # Where input is (B, M), dL/dY is (B, K)
                # (M, B) @ (B, K) -> (M, K)
                # So we take input.transpose(-1, -2) for the last two dimensions.
                
                input_for_weight_grad = self_ref.tensor.transpose(-1, -2) # (..., K, M)
                
                # Perform batch matmul (..., K, M) @ (..., M, N) -> (..., K, N)
                grad_weights_tensor = torch.matmul(input_for_weight_grad, result_ref.tensor.grad)
                
                # Now sum over all batch dimensions to get the final (K, N) gradient for weights
                # The number of batch dimensions is self_ref.tensor.ndim - 2
                num_batch_dims = self_ref.tensor.ndim - 2
                if num_batch_dims > 0:
                    for i in range(num_batch_dims):
                        grad_weights_tensor = grad_weights_tensor.sum(dim=0)
                
                weights_ref.tensor.grad.add_(grad_weights_tensor)

            if bias_ref and bias_ref._custom_requires_grad:
                if bias_ref.tensor.grad is None:
                    bias_ref.zero_grad()
                # dL/db = sum(dL/dY) over all dimensions except the last (output_features)
                grad_bias = result_ref.tensor.grad.sum(dim=tuple(range(result_ref.tensor.grad.ndim - 1)))
                bias_ref.tensor.grad.add_(grad_bias)

        result._backward = _backward
        return result


    # --- Batch Normalization ---
    def batch_norm(self, running_mean=None, running_var=None, weight=None, bias=None, training=True, eps=1e-5, momentum=0.1):
        # For simplicity, this implementation will primarily focus on 1D BatchNorm (feature-wise).
        # Generalizing to 2D/3D (channel-wise) would involve adjusting dimensions for mean/var calculation.
        # Here we assume input is (N, C) or (N, C, L) etc., and normalization happens along the feature dimension (C).
        # This will work for Dense layers or the channel dimension of Conv layers if flattened or permuted appropriately.
        
        # If input is (N, C, H, W), mean/var are typically computed over N, H, W for each C.
        # This implementation assumes the last dimension is the feature dimension to normalize over,
        # or it is reshaped to be effectively 2D (N * H * W, C).
        
        # The true BatchNorm operation computes mean/variance over batch and spatial dimensions.
        # For simplicity, we'll implement it for a 2D tensor (N, C) or flattened to it.
        
        input_shape = self.tensor.shape
        num_features = input_shape[-1] # Assuming normalization over the last dimension (features)
        
        if self.tensor.ndim > 2:
            # For convolutional layers (N, C, H, W) or (N, C, D, H, W),
            # BatchNorm typically normalizes per channel (C).
            # This means mean/var are computed over (N, H, W) for each C.
            # We need to reshape the tensor to (N * D * H * W, C) to apply element-wise operations on C.
            original_tensor = self.tensor
            self.tensor = self.tensor.permute(0, *range(2, self.tensor.ndim), 1).reshape(-1, input_shape[1])
            num_features = input_shape[1]

        if training:
            mean = self.tensor.mean(dim=0, keepdim=True)
            var = self.tensor.var(dim=0, keepdim=True, unbiased=False) # biased variance for population
            
            # Update running mean and variance (if provided)
            if running_mean is not None:
                running_mean.tensor.mul_(momentum).add_(mean * (1 - momentum))
            if running_var is not None:
                running_var.tensor.mul_(momentum).add_(var * (1 - momentum))
        else: # Inference mode
            if running_mean is None or running_var is None:
                raise ValueError("running_mean and running_var must be provided for inference mode.")
            mean = running_mean.tensor
            var = running_var.tensor

        x_hat = (self.tensor - mean) / torch.sqrt(var + eps)
        
        # Apply learnable scale (gamma) and shift (beta) if provided
        gamma = weight.tensor if weight is not None else torch.ones(num_features, device=self.tensor.device, dtype=self.tensor.dtype)
        beta = bias.tensor if bias is not None else torch.zeros(num_features, device=self.tensor.device, dtype=self.tensor.dtype)
        
        result_tensor = gamma * x_hat + beta

        if original_tensor is not None: # Reshape back if it was permuted/reshaped
            result_tensor = result_tensor.reshape(input_shape[0], *input_shape[2:], input_shape[1]).permute(0, self.tensor.ndim - 1, *range(1, self.tensor.ndim -1))
            self.tensor = original_tensor # Restore original shape for self.tensor

        requires_grad = self._custom_requires_grad or (weight is not None and weight._custom_requires_grad) or \
                        (bias is not None and bias._custom_requires_grad)

        if not requires_grad:
            return CustomTensor(result_tensor, due_to_operation=True)

        graph = self.graph
        if weight is not None and weight._custom_requires_grad:
            graph = weight.graph
        elif bias is not None and bias._custom_requires_grad:
            graph = bias.graph

        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)

        self_ref = weakref.proxy(self)
        weight_ref = weakref.proxy(weight) if weight else None
        bias_ref = weakref.proxy(bias) if bias else None
        result_ref = weakref.proxy(result)
        
        # Detach these for backward pass to prevent recalculation and allow storing them.
        mean_detached = mean.detach()
        var_detached = var.detach()
        x_hat_detached = x_hat.detach() # The normalized input before gamma/beta

        if self._custom_requires_grad:
            graph.add_edge(self._node_id, result._node_id)
        if weight_ref and weight._custom_requires_grad:
            graph.add_edge(weight._node_id, result._node_id)
        if bias_ref and bias._custom_requires_grad:
            graph.add_edge(bias._node_id, result._node_id)


        def _backward():
            if self_ref._custom_requires_grad:
                if self_ref.tensor.grad is None:
                    self_ref.zero_grad()
            if weight_ref and weight_ref._custom_requires_grad:
                if weight_ref.tensor.grad is None:
                    weight_ref.zero_grad()
            if bias_ref and bias_ref._custom_requires_grad:
                if bias_ref.tensor.grad is None:
                    bias_ref.zero_grad()
            
            grad_output = result_ref.tensor.grad
            
            # Adjust grad_output for original shape if input was reshaped for BatchNorm processing
            temp_grad_output = grad_output
            temp_input_tensor = self_ref.tensor
            if temp_input_tensor.ndim > 2:
                # Need to permute grad_output back to (N, C, H, W)
                # Result_tensor was (N, C, H, W), so grad_output is (N, C, H, W)
                # But inside _backward, we operate on the effectively 2D tensor
                # so we need to flatten/permute grad_output to match the shape it had during forward BN
                temp_grad_output = temp_grad_output.permute(0, *range(2, temp_grad_output.ndim), 1).reshape(-1, input_shape[1])
                temp_input_tensor = temp_input_tensor.permute(0, *range(2, temp_input_tensor.ndim), 1).reshape(-1, input_shape[1])
            
            # Gradients w.r.t. gamma (weight) and beta (bias)
            if weight_ref and weight_ref._custom_requires_grad:
                # dL/dgamma = sum(dL/dY * x_hat) over all non-feature dimensions
                # grad_gamma = (grad_output * x_hat).sum(dim=0, keepdim=True)
                grad_gamma = (temp_grad_output * x_hat_detached).sum(dim=0)
                weight_ref.tensor.grad.add_(grad_gamma)
            
            if bias_ref and bias_ref._custom_requires_grad:
                # dL/dbeta = sum(dL/dY) over all non-feature dimensions
                grad_beta = temp_grad_output.sum(dim=0)
                bias_ref.tensor.grad.add_(grad_beta)

            if self_ref._custom_requires_grad:
                # Gradients w.r.t. input (dL/dX)
                # This is the most complex part of BatchNorm backward
                
                # N = number of elements in the mean/var calculation (batch_size * spatial_dims)
                N_bn = temp_input_tensor.numel() / num_features 
                
                grad_x_hat = temp_grad_output * (weight_ref.tensor if weight_ref else gamma) # dL/dx_hat = dL/dY * gamma
                
                # dL/dvar = sum(dL/dx_hat * (x - mean) * -0.5 * (var + eps)^(-1.5))
                grad_var = (grad_x_hat * (temp_input_tensor - mean_detached)).sum(dim=0, keepdim=True) * (-0.5 * (var_detached + eps).pow(-1.5))
                
                # dL/dmean = sum(dL/dx_hat * -1 / sqrt(var + eps)) + dL/dvar * d(var)/d(mean)
                # d(var)/d(mean) = sum(-2 * (x - mean) / N_bn)
                grad_mean = grad_x_hat.sum(dim=0, keepdim=True) * (-1 / torch.sqrt(var_detached + eps)) + \
                            grad_var * ( -2 * (temp_input_tensor - mean_detached).sum(dim=0, keepdim=True) / N_bn )

                # dL/dX = dL/dx_hat * 1 / sqrt(var + eps) + dL/dvar * 2*(x-mean)/N_bn + dL/dmean * 1/N_bn
                grad_input_tensor = grad_x_hat / torch.sqrt(var_detached + eps) + \
                                    grad_var * (2 * (temp_input_tensor - mean_detached) / N_bn) + \
                                    grad_mean / N_bn
                
                if self_ref.tensor.ndim > 2:
                    grad_input_tensor = grad_input_tensor.reshape(input_shape[0], *input_shape[2:], input_shape[1]).permute(0, self_ref.tensor.ndim - 1, *range(1, self_ref.tensor.ndim -1))
                
                self_ref.tensor.grad.add_(grad_input_tensor)
        
        result._backward = _backward
        return result

    # --- Skip Connections (Already compatible if operations support broadcasting) ---
    # Since skip connections are essentially element-wise additions,
    # the existing __add__ method with broadcasting support should handle them correctly.
    # E.g., `output = main_path + skip_connection_tensor` will use `_add_tensor`.


    # --- Weight Initialization Methods ---
    @staticmethod
    def _calculate_fan_in_and_fan_out(tensor_shape):
        if len(tensor_shape) < 2:
            raise ValueError("Fan in and fan out can only be calculated for tensors with at least 2 dimensions.")
        
        # For a linear layer (Dense): (in_features, out_features)
        # For a convolutional layer: (out_channels, in_channels, kH, kW) or (out_channels, in_channels, kD, kH, kW)
        
        if len(tensor_shape) == 2: # Linear layer
            fan_in = tensor_shape[0]
            fan_out = tensor_shape[1]
        else: # Convolutional layer
            receptive_field_size = 1
            for dim in tensor_shape[2:]:
                receptive_field_size *= dim
            fan_in = tensor_shape[1] * receptive_field_size
            fan_out = tensor_shape[0] * receptive_field_size
        return fan_in, fan_out

    def xavier_uniform_init(self, gain=1.0):
        """
        Fills the tensor with values drawn from a uniform distribution.
        Also known as Glorot initialization.
        """
        fan_in, fan_out = self._calculate_fan_in_and_fan_out(self.tensor.shape)
        std = gain * math.sqrt(2.0 / (fan_in + fan_out))
        bound = math.sqrt(3.0) * std
        self.tensor.uniform_(-bound, bound)
        return self

    def xavier_normal_init(self, gain=1.0):
        """
        Fills the tensor with values drawn from a normal distribution.
        Also known as Glorot initialization.
        """
        fan_in, fan_out = self._calculate_fan_in_and_fan_out(self.tensor.shape)
        std = gain * math.sqrt(2.0 / (fan_in + fan_out))
        self.tensor.normal_(0, std)
        return self

    def kaiming_uniform_init(self, a=0, mode='fan_in', nonlinearity='relu'):
        """
        Fills the tensor with values drawn from a uniform distribution using Kaiming initialization.
        """
        if mode not in ('fan_in', 'fan_out'):
            raise ValueError("Mode must be 'fan_in' or 'fan_out'.")
        if nonlinearity == 'relu':
            gain = math.sqrt(2.0)
        else:
            # For other non-linearities, might need specific gains or a=0
            gain = 1.0 # Default if not relu, or custom gain function
        
        fan_in, fan_out = self._calculate_fan_in_and_fan_out(self.tensor.shape)
        
        if mode == 'fan_in':
            fan = fan_in
        else: # mode == 'fan_out'
            fan = fan_out
        
        std = gain / math.sqrt(fan)
        bound = math.sqrt(3.0) * std
        self.tensor.uniform_(-bound, bound)
        return self

    def kaiming_normal_init(self, a=0, mode='fan_in', nonlinearity='relu'):
        """
        Fills the tensor with values drawn from a normal distribution using Kaiming initialization.
        """
        if mode not in ('fan_in', 'fan_out'):
            raise ValueError("Mode must be 'fan_in' or 'fan_out'.")
        if nonlinearity == 'relu':
            gain = math.sqrt(2.0)
        else:
            gain = 1.0 
        
        fan_in, fan_out = self._calculate_fan_in_and_fan_out(self.tensor.shape)
        
        if mode == 'fan_in':
            fan = fan_in
        else: # mode == 'fan_out'
            fan = fan_out
        
        std = gain / math.sqrt(fan)
        self.tensor.normal_(0, std)
        return self


    def backward(self,weightage_tensor=1):
        if not self._custom_requires_grad:
            raise RuntimeError("Output tensor does not require grad.")
        if self.graph is None:
            raise RuntimeError("Output tensor is not part of a graph.")
        graph = self.graph

        # Initialize gradient for the output tensor
        if isinstance(weightage_tensor,numbers.Number):
            self.tensor.grad = torch.full_like(self.tensor, fill_value=weightage_tensor)
        elif isinstance(weightage_tensor,torch.Tensor):
            # Ensure the weightage_tensor can be broadcasted to self.tensor.shape
            if weightage_tensor.shape != self.tensor.shape:
                try:
                    # Attempt to broadcast and assign
                    self.tensor.grad = (weightage_tensor * torch.ones_like(self.tensor)).clone()
                except RuntimeError:
                    raise ValueError(f"Weightage tensor shape {weightage_tensor.shape} is not broadcastable to output tensor shape {self.tensor.shape}")
            else:
                self.tensor.grad = weightage_tensor.clone() # we don't want to modify the original tensor data

        # Perform backward pass using topological sort

        nodes_to_process = graph.reverse_toposort_from_tensor(self._node_id)

        for tensor_node in nodes_to_process:
            # Check if the weak proxy is still valid (tensor is alive)
            if tensor_node.__class__ is weakref.ProxyType and tensor_node.__repr__() is None:
                continue # Skip if the weak reference is dead

            # Only call _backward for tensors that require grad and have a node in the graph
            if tensor_node._custom_requires_grad:
                tensor_node._backward()
            
            # After computing gradients for a node, if it's an intermediate tensor,
            # clear its gradient and remove its reference from the graph to save memory.
            # This is part of auto_cleanup for non-leaf nodes.
            if not tensor_node._is_leaf and tensor_node._node_id in graph.intermediate_tensors:
                tensor_node.del_non_leaf_tensor_reference(tensor_node._node_id)
                # Optionally, delete the node from graph, but rx.ancestors might need it
                # for subsequent backward passes if the graph is reused for multiple backward calls.
                # For a single backward pass, clearing intermediate_tensors is usually sufficient.

    def to_device(self, device):
        self.tensor = self.tensor.to(device)
        return self # Allow chaining

    @property
    def dtype(self):
        return self.tensor.dtype

    @property
    def ndim(self):
        return self.tensor.ndim

    @property
    def shape(self):
        return self.tensor.shape
    
    @property
    def grad(self):
        return self.tensor.grad

    def __del__(self):
      if self._node_id is not None and self._is_leaf: 
        try:
              self.graph.delete_node(self._node_id)
        except ReferenceError:
              pass
      # print(f"Garbage Collector has decided that reference counts for {self._node_id} and id {id(self)} are zero so Goodbye!!") # This print can be noisy