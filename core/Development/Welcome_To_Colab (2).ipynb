{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5JkdP6sXmSoX",
        "outputId": "049a0c4d-939a-4382-e456-c62fa2cbdf9e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting rustworkx\n",
            "  Downloading rustworkx-0.16.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy<3,>=1.16.0 in /usr/local/lib/python3.11/dist-packages (from rustworkx) (2.0.2)\n",
            "Downloading rustworkx-0.16.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rustworkx\n",
            "Successfully installed rustworkx-0.16.0\n"
          ]
        }
      ],
      "source": [
        "! pip install rustworkx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "ulR6Lvv1lxRA"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import weakref\n",
        "import numbers\n",
        "import rustworkx as rx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFj9utaNld3a"
      },
      "source": [
        "#custom Tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "Zyo4n4MclcEK"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "class CustomTensor:\n",
        "    \"\"\"\n",
        "    CustomTensor(data, ...) will return the same object if `data` is already a CustomTensor.\n",
        "    This avoids memory reallocation and graph duplication.\n",
        "    Do not use it to rewrap or re-register nodes. Use only for symbolic reference.\n",
        "    \"\"\"\n",
        "    __slots__ = ('tensor', '_node_id', '_custom_requires_grad', '_backward', 'graph','__weakref__')\n",
        "\n",
        "    def __new__(cls, data, *, _custom_requires_grad=False, device=None, dtype=None, graph=None,due_to_operation=False):\n",
        "        if isinstance(data, CustomTensor):\n",
        "            return data  # âœ… Prevent new allocation, return existing one\n",
        "        return super().__new__(cls)\n",
        "\n",
        "    def __init__(self, data, *, _custom_requires_grad=False, device=None, dtype=None, graph=None,due_to_operation=False):\n",
        "        if isinstance(data, CustomTensor):\n",
        "            # Already returned in __new__, skip init\n",
        "            return\n",
        "        if due_to_operation:\n",
        "          self.tensor = data\n",
        "        else:\n",
        "          self.tensor = torch.as_tensor(data, dtype=dtype, device=device)\n",
        "          self.tensor.requires_grad_(False)\n",
        "        self._custom_requires_grad = _custom_requires_grad\n",
        "        self._node_id = None\n",
        "        self._backward = lambda: None\n",
        "        self.graph = None\n",
        "\n",
        "        if _custom_requires_grad:\n",
        "            self._init_graph(graph)\n",
        "\n",
        "    def _init_graph(self, graph):\n",
        "        if graph is None:\n",
        "            raise ValueError(\"Graph must be provided when _custom_requires_grad is True.\")\n",
        "        self.graph = weakref.ref(graph)\n",
        "        graph.add_tensor_graph(self)\n",
        "        graph.add_non_leaf_tensor_references(self)\n",
        "    def _zero_grad(self):\n",
        "        self.tensor.grad = torch.zeros_like(self.tensor)\n",
        "\n",
        "    def __add__(self,other):\n",
        "\n",
        "        if isinstance(other, numbers.Number):\n",
        "            result = Operations.add_tensor_and_scalar(self.tensor,other)\n",
        "            requires_grad = self._custom_requires_grad\n",
        "            if requires_grad:\n",
        "                graph = self.graph()\n",
        "                result = CustomTensor(result,_custom_requires_grad=True,graph=graph,due_to_operation=True)\n",
        "                graph.add_edge(self._node_id, result._node_id)\n",
        "                def _backward():\n",
        "                    if self.tensor.grad is None:\n",
        "                        self._zero_grad()\n",
        "                    self.tensor.grad = Operations.add_tensor_and_tensor(self.tensor.grad,result.tensor.grad)\n",
        "                result._backward = _backward\n",
        "                return result\n",
        "            else:\n",
        "                return CustomTensor(result,_custom_requires_grad=requires_grad)\n",
        "        elif isinstance(other,CustomTensor):\n",
        "            result = Operations.add_tensor_and_tensor(self.tensor,other.tensor)\n",
        "            self_requires_grad = self._custom_requires_grad\n",
        "            other_requires_grad = other._custom_requires_grad\n",
        "            if self_requires_grad and other_requires_grad:\n",
        "                graph = self.graph()\n",
        "                result = CustomTensor(result,_custom_requires_grad=True,graph=graph,due_to_operation=True)\n",
        "                graph.add_edge(self._node_id, result._node_id)\n",
        "                graph.add_edge(other._node_id, result._node_id)\n",
        "                def _backward():\n",
        "                    if self.tensor.grad is None:\n",
        "                        self._zero_grad()\n",
        "                    if other.tensor.grad is None:\n",
        "                        other._zero_grad()\n",
        "                    self.tensor.grad.add_(result.tensor.grad)\n",
        "                    other.tensor.grad.add_(result.tensor.grad)\n",
        "                result._backward = _backward\n",
        "                return result\n",
        "            elif self_requires_grad and not other_requires_grad:\n",
        "                graph =self.graph()\n",
        "                result = CustomTensor(result,_custom_requires_grad = True, graph=graph,due_to_operation=True)\n",
        "                graph.add_edge(self._node_id,result._node_id)\n",
        "                def _backward():\n",
        "                    if self.tensor.grad is None:\n",
        "                        self._zero_grad()\n",
        "                    self.tensor.grad.add_(result.tensor.grad)# = Operations.add_tensor_and_tensor(self.tensor.grad,result.tensor.grad)\n",
        "                result._backward =_backward\n",
        "                return result\n",
        "            elif other_requires_grad and not self_requires_grad:\n",
        "                graph = other.graph()\n",
        "                result = CustomTensor(result,_custom_requires_grad =True, graph = graph,due_to_operation=True)\n",
        "                graph.add_edge(other._node_id,result._node_id)\n",
        "                def _backward():\n",
        "                    if other.tensor.grad is None:\n",
        "                        other._zero_grad()\n",
        "                    other.tensor.grad.add_(result.tensor.grad) #= Operations.add_tensor_and_tensor(other.tensor.grad,result.tensor.grad)\n",
        "                result._backward =_backward\n",
        "                return result\n",
        "            else:\n",
        "                return CustomTensor(result,_custom_requires_grad=False,graph=None,due_to_operation=True)\n",
        "    def __del__(self):\n",
        "        print(f\"CustomTensor with id={id(self)} is being garbage collected\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wctU5vHZlhfL"
      },
      "source": [
        "# AutogradGraph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "GN6M4TQFlmwM"
      },
      "outputs": [],
      "source": [
        "class AutogradGraph:\n",
        "    def __init__(self,check_for_cycles=True, auto_cleanup=True):\n",
        "        self.graph = rx.PyDiGraph()\n",
        "        self.intermediate_tensors = dict()\n",
        "        self._check_cycles = check_for_cycles\n",
        "        self._auto_cleanup = auto_cleanup\n",
        "\n",
        "\n",
        "    def __enter__(self):\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, exc_type, exc_value, traceback):\n",
        "        if self._check_cycles:\n",
        "            if not self.check_cycle():\n",
        "                raise RuntimeError(\"Cycle detected in autograd graph on context exit.\")\n",
        "        if self._auto_cleanup:\n",
        "            self.intermediate_tensors.clear()\n",
        "            self.graph.clear()\n",
        "\n",
        "    def add_tensor_graph(self, tensor):\n",
        "        requires_grad = tensor._custom_requires_grad\n",
        "        if not requires_grad:\n",
        "            raise ValueError(\"Tensor with require grad False cannot to be added to the graph.\")\n",
        "\n",
        "        tensor_index = self.graph.add_node(weakref.ref(tensor))\n",
        "        tensor._node_id  = tensor_index\n",
        "\n",
        "    def add_non_leaf_tensor_references(self, tensor):\n",
        "        requires_grad = tensor._custom_requires_grad\n",
        "\n",
        "        node_id = tensor._node_id\n",
        "\n",
        "        if not requires_grad:\n",
        "            raise ValueError(\"Tensor must be a non leaf tensor.\")\n",
        "\n",
        "        if node_id in self.intermediate_tensors:\n",
        "            raise ValueError(\"Tensor reference to persist in memory already exists.\")\n",
        "\n",
        "        self.intermediate_tensors[node_id] = tensor\n",
        "\n",
        "    def add_edge(self, node_from, node_to, weight=None):\n",
        "        if not isinstance(node_from, int) or not isinstance(node_to, int):\n",
        "            raise TypeError(\"Node indices must be integers.\")\n",
        "\n",
        "        graph = self.graph\n",
        "        if not graph.has_node(node_from) or not graph.has_node(node_to):\n",
        "            raise ValueError(\"Both nodes must exist in the graph before adding an edge.\")\n",
        "\n",
        "        graph.add_edge(node_from, node_to, weight)\n",
        "\n",
        "    def check_cycle(self):\n",
        "        return rx.is_directed_acyclic_graph(self.graph)\n",
        "\n",
        "    def reverse_toposort(self):\n",
        "\n",
        "        if not self.check_cycle():\n",
        "            raise RuntimeError(\"Cannot perform topological sort on a graph with cycles.\")\n",
        "        graph = self.graph\n",
        "        node_indexes = rx.topological_sort(graph)\n",
        "        return [graph[node_index] for node_index in reversed(node_indexes)]\n",
        "\n",
        "    def delete_node(self, node_index):\n",
        "        if not isinstance(node_index, int):\n",
        "            raise TypeError(\"Node index must be an integer.\")\n",
        "\n",
        "        graph = self.graph\n",
        "        if not graph.has_node(node_index):\n",
        "            raise ValueError(f\"Node index {node_index} does not exist in the graph.\")\n",
        "\n",
        "        graph.remove_node(node_index)\n",
        "\n",
        "    def delete_edge(self, node_from, node_to):\n",
        "        if not isinstance(node_from, int) or not isinstance(node_to, int):\n",
        "            raise TypeError(\"Node indices must be integers.\")\n",
        "\n",
        "        graph = self.graph\n",
        "        if not graph.has_edge(node_from, node_to):\n",
        "            raise ValueError(f\"Edge ({node_from}, {node_to}) does not exist in the graph.\")\n",
        "\n",
        "        graph.remove_edge(node_from, node_to)\n",
        "\n",
        "    def del_non_leaf_tensor_reference(self, tensor_node_id):\n",
        "        try:\n",
        "            del self.intermediate_tensors[tensor_node_id]\n",
        "        except KeyError:\n",
        "            raise KeyError(f\"No tensor reference found for node ID {tensor_node_id}\")\n",
        "\n",
        "    def __repr__(self):\n",
        "        graph = self.graph\n",
        "        return f\"CustomAutogradGraph(nodes={graph.num_nodes()}, edges={graph.num_edges()})\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3eqsMV42lnfh"
      },
      "source": [
        "# Operations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "FkA57SKdlpUJ"
      },
      "outputs": [],
      "source": [
        "class Operations:\n",
        "    @torch.jit.script\n",
        "    def add_tensor_and_scalar(tensor: torch.Tensor, scaler: float) -> torch.Tensor:\n",
        "        return tensor + scaler\n",
        "\n",
        "    @torch.jit.script\n",
        "    def add_tensor_and_tensor(tensor1: torch.Tensor, tensor2: torch.Tensor) -> torch.Tensor:\n",
        "        return tensor1 + tensor2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEdjOowPmgZ-"
      },
      "source": [
        "# Tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "XnNWDQWtme0s"
      },
      "outputs": [],
      "source": [
        "device = \"cpu\"\n",
        "dtype =  torch.float16\n",
        "a = CustomTensor([1,2,3],_custom_requires_grad=False,device=\"cpu\",dtype=dtype,graph = None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "kbQPucrMEpz7"
      },
      "outputs": [],
      "source": [
        "b = CustomTensor([-5,6.8,-3.5],_custom_requires_grad=False,device = device,dtype=dtype,graph = None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AzH1083JE7vu",
        "outputId": "ab78e940-ce53-4ec0-f033-3620612a50b4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<__main__.CustomTensor at 0x78a842ead350>"
            ]
          },
          "execution_count": 93,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vlX4MIS7rW-p",
        "outputId": "3ee5b965-b470-4372-c0ff-f576e20be642"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<__main__.CustomTensor at 0x78a842ad2750>"
            ]
          },
          "execution_count": 94,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ChS1tzQr84E",
        "outputId": "70d79387-dd10-460c-98bd-5ba3a120cf7d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([0., 0., 0.], dtype=torch.float16)"
            ]
          },
          "execution_count": 121,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "a.tensor.grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "id": "MZLnqJQsHiv5"
      },
      "outputs": [],
      "source": [
        "a._zero_grad()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_kKXynC8H5El",
        "outputId": "244c5f26-a5c9-41fb-b6b7-449d1551a6fc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([1., 1., 1.], dtype=torch.float16)"
            ]
          },
          "execution_count": 122,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "a.tensor.grad.add_(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "id": "-yPyYqlWE96d"
      },
      "outputs": [],
      "source": [
        "c=a+b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dlbuVFyLHSO1",
        "outputId": "2d355207-5ae6-4fc8-bd28-bc0b6650c2bb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([-4.0000,  8.7969, -0.5000], dtype=torch.float16)"
            ]
          },
          "execution_count": 116,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "c.tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wA1OhLpKHVJj",
        "outputId": "d5ed4e04-0292-4f48-8108-b36d1bab4f07"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<__main__.CustomTensor at 0x78a842b328e0>"
            ]
          },
          "execution_count": 117,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "c"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "id": "ObERoNbAHWMn"
      },
      "outputs": [],
      "source": [
        "d=c"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8dZr8Vp1HXEV",
        "outputId": "c7ee1a6e-e283-4478-d5fd-c71b2730ae17"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<__main__.CustomTensor at 0x78a842b328e0>"
            ]
          },
          "execution_count": 119,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "LjWyo2qXoWm-"
      },
      "outputs": [],
      "source": [
        "k=CustomTensor(a)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7fmEKvtSrlOa",
        "outputId": "5c95d9f4-7d8d-4988-8042-c8eaab002335"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([1., 2., 3.], dtype=torch.float16)"
            ]
          },
          "execution_count": 72,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "k.tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pEl3Xrj1u5WU",
        "outputId": "2fe5d151-8387-4bb4-a348-2cbb96f28f20"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<__main__.CustomTensor at 0x78a861876f20>"
            ]
          },
          "execution_count": 73,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "k"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "MR1V5liRDvk-"
      },
      "outputs": [],
      "source": [
        "k=None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "UiKj_s8CEbvz"
      },
      "outputs": [],
      "source": [
        "del a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "sZL89CWED6Z-"
      },
      "outputs": [],
      "source": [
        "a=None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "id": "vtk2H0-4ES7S"
      },
      "outputs": [],
      "source": [
        "import gc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q9O1vGfHvZHU",
        "outputId": "d959b26d-39a1-4588-ab7d-0b480a55f09c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "print(sys.getrefcount(a))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "5JpEC2NnvhlI"
      },
      "outputs": [],
      "source": [
        "import gc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LtUC5Ly3vjFz",
        "outputId": "a703bbac-b815-434c-c31f-80f2caf066c4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "execution_count": 114,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "eDfwBP41uyA4"
      },
      "outputs": [],
      "source": [
        "k=None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "bIvuQ3Xju0rL"
      },
      "outputs": [],
      "source": [
        "del a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "Z_Wh9bsVpljS"
      },
      "outputs": [],
      "source": [
        "k=None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kswkZWYep2J6",
        "outputId": "453f71e3-3ade-451a-b653-d98433a45e75"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<__main__.CustomTensor at 0x7bdc07166ed0>"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pa5SncS0pnIG",
        "outputId": "cc91be06-9788-4dd3-e525-bbc46d886c59"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<__main__.CustomTensor at 0x7979064d74c0>"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "C`ustomTensor(a)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5wsoA6-woXCF",
        "outputId": "19400a38-1082-4178-a613-6005cbd6d1bf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([1., 2., 3.], dtype=torch.float16)"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "a.tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wQLLFJWapHti",
        "outputId": "0992b15f-ad49-4958-97c0-a0bb23bb0c7c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ref count after a: 2\n",
            "Ref count after k: 3\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import torch\n",
        "import weakref\n",
        "\n",
        "class CustomTensor:\n",
        "    __slots__ = ('tensor', '_node_id', '_custom_requires_grad', '_backward', 'graph')\n",
        "\n",
        "    def __new__(cls, data, *, _custom_requires_grad=False, device=None, dtype=None, graph=None):\n",
        "        if isinstance(data, CustomTensor):\n",
        "            return data\n",
        "        return super().__new__(cls)\n",
        "\n",
        "    def __init__(self, data, *, _custom_requires_grad=False, device=None, dtype=None, graph=None):\n",
        "        if isinstance(data, CustomTensor):\n",
        "            return\n",
        "        self.tensor = torch.as_tensor(data, dtype=dtype, device=device)\n",
        "        self._custom_requires_grad = _custom_requires_grad\n",
        "        self._node_id = None\n",
        "        self._backward = lambda: None\n",
        "        self.graph = None\n",
        "\n",
        "a = CustomTensor([1,2,3], _custom_requires_grad=False, device=\"cpu\", dtype=torch.float16, graph=None)\n",
        "print(\"Ref count after a:\", sys.getrefcount(a))\n",
        "\n",
        "k = CustomTensor(a)\n",
        "print(\"Ref count after k:\", sys.getrefcount(a))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46H4koJCY40y"
      },
      "source": [
        "# testing reference and weakref.proxy use case possibility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PwIAQNYhPLp4",
        "outputId": "bc69ba2a-f6f3-41f3-e124-1c4b1247abdf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "CREATION OVERHEAD BENCHMARK\n",
            "==================================================\n",
            "Object creation:      0.0027s\n",
            "Weak ref creation:    0.0041s\n",
            "Weak proxy creation:  0.0032s\n",
            "\n",
            "Creation overhead:\n",
            "Weak ref overhead:   51.3%\n",
            "Weak proxy overhead: 17.4%\n",
            "\n",
            "==================================================\n",
            "PRACTICAL RECOMMENDATIONS\n",
            "==================================================\n",
            "Running benchmarks...\n",
            "Iterations per run: 100,000\n",
            "Number of runs: 10\n",
            "--------------------------------------------------\n",
            "Direct access:     0.0066s Â± 0.0014s\n",
            "Weak ref access:   0.0123s Â± 0.0042s\n",
            "Weak proxy access: 0.0127s Â± 0.0083s\n",
            "\n",
            "Overhead compared to direct access:\n",
            "Weak ref overhead:   87.9%\n",
            "Weak proxy overhead: 93.4%\n",
            "\n",
            "Per-operation time (nanoseconds):\n",
            "Direct access:     65.7 ns\n",
            "Weak ref access:   123.4 ns\n",
            "Weak proxy access: 127.0 ns\n",
            "\n",
            "ðŸŽ¯ Key Takeaways:\n",
            "â€¢ Weak references add ~88% overhead\n",
            "â€¢ Weak proxies add ~93% overhead\n",
            "â€¢ Per-operation cost: ~127 nanoseconds\n",
            "\n",
            "ðŸ’¡ When to use weak references:\n",
            "âœ… Breaking circular references (essential)\n",
            "âœ… Cache implementations\n",
            "âœ… Observer patterns\n",
            "âœ… When object lifetime management is critical\n",
            "\n",
            "âš ï¸  When overhead might matter:\n",
            "â€¢ Tight loops with millions of attribute accesses\n",
            "â€¢ Real-time systems with microsecond requirements\n",
            "â€¢ High-frequency trading algorithms\n",
            "\n",
            "ðŸš€ For your autograd system:\n",
            "â€¢ The overhead is negligible compared to tensor operations\n",
            "â€¢ Preventing memory leaks is much more important\n",
            "â€¢ Use weak references - the benefits outweigh the cost\n"
          ]
        }
      ],
      "source": [
        "import weakref\n",
        "import timeit\n",
        "import gc\n",
        "import statistics\n",
        "\n",
        "class TestObject:\n",
        "    def __init__(self, value):\n",
        "        self.value = value\n",
        "\n",
        "def benchmark_access_methods():\n",
        "    \"\"\"Comprehensive benchmark of different access methods\"\"\"\n",
        "\n",
        "    # Create test objects\n",
        "    obj = TestObject(42)\n",
        "    weak_ref = weakref.ref(obj)\n",
        "    weak_proxy = weakref.proxy(obj)\n",
        "\n",
        "    # Number of iterations\n",
        "    iterations = 100000\n",
        "\n",
        "    # Number of benchmark runs to calculate statistics\n",
        "    runs = 10\n",
        "\n",
        "    print(\"Running benchmarks...\")\n",
        "    print(f\"Iterations per run: {iterations:,}\")\n",
        "    print(f\"Number of runs: {runs}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    # Test 1: Direct object access\n",
        "    direct_times = []\n",
        "    for _ in range(runs):\n",
        "        gc.collect()  # Force garbage collection before each run\n",
        "        time = timeit.timeit(lambda: obj.value, number=iterations)\n",
        "        direct_times.append(time)\n",
        "\n",
        "    # Test 2: Weak reference access (with call)\n",
        "    weak_ref_times = []\n",
        "    for _ in range(runs):\n",
        "        gc.collect()\n",
        "        time = timeit.timeit(lambda: weak_ref().value, number=iterations)\n",
        "        weak_ref_times.append(time)\n",
        "\n",
        "    # Test 3: Weak proxy access\n",
        "    weak_proxy_times = []\n",
        "    for _ in range(runs):\n",
        "        gc.collect()\n",
        "        time = timeit.timeit(lambda: weak_proxy.value, number=iterations)\n",
        "        weak_proxy_times.append(time)\n",
        "\n",
        "    # Calculate statistics\n",
        "    direct_mean = statistics.mean(direct_times)\n",
        "    direct_std = statistics.stdev(direct_times)\n",
        "\n",
        "    weak_ref_mean = statistics.mean(weak_ref_times)\n",
        "    weak_ref_std = statistics.stdev(weak_ref_times)\n",
        "\n",
        "    weak_proxy_mean = statistics.mean(weak_proxy_times)\n",
        "    weak_proxy_std = statistics.stdev(weak_proxy_times)\n",
        "\n",
        "    # Print results\n",
        "    print(f\"Direct access:     {direct_mean:.4f}s Â± {direct_std:.4f}s\")\n",
        "    print(f\"Weak ref access:   {weak_ref_mean:.4f}s Â± {weak_ref_std:.4f}s\")\n",
        "    print(f\"Weak proxy access: {weak_proxy_mean:.4f}s Â± {weak_proxy_std:.4f}s\")\n",
        "    print()\n",
        "\n",
        "    # Calculate overhead percentages\n",
        "    ref_overhead = ((weak_ref_mean - direct_mean) / direct_mean) * 100\n",
        "    proxy_overhead = ((weak_proxy_mean - direct_mean) / direct_mean) * 100\n",
        "\n",
        "    print(\"Overhead compared to direct access:\")\n",
        "    print(f\"Weak ref overhead:   {ref_overhead:.1f}%\")\n",
        "    print(f\"Weak proxy overhead: {proxy_overhead:.1f}%\")\n",
        "    print()\n",
        "\n",
        "    # Performance per operation (nanoseconds)\n",
        "    print(\"Per-operation time (nanoseconds):\")\n",
        "    print(f\"Direct access:     {(direct_mean / iterations) * 1e9:.1f} ns\")\n",
        "    print(f\"Weak ref access:   {(weak_ref_mean / iterations) * 1e9:.1f} ns\")\n",
        "    print(f\"Weak proxy access: {(weak_proxy_mean / iterations) * 1e9:.1f} ns\")\n",
        "\n",
        "    return {\n",
        "        'direct': direct_mean,\n",
        "        'weak_ref': weak_ref_mean,\n",
        "        'weak_proxy': weak_proxy_mean,\n",
        "        'ref_overhead': ref_overhead,\n",
        "        'proxy_overhead': proxy_overhead\n",
        "    }\n",
        "\n",
        "def benchmark_creation_overhead():\n",
        "    \"\"\"Benchmark the cost of creating weak references\"\"\"\n",
        "\n",
        "    iterations = 10000\n",
        "    runs = 5\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"CREATION OVERHEAD BENCHMARK\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # Test object creation\n",
        "    obj_times = []\n",
        "    for _ in range(runs):\n",
        "        gc.collect()\n",
        "        time = timeit.timeit(lambda: TestObject(42), number=iterations)\n",
        "        obj_times.append(time)\n",
        "\n",
        "    # Test weak reference creation\n",
        "    weak_ref_times = []\n",
        "    for _ in range(runs):\n",
        "        gc.collect()\n",
        "        def create_weak_ref():\n",
        "            obj = TestObject(42)\n",
        "            return weakref.ref(obj)\n",
        "        time = timeit.timeit(create_weak_ref, number=iterations)\n",
        "        weak_ref_times.append(time)\n",
        "\n",
        "    # Test weak proxy creation\n",
        "    weak_proxy_times = []\n",
        "    for _ in range(runs):\n",
        "        gc.collect()\n",
        "        def create_weak_proxy():\n",
        "            obj = TestObject(42)\n",
        "            return weakref.proxy(obj)\n",
        "        time = timeit.timeit(create_weak_proxy, number=iterations)\n",
        "        weak_proxy_times.append(time)\n",
        "\n",
        "    obj_mean = statistics.mean(obj_times)\n",
        "    weak_ref_mean = statistics.mean(weak_ref_times)\n",
        "    weak_proxy_mean = statistics.mean(weak_proxy_times)\n",
        "\n",
        "    print(f\"Object creation:      {obj_mean:.4f}s\")\n",
        "    print(f\"Weak ref creation:    {weak_ref_mean:.4f}s\")\n",
        "    print(f\"Weak proxy creation:  {weak_proxy_mean:.4f}s\")\n",
        "    print()\n",
        "\n",
        "    ref_overhead = ((weak_ref_mean - obj_mean) / obj_mean) * 100\n",
        "    proxy_overhead = ((weak_proxy_mean - obj_mean) / obj_mean) * 100\n",
        "\n",
        "    print(\"Creation overhead:\")\n",
        "    print(f\"Weak ref overhead:   {ref_overhead:.1f}%\")\n",
        "    print(f\"Weak proxy overhead: {proxy_overhead:.1f}%\")\n",
        "\n",
        "def practical_recommendation():\n",
        "    \"\"\"Provide practical recommendations based on benchmarks\"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"PRACTICAL RECOMMENDATIONS\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    results = benchmark_access_methods()\n",
        "\n",
        "    print(\"\\nðŸŽ¯ Key Takeaways:\")\n",
        "    print(f\"â€¢ Weak references add ~{results['ref_overhead']:.0f}% overhead\")\n",
        "    print(f\"â€¢ Weak proxies add ~{results['proxy_overhead']:.0f}% overhead\")\n",
        "    print(f\"â€¢ Per-operation cost: ~{(results['weak_proxy'] / 100000) * 1e9:.0f} nanoseconds\")\n",
        "\n",
        "    print(\"\\nðŸ’¡ When to use weak references:\")\n",
        "    print(\"âœ… Breaking circular references (essential)\")\n",
        "    print(\"âœ… Cache implementations\")\n",
        "    print(\"âœ… Observer patterns\")\n",
        "    print(\"âœ… When object lifetime management is critical\")\n",
        "\n",
        "    print(\"\\nâš ï¸  When overhead might matter:\")\n",
        "    print(\"â€¢ Tight loops with millions of attribute accesses\")\n",
        "    print(\"â€¢ Real-time systems with microsecond requirements\")\n",
        "    print(\"â€¢ High-frequency trading algorithms\")\n",
        "\n",
        "    print(\"\\nðŸš€ For your autograd system:\")\n",
        "    print(\"â€¢ The overhead is negligible compared to tensor operations\")\n",
        "    print(\"â€¢ Preventing memory leaks is much more important\")\n",
        "    print(\"â€¢ Use weak references - the benefits outweigh the cost\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    benchmark_creation_overhead()\n",
        "    practical_recommendation()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YStZIH8DXpXj"
      },
      "source": [
        "# chat gpt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MvzZ4DZuXsv_"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import weakref\n",
        "import numbers\n",
        "import rustworkx as rx\n",
        "\n",
        "class Operations:\n",
        "    @torch.jit.script\n",
        "    def add_tensor_and_scalar(tensor: torch.Tensor, scalar: float) -> torch.Tensor:\n",
        "        return tensor + scalar\n",
        "\n",
        "    @torch.jit.script\n",
        "    def add_tensor_and_tensor(tensor1: torch.Tensor, tensor2: torch.Tensor) -> torch.Tensor:\n",
        "        return tensor1 + tensor2\n",
        "\n",
        "class AutogradGraph:\n",
        "    def __init__(self, check_for_cycles=True, auto_cleanup=True):\n",
        "        self.graph = rx.PyDiGraph()\n",
        "        self.intermediate_tensors = {}\n",
        "        self._check_cycles = check_for_cycles\n",
        "        self._auto_cleanup = auto_cleanup\n",
        "\n",
        "    def __enter__(self):\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, exc_type, exc_value, traceback):\n",
        "        if self._check_cycles and not self.check_cycle():\n",
        "            raise RuntimeError(\"Cycle detected in autograd graph on context exit.\")\n",
        "        if self._auto_cleanup:\n",
        "            self.intermediate_tensors.clear()\n",
        "            self.graph.clear()\n",
        "\n",
        "    def add_tensor_graph(self, tensor, is_leaf):\n",
        "        if not tensor._custom_requires_grad:\n",
        "            raise ValueError(\"Tensor with requires_grad=False cannot be added to the graph.\")\n",
        "\n",
        "        ref = tensor if is_leaf else weakref.proxy(tensor)\n",
        "        tensor_index = self.graph.add_node(ref)\n",
        "        tensor._node_id = tensor_index\n",
        "\n",
        "    def add_non_leaf_tensor_reference(self, tensor):\n",
        "        if not tensor._custom_requires_grad:\n",
        "            raise ValueError(\"Tensor must require grad.\")\n",
        "\n",
        "        if tensor._node_id in self.intermediate_tensors:\n",
        "            raise ValueError(\"Tensor reference already exists in intermediate tensors.\")\n",
        "\n",
        "        self.intermediate_tensors[tensor._node_id] = tensor\n",
        "\n",
        "    def add_edge(self, node_from, node_to, weight=None):\n",
        "        if not all(isinstance(n, int) for n in (node_from, node_to)):\n",
        "            raise TypeError(\"Node indices must be integers.\")\n",
        "        if not self.graph.has_node(node_from) or not self.graph.has_node(node_to):\n",
        "            raise ValueError(\"Nodes must exist before adding edge.\")\n",
        "        self.graph.add_edge(node_from, node_to, weight)\n",
        "\n",
        "    def check_cycle(self):\n",
        "        return rx.is_directed_acyclic_graph(self.graph)\n",
        "\n",
        "    def reverse_toposort(self):\n",
        "        if not self.check_cycle():\n",
        "            raise RuntimeError(\"Cannot perform topological sort on cyclic graph.\")\n",
        "        return [self.graph[n] for n in reversed(rx.topological_sort(self.graph))]\n",
        "\n",
        "    def delete_node(self, node_index):\n",
        "        if not isinstance(node_index, int):\n",
        "            raise TypeError(\"Node index must be an integer.\")\n",
        "        if not self.graph.has_node(node_index):\n",
        "            raise ValueError(\"Node does not exist.\")\n",
        "        self.graph.remove_node(node_index)\n",
        "\n",
        "    def delete_edge(self, node_from, node_to):\n",
        "        if not self.graph.has_edge(node_from, node_to):\n",
        "            raise ValueError(\"Edge does not exist.\")\n",
        "        self.graph.remove_edge(node_from, node_to)\n",
        "\n",
        "    def del_non_leaf_tensor_reference(self, tensor_node_id):\n",
        "        self.intermediate_tensors.pop(tensor_node_id, None)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"CustomAutogradGraph(nodes={self.graph.num_nodes()}, edges={self.graph.num_edges()})\"\n",
        "\n",
        "class CustomTensor:\n",
        "    __slots__ = ('tensor', '_node_id', '_custom_requires_grad', '_backward', 'graph', '__weakref__')\n",
        "\n",
        "    def __new__(cls, data, *, _custom_requires_grad=False, device=None, dtype=None, graph=None, due_to_operation=False, is_leaf=False):\n",
        "        if isinstance(data, CustomTensor):\n",
        "            return data  # Don't rewrap\n",
        "        return super().__new__(cls)\n",
        "\n",
        "    def __init__(self, data, *, _custom_requires_grad=False, device=None, dtype=None, graph=None, due_to_operation=False, is_leaf=False):\n",
        "        if isinstance(data, CustomTensor):\n",
        "            return\n",
        "\n",
        "        self.tensor = data if due_to_operation else torch.as_tensor(data, dtype=dtype, device=device)\n",
        "        self.tensor.requires_grad_(False)\n",
        "        self._custom_requires_grad = _custom_requires_grad\n",
        "        self._node_id = None\n",
        "        self._backward = lambda: None\n",
        "        self.graph = None\n",
        "\n",
        "        if _custom_requires_grad:\n",
        "            self._init_graph(graph, is_leaf)\n",
        "\n",
        "    def _init_graph(self, graph, is_leaf):\n",
        "        if graph is None:\n",
        "            raise ValueError(\"Graph must be provided if requires_grad is True.\")\n",
        "        self.graph = weakref.proxy(graph)\n",
        "        graph.add_tensor_graph(self, is_leaf=is_leaf)\n",
        "        if not is_leaf:\n",
        "            graph.add_non_leaf_tensor_reference(self)\n",
        "\n",
        "    def _zero_grad(self):\n",
        "        self.tensor.grad = torch.zeros_like(self.tensor)\n",
        "\n",
        "    def __add__(self, other):\n",
        "        if isinstance(other, numbers.Number):\n",
        "            return self._binary_op_scalar(other, op=Operations.add_tensor_and_scalar)\n",
        "        elif isinstance(other, CustomTensor):\n",
        "            return self._binary_op_tensor(other, op=Operations.add_tensor_and_tensor)\n",
        "        return NotImplemented\n",
        "\n",
        "    def _binary_op_scalar(self, scalar, op):\n",
        "        result_tensor = op(self.tensor, scalar)\n",
        "        if not self._custom_requires_grad:\n",
        "            return CustomTensor(result_tensor)\n",
        "\n",
        "        graph = self.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "        graph.add_edge(self._node_id, result._node_id)\n",
        "\n",
        "        self_ref = weakref.proxy(self)\n",
        "        def _backward():\n",
        "            if self_ref.tensor.grad is None:\n",
        "                self_ref._zero_grad()\n",
        "            self_ref.tensor.grad.add_(result.tensor.grad)\n",
        "\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "\n",
        "    def _binary_op_tensor(self, other, op):\n",
        "        result_tensor = op(self.tensor, other.tensor)\n",
        "        requires_grad = self._custom_requires_grad or other._custom_requires_grad\n",
        "\n",
        "        if not requires_grad:\n",
        "            return CustomTensor(result_tensor)\n",
        "\n",
        "        graph = self.graph if self._custom_requires_grad else other.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "\n",
        "        self_ref = weakref.proxy(self)\n",
        "        other_ref = weakref.proxy(other)\n",
        "\n",
        "        if self._custom_requires_grad:\n",
        "            graph.add_edge(self._node_id, result._node_id)\n",
        "        if other._custom_requires_grad:\n",
        "            graph.add_edge(other._node_id, result._node_id)\n",
        "\n",
        "        def _backward():\n",
        "            if self._custom_requires_grad:\n",
        "                if self_ref.tensor.grad is None:\n",
        "                    self_ref._zero_grad()\n",
        "                self_ref.tensor.grad.add_(result.tensor.grad)\n",
        "            if other._custom_requires_grad:\n",
        "                if other_ref.tensor.grad is None:\n",
        "                    other_ref._zero_grad()\n",
        "                other_ref.tensor.grad.add_(result.tensor.grad)\n",
        "\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "\n",
        "    def __del__(self):\n",
        "        print(f\"CustomTensor with id={id(self)} is being garbage collected\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmpTbqadYn47"
      },
      "source": [
        "#gemini"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 210,
      "metadata": {
        "id": "QYdGgt65Yo1t"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import weakref\n",
        "import numbers\n",
        "import rustworkx as rx\n",
        "import pytest\n",
        "\n",
        "# Original code (assuming it's in a file named 'autograd_system.py' or similar)\n",
        "# For the purpose of this test code, I'll include it directly.\n",
        "\n",
        "class Operations:\n",
        "    @torch.jit.script\n",
        "    def add_tensor_and_scalar(tensor: torch.Tensor, scalar: float) -> torch.Tensor:\n",
        "        return tensor + scalar\n",
        "\n",
        "    @torch.jit.script\n",
        "    def add_tensor_and_tensor(tensor1: torch.Tensor, tensor2: torch.Tensor) -> torch.Tensor:\n",
        "        return tensor1 + tensor2\n",
        "\n",
        "class AutogradGraph:\n",
        "    def __init__(self, check_for_cycles=True, auto_cleanup=True):\n",
        "        self.graph = rx.PyDiGraph()\n",
        "        self.intermediate_tensors = {}\n",
        "        self._check_cycles = check_for_cycles\n",
        "        self._auto_cleanup = auto_cleanup\n",
        "\n",
        "    def __enter__(self):\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, exc_type, exc_value, traceback):\n",
        "        if self._check_cycles and not self.check_cycle():\n",
        "            raise RuntimeError(\"Cycle detected in autograd graph on context exit.\")\n",
        "        if self._auto_cleanup:\n",
        "            self.intermediate_tensors.clear()\n",
        "            self.graph.clear()\n",
        "\n",
        "    def add_tensor_graph(self, tensor, is_leaf):\n",
        "        if not tensor._custom_requires_grad:\n",
        "            raise ValueError(\"Tensor with requires_grad=False cannot be added to the graph.\")\n",
        "\n",
        "        ref = tensor if is_leaf else weakref.proxy(tensor)\n",
        "        tensor_index = self.graph.add_node(ref)\n",
        "        tensor._node_id = tensor_index\n",
        "\n",
        "    def add_non_leaf_tensor_reference(self, tensor):\n",
        "        if not tensor._custom_requires_grad:\n",
        "            raise ValueError(\"Tensor must require grad.\")\n",
        "\n",
        "        if tensor._node_id in self.intermediate_tensors:\n",
        "            raise ValueError(\"Tensor reference already exists in intermediate tensors.\")\n",
        "\n",
        "        self.intermediate_tensors[tensor._node_id] = tensor\n",
        "\n",
        "    def add_edge(self, node_from, node_to, weight=None):\n",
        "        if not all(isinstance(n, int) for n in (node_from, node_to)):\n",
        "            raise TypeError(\"Node indices must be integers.\")\n",
        "        if not self.graph.has_node(node_from) or not self.graph.has_node(node_to):\n",
        "            raise ValueError(\"Nodes must exist before adding edge.\")\n",
        "        self.graph.add_edge(node_from, node_to, weight)\n",
        "\n",
        "    def check_cycle(self):\n",
        "        return rx.is_directed_acyclic_graph(self.graph)\n",
        "\n",
        "    def reverse_toposort(self):\n",
        "        if not self.check_cycle():\n",
        "            raise RuntimeError(\"Cannot perform topological sort on cyclic graph.\")\n",
        "        # rustworkx.topological_sort already works on node indices and dependencies\n",
        "        # The result of rx.topological_sort is a list of node indices in topological order.\n",
        "        # We then retrieve the actual tensor objects using self.graph[n].\n",
        "        return [self.graph[n] for n in reversed(rx.topological_sort(self.graph))]\n",
        "\n",
        "    def delete_node(self, node_index):\n",
        "        if not isinstance(node_index, int):\n",
        "            raise TypeError(\"Node index must be an integer.\")\n",
        "        if not self.graph.has_node(node_index):\n",
        "            raise ValueError(\"Node does not exist.\")\n",
        "        self.graph.remove_node(node_index)\n",
        "\n",
        "    def delete_edge(self, node_from, node_to):\n",
        "        if not self.graph.has_edge(node_from, node_to):\n",
        "            raise ValueError(\"Edge does not exist.\")\n",
        "        self.graph.remove_edge(node_from, node_to)\n",
        "\n",
        "    def del_non_leaf_tensor_reference(self, tensor_node_id):\n",
        "        self.intermediate_tensors.pop(tensor_node_id, None)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"CustomAutogradGraph(nodes={self.graph.num_nodes()}, edges={self.graph.num_edges()})\"\n",
        "\n",
        "class CustomTensor:\n",
        "    __slots__ = ('tensor', '_node_id', '_custom_requires_grad', '_backward', 'graph', '__weakref__')\n",
        "\n",
        "    def __new__(cls, data, *, _custom_requires_grad=False, device=None, dtype=None, graph=None, due_to_operation=False, is_leaf=False):\n",
        "        if isinstance(data, CustomTensor):\n",
        "            return data  # Don't rewrap\n",
        "        return super().__new__(cls)\n",
        "\n",
        "    def __init__(self, data, *, _custom_requires_grad=False, device=None, dtype=None, graph=None, due_to_operation=False, is_leaf=False):\n",
        "        if isinstance(data, CustomTensor):\n",
        "            return\n",
        "\n",
        "        self.tensor = data if due_to_operation else torch.as_tensor(data, dtype=dtype, device=device)\n",
        "        self.tensor.requires_grad_(False)\n",
        "        self._custom_requires_grad = _custom_requires_grad\n",
        "        self._node_id = None\n",
        "        self._backward = lambda: None\n",
        "        self.graph = None\n",
        "\n",
        "        if _custom_requires_grad:\n",
        "            self._init_graph(graph, is_leaf)\n",
        "\n",
        "    def _init_graph(self, graph, is_leaf):\n",
        "        if graph is None:\n",
        "            raise ValueError(\"Graph must be provided if requires_grad is True.\")\n",
        "        if is_leaf:\n",
        "          self.graph = weakref.proxy(graph)\n",
        "        else:\n",
        "          self.graph = graph\n",
        "        graph.add_tensor_graph(self, is_leaf=is_leaf)\n",
        "        if not is_leaf:\n",
        "            graph.add_non_leaf_tensor_reference(self)\n",
        "\n",
        "    def _zero_grad(self):\n",
        "        self.tensor.grad = torch.zeros_like(self.tensor)\n",
        "\n",
        "    def __add__(self, other):\n",
        "        if isinstance(other, numbers.Number):\n",
        "            return self._binary_op_scalar(other, op=Operations.add_tensor_and_scalar)\n",
        "        elif isinstance(other, CustomTensor):\n",
        "            return self._binary_op_tensor(other, op=Operations.add_tensor_and_tensor)\n",
        "        return NotImplemented\n",
        "\n",
        "    def _binary_op_scalar(self, scalar, op):\n",
        "        result_tensor = op(self.tensor, scalar)\n",
        "        if not self._custom_requires_grad:\n",
        "            return CustomTensor(result_tensor)\n",
        "\n",
        "        graph = self.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "        graph.add_edge(self._node_id, result._node_id)\n",
        "\n",
        "        self_ref = weakref.proxy(self)\n",
        "        result_ref = weakref.proxy(result)\n",
        "        def _backward():\n",
        "            if self_ref.tensor.grad is None:\n",
        "                self_ref._zero_grad()\n",
        "            # print(f\"Backward for scalar add: result_grad={result.tensor.grad}, self_grad_before={self_ref.tensor.grad}\") # Debugging\n",
        "            self_ref.tensor.grad.add_(result_ref.tensor.grad)\n",
        "            # print(f\"Backward for scalar add: self_grad_after={self_ref.tensor.grad}\") # Debugging\n",
        "\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "\n",
        "    def _binary_op_tensor(self, other, op):\n",
        "        result_tensor = op(self.tensor, other.tensor)\n",
        "        requires_grad = self._custom_requires_grad or other._custom_requires_grad\n",
        "\n",
        "        if not requires_grad:\n",
        "            return CustomTensor(result_tensor)\n",
        "\n",
        "        # Graph selection logic - assuming operations happen within a single graph context\n",
        "        graph = None\n",
        "        if self._custom_requires_grad:\n",
        "            graph = self.graph\n",
        "        elif other._custom_requires_grad:\n",
        "            graph = other.graph\n",
        "        else:\n",
        "            # This case should ideally not be reached if requires_grad is True\n",
        "            # and at least one operand has requires_grad\n",
        "            pass # Or raise an error if graph is truly missing\n",
        "\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "\n",
        "        self_ref = weakref.proxy(self)\n",
        "        other_ref = weakref.proxy(other)\n",
        "\n",
        "        if self._custom_requires_grad:\n",
        "            graph.add_edge(self._node_id, result._node_id)\n",
        "        if other._custom_requires_grad:\n",
        "            graph.add_edge(other._node_id, result._node_id)\n",
        "        result_ref = weakref.proxy(result)\n",
        "        def _backward():\n",
        "            # print(f\"Backward for tensor add: result_grad={result.tensor.grad}\") # Debugging\n",
        "            if self._custom_requires_grad:\n",
        "                if self_ref.tensor.grad is None:\n",
        "                    self_ref._zero_grad()\n",
        "                self_ref.tensor.grad.add_(result_ref.tensor.grad)\n",
        "                # print(f\"  self_grad_after={self_ref.tensor.grad}\") # Debugging\n",
        "            if other._custom_requires_grad:\n",
        "                if other_ref.tensor.grad is None:\n",
        "                    other_ref._zero_grad()\n",
        "                other_ref.tensor.grad.add_(result_ref.tensor.grad)\n",
        "                # print(f\"  other_grad_after={other_ref.tensor.grad}\") # Debugging\n",
        "\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "\n",
        "    def __del__(self):\n",
        "        # print(f\"CustomTensor with node_id={self._node_id} (id={id(self)}) is being garbage collected\") # Debugging\n",
        "        pass # Suppress print for cleaner test output\n",
        "\n",
        "\n",
        "# --- Test Code ---\n",
        "\n",
        "def run_backward(output_tensor: CustomTensor):\n",
        "    \"\"\"\n",
        "    Simulates the backward pass for the custom autograd system.\n",
        "    This function needs to be explicitly defined as it's not part of the original classes.\n",
        "    \"\"\"\n",
        "    if not output_tensor._custom_requires_grad:\n",
        "        raise RuntimeError(\"Output tensor does not require grad.\")\n",
        "    if output_tensor.graph is None:\n",
        "        raise RuntimeError(\"Output tensor is not part of a graph.\")\n",
        "\n",
        "    # Initialize gradient for the output tensor\n",
        "    output_tensor.tensor.grad = torch.ones_like(output_tensor.tensor)\n",
        "\n",
        "    # Perform backward pass using topological sort\n",
        "    nodes_to_process = output_tensor.graph.reverse_toposort()\n",
        "\n",
        "    # Create a strong reference to intermediate tensors needed for backward pass\n",
        "    # This simulates how a real autograd engine would keep track of them\n",
        "    # The graph context's intermediate_tensors dict already serves this purpose.\n",
        "\n",
        "    for tensor_node in nodes_to_process:\n",
        "        # Check if the weak proxy is still valid (tensor is alive)\n",
        "        if isinstance(tensor_node, weakref.ProxyTypes) and tensor_node.__slots__ is None:\n",
        "            # print(f\"Skipping dead proxy: {tensor_node}\") # Debugging\n",
        "            continue # Skip if the weak reference is dead\n",
        "\n",
        "        if tensor_node.tensor.grad is None and tensor_node is not output_tensor:\n",
        "            # This can happen if a tensor is part of the graph but its grad hasn't been set yet\n",
        "            # and it's not the root of the backward call. This typically means it's a leaf\n",
        "            # that wasn't used to compute the output or an intermediate that accumulated no grad.\n",
        "            # For simplicity in this test, we assume grads propagate.\n",
        "            # print(f\"Warning: Tensor node {tensor_node._node_id} has no grad before _backward call.\")\n",
        "            pass # A no-op for now. In a real system, you might want to handle this.\n",
        "\n",
        "        # Ensure that non-leaf tensors are still alive when their _backward is called\n",
        "        # The `intermediate_tensors` in `AutogradGraph` should keep them alive.\n",
        "        tensor_node._backward()\n",
        "\n",
        "    # Clean up intermediate tensors references after backward pass\n",
        "    # This would typically be handled by the graph context's exit, but\n",
        "    # if `_auto_cleanup` is False, you might need manual cleanup.\n",
        "    # Here, for testing GC, we'll let the context manager handle it.\n",
        "\n",
        "\n",
        "class TestCustomAutogradSystem:\n",
        "\n",
        "    def test_basic_add_scalar_grad(self):\n",
        "        with AutogradGraph() as graph:\n",
        "            a = CustomTensor(torch.tensor([2.0, 3.0]), _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            b = a + 5.0 # (a + 5)\n",
        "            c = b + 10.0 # (a + 5 + 10)\n",
        "\n",
        "            # Manually run backward pass\n",
        "            run_backward(c)\n",
        "\n",
        "            # Expected gradients:\n",
        "            # dC/dA = 1.0 (for each element)\n",
        "            assert torch.allclose(a.tensor.grad, torch.tensor([1.0, 1.0]))\n",
        "            assert b.tensor.grad is not None\n",
        "            assert torch.allclose(b.tensor.grad, torch.tensor([1.0, 1.0])) # dC/dB = 1.0\n",
        "\n",
        "            # Verify graph structure\n",
        "            assert graph.graph.num_nodes() == 3\n",
        "            assert graph.graph.num_edges() == 2\n",
        "            assert graph.graph.has_edge(a._node_id, b._node_id)\n",
        "            assert graph.graph.has_edge(b._node_id, c._node_id)\n",
        "            assert graph.check_cycle() is True\n",
        "\n",
        "    def test_basic_add_tensor_grad(self):\n",
        "        with AutogradGraph() as graph:\n",
        "            a = CustomTensor(torch.tensor([2.0, 3.0]), _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            b = CustomTensor(torch.tensor([1.0, 2.0]), _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            c = a + b # (a + b)\n",
        "            d = c + 5.0 # (a + b + 5)\n",
        "\n",
        "            run_backward(d)\n",
        "\n",
        "            # Expected gradients:\n",
        "            # dD/dA = 1.0\n",
        "            # dD/dB = 1.0\n",
        "            assert torch.allclose(a.tensor.grad, torch.tensor([1.0, 1.0]))\n",
        "            assert torch.allclose(b.tensor.grad, torch.tensor([1.0, 1.0]))\n",
        "\n",
        "            # Verify graph structure\n",
        "            assert graph.graph.num_nodes() == 4\n",
        "            assert graph.graph.num_edges() == 3\n",
        "            assert graph.graph.has_edge(a._node_id, c._node_id)\n",
        "            assert graph.graph.has_edge(b._node_id, c._node_id)\n",
        "            assert graph.graph.has_edge(c._node_id, d._node_id)\n",
        "            assert graph.check_cycle() is True\n",
        "\n",
        "    def test_mixed_requires_grad_tensor_add(self):\n",
        "        with AutogradGraph() as graph:\n",
        "            a = CustomTensor(torch.tensor([2.0, 3.0]), _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            b = CustomTensor(torch.tensor([1.0, 2.0]), _custom_requires_grad=False) # Does not require grad\n",
        "            c = a + b # c should require grad, b's grad should be None\n",
        "\n",
        "            run_backward(c)\n",
        "\n",
        "            assert torch.allclose(a.tensor.grad, torch.tensor([1.0, 1.0]))\n",
        "            assert b.tensor.grad is None # b should not have a grad\n",
        "            assert c._custom_requires_grad is True\n",
        "\n",
        "            # Verify graph structure\n",
        "            assert graph.graph.num_nodes() == 2 # Only a and c in the graph\n",
        "            assert graph.graph.num_edges() == 1\n",
        "            assert graph.graph.has_node(a._node_id)\n",
        "            assert graph.graph.has_node(c._node_id)\n",
        "            assert graph.graph.has_edge(a._node_id, c._node_id)\n",
        "            #assert not graph.graph.has_node(b._node_id) # b should not be in graph\n",
        "\n",
        "    def test_no_requires_grad(self):\n",
        "        with AutogradGraph() as graph: # Graph created, but no tensors with requires_grad=True added\n",
        "            a = CustomTensor(torch.tensor([1.0]))\n",
        "            b = CustomTensor(torch.tensor([2.0]))\n",
        "            c = a + b\n",
        "            d = c + 3.0\n",
        "\n",
        "            assert not a._custom_requires_grad\n",
        "            assert not b._custom_requires_grad\n",
        "            assert not c._custom_requires_grad\n",
        "            assert not d._custom_requires_grad\n",
        "            assert graph.graph.num_nodes() == 0 # Graph should remain empty\n",
        "            assert graph.graph.num_edges() == 0\n",
        "\n",
        "            with pytest.raises(RuntimeError, match=\"Output tensor does not require grad.\"):\n",
        "                run_backward(d)\n",
        "\n",
        "    def test_autograd_graph_context_manager(self):\n",
        "        graph = None\n",
        "        with AutogradGraph(check_for_cycles=True, auto_cleanup=True) as g:\n",
        "            graph = g\n",
        "            a = CustomTensor(torch.tensor([1.0]), _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            b = a + 1.0\n",
        "            assert graph.graph.num_nodes() == 2\n",
        "            assert graph.graph.num_edges() == 1\n",
        "            assert len(graph.intermediate_tensors) == 1 # b should be in intermediate_tensors\n",
        "\n",
        "        # After exiting the context, graph should be empty\n",
        "        assert graph.graph.num_nodes() == 0\n",
        "        assert graph.graph.num_edges() == 0\n",
        "        assert len(graph.intermediate_tensors) == 0\n",
        "\n",
        "    def test_cycle_detection(self):\n",
        "      try:\n",
        "        with AutogradGraph(check_for_cycles=True, auto_cleanup=False) as graph: # auto_cleanup=False to inspect after error\n",
        "            a = CustomTensor(torch.tensor([1.0]), _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            b = CustomTensor(torch.tensor([2.0]), _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "\n",
        "            # Manually create a cycle (a -> b -> a)\n",
        "            graph.add_edge(a._node_id, b._node_id)\n",
        "            graph.add_edge(b._node_id, a._node_id)\n",
        "      except RuntimeError as e:\n",
        "        print(f\"Raised the error of cycle detected as {e}\")\n",
        "            # with pytest.raises(RuntimeError, match=\"Cycle detected in autograd graph on context exit.\"):\n",
        "            #     pass # The __exit__ method will be called here\n",
        "\n",
        "    def test_no_circular_references_non_leaf_tensors_die(self):\n",
        "          # This test relies on the garbage collector. It's a heuristic test\n",
        "        # as Python's GC timing is not strictly deterministic.\n",
        "        # However, with weakrefs, it should work for non-leaf tensors.\n",
        "\n",
        "      print(\"\\n--- Starting GC Test: No Circular References (Part 1) ---\")\n",
        "\n",
        "      graph_ref = None\n",
        "      output_tensor_weak_ref = None\n",
        "      node_id_d = -1 # To store node_id before d is deleted\n",
        "\n",
        "      # BLOCK 1: Create graph and tensors\n",
        "      with AutogradGraph(auto_cleanup=False) as graph: # Keep graph for inspection\n",
        "          graph_ref = weakref.ref(graph)\n",
        "          a = CustomTensor(torch.tensor([1.0]), _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "          b = a + 1.0 # Intermediate tensor\n",
        "          c = b + 2.0 # Intermediate tensor\n",
        "          d = c + 3.0 # Output tensor (also intermediate from graph's perspective)\n",
        "\n",
        "          # Store weak reference to 'd' BEFORE its strong reference is potentially removed\n",
        "          output_tensor_weak_ref = weakref.ref(d)\n",
        "          node_id_d = d._node_id # Store node_id while d is alive\n",
        "\n",
        "          print(f\"Initial: d object: {d}\")\n",
        "          print(f\"Initial: d._node_id: {node_id_d}\")\n",
        "          print(f\"Initial: graph.intermediate_tensors keys: {list(graph.intermediate_tensors.keys())}\")\n",
        "          # The ref count for `d` object itself will be high here because it's in `graph.intermediate_tensors`,\n",
        "          # and held by variable `d`, and by the temporary ref in `getrefcount`.\n",
        "          print(f\"Initial: refcount of d (via output_tensor_weak_ref.test_ref): {sys.getrefcount(output_tensor_weak_ref()) if output_tensor_weak_ref() else 'N/A'}\")\n",
        "          assert len(graph.intermediate_tensors) == 3 # b, c, d should be in intermediate_tensors\n",
        "\n",
        "      # BLOCK 2: After exiting context manager (auto_cleanup=False)\n",
        "      print(\"\\n--- After exiting 'with' block (auto_cleanup=False) ---\")\n",
        "      # The 'graph' variable still holds a strong reference to the AutogradGraph instance.\n",
        "      # graph_ref() should return the graph object.\n",
        "      assert graph_ref() is not None, \"Graph object should still be alive.\"\n",
        "      assert len(graph_ref().intermediate_tensors) == 3, \"Intermediate tensors should still be referenced by the graph.\"\n",
        "      print(f\"After 'with' block: d object (via weakref): {output_tensor_weak_ref()}\")\n",
        "      print(f\"After 'with' block: refcount of d (via output_tensor_weak_ref.test_ref): {sys.getrefcount(output_tensor_weak_ref())}\")\n",
        "\n",
        "      # BLOCK 3: Remove strong reference 'd' from local scope\n",
        "      print(\"\\n--- Deleting 'd' variable ---\")\n",
        "      del d # Remove the local strong reference to the CustomTensor object.\n",
        "      gc.collect() # Force garbage collection\n",
        "\n",
        "      # Now, output_tensor_weak_ref() *still* shouldn't be None because `graph_ref().intermediate_tensors`\n",
        "      # holds the strong reference.\n",
        "      print(f\"After del d + gc.collect(): d object (via weakref): {output_tensor_weak_ref()}\")\n",
        "      # We expect this to *not* be None yet, and to still show a refcount reflecting intermediate_tensors.\n",
        "      assert output_tensor_weak_ref() is not None, \"d should still be alive due to intermediate_tensors.\"\n",
        "      current_d_refcount_after_del_d = sys.getrefcount(output_tensor_weak_ref()) if output_tensor_weak_ref() else 'N/A'\n",
        "      print(f\"After del d + gc.collect(): refcount of d: {current_d_refcount_after_del_d}\")\n",
        "      # Expected refcount should be 2: one from intermediate_tensors, one from getrefcount()\n",
        "      assert current_d_refcount_after_del_d == 2, f\"Expected refcount 2, got {current_d_refcount_after_del_d}\"\n",
        "\n",
        "      # BLOCK 4: Remove strong reference from intermediate_tensors\n",
        "      print(f\"\\n--- Deleting strong reference from graph.intermediate_tensors for node {node_id_d} ---\")\n",
        "      graph_ref().del_non_leaf_tensor_reference(node_id_d) # THIS IS THE CRUCIAL STEP\n",
        "      print(f\"After del_non_leaf_tensor_reference: graph.intermediate_tensors keys: {list(graph_ref().intermediate_tensors.keys())}\")\n",
        "      #gc.collect() # Force garbage collection again\n",
        "\n",
        "      # Now, with the last strong reference gone, 'd' should be garbage collected.\n",
        "      print(f\"After del_non_leaf_tensor_reference + gc.collect(): d object (via weakref): {output_tensor_weak_ref()}\")\n",
        "      # This is where your original assertion was. It *should* pass now.\n",
        "      assert output_tensor_weak_ref() is None, \"Output tensor (non-leaf) should be garbage collected after its strong reference is deleted from intermediate_tensors.\"\n",
        "      print(\"Assertion Passed: Output tensor (d) was garbage collected.\")\n",
        "\n",
        "      # BLOCK 5: Verify other intermediate tensors are collected when graph is cleared\n",
        "      print(\"\\n--- Starting GC Test: All Intermediate Tensors ---\")\n",
        "      intermediate_tensors_wrefs = []\n",
        "      # Create a new graph and new tensors to avoid interference from previous block\n",
        "      with AutogradGraph(auto_cleanup=False) as graph_new:\n",
        "          a_new = CustomTensor(torch.tensor([1.0]), _custom_requires_grad=True, graph=graph_new, is_leaf=True)\n",
        "          b_new = a_new + 1.0 # Intermediate\n",
        "          c_new = b_new + 2.0 # Intermediate\n",
        "          d_new = c_new + 3.0 # Intermediate (output of a chain)\n",
        "\n",
        "          # Store weak references to the intermediate tensors\n",
        "          intermediate_tensors_wrefs.append(weakref.ref(b_new))\n",
        "          intermediate_tensors_wrefs.append(weakref.ref(c_new))\n",
        "          intermediate_tensors_wrefs.append(weakref.ref(d_new))\n",
        "\n",
        "          # Verify they are initially alive\n",
        "          assert all(wref() is not None for wref in intermediate_tensors_wrefs)\n",
        "          assert len(graph_new.intermediate_tensors) == 3\n",
        "\n",
        "      print(f\"After 'with' block (new graph): graph_new object: {graph_new}\")\n",
        "      assert graph_new is not None, \"New graph object should still be alive after 'with' block.\"\n",
        "      assert len(graph_new.intermediate_tensors) == 3, \"New graph intermediate_tensors should still hold refs.\"\n",
        "\n",
        "      # Manually clear the intermediate_tensors dictionary and remove graph reference\n",
        "      print(\"\\n--- Manually clearing graph.intermediate_tensors and deleting graph ---\")\n",
        "      graph_new.intermediate_tensors.clear()\n",
        "      del graph_new # Remove the strong reference to the graph itself\n",
        "      del b_new , c_new , d_new # deleting the local variable strong references\n",
        "      #gc.collect()\n",
        "\n",
        "      # Now, all non-leaf tensors should be garbage collected\n",
        "      for i, wref in enumerate(intermediate_tensors_wrefs):\n",
        "          print(f\"Intermediate tensor {i} (via weakref): {wref()}\")\n",
        "          assert wref() is None, f\"Intermediate tensor {i} should be garbage collected after graph context and intermediate_tensors are cleared.\"\n",
        "      print(\"Assertion Passed: All intermediate tensors were garbage collected.\")\n",
        "\n",
        "    def test_topological_sort_order(self):\n",
        "        with AutogradGraph() as graph:\n",
        "            t1 = CustomTensor(torch.tensor([1.0]), _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            t2 = CustomTensor(torch.tensor([2.0]), _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            t3 = t1 + t2\n",
        "            t4 = t3 + 5.0\n",
        "            t5 = t2 + 10.0 # Another branch\n",
        "            t6 = t4 + t5\n",
        "\n",
        "            # The topological sort should produce an order where dependencies come before their dependents.\n",
        "            # Reversed topological sort should produce an order where outputs come before their inputs.\n",
        "            # Example expected order: t6, t4, t5, t3, t2, t1 (or variations respecting dependencies)\n",
        "            sorted_tensors = graph.reverse_toposort()\n",
        "\n",
        "            # Check if dependencies are respected in reverse order\n",
        "            # If A -> B, then B should appear before A in reverse topological sort.\n",
        "            # t6 depends on t4, t5. So t6 should be before t4 and t5.\n",
        "            # t4 depends on t3. So t4 should be before t3.\n",
        "            # t5 depends on t2. So t5 should be before t2.\n",
        "            # t3 depends on t1, t2. So t3 should be before t1 and t2.\n",
        "\n",
        "            # Simple check: The first element should be t6 (the ultimate output).\n",
        "            assert sorted_tensors[0] is t6\n",
        "\n",
        "            # Check positions:\n",
        "            pos = {t: i for i, t in enumerate(sorted_tensors)}\n",
        "\n",
        "            assert pos[t6] < pos[t4]\n",
        "            assert pos[t6] < pos[t5]\n",
        "            assert pos[t4] < pos[t3]\n",
        "            assert pos[t5] < pos[t2]\n",
        "            assert pos[t3] < pos[t1]\n",
        "            assert pos[t3] < pos[t2] # t3 also depends on t2\n",
        "\n",
        "            # Additional check: t2 is a dependency for both t3 and t5.\n",
        "            # In reverse topo sort, t3 and t5 must appear before t2.\n",
        "            assert pos[t3] < pos[t2]\n",
        "            assert pos[t5] < pos[t2]\n",
        "\n",
        "            # t1 is only a dependency for t3.\n",
        "            assert pos[t3] < pos[t1]\n",
        "\n",
        "            # Check if all 6 tensors are in the sorted list\n",
        "            assert len(sorted_tensors) == 6\n",
        "            assert set(sorted_tensors) == {t1, t2, t3, t4, t5, t6}\n",
        "\n",
        "# To run these tests, save the code as a Python file (e.g., `test_autograd.py`)\n",
        "# and run `pytest` from your terminal in the same directory.\n",
        "# `pip install pytest torch rustworkx` if you don't have them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 211,
      "metadata": {
        "id": "qlerSJMBZVyu"
      },
      "outputs": [],
      "source": [
        "k=TestCustomAutogradSystem()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 188,
      "metadata": {
        "id": "JqLizMhYZYFt"
      },
      "outputs": [],
      "source": [
        "k.test_basic_add_scalar_grad()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 189,
      "metadata": {
        "id": "-RzvJuk8ZcxQ"
      },
      "outputs": [],
      "source": [
        "k.test_basic_add_tensor_grad()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 190,
      "metadata": {
        "id": "FRSjwXxXdNRr"
      },
      "outputs": [],
      "source": [
        "k.test_mixed_requires_grad_tensor_add()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 191,
      "metadata": {
        "id": "NeI2AJpgdStt"
      },
      "outputs": [],
      "source": [
        "k.test_no_requires_grad()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 192,
      "metadata": {
        "id": "VDDJgK0xd2ob"
      },
      "outputs": [],
      "source": [
        "k.test_autograd_graph_context_manager()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 193,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "khP3jZxDd6hC",
        "outputId": "4041e6ae-f87d-4254-885f-cab08ed7ca13"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Raised the error of cycle detected as Cycle detected in autograd graph on context exit.\n"
          ]
        }
      ],
      "source": [
        "k.test_cycle_detection()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 194,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nfG1rNMXiCa9",
        "outputId": "54e6b624-fbd3-46d2-e6c5-b843bb30f6a2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "182"
            ]
          },
          "execution_count": 194,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gc\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 212,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EJQ9maoLeEGk",
        "outputId": "1b9de0eb-ce2a-4572-867d-bdf6d5354ee2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3\n",
            "d <__main__.CustomTensor object at 0x78a841a447c0>\n",
            "3\n",
            "2\n",
            "<weakref at 0x78a841a47420; to 'CustomTensor' at 0x78a841a447c0>\n",
            "3\n",
            "2\n",
            "Outut_Tensor_weak_ref <weakref at 0x78a841a47420; dead>\n",
            "\n",
            "Running advanced GC test for intermediate tensors...\n"
          ]
        }
      ],
      "source": [
        "k.test_no_circular_references_non_leaf_tensors_die()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 208,
      "metadata": {
        "id": "k0bgwgYpjLow"
      },
      "outputs": [],
      "source": [
        "def test_no_circular_references_non_leaf_tensors_die(self):\n",
        "          # This test relies on the garbage collector. It's a heuristic test\n",
        "        # as Python's GC timing is not strictly deterministic.\n",
        "        # However, with weakrefs, it should work for non-leaf tensors.\n",
        "\n",
        "    print(\"\\n--- Starting GC Test: No Circular References (Part 1) ---\")\n",
        "\n",
        "    graph_ref = None\n",
        "    output_tensor_weak_ref = None\n",
        "    node_id_d = -1 # To store node_id before d is deleted\n",
        "\n",
        "    # BLOCK 1: Create graph and tensors\n",
        "    with AutogradGraph(auto_cleanup=False) as graph: # Keep graph for inspection\n",
        "        graph_ref = weakref.ref(graph)\n",
        "        a = CustomTensor(torch.tensor([1.0]), _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "        b = a + 1.0 # Intermediate tensor\n",
        "        c = b + 2.0 # Intermediate tensor\n",
        "        d = c + 3.0 # Output tensor (also intermediate from graph's perspective)\n",
        "\n",
        "        # Store weak reference to 'd' BEFORE its strong reference is potentially removed\n",
        "        output_tensor_weak_ref = weakref.ref(d)\n",
        "        node_id_d = d._node_id # Store node_id while d is alive\n",
        "\n",
        "        print(f\"Initial: d object: {d}\")\n",
        "        print(f\"Initial: d._node_id: {node_id_d}\")\n",
        "        print(f\"Initial: graph.intermediate_tensors keys: {list(graph.intermediate_tensors.keys())}\")\n",
        "        # The ref count for `d` object itself will be high here because it's in `graph.intermediate_tensors`,\n",
        "        # and held by variable `d`, and by the temporary ref in `getrefcount`.\n",
        "        print(f\"Initial: refcount of d (via output_tensor_weak_ref.test_ref): {sys.getrefcount(output_tensor_weak_ref()) if output_tensor_weak_ref() else 'N/A'}\")\n",
        "        assert len(graph.intermediate_tensors) == 3 # b, c, d should be in intermediate_tensors\n",
        "\n",
        "    # BLOCK 2: After exiting context manager (auto_cleanup=False)\n",
        "    print(\"\\n--- After exiting 'with' block (auto_cleanup=False) ---\")\n",
        "    # The 'graph' variable still holds a strong reference to the AutogradGraph instance.\n",
        "    # graph_ref() should return the graph object.\n",
        "    assert graph_ref() is not None, \"Graph object should still be alive.\"\n",
        "    assert len(graph_ref().intermediate_tensors) == 3, \"Intermediate tensors should still be referenced by the graph.\"\n",
        "    print(f\"After 'with' block: d object (via weakref): {output_tensor_weak_ref()}\")\n",
        "    print(f\"After 'with' block: refcount of d (via output_tensor_weak_ref.test_ref): {sys.getrefcount(output_tensor_weak_ref())}\")\n",
        "\n",
        "    # BLOCK 3: Remove strong reference 'd' from local scope\n",
        "    print(\"\\n--- Deleting 'd' variable ---\")\n",
        "    del d # Remove the local strong reference to the CustomTensor object.\n",
        "    gc.collect() # Force garbage collection\n",
        "\n",
        "    # Now, output_tensor_weak_ref() *still* shouldn't be None because `graph_ref().intermediate_tensors`\n",
        "    # holds the strong reference.\n",
        "    print(f\"After del d + gc.collect(): d object (via weakref): {output_tensor_weak_ref()}\")\n",
        "    # We expect this to *not* be None yet, and to still show a refcount reflecting intermediate_tensors.\n",
        "    assert output_tensor_weak_ref() is not None, \"d should still be alive due to intermediate_tensors.\"\n",
        "    current_d_refcount_after_del_d = sys.getrefcount(output_tensor_weak_ref()) if output_tensor_weak_ref() else 'N/A'\n",
        "    print(f\"After del d + gc.collect(): refcount of d: {current_d_refcount_after_del_d}\")\n",
        "    # Expected refcount should be 2: one from intermediate_tensors, one from getrefcount()\n",
        "    assert current_d_refcount_after_del_d == 2, f\"Expected refcount 2, got {current_d_refcount_after_del_d}\"\n",
        "\n",
        "    # BLOCK 4: Remove strong reference from intermediate_tensors\n",
        "    print(f\"\\n--- Deleting strong reference from graph.intermediate_tensors for node {node_id_d} ---\")\n",
        "    graph_ref().del_non_leaf_tensor_reference(node_id_d) # THIS IS THE CRUCIAL STEP\n",
        "    print(f\"After del_non_leaf_tensor_reference: graph.intermediate_tensors keys: {list(graph_ref().intermediate_tensors.keys())}\")\n",
        "    #gc.collect() # Force garbage collection again\n",
        "\n",
        "    # Now, with the last strong reference gone, 'd' should be garbage collected.\n",
        "    print(f\"After del_non_leaf_tensor_reference + gc.collect(): d object (via weakref): {output_tensor_weak_ref()}\")\n",
        "    # This is where your original assertion was. It *should* pass now.\n",
        "    assert output_tensor_weak_ref() is None, \"Output tensor (non-leaf) should be garbage collected after its strong reference is deleted from intermediate_tensors.\"\n",
        "    print(\"Assertion Passed: Output tensor (d) was garbage collected.\")\n",
        "\n",
        "    # BLOCK 5: Verify other intermediate tensors are collected when graph is cleared\n",
        "    print(\"\\n--- Starting GC Test: All Intermediate Tensors ---\")\n",
        "    intermediate_tensors_wrefs = []\n",
        "    # Create a new graph and new tensors to avoid interference from previous block\n",
        "    with AutogradGraph(auto_cleanup=False) as graph_new:\n",
        "        a_new = CustomTensor(torch.tensor([1.0]), _custom_requires_grad=True, graph=graph_new, is_leaf=True)\n",
        "        b_new = a_new + 1.0 # Intermediate\n",
        "        c_new = b_new + 2.0 # Intermediate\n",
        "        d_new = c_new + 3.0 # Intermediate (output of a chain)\n",
        "\n",
        "        # Store weak references to the intermediate tensors\n",
        "        intermediate_tensors_wrefs.append(weakref.ref(b_new))\n",
        "        intermediate_tensors_wrefs.append(weakref.ref(c_new))\n",
        "        intermediate_tensors_wrefs.append(weakref.ref(d_new))\n",
        "\n",
        "        # Verify they are initially alive\n",
        "        assert all(wref() is not None for wref in intermediate_tensors_wrefs)\n",
        "        assert len(graph_new.intermediate_tensors) == 3\n",
        "\n",
        "    print(f\"After 'with' block (new graph): graph_new object: {graph_new}\")\n",
        "    assert graph_new is not None, \"New graph object should still be alive after 'with' block.\"\n",
        "    assert len(graph_new.intermediate_tensors) == 3, \"New graph intermediate_tensors should still hold refs.\"\n",
        "\n",
        "    # Manually clear the intermediate_tensors dictionary and remove graph reference\n",
        "    print(\"\\n--- Manually clearing graph.intermediate_tensors and deleting graph ---\")\n",
        "    graph_new.intermediate_tensors.clear()\n",
        "    del graph_new # Remove the strong reference to the graph itself\n",
        "    del b_new , c_new , d_new # deleting the local variable strong references\n",
        "    #gc.collect()\n",
        "\n",
        "    # Now, all non-leaf tensors should be garbage collected\n",
        "    for i, wref in enumerate(intermediate_tensors_wrefs):\n",
        "        print(f\"Intermediate tensor {i} (via weakref): {wref()}\")\n",
        "        assert wref() is None, f\"Intermediate tensor {i} should be garbage collected after graph context and intermediate_tensors are cleared.\"\n",
        "    print(\"Assertion Passed: All intermediate tensors were garbage collected.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 209,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ee8MD9T3jWsG",
        "outputId": "ff13f91f-728d-4181-9e7f-6e8e3db70690"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Starting GC Test: No Circular References (Part 1) ---\n",
            "Initial: d object: <__main__.CustomTensor object at 0x78a841a2f4c0>\n",
            "Initial: d._node_id: 3\n",
            "Initial: graph.intermediate_tensors keys: [1, 2, 3]\n",
            "Initial: refcount of d (via output_tensor_weak_ref.test_ref): 3\n",
            "\n",
            "--- After exiting 'with' block (auto_cleanup=False) ---\n",
            "After 'with' block: d object (via weakref): <__main__.CustomTensor object at 0x78a841a2f4c0>\n",
            "After 'with' block: refcount of d (via output_tensor_weak_ref.test_ref): 3\n",
            "\n",
            "--- Deleting 'd' variable ---\n",
            "After del d + gc.collect(): d object (via weakref): <__main__.CustomTensor object at 0x78a841a2f4c0>\n",
            "After del d + gc.collect(): refcount of d: 2\n",
            "\n",
            "--- Deleting strong reference from graph.intermediate_tensors for node 3 ---\n",
            "After del_non_leaf_tensor_reference: graph.intermediate_tensors keys: [1, 2]\n",
            "After del_non_leaf_tensor_reference + gc.collect(): d object (via weakref): None\n",
            "Assertion Passed: Output tensor (d) was garbage collected.\n",
            "\n",
            "--- Starting GC Test: All Intermediate Tensors ---\n",
            "After 'with' block (new graph): graph_new object: CustomAutogradGraph(nodes=4, edges=3)\n",
            "\n",
            "--- Manually clearing graph.intermediate_tensors and deleting graph ---\n",
            "Intermediate tensor 0 (via weakref): None\n",
            "Intermediate tensor 1 (via weakref): None\n",
            "Intermediate tensor 2 (via weakref): None\n",
            "Assertion Passed: All intermediate tensors were garbage collected.\n"
          ]
        }
      ],
      "source": [
        "test_no_circular_references_non_leaf_tensors_die()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w638ekDnjXKm"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
