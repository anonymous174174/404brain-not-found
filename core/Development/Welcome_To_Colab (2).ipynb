{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5JkdP6sXmSoX",
        "outputId": "049a0c4d-939a-4382-e456-c62fa2cbdf9e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting rustworkx\n",
            "  Downloading rustworkx-0.16.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy<3,>=1.16.0 in /usr/local/lib/python3.11/dist-packages (from rustworkx) (2.0.2)\n",
            "Downloading rustworkx-0.16.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rustworkx\n",
            "Successfully installed rustworkx-0.16.0\n"
          ]
        }
      ],
      "source": [
        "! pip install rustworkx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "ulR6Lvv1lxRA"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import weakref\n",
        "import numbers\n",
        "import rustworkx as rx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFj9utaNld3a"
      },
      "source": [
        "# custom Tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "Zyo4n4MclcEK"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "class CustomTensor:\n",
        "    \"\"\"\n",
        "    CustomTensor(data, ...) will return the same object if `data` is already a CustomTensor.\n",
        "    This avoids memory reallocation and graph duplication.\n",
        "    Do not use it to rewrap or re-register nodes. Use only for symbolic reference.\n",
        "    \"\"\"\n",
        "    __slots__ = ('tensor', '_node_id', '_custom_requires_grad', '_backward', 'graph','__weakref__')\n",
        "\n",
        "    def __new__(cls, data, *, _custom_requires_grad=False, device=None, dtype=None, graph=None,due_to_operation=False):\n",
        "        if isinstance(data, CustomTensor):\n",
        "            return data  # ✅ Prevent new allocation, return existing one\n",
        "        return super().__new__(cls)\n",
        "\n",
        "    def __init__(self, data, *, _custom_requires_grad=False, device=None, dtype=None, graph=None,due_to_operation=False):\n",
        "        if isinstance(data, CustomTensor):\n",
        "            # Already returned in __new__, skip init\n",
        "            return\n",
        "        if due_to_operation:\n",
        "          self.tensor = data\n",
        "        else:\n",
        "          self.tensor = torch.as_tensor(data, dtype=dtype, device=device)\n",
        "          self.tensor.requires_grad_(False)\n",
        "        self._custom_requires_grad = _custom_requires_grad\n",
        "        self._node_id = None\n",
        "        self._backward = lambda: None\n",
        "        self.graph = None\n",
        "\n",
        "        if _custom_requires_grad:\n",
        "            self._init_graph(graph)\n",
        "\n",
        "    def _init_graph(self, graph):\n",
        "        if graph is None:\n",
        "            raise ValueError(\"Graph must be provided when _custom_requires_grad is True.\")\n",
        "        self.graph = weakref.ref(graph)\n",
        "        graph.add_tensor_graph(self)\n",
        "        graph.add_non_leaf_tensor_references(self)\n",
        "    def _zero_grad(self):\n",
        "        self.tensor.grad = torch.zeros_like(self.tensor)\n",
        "\n",
        "    def __add__(self,other):\n",
        "\n",
        "        if isinstance(other, numbers.Number):\n",
        "            result = Operations.add_tensor_and_scalar(self.tensor,other)\n",
        "            requires_grad = self._custom_requires_grad\n",
        "            if requires_grad:\n",
        "                graph = self.graph()\n",
        "                result = CustomTensor(result,_custom_requires_grad=True,graph=graph,due_to_operation=True)\n",
        "                graph.add_edge(self._node_id, result._node_id)\n",
        "                def _backward():\n",
        "                    if self.tensor.grad is None:\n",
        "                        self._zero_grad()\n",
        "                    self.tensor.grad = Operations.add_tensor_and_tensor(self.tensor.grad,result.tensor.grad)\n",
        "                result._backward = _backward\n",
        "                return result\n",
        "            else:\n",
        "                return CustomTensor(result,_custom_requires_grad=requires_grad)\n",
        "        elif isinstance(other,CustomTensor):\n",
        "            result = Operations.add_tensor_and_tensor(self.tensor,other.tensor)\n",
        "            self_requires_grad = self._custom_requires_grad\n",
        "            other_requires_grad = other._custom_requires_grad\n",
        "            if self_requires_grad and other_requires_grad:\n",
        "                graph = self.graph()\n",
        "                result = CustomTensor(result,_custom_requires_grad=True,graph=graph,due_to_operation=True)\n",
        "                graph.add_edge(self._node_id, result._node_id)\n",
        "                graph.add_edge(other._node_id, result._node_id)\n",
        "                def _backward():\n",
        "                    if self.tensor.grad is None:\n",
        "                        self._zero_grad()\n",
        "                    if other.tensor.grad is None:\n",
        "                        other._zero_grad()\n",
        "                    self.tensor.grad.add_(result.tensor.grad)\n",
        "                    other.tensor.grad.add_(result.tensor.grad)\n",
        "                result._backward = _backward\n",
        "                return result\n",
        "            elif self_requires_grad and not other_requires_grad:\n",
        "                graph =self.graph()\n",
        "                result = CustomTensor(result,_custom_requires_grad = True, graph=graph,due_to_operation=True)\n",
        "                graph.add_edge(self._node_id,result._node_id)\n",
        "                def _backward():\n",
        "                    if self.tensor.grad is None:\n",
        "                        self._zero_grad()\n",
        "                    self.tensor.grad.add_(result.tensor.grad)# = Operations.add_tensor_and_tensor(self.tensor.grad,result.tensor.grad)\n",
        "                result._backward =_backward\n",
        "                return result\n",
        "            elif other_requires_grad and not self_requires_grad:\n",
        "                graph = other.graph()\n",
        "                result = CustomTensor(result,_custom_requires_grad =True, graph = graph,due_to_operation=True)\n",
        "                graph.add_edge(other._node_id,result._node_id)\n",
        "                def _backward():\n",
        "                    if other.tensor.grad is None:\n",
        "                        other._zero_grad()\n",
        "                    other.tensor.grad.add_(result.tensor.grad) #= Operations.add_tensor_and_tensor(other.tensor.grad,result.tensor.grad)\n",
        "                result._backward =_backward\n",
        "                return result\n",
        "            else:\n",
        "                return CustomTensor(result,_custom_requires_grad=False,graph=None,due_to_operation=True)\n",
        "    def __del__(self):\n",
        "        print(f\"CustomTensor with id={id(self)} is being garbage collected\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wctU5vHZlhfL"
      },
      "source": [
        "# AutogradGraph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "GN6M4TQFlmwM"
      },
      "outputs": [],
      "source": [
        "class AutogradGraph:\n",
        "    def __init__(self,check_for_cycles=True, auto_cleanup=True):\n",
        "        self.graph = rx.PyDiGraph()\n",
        "        self.intermediate_tensors = dict()\n",
        "        self._check_cycles = check_for_cycles\n",
        "        self._auto_cleanup = auto_cleanup\n",
        "\n",
        "\n",
        "    def __enter__(self):\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, exc_type, exc_value, traceback):\n",
        "        if self._check_cycles:\n",
        "            if not self.check_cycle():\n",
        "                raise RuntimeError(\"Cycle detected in autograd graph on context exit.\")\n",
        "        if self._auto_cleanup:\n",
        "            self.intermediate_tensors.clear()\n",
        "            self.graph.clear()\n",
        "\n",
        "    def add_tensor_graph(self, tensor):\n",
        "        requires_grad = tensor._custom_requires_grad\n",
        "        if not requires_grad:\n",
        "            raise ValueError(\"Tensor with require grad False cannot to be added to the graph.\")\n",
        "\n",
        "        tensor_index = self.graph.add_node(weakref.ref(tensor))\n",
        "        tensor._node_id  = tensor_index\n",
        "\n",
        "    def add_non_leaf_tensor_references(self, tensor):\n",
        "        requires_grad = tensor._custom_requires_grad\n",
        "\n",
        "        node_id = tensor._node_id\n",
        "\n",
        "        if not requires_grad:\n",
        "            raise ValueError(\"Tensor must be a non leaf tensor.\")\n",
        "\n",
        "        if node_id in self.intermediate_tensors:\n",
        "            raise ValueError(\"Tensor reference to persist in memory already exists.\")\n",
        "\n",
        "        self.intermediate_tensors[node_id] = tensor\n",
        "\n",
        "    def add_edge(self, node_from, node_to, weight=None):\n",
        "        if not isinstance(node_from, int) or not isinstance(node_to, int):\n",
        "            raise TypeError(\"Node indices must be integers.\")\n",
        "\n",
        "        graph = self.graph\n",
        "        if not graph.has_node(node_from) or not graph.has_node(node_to):\n",
        "            raise ValueError(\"Both nodes must exist in the graph before adding an edge.\")\n",
        "\n",
        "        graph.add_edge(node_from, node_to, weight)\n",
        "\n",
        "    def check_cycle(self):\n",
        "        return rx.is_directed_acyclic_graph(self.graph)\n",
        "\n",
        "    def reverse_toposort(self):\n",
        "\n",
        "        if not self.check_cycle():\n",
        "            raise RuntimeError(\"Cannot perform topological sort on a graph with cycles.\")\n",
        "        graph = self.graph\n",
        "        node_indexes = rx.topological_sort(graph)\n",
        "        return [graph[node_index] for node_index in reversed(node_indexes)]\n",
        "\n",
        "    def delete_node(self, node_index):\n",
        "        if not isinstance(node_index, int):\n",
        "            raise TypeError(\"Node index must be an integer.\")\n",
        "\n",
        "        graph = self.graph\n",
        "        if not graph.has_node(node_index):\n",
        "            raise ValueError(f\"Node index {node_index} does not exist in the graph.\")\n",
        "\n",
        "        graph.remove_node(node_index)\n",
        "\n",
        "    def delete_edge(self, node_from, node_to):\n",
        "        if not isinstance(node_from, int) or not isinstance(node_to, int):\n",
        "            raise TypeError(\"Node indices must be integers.\")\n",
        "\n",
        "        graph = self.graph\n",
        "        if not graph.has_edge(node_from, node_to):\n",
        "            raise ValueError(f\"Edge ({node_from}, {node_to}) does not exist in the graph.\")\n",
        "\n",
        "        graph.remove_edge(node_from, node_to)\n",
        "\n",
        "    def del_non_leaf_tensor_reference(self, tensor_node_id):\n",
        "        try:\n",
        "            del self.intermediate_tensors[tensor_node_id]\n",
        "        except KeyError:\n",
        "            raise KeyError(f\"No tensor reference found for node ID {tensor_node_id}\")\n",
        "\n",
        "    def __repr__(self):\n",
        "        graph = self.graph\n",
        "        return f\"CustomAutogradGraph(nodes={graph.num_nodes()}, edges={graph.num_edges()})\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3eqsMV42lnfh"
      },
      "source": [
        "# Operations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "FkA57SKdlpUJ"
      },
      "outputs": [],
      "source": [
        "class Operations:\n",
        "    @torch.jit.script\n",
        "    def add_tensor_and_scalar(tensor: torch.Tensor, scaler: float) -> torch.Tensor:\n",
        "        return tensor + scaler\n",
        "\n",
        "    @torch.jit.script\n",
        "    def add_tensor_and_tensor(tensor1: torch.Tensor, tensor2: torch.Tensor) -> torch.Tensor:\n",
        "        return tensor1 + tensor2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEdjOowPmgZ-"
      },
      "source": [
        "# Tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "XnNWDQWtme0s"
      },
      "outputs": [],
      "source": [
        "device = \"cpu\"\n",
        "dtype =  torch.float16\n",
        "a = CustomTensor([1,2,3],_custom_requires_grad=False,device=\"cpu\",dtype=dtype,graph = None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "kbQPucrMEpz7"
      },
      "outputs": [],
      "source": [
        "b = CustomTensor([-5,6.8,-3.5],_custom_requires_grad=False,device = device,dtype=dtype,graph = None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AzH1083JE7vu",
        "outputId": "ab78e940-ce53-4ec0-f033-3620612a50b4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<__main__.CustomTensor at 0x78a842ead350>"
            ]
          },
          "execution_count": 93,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vlX4MIS7rW-p",
        "outputId": "3ee5b965-b470-4372-c0ff-f576e20be642"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<__main__.CustomTensor at 0x78a842ad2750>"
            ]
          },
          "execution_count": 94,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ChS1tzQr84E",
        "outputId": "70d79387-dd10-460c-98bd-5ba3a120cf7d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([0., 0., 0.], dtype=torch.float16)"
            ]
          },
          "execution_count": 121,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "a.tensor.grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "id": "MZLnqJQsHiv5"
      },
      "outputs": [],
      "source": [
        "a._zero_grad()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_kKXynC8H5El",
        "outputId": "244c5f26-a5c9-41fb-b6b7-449d1551a6fc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([1., 1., 1.], dtype=torch.float16)"
            ]
          },
          "execution_count": 122,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "a.tensor.grad.add_(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "id": "-yPyYqlWE96d"
      },
      "outputs": [],
      "source": [
        "c=a+b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dlbuVFyLHSO1",
        "outputId": "2d355207-5ae6-4fc8-bd28-bc0b6650c2bb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([-4.0000,  8.7969, -0.5000], dtype=torch.float16)"
            ]
          },
          "execution_count": 116,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "c.tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wA1OhLpKHVJj",
        "outputId": "d5ed4e04-0292-4f48-8108-b36d1bab4f07"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<__main__.CustomTensor at 0x78a842b328e0>"
            ]
          },
          "execution_count": 117,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "c"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "id": "ObERoNbAHWMn"
      },
      "outputs": [],
      "source": [
        "d=c"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8dZr8Vp1HXEV",
        "outputId": "c7ee1a6e-e283-4478-d5fd-c71b2730ae17"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<__main__.CustomTensor at 0x78a842b328e0>"
            ]
          },
          "execution_count": 119,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "LjWyo2qXoWm-"
      },
      "outputs": [],
      "source": [
        "k=CustomTensor(a)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7fmEKvtSrlOa",
        "outputId": "5c95d9f4-7d8d-4988-8042-c8eaab002335"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([1., 2., 3.], dtype=torch.float16)"
            ]
          },
          "execution_count": 72,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "k.tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pEl3Xrj1u5WU",
        "outputId": "2fe5d151-8387-4bb4-a348-2cbb96f28f20"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<__main__.CustomTensor at 0x78a861876f20>"
            ]
          },
          "execution_count": 73,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "k"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "MR1V5liRDvk-"
      },
      "outputs": [],
      "source": [
        "k=None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "UiKj_s8CEbvz"
      },
      "outputs": [],
      "source": [
        "del a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "sZL89CWED6Z-"
      },
      "outputs": [],
      "source": [
        "a=None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "id": "vtk2H0-4ES7S"
      },
      "outputs": [],
      "source": [
        "import gc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q9O1vGfHvZHU",
        "outputId": "d959b26d-39a1-4588-ab7d-0b480a55f09c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "print(sys.getrefcount(a))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "5JpEC2NnvhlI"
      },
      "outputs": [],
      "source": [
        "import gc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LtUC5Ly3vjFz",
        "outputId": "a703bbac-b815-434c-c31f-80f2caf066c4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "execution_count": 114,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "eDfwBP41uyA4"
      },
      "outputs": [],
      "source": [
        "k=None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "bIvuQ3Xju0rL"
      },
      "outputs": [],
      "source": [
        "del a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "Z_Wh9bsVpljS"
      },
      "outputs": [],
      "source": [
        "k=None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kswkZWYep2J6",
        "outputId": "453f71e3-3ade-451a-b653-d98433a45e75"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<__main__.CustomTensor at 0x7bdc07166ed0>"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pa5SncS0pnIG",
        "outputId": "cc91be06-9788-4dd3-e525-bbc46d886c59"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<__main__.CustomTensor at 0x7979064d74c0>"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "C`ustomTensor(a)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5wsoA6-woXCF",
        "outputId": "19400a38-1082-4178-a613-6005cbd6d1bf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([1., 2., 3.], dtype=torch.float16)"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "a.tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wQLLFJWapHti",
        "outputId": "0992b15f-ad49-4958-97c0-a0bb23bb0c7c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ref count after a: 2\n",
            "Ref count after k: 3\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import torch\n",
        "import weakref\n",
        "\n",
        "class CustomTensor:\n",
        "    __slots__ = ('tensor', '_node_id', '_custom_requires_grad', '_backward', 'graph')\n",
        "\n",
        "    def __new__(cls, data, *, _custom_requires_grad=False, device=None, dtype=None, graph=None):\n",
        "        if isinstance(data, CustomTensor):\n",
        "            return data\n",
        "        return super().__new__(cls)\n",
        "\n",
        "    def __init__(self, data, *, _custom_requires_grad=False, device=None, dtype=None, graph=None):\n",
        "        if isinstance(data, CustomTensor):\n",
        "            return\n",
        "        self.tensor = torch.as_tensor(data, dtype=dtype, device=device)\n",
        "        self._custom_requires_grad = _custom_requires_grad\n",
        "        self._node_id = None\n",
        "        self._backward = lambda: None\n",
        "        self.graph = None\n",
        "\n",
        "a = CustomTensor([1,2,3], _custom_requires_grad=False, device=\"cpu\", dtype=torch.float16, graph=None)\n",
        "print(\"Ref count after a:\", sys.getrefcount(a))\n",
        "\n",
        "k = CustomTensor(a)\n",
        "print(\"Ref count after k:\", sys.getrefcount(a))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46H4koJCY40y"
      },
      "source": [
        "# testing reference and weakref.proxy use case possibility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MvzZ4DZuXsv_"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import weakref\n",
        "import numbers\n",
        "import rustworkx as rx\n",
        "\n",
        "class Operations:\n",
        "    @torch.jit.script\n",
        "    def add_tensor_and_scalar(tensor: torch.Tensor, scalar: float) -> torch.Tensor:\n",
        "        return tensor + scalar\n",
        "\n",
        "    @torch.jit.script\n",
        "    def add_tensor_and_tensor(tensor1: torch.Tensor, tensor2: torch.Tensor) -> torch.Tensor:\n",
        "        return tensor1 + tensor2\n",
        "\n",
        "class AutogradGraph:\n",
        "    def __init__(self, check_for_cycles=True, auto_cleanup=True):\n",
        "        self.graph = rx.PyDiGraph()\n",
        "        self.intermediate_tensors = {}\n",
        "        self._check_cycles = check_for_cycles\n",
        "        self._auto_cleanup = auto_cleanup\n",
        "\n",
        "    def __enter__(self):\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, exc_type, exc_value, traceback):\n",
        "        if self._check_cycles and not self.check_cycle():\n",
        "            raise RuntimeError(\"Cycle detected in autograd graph on context exit.\")\n",
        "        if self._auto_cleanup:\n",
        "            self.intermediate_tensors.clear()\n",
        "            self.graph.clear()\n",
        "\n",
        "    def add_tensor_graph(self, tensor, is_leaf):\n",
        "        if not tensor._custom_requires_grad:\n",
        "            raise ValueError(\"Tensor with requires_grad=False cannot be added to the graph.\")\n",
        "\n",
        "        ref = tensor if is_leaf else weakref.proxy(tensor)\n",
        "        tensor_index = self.graph.add_node(ref)\n",
        "        tensor._node_id = tensor_index\n",
        "\n",
        "    def add_non_leaf_tensor_reference(self, tensor):\n",
        "        if not tensor._custom_requires_grad:\n",
        "            raise ValueError(\"Tensor must require grad.\")\n",
        "\n",
        "        if tensor._node_id in self.intermediate_tensors:\n",
        "            raise ValueError(\"Tensor reference already exists in intermediate tensors.\")\n",
        "\n",
        "        self.intermediate_tensors[tensor._node_id] = tensor\n",
        "\n",
        "    def add_edge(self, node_from, node_to, weight=None):\n",
        "        if not all(isinstance(n, int) for n in (node_from, node_to)):\n",
        "            raise TypeError(\"Node indices must be integers.\")\n",
        "        if not self.graph.has_node(node_from) or not self.graph.has_node(node_to):\n",
        "            raise ValueError(\"Nodes must exist before adding edge.\")\n",
        "        self.graph.add_edge(node_from, node_to, weight)\n",
        "\n",
        "    def check_cycle(self):\n",
        "        return rx.is_directed_acyclic_graph(self.graph)\n",
        "\n",
        "    def reverse_toposort(self):\n",
        "        if not self.check_cycle():\n",
        "            raise RuntimeError(\"Cannot perform topological sort on cyclic graph.\")\n",
        "        return [self.graph[n] for n in reversed(rx.topological_sort(self.graph))]\n",
        "\n",
        "    def delete_node(self, node_index):\n",
        "        if not isinstance(node_index, int):\n",
        "            raise TypeError(\"Node index must be an integer.\")\n",
        "        if not self.graph.has_node(node_index):\n",
        "            raise ValueError(\"Node does not exist.\")\n",
        "        self.graph.remove_node(node_index)\n",
        "\n",
        "    def delete_edge(self, node_from, node_to):\n",
        "        if not self.graph.has_edge(node_from, node_to):\n",
        "            raise ValueError(\"Edge does not exist.\")\n",
        "        self.graph.remove_edge(node_from, node_to)\n",
        "\n",
        "    def del_non_leaf_tensor_reference(self, tensor_node_id):\n",
        "        self.intermediate_tensors.pop(tensor_node_id, None)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"CustomAutogradGraph(nodes={self.graph.num_nodes()}, edges={self.graph.num_edges()})\"\n",
        "\n",
        "class CustomTensor:\n",
        "    __slots__ = ('tensor', '_node_id', '_custom_requires_grad', '_backward', 'graph', '__weakref__')\n",
        "\n",
        "    def __new__(cls, data, *, _custom_requires_grad=False, device=None, dtype=None, graph=None, due_to_operation=False, is_leaf=False):\n",
        "        if isinstance(data, CustomTensor):\n",
        "            return data  # Don't rewrap\n",
        "        return super().__new__(cls)\n",
        "\n",
        "    def __init__(self, data, *, _custom_requires_grad=False, device=None, dtype=None, graph=None, due_to_operation=False, is_leaf=False):\n",
        "        if isinstance(data, CustomTensor):\n",
        "            return\n",
        "\n",
        "        self.tensor = data if due_to_operation else torch.as_tensor(data, dtype=dtype, device=device)\n",
        "        self.tensor.requires_grad_(False)\n",
        "        self._custom_requires_grad = _custom_requires_grad\n",
        "        self._node_id = None\n",
        "        self._backward = lambda: None\n",
        "        self.graph = None\n",
        "\n",
        "        if _custom_requires_grad:\n",
        "            self._init_graph(graph, is_leaf)\n",
        "\n",
        "    def _init_graph(self, graph, is_leaf):\n",
        "        if graph is None:\n",
        "            raise ValueError(\"Graph must be provided if requires_grad is True.\")\n",
        "        self.graph = weakref.proxy(graph)\n",
        "        graph.add_tensor_graph(self, is_leaf=is_leaf)\n",
        "        if not is_leaf:\n",
        "            graph.add_non_leaf_tensor_reference(self)\n",
        "\n",
        "    def _zero_grad(self):\n",
        "        self.tensor.grad = torch.zeros_like(self.tensor)\n",
        "\n",
        "    def __add__(self, other):\n",
        "        if isinstance(other, numbers.Number):\n",
        "            return self._binary_op_scalar(other, op=Operations.add_tensor_and_scalar)\n",
        "        elif isinstance(other, CustomTensor):\n",
        "            return self._binary_op_tensor(other, op=Operations.add_tensor_and_tensor)\n",
        "        return NotImplemented\n",
        "\n",
        "    def _binary_op_scalar(self, scalar, op):\n",
        "        result_tensor = op(self.tensor, scalar)\n",
        "        if not self._custom_requires_grad:\n",
        "            return CustomTensor(result_tensor)\n",
        "\n",
        "        graph = self.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "        graph.add_edge(self._node_id, result._node_id)\n",
        "\n",
        "        self_ref = weakref.proxy(self)\n",
        "        def _backward():\n",
        "            if self_ref.tensor.grad is None:\n",
        "                self_ref._zero_grad()\n",
        "            self_ref.tensor.grad.add_(result.tensor.grad)\n",
        "\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "\n",
        "    def _binary_op_tensor(self, other, op):\n",
        "        result_tensor = op(self.tensor, other.tensor)\n",
        "        requires_grad = self._custom_requires_grad or other._custom_requires_grad\n",
        "\n",
        "        if not requires_grad:\n",
        "            return CustomTensor(result_tensor)\n",
        "\n",
        "        graph = self.graph if self._custom_requires_grad else other.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "\n",
        "        self_ref = weakref.proxy(self)\n",
        "        other_ref = weakref.proxy(other)\n",
        "\n",
        "        if self._custom_requires_grad:\n",
        "            graph.add_edge(self._node_id, result._node_id)\n",
        "        if other._custom_requires_grad:\n",
        "            graph.add_edge(other._node_id, result._node_id)\n",
        "\n",
        "        def _backward():\n",
        "            if self._custom_requires_grad:\n",
        "                if self_ref.tensor.grad is None:\n",
        "                    self_ref._zero_grad()\n",
        "                self_ref.tensor.grad.add_(result.tensor.grad)\n",
        "            if other._custom_requires_grad:\n",
        "                if other_ref.tensor.grad is None:\n",
        "                    other_ref._zero_grad()\n",
        "                other_ref.tensor.grad.add_(result.tensor.grad)\n",
        "\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "\n",
        "    def __del__(self):\n",
        "        print(f\"CustomTensor with id={id(self)} is being garbage collected\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmpTbqadYn47"
      },
      "source": [
        "# working build 7th july"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QYdGgt65Yo1t"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'base (Python 3.13.2)' requires the ipykernel package.\n",
            "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
            "\u001b[1;31mOr install 'ipykernel' using the command: 'conda install -n base ipykernel --update-deps --force-reinstall'"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import weakref\n",
        "import numbers\n",
        "import rustworkx as rx\n",
        "import pytest\n",
        "\n",
        "# Original code (assuming it's in a file named 'autograd_system.py' or similar)\n",
        "# For the purpose of this test code, I'll include it directly.\n",
        "\n",
        "class Operations:\n",
        "    @torch.jit.script\n",
        "    def add_tensor_and_scalar(tensor: torch.Tensor, scalar: float) -> torch.Tensor:\n",
        "        return tensor + scalar\n",
        "\n",
        "    @torch.jit.script\n",
        "    def add_tensor_and_tensor(tensor1: torch.Tensor, tensor2: torch.Tensor) -> torch.Tensor:\n",
        "        return tensor1 + tensor2\n",
        "\n",
        "    @torch.jit.script\n",
        "    def mul_tensor_and_scaler(tensor: torch.Tensor, scaler: float) -> torch.Tensor:\n",
        "        return tensor * scaler\n",
        "\n",
        "    @torch.jit.script\n",
        "    def mul_tensor_and_tensor(tensor1: torch.Tensor, tensor2: torch.Tensor) -> torch.Tensor:\n",
        "        return tensor1 * tensor2\n",
        "\n",
        "    @torch.jit.script\n",
        "    def sub_tensor_and_scalar(tensor: torch.Tensor, scalar: float) -> torch.Tensor:\n",
        "        return tensor - scalar\n",
        "\n",
        "    @torch.jit.script\n",
        "    def sub_tensor_and_tensor(tensor1: torch.Tensor, tensor2: torch.Tensor) -> torch.Tensor:\n",
        "        return tensor1 - tensor2\n",
        "\n",
        "    @torch.jit.script\n",
        "    def div_tensor_and_scalar(tensor: torch.Tensor, scalar: float) -> torch.Tensor:\n",
        "        return tensor / scalar\n",
        "\n",
        "    @torch.jit.script\n",
        "    def pow_tensor_and_scalar(tensor: torch.Tensor, scalar: float) -> torch.Tensor:\n",
        "        return tensor.pow(scalar)\n",
        "\n",
        "    @torch.jit.script\n",
        "    def exp_tensor(tensor: torch.Tensor) -> torch.Tensor:\n",
        "        return torch.exp(tensor)\n",
        "\n",
        "    @torch.jit.script\n",
        "    def log_tensor(tensor: torch.Tensor) -> torch.Tensor:\n",
        "        return torch.log(tensor)\n",
        "\n",
        "    @torch.jit.script\n",
        "    def sin_tensor(tensor: torch.Tensor) -> torch.Tensor:\n",
        "        return torch.sin(tensor)\n",
        "\n",
        "    @torch.jit.script\n",
        "    def cos_tensor(tensor: torch.Tensor) -> torch.Tensor:\n",
        "        return torch.cos(tensor)\n",
        "\n",
        "    @torch.jit.script\n",
        "    def dot_tensor_and_tensor(tensor1: torch.Tensor, tensor2: torch.Tensor) -> torch.Tensor:\n",
        "        return torch.dot(tensor1, tensor2)\n",
        "\n",
        "    @torch.jit.script\n",
        "    def apply_binary_mask(tensor: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
        "        return tensor * mask\n",
        "\n",
        "    @torch.jit.script\n",
        "    def sqrt_tensor(tensor: torch.Tensor) -> torch.Tensor:\n",
        "        return torch.sqrt(tensor)\n",
        "\n",
        "    @torch.jit.script\n",
        "    def matmul_tensor_and_tensor(tensor1: torch.Tensor, tensor2: torch.Tensor) -> torch.Tensor:\n",
        "        return torch.matmul(tensor1, tensor2)\n",
        "\n",
        "    def einsum(equation: str, *operands) -> torch.Tensor:\n",
        "        # torch.jit.script does not support variable arguments or string arguments well,\n",
        "        # so einsum cannot be scripted. Leave as a static method.\n",
        "        return torch.einsum(equation, *operands)\n",
        "\n",
        "class AutogradGraph:\n",
        "    def __init__(self, check_for_cycles=True, auto_cleanup=True):\n",
        "        self.graph = rx.PyDiGraph()\n",
        "        self.intermediate_tensors = {}\n",
        "        self._check_cycles = check_for_cycles\n",
        "        self._auto_cleanup = auto_cleanup\n",
        "\n",
        "    def __enter__(self):\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, exc_type, exc_value, traceback):\n",
        "        if self._check_cycles and not self.check_cycle():\n",
        "            raise RuntimeError(\"Cycle detected in autograd graph on context exit.\")\n",
        "        if self._auto_cleanup:\n",
        "            self.intermediate_tensors.clear()\n",
        "            self.graph.clear()\n",
        "\n",
        "    def add_tensor_graph(self, tensor, is_leaf):\n",
        "        if not tensor._custom_requires_grad:\n",
        "            raise ValueError(\"Tensor with requires_grad=False cannot be added to the graph.\")\n",
        "\n",
        "        ref = tensor if is_leaf else weakref.proxy(tensor)\n",
        "        tensor_index = self.graph.add_node(ref)\n",
        "        tensor._node_id = tensor_index\n",
        "\n",
        "    def add_non_leaf_tensor_reference(self, tensor):\n",
        "        if not tensor._custom_requires_grad:\n",
        "            raise ValueError(\"Tensor must require grad.\")\n",
        "\n",
        "        if tensor._node_id in self.intermediate_tensors:\n",
        "            raise ValueError(\"Tensor reference already exists in intermediate tensors.\")\n",
        "\n",
        "        self.intermediate_tensors[tensor._node_id] = tensor\n",
        "\n",
        "    def add_edge(self, node_from, node_to, weight=None):\n",
        "        if not all(isinstance(n, int) for n in (node_from, node_to)):\n",
        "            raise TypeError(\"Node indices must be integers.\")\n",
        "        if not self.graph.has_node(node_from) or not self.graph.has_node(node_to):\n",
        "            raise ValueError(\"Nodes must exist before adding edge.\")\n",
        "        self.graph.add_edge(node_from, node_to, weight)\n",
        "\n",
        "    def check_cycle(self):\n",
        "        return rx.is_directed_acyclic_graph(self.graph)\n",
        "\n",
        "    def reverse_toposort(self):\n",
        "        if not self.check_cycle():\n",
        "            raise RuntimeError(\"Cannot perform topological sort on cyclic graph.\")\n",
        "        # rustworkx.topological_sort already works on node indices and dependencies\n",
        "        # The result of rx.topological_sort is a list of node indices in topological order.\n",
        "        # We then retrieve the actual tensor objects using self.graph[n].\n",
        "        return [self.graph[n] for n in reversed(rx.topological_sort(self.graph))]\n",
        "\n",
        "    def delete_node(self, node_index):\n",
        "        if not isinstance(node_index, int):\n",
        "            raise TypeError(\"Node index must be an integer.\")\n",
        "        if not self.graph.has_node(node_index):\n",
        "            raise ValueError(\"Node does not exist.\")\n",
        "        self.graph.remove_node(node_index)\n",
        "\n",
        "    def delete_edge(self, node_from, node_to):\n",
        "        if not self.graph.has_edge(node_from, node_to):\n",
        "            raise ValueError(\"Edge does not exist.\")\n",
        "        self.graph.remove_edge(node_from, node_to)\n",
        "\n",
        "    def del_non_leaf_tensor_reference(self, tensor_node_id):\n",
        "        self.intermediate_tensors.pop(tensor_node_id, None)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"CustomAutogradGraph(nodes={self.graph.num_nodes()}, edges={self.graph.num_edges()})\"\n",
        "\n",
        "class CustomTensor:\n",
        "    __slots__ = ('tensor', '_node_id', '_custom_requires_grad', '_backward', 'graph', '__weakref__')\n",
        "\n",
        "    def __new__(cls, data, *, _custom_requires_grad=False, device=None, dtype=None, graph=None, due_to_operation=False, is_leaf=False):\n",
        "        if isinstance(data, CustomTensor):\n",
        "            return data  # Don't rewrap\n",
        "        return super().__new__(cls)\n",
        "\n",
        "    def __init__(self, data, *, _custom_requires_grad=False, device=None, dtype=None, graph=None, due_to_operation=False, is_leaf=False):\n",
        "        if isinstance(data, CustomTensor):\n",
        "            return\n",
        "\n",
        "        self.tensor = data if due_to_operation else torch.as_tensor(data, dtype=dtype, device=device)\n",
        "        self.tensor.requires_grad_(False)\n",
        "        self._custom_requires_grad = _custom_requires_grad\n",
        "        self._node_id = None\n",
        "        self._backward = lambda: None\n",
        "        self.graph = None\n",
        "\n",
        "        if _custom_requires_grad:\n",
        "            self._init_graph(graph, is_leaf)\n",
        "\n",
        "    def _init_graph(self, graph, is_leaf):\n",
        "        if graph is None:\n",
        "            raise ValueError(\"Graph must be provided if requires_grad is True.\")\n",
        "        if is_leaf:\n",
        "          self.graph = weakref.proxy(graph)\n",
        "        else:\n",
        "          self.graph = graph\n",
        "        graph.add_tensor_graph(self, is_leaf=is_leaf)\n",
        "        if not is_leaf:\n",
        "            graph.add_non_leaf_tensor_reference(self)\n",
        "\n",
        "    def _zero_grad(self):\n",
        "        self.tensor.grad = torch.zeros_like(self.tensor)\n",
        "\n",
        "    def __add__(self, other):\n",
        "        if isinstance(other, numbers.Number):\n",
        "            return self._binary_op_scalar(other, op=Operations.add_tensor_and_scalar)\n",
        "        elif isinstance(other, CustomTensor):\n",
        "            return self._binary_op_tensor(other, op=Operations.add_tensor_and_tensor)\n",
        "        return NotImplemented\n",
        "\n",
        "    def _binary_op_scalar(self, scalar, op):\n",
        "        result_tensor = op(self.tensor, scalar)\n",
        "        if not self._custom_requires_grad:\n",
        "            return CustomTensor(result_tensor)\n",
        "\n",
        "        graph = self.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "        graph.add_edge(self._node_id, result._node_id)\n",
        "\n",
        "        self_ref = weakref.proxy(self)\n",
        "        result_ref = weakref.proxy(result)\n",
        "        def _backward():\n",
        "            if self_ref.tensor.grad is None:\n",
        "                self_ref._zero_grad()\n",
        "            # print(f\"Backward for scalar add: result_grad={result.tensor.grad}, self_grad_before={self_ref.tensor.grad}\") # Debugging\n",
        "            self_ref.tensor.grad.add_(result_ref.tensor.grad)\n",
        "            # print(f\"Backward for scalar add: self_grad_after={self_ref.tensor.grad}\") # Debugging\n",
        "\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "\n",
        "    def _binary_op_tensor(self, other, op):\n",
        "        result_tensor = op(self.tensor, other.tensor)\n",
        "        requires_grad = self._custom_requires_grad or other._custom_requires_grad\n",
        "\n",
        "        if not requires_grad:\n",
        "            return CustomTensor(result_tensor)\n",
        "\n",
        "        # Graph selection logic - assuming operations happen within a single graph context\n",
        "        graph = None\n",
        "        if self._custom_requires_grad:\n",
        "            graph = self.graph\n",
        "        elif other._custom_requires_grad:\n",
        "            graph = other.graph\n",
        "        else:\n",
        "            # This case should ideally not be reached if requires_grad is True\n",
        "            # and at least one operand has requires_grad\n",
        "            pass # Or raise an error if graph is truly missing\n",
        "\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "\n",
        "        self_ref = weakref.proxy(self)\n",
        "        other_ref = weakref.proxy(other)\n",
        "\n",
        "        if self._custom_requires_grad:\n",
        "            graph.add_edge(self._node_id, result._node_id)\n",
        "        if other._custom_requires_grad:\n",
        "            graph.add_edge(other._node_id, result._node_id)\n",
        "        result_ref = weakref.proxy(result)\n",
        "        def _backward():\n",
        "            # print(f\"Backward for tensor add: result_grad={result.tensor.grad}\") # Debugging\n",
        "            if self._custom_requires_grad:\n",
        "                if self_ref.tensor.grad is None:\n",
        "                    self_ref._zero_grad()\n",
        "                self_ref.tensor.grad.add_(result_ref.tensor.grad)\n",
        "                # print(f\"  self_grad_after={self_ref.tensor.grad}\") # Debugging\n",
        "            if other._custom_requires_grad:\n",
        "                if other_ref.tensor.grad is None:\n",
        "                    other_ref._zero_grad()\n",
        "                other_ref.tensor.grad.add_(result_ref.tensor.grad)\n",
        "                # print(f\"  other_grad_after={other_ref.tensor.grad}\") # Debugging\n",
        "\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "    def __mul__(self,other):\n",
        "        if isinstance(other, numbers.Number):\n",
        "            return self._binary_op_scalar(other, op=Operations.add_tensor_and_scalar)\n",
        "        if isinstance(other, CustomTensor):\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def __del__(self):\n",
        "        # print(f\"CustomTensor with node_id={self._node_id} (id={id(self)}) is being garbage collected\") # Debugging\n",
        "        pass # Suppress print for cleaner test output\n",
        "\n",
        "\n",
        "# --- Test Code ---\n",
        "\n",
        "def run_backward(output_tensor: CustomTensor):\n",
        "    \"\"\"\n",
        "    Simulates the backward pass for the custom autograd system.\n",
        "    This function needs to be explicitly defined as it's not part of the original classes.\n",
        "    \"\"\"\n",
        "    if not output_tensor._custom_requires_grad:\n",
        "        raise RuntimeError(\"Output tensor does not require grad.\")\n",
        "    if output_tensor.graph is None:\n",
        "        raise RuntimeError(\"Output tensor is not part of a graph.\")\n",
        "\n",
        "    # Initialize gradient for the output tensor\n",
        "    output_tensor.tensor.grad = torch.ones_like(output_tensor.tensor)\n",
        "\n",
        "    # Perform backward pass using topological sort\n",
        "    nodes_to_process = output_tensor.graph.reverse_toposort()\n",
        "\n",
        "    # Create a strong reference to intermediate tensors needed for backward pass\n",
        "    # This simulates how a real autograd engine would keep track of them\n",
        "    # The graph context's intermediate_tensors dict already serves this purpose.\n",
        "\n",
        "    for tensor_node in nodes_to_process:\n",
        "        # Check if the weak proxy is still valid (tensor is alive)\n",
        "        if isinstance(tensor_node, weakref.ProxyTypes) and tensor_node.__slots__ is None:\n",
        "            # print(f\"Skipping dead proxy: {tensor_node}\") # Debugging\n",
        "            continue # Skip if the weak reference is dead\n",
        "\n",
        "        if tensor_node.tensor.grad is None and tensor_node is not output_tensor:\n",
        "            # This can happen if a tensor is part of the graph but its grad hasn't been set yet\n",
        "            # and it's not the root of the backward call. This typically means it's a leaf\n",
        "            # that wasn't used to compute the output or an intermediate that accumulated no grad.\n",
        "            # For simplicity in this test, we assume grads propagate.\n",
        "            # print(f\"Warning: Tensor node {tensor_node._node_id} has no grad before _backward call.\")\n",
        "            pass # A no-op for now. In a real system, you might want to handle this.\n",
        "\n",
        "        # Ensure that non-leaf tensors are still alive when their _backward is called\n",
        "        # The `intermediate_tensors` in `AutogradGraph` should keep them alive.\n",
        "        tensor_node._backward()\n",
        "\n",
        "    # Clean up intermediate tensors references after backward pass\n",
        "    # This would typically be handled by the graph context's exit, but\n",
        "    # if `_auto_cleanup` is False, you might need manual cleanup.\n",
        "    # Here, for testing GC, we'll let the context manager handle it.\n",
        "\n",
        "\n",
        "class TestCustomAutogradSystem:\n",
        "\n",
        "    def test_basic_add_scalar_grad(self):\n",
        "        with AutogradGraph() as graph:\n",
        "            a = CustomTensor(torch.tensor([2.0, 3.0]), _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            b = a + 5.0 # (a + 5)\n",
        "            c = b + 10.0 # (a + 5 + 10)\n",
        "\n",
        "            # Manually run backward pass\n",
        "            run_backward(c)\n",
        "\n",
        "            # Expected gradients:\n",
        "            # dC/dA = 1.0 (for each element)\n",
        "            assert torch.allclose(a.tensor.grad, torch.tensor([1.0, 1.0]))\n",
        "            assert b.tensor.grad is not None\n",
        "            assert torch.allclose(b.tensor.grad, torch.tensor([1.0, 1.0])) # dC/dB = 1.0\n",
        "\n",
        "            # Verify graph structure\n",
        "            assert graph.graph.num_nodes() == 3\n",
        "            assert graph.graph.num_edges() == 2\n",
        "            assert graph.graph.has_edge(a._node_id, b._node_id)\n",
        "            assert graph.graph.has_edge(b._node_id, c._node_id)\n",
        "            assert graph.check_cycle() is True\n",
        "\n",
        "    def test_basic_add_tensor_grad(self):\n",
        "        with AutogradGraph() as graph:\n",
        "            a = CustomTensor(torch.tensor([2.0, 3.0]), _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            b = CustomTensor(torch.tensor([1.0, 2.0]), _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            c = a + b # (a + b)\n",
        "            d = c + 5.0 # (a + b + 5)\n",
        "\n",
        "            run_backward(d)\n",
        "\n",
        "            # Expected gradients:\n",
        "            # dD/dA = 1.0\n",
        "            # dD/dB = 1.0\n",
        "            assert torch.allclose(a.tensor.grad, torch.tensor([1.0, 1.0]))\n",
        "            assert torch.allclose(b.tensor.grad, torch.tensor([1.0, 1.0]))\n",
        "\n",
        "            # Verify graph structure\n",
        "            assert graph.graph.num_nodes() == 4\n",
        "            assert graph.graph.num_edges() == 3\n",
        "            assert graph.graph.has_edge(a._node_id, c._node_id)\n",
        "            assert graph.graph.has_edge(b._node_id, c._node_id)\n",
        "            assert graph.graph.has_edge(c._node_id, d._node_id)\n",
        "            assert graph.check_cycle() is True\n",
        "\n",
        "    def test_mixed_requires_grad_tensor_add(self):\n",
        "        with AutogradGraph() as graph:\n",
        "            a = CustomTensor(torch.tensor([2.0, 3.0]), _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            b = CustomTensor(torch.tensor([1.0, 2.0]), _custom_requires_grad=False) # Does not require grad\n",
        "            c = a + b # c should require grad, b's grad should be None\n",
        "\n",
        "            run_backward(c)\n",
        "\n",
        "            assert torch.allclose(a.tensor.grad, torch.tensor([1.0, 1.0]))\n",
        "            assert b.tensor.grad is None # b should not have a grad\n",
        "            assert c._custom_requires_grad is True\n",
        "\n",
        "            # Verify graph structure\n",
        "            assert graph.graph.num_nodes() == 2 # Only a and c in the graph\n",
        "            assert graph.graph.num_edges() == 1\n",
        "            assert graph.graph.has_node(a._node_id)\n",
        "            assert graph.graph.has_node(c._node_id)\n",
        "            assert graph.graph.has_edge(a._node_id, c._node_id)\n",
        "            #assert not graph.graph.has_node(b._node_id) # b should not be in graph\n",
        "\n",
        "    def test_no_requires_grad(self):\n",
        "        with AutogradGraph() as graph: # Graph created, but no tensors with requires_grad=True added\n",
        "            a = CustomTensor(torch.tensor([1.0]))\n",
        "            b = CustomTensor(torch.tensor([2.0]))\n",
        "            c = a + b\n",
        "            d = c + 3.0\n",
        "\n",
        "            assert not a._custom_requires_grad\n",
        "            assert not b._custom_requires_grad\n",
        "            assert not c._custom_requires_grad\n",
        "            assert not d._custom_requires_grad\n",
        "            assert graph.graph.num_nodes() == 0 # Graph should remain empty\n",
        "            assert graph.graph.num_edges() == 0\n",
        "\n",
        "            with pytest.raises(RuntimeError, match=\"Output tensor does not require grad.\"):\n",
        "                run_backward(d)\n",
        "\n",
        "    def test_autograd_graph_context_manager(self):\n",
        "        graph = None\n",
        "        with AutogradGraph(check_for_cycles=True, auto_cleanup=True) as g:\n",
        "            graph = g\n",
        "            a = CustomTensor(torch.tensor([1.0]), _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            b = a + 1.0\n",
        "            assert graph.graph.num_nodes() == 2\n",
        "            assert graph.graph.num_edges() == 1\n",
        "            assert len(graph.intermediate_tensors) == 1 # b should be in intermediate_tensors\n",
        "\n",
        "        # After exiting the context, graph should be empty\n",
        "        assert graph.graph.num_nodes() == 0\n",
        "        assert graph.graph.num_edges() == 0\n",
        "        assert len(graph.intermediate_tensors) == 0\n",
        "\n",
        "    def test_cycle_detection(self):\n",
        "      try:\n",
        "        with AutogradGraph(check_for_cycles=True, auto_cleanup=False) as graph: # auto_cleanup=False to inspect after error\n",
        "            a = CustomTensor(torch.tensor([1.0]), _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            b = CustomTensor(torch.tensor([2.0]), _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "\n",
        "            # Manually create a cycle (a -> b -> a)\n",
        "            graph.add_edge(a._node_id, b._node_id)\n",
        "            graph.add_edge(b._node_id, a._node_id)\n",
        "      except RuntimeError as e:\n",
        "        print(f\"Raised the error of cycle detected as {e}\")\n",
        "            # with pytest.raises(RuntimeError, match=\"Cycle detected in autograd graph on context exit.\"):\n",
        "            #     pass # The __exit__ method will be called here\n",
        "\n",
        "    def test_no_circular_references_non_leaf_tensors_die(self):\n",
        "          # This test relies on the garbage collector. It's a heuristic test\n",
        "        # as Python's GC timing is not strictly deterministic.\n",
        "        # However, with weakrefs, it should work for non-leaf tensors.\n",
        "\n",
        "      print(\"\\n--- Starting GC Test: No Circular References (Part 1) ---\")\n",
        "\n",
        "      graph_ref = None\n",
        "      output_tensor_weak_ref = None\n",
        "      node_id_d = -1 # To store node_id before d is deleted\n",
        "\n",
        "      # BLOCK 1: Create graph and tensors\n",
        "      with AutogradGraph(auto_cleanup=False) as graph: # Keep graph for inspection\n",
        "          graph_ref = weakref.ref(graph)\n",
        "          a = CustomTensor(torch.tensor([1.0]), _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "          b = a + 1.0 # Intermediate tensor\n",
        "          c = b + 2.0 # Intermediate tensor\n",
        "          d = c + 3.0 # Output tensor (also intermediate from graph's perspective)\n",
        "\n",
        "          # Store weak reference to 'd' BEFORE its strong reference is potentially removed\n",
        "          output_tensor_weak_ref = weakref.ref(d)\n",
        "          node_id_d = d._node_id # Store node_id while d is alive\n",
        "\n",
        "          print(f\"Initial: d object: {d}\")\n",
        "          print(f\"Initial: d._node_id: {node_id_d}\")\n",
        "          print(f\"Initial: graph.intermediate_tensors keys: {list(graph.intermediate_tensors.keys())}\")\n",
        "          # The ref count for `d` object itself will be high here because it's in `graph.intermediate_tensors`,\n",
        "          # and held by variable `d`, and by the temporary ref in `getrefcount`.\n",
        "          print(f\"Initial: refcount of d (via output_tensor_weak_ref.test_ref): {sys.getrefcount(output_tensor_weak_ref()) if output_tensor_weak_ref() else 'N/A'}\")\n",
        "          assert len(graph.intermediate_tensors) == 3 # b, c, d should be in intermediate_tensors\n",
        "\n",
        "      # BLOCK 2: After exiting context manager (auto_cleanup=False)\n",
        "      print(\"\\n--- After exiting 'with' block (auto_cleanup=False) ---\")\n",
        "      # The 'graph' variable still holds a strong reference to the AutogradGraph instance.\n",
        "      # graph_ref() should return the graph object.\n",
        "      assert graph_ref() is not None, \"Graph object should still be alive.\"\n",
        "      assert len(graph_ref().intermediate_tensors) == 3, \"Intermediate tensors should still be referenced by the graph.\"\n",
        "      print(f\"After 'with' block: d object (via weakref): {output_tensor_weak_ref()}\")\n",
        "      print(f\"After 'with' block: refcount of d (via output_tensor_weak_ref.test_ref): {sys.getrefcount(output_tensor_weak_ref())}\")\n",
        "\n",
        "      # BLOCK 3: Remove strong reference 'd' from local scope\n",
        "      print(\"\\n--- Deleting 'd' variable ---\")\n",
        "      del d # Remove the local strong reference to the CustomTensor object.\n",
        "      gc.collect() # Force garbage collection\n",
        "\n",
        "      # Now, output_tensor_weak_ref() *still* shouldn't be None because `graph_ref().intermediate_tensors`\n",
        "      # holds the strong reference.\n",
        "      print(f\"After del d + gc.collect(): d object (via weakref): {output_tensor_weak_ref()}\")\n",
        "      # We expect this to *not* be None yet, and to still show a refcount reflecting intermediate_tensors.\n",
        "      assert output_tensor_weak_ref() is not None, \"d should still be alive due to intermediate_tensors.\"\n",
        "      current_d_refcount_after_del_d = sys.getrefcount(output_tensor_weak_ref()) if output_tensor_weak_ref() else 'N/A'\n",
        "      print(f\"After del d + gc.collect(): refcount of d: {current_d_refcount_after_del_d}\")\n",
        "      # Expected refcount should be 2: one from intermediate_tensors, one from getrefcount()\n",
        "      assert current_d_refcount_after_del_d == 2, f\"Expected refcount 2, got {current_d_refcount_after_del_d}\"\n",
        "\n",
        "      # BLOCK 4: Remove strong reference from intermediate_tensors\n",
        "      print(f\"\\n--- Deleting strong reference from graph.intermediate_tensors for node {node_id_d} ---\")\n",
        "      graph_ref().del_non_leaf_tensor_reference(node_id_d) # THIS IS THE CRUCIAL STEP\n",
        "      print(f\"After del_non_leaf_tensor_reference: graph.intermediate_tensors keys: {list(graph_ref().intermediate_tensors.keys())}\")\n",
        "      #gc.collect() # Force garbage collection again\n",
        "\n",
        "      # Now, with the last strong reference gone, 'd' should be garbage collected.\n",
        "      print(f\"After del_non_leaf_tensor_reference + gc.collect(): d object (via weakref): {output_tensor_weak_ref()}\")\n",
        "      # This is where your original assertion was. It *should* pass now.\n",
        "      assert output_tensor_weak_ref() is None, \"Output tensor (non-leaf) should be garbage collected after its strong reference is deleted from intermediate_tensors.\"\n",
        "      print(\"Assertion Passed: Output tensor (d) was garbage collected.\")\n",
        "\n",
        "      # BLOCK 5: Verify other intermediate tensors are collected when graph is cleared\n",
        "      print(\"\\n--- Starting GC Test: All Intermediate Tensors ---\")\n",
        "      intermediate_tensors_wrefs = []\n",
        "      # Create a new graph and new tensors to avoid interference from previous block\n",
        "      with AutogradGraph(auto_cleanup=False) as graph_new:\n",
        "          a_new = CustomTensor(torch.tensor([1.0]), _custom_requires_grad=True, graph=graph_new, is_leaf=True)\n",
        "          b_new = a_new + 1.0 # Intermediate\n",
        "          c_new = b_new + 2.0 # Intermediate\n",
        "          d_new = c_new + 3.0 # Intermediate (output of a chain)\n",
        "\n",
        "          # Store weak references to the intermediate tensors\n",
        "          intermediate_tensors_wrefs.append(weakref.ref(b_new))\n",
        "          intermediate_tensors_wrefs.append(weakref.ref(c_new))\n",
        "          intermediate_tensors_wrefs.append(weakref.ref(d_new))\n",
        "\n",
        "          # Verify they are initially alive\n",
        "          assert all(wref() is not None for wref in intermediate_tensors_wrefs)\n",
        "          assert len(graph_new.intermediate_tensors) == 3\n",
        "\n",
        "      print(f\"After 'with' block (new graph): graph_new object: {graph_new}\")\n",
        "      assert graph_new is not None, \"New graph object should still be alive after 'with' block.\"\n",
        "      assert len(graph_new.intermediate_tensors) == 3, \"New graph intermediate_tensors should still hold refs.\"\n",
        "\n",
        "      # Manually clear the intermediate_tensors dictionary and remove graph reference\n",
        "      print(\"\\n--- Manually clearing graph.intermediate_tensors and deleting graph ---\")\n",
        "      graph_new.intermediate_tensors.clear()\n",
        "      del graph_new # Remove the strong reference to the graph itself\n",
        "      del b_new , c_new , d_new # deleting the local variable strong references\n",
        "      #gc.collect()\n",
        "\n",
        "      # Now, all non-leaf tensors should be garbage collected\n",
        "      for i, wref in enumerate(intermediate_tensors_wrefs):\n",
        "          print(f\"Intermediate tensor {i} (via weakref): {wref()}\")\n",
        "          assert wref() is None, f\"Intermediate tensor {i} should be garbage collected after graph context and intermediate_tensors are cleared.\"\n",
        "      print(\"Assertion Passed: All intermediate tensors were garbage collected.\")\n",
        "\n",
        "    def test_topological_sort_order(self):\n",
        "        with AutogradGraph() as graph:\n",
        "            t1 = CustomTensor(torch.tensor([1.0]), _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            t2 = CustomTensor(torch.tensor([2.0]), _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            t3 = t1 + t2\n",
        "            t4 = t3 + 5.0\n",
        "            t5 = t2 + 10.0 # Another branch\n",
        "            t6 = t4 + t5\n",
        "\n",
        "            # The topological sort should produce an order where dependencies come before their dependents.\n",
        "            # Reversed topological sort should produce an order where outputs come before their inputs.\n",
        "            # Example expected order: t6, t4, t5, t3, t2, t1 (or variations respecting dependencies)\n",
        "            sorted_tensors = graph.reverse_toposort()\n",
        "\n",
        "            # Check if dependencies are respected in reverse order\n",
        "            # If A -> B, then B should appear before A in reverse topological sort.\n",
        "            # t6 depends on t4, t5. So t6 should be before t4 and t5.\n",
        "            # t4 depends on t3. So t4 should be before t3.\n",
        "            # t5 depends on t2. So t5 should be before t2.\n",
        "            # t3 depends on t1, t2. So t3 should be before t1 and t2.\n",
        "\n",
        "            # Simple check: The first element should be t6 (the ultimate output).\n",
        "            assert sorted_tensors[0] is t6\n",
        "\n",
        "            # Check positions:\n",
        "            pos = {t: i for i, t in enumerate(sorted_tensors)}\n",
        "\n",
        "            assert pos[t6] < pos[t4]\n",
        "            assert pos[t6] < pos[t5]\n",
        "            assert pos[t4] < pos[t3]\n",
        "            assert pos[t5] < pos[t2]\n",
        "            assert pos[t3] < pos[t1]\n",
        "            assert pos[t3] < pos[t2] # t3 also depends on t2\n",
        "\n",
        "            # Additional check: t2 is a dependency for both t3 and t5.\n",
        "            # In reverse topo sort, t3 and t5 must appear before t2.\n",
        "            assert pos[t3] < pos[t2]\n",
        "            assert pos[t5] < pos[t2]\n",
        "\n",
        "            # t1 is only a dependency for t3.\n",
        "            assert pos[t3] < pos[t1]\n",
        "\n",
        "            # Check if all 6 tensors are in the sorted list\n",
        "            assert len(sorted_tensors) == 6\n",
        "            assert set(sorted_tensors) == {t1, t2, t3, t4, t5, t6}\n",
        "\n",
        "# To run these tests, save the code as a Python file (e.g., `test_autograd.py`)\n",
        "# and run `pytest` from your terminal in the same directory.\n",
        "# `pip install pytest torch rustworkx` if you don't have them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 211,
      "metadata": {
        "id": "qlerSJMBZVyu"
      },
      "outputs": [],
      "source": [
        "k=TestCustomAutogradSystem()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 188,
      "metadata": {
        "id": "JqLizMhYZYFt"
      },
      "outputs": [],
      "source": [
        "k.test_basic_add_scalar_grad()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 189,
      "metadata": {
        "id": "-RzvJuk8ZcxQ"
      },
      "outputs": [],
      "source": [
        "k.test_basic_add_tensor_grad()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 190,
      "metadata": {
        "id": "FRSjwXxXdNRr"
      },
      "outputs": [],
      "source": [
        "k.test_mixed_requires_grad_tensor_add()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 191,
      "metadata": {
        "id": "NeI2AJpgdStt"
      },
      "outputs": [],
      "source": [
        "k.test_no_requires_grad()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 192,
      "metadata": {
        "id": "VDDJgK0xd2ob"
      },
      "outputs": [],
      "source": [
        "k.test_autograd_graph_context_manager()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 193,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "khP3jZxDd6hC",
        "outputId": "4041e6ae-f87d-4254-885f-cab08ed7ca13"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Raised the error of cycle detected as Cycle detected in autograd graph on context exit.\n"
          ]
        }
      ],
      "source": [
        "k.test_cycle_detection()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 194,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nfG1rNMXiCa9",
        "outputId": "54e6b624-fbd3-46d2-e6c5-b843bb30f6a2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "182"
            ]
          },
          "execution_count": 194,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gc\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 212,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EJQ9maoLeEGk",
        "outputId": "1b9de0eb-ce2a-4572-867d-bdf6d5354ee2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3\n",
            "d <__main__.CustomTensor object at 0x78a841a447c0>\n",
            "3\n",
            "2\n",
            "<weakref at 0x78a841a47420; to 'CustomTensor' at 0x78a841a447c0>\n",
            "3\n",
            "2\n",
            "Outut_Tensor_weak_ref <weakref at 0x78a841a47420; dead>\n",
            "\n",
            "Running advanced GC test for intermediate tensors...\n"
          ]
        }
      ],
      "source": [
        "k.test_no_circular_references_non_leaf_tensors_die()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 208,
      "metadata": {
        "id": "k0bgwgYpjLow"
      },
      "outputs": [],
      "source": [
        "def test_no_circular_references_non_leaf_tensors_die(self):\n",
        "          # This test relies on the garbage collector. It's a heuristic test\n",
        "        # as Python's GC timing is not strictly deterministic.\n",
        "        # However, with weakrefs, it should work for non-leaf tensors.\n",
        "\n",
        "    print(\"\\n--- Starting GC Test: No Circular References (Part 1) ---\")\n",
        "\n",
        "    graph_ref = None\n",
        "    output_tensor_weak_ref = None\n",
        "    node_id_d = -1 # To store node_id before d is deleted\n",
        "\n",
        "    # BLOCK 1: Create graph and tensors\n",
        "    with AutogradGraph(auto_cleanup=False) as graph: # Keep graph for inspection\n",
        "        graph_ref = weakref.ref(graph)\n",
        "        a = CustomTensor(torch.tensor([1.0]), _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "        b = a + 1.0 # Intermediate tensor\n",
        "        c = b + 2.0 # Intermediate tensor\n",
        "        d = c + 3.0 # Output tensor (also intermediate from graph's perspective)\n",
        "\n",
        "        # Store weak reference to 'd' BEFORE its strong reference is potentially removed\n",
        "        output_tensor_weak_ref = weakref.ref(d)\n",
        "        node_id_d = d._node_id # Store node_id while d is alive\n",
        "\n",
        "        print(f\"Initial: d object: {d}\")\n",
        "        print(f\"Initial: d._node_id: {node_id_d}\")\n",
        "        print(f\"Initial: graph.intermediate_tensors keys: {list(graph.intermediate_tensors.keys())}\")\n",
        "        # The ref count for `d` object itself will be high here because it's in `graph.intermediate_tensors`,\n",
        "        # and held by variable `d`, and by the temporary ref in `getrefcount`.\n",
        "        print(f\"Initial: refcount of d (via output_tensor_weak_ref.test_ref): {sys.getrefcount(output_tensor_weak_ref()) if output_tensor_weak_ref() else 'N/A'}\")\n",
        "        assert len(graph.intermediate_tensors) == 3 # b, c, d should be in intermediate_tensors\n",
        "\n",
        "    # BLOCK 2: After exiting context manager (auto_cleanup=False)\n",
        "    print(\"\\n--- After exiting 'with' block (auto_cleanup=False) ---\")\n",
        "    # The 'graph' variable still holds a strong reference to the AutogradGraph instance.\n",
        "    # graph_ref() should return the graph object.\n",
        "    assert graph_ref() is not None, \"Graph object should still be alive.\"\n",
        "    assert len(graph_ref().intermediate_tensors) == 3, \"Intermediate tensors should still be referenced by the graph.\"\n",
        "    print(f\"After 'with' block: d object (via weakref): {output_tensor_weak_ref()}\")\n",
        "    print(f\"After 'with' block: refcount of d (via output_tensor_weak_ref.test_ref): {sys.getrefcount(output_tensor_weak_ref())}\")\n",
        "\n",
        "    # BLOCK 3: Remove strong reference 'd' from local scope\n",
        "    print(\"\\n--- Deleting 'd' variable ---\")\n",
        "    del d # Remove the local strong reference to the CustomTensor object.\n",
        "    gc.collect() # Force garbage collection\n",
        "\n",
        "    # Now, output_tensor_weak_ref() *still* shouldn't be None because `graph_ref().intermediate_tensors`\n",
        "    # holds the strong reference.\n",
        "    print(f\"After del d + gc.collect(): d object (via weakref): {output_tensor_weak_ref()}\")\n",
        "    # We expect this to *not* be None yet, and to still show a refcount reflecting intermediate_tensors.\n",
        "    assert output_tensor_weak_ref() is not None, \"d should still be alive due to intermediate_tensors.\"\n",
        "    current_d_refcount_after_del_d = sys.getrefcount(output_tensor_weak_ref()) if output_tensor_weak_ref() else 'N/A'\n",
        "    print(f\"After del d + gc.collect(): refcount of d: {current_d_refcount_after_del_d}\")\n",
        "    # Expected refcount should be 2: one from intermediate_tensors, one from getrefcount()\n",
        "    assert current_d_refcount_after_del_d == 2, f\"Expected refcount 2, got {current_d_refcount_after_del_d}\"\n",
        "\n",
        "    # BLOCK 4: Remove strong reference from intermediate_tensors\n",
        "    print(f\"\\n--- Deleting strong reference from graph.intermediate_tensors for node {node_id_d} ---\")\n",
        "    graph_ref().del_non_leaf_tensor_reference(node_id_d) # THIS IS THE CRUCIAL STEP\n",
        "    print(f\"After del_non_leaf_tensor_reference: graph.intermediate_tensors keys: {list(graph_ref().intermediate_tensors.keys())}\")\n",
        "    #gc.collect() # Force garbage collection again\n",
        "\n",
        "    # Now, with the last strong reference gone, 'd' should be garbage collected.\n",
        "    print(f\"After del_non_leaf_tensor_reference + gc.collect(): d object (via weakref): {output_tensor_weak_ref()}\")\n",
        "    # This is where your original assertion was. It *should* pass now.\n",
        "    assert output_tensor_weak_ref() is None, \"Output tensor (non-leaf) should be garbage collected after its strong reference is deleted from intermediate_tensors.\"\n",
        "    print(\"Assertion Passed: Output tensor (d) was garbage collected.\")\n",
        "\n",
        "    # BLOCK 5: Verify other intermediate tensors are collected when graph is cleared\n",
        "    print(\"\\n--- Starting GC Test: All Intermediate Tensors ---\")\n",
        "    intermediate_tensors_wrefs = []\n",
        "    # Create a new graph and new tensors to avoid interference from previous block\n",
        "    with AutogradGraph(auto_cleanup=False) as graph_new:\n",
        "        a_new = CustomTensor(torch.tensor([1.0]), _custom_requires_grad=True, graph=graph_new, is_leaf=True)\n",
        "        b_new = a_new + 1.0 # Intermediate\n",
        "        c_new = b_new + 2.0 # Intermediate\n",
        "        d_new = c_new + 3.0 # Intermediate (output of a chain)\n",
        "\n",
        "        # Store weak references to the intermediate tensors\n",
        "        intermediate_tensors_wrefs.append(weakref.ref(b_new))\n",
        "        intermediate_tensors_wrefs.append(weakref.ref(c_new))\n",
        "        intermediate_tensors_wrefs.append(weakref.ref(d_new))\n",
        "\n",
        "        # Verify they are initially alive\n",
        "        assert all(wref() is not None for wref in intermediate_tensors_wrefs)\n",
        "        assert len(graph_new.intermediate_tensors) == 3\n",
        "\n",
        "    print(f\"After 'with' block (new graph): graph_new object: {graph_new}\")\n",
        "    assert graph_new is not None, \"New graph object should still be alive after 'with' block.\"\n",
        "    assert len(graph_new.intermediate_tensors) == 3, \"New graph intermediate_tensors should still hold refs.\"\n",
        "\n",
        "    # Manually clear the intermediate_tensors dictionary and remove graph reference\n",
        "    print(\"\\n--- Manually clearing graph.intermediate_tensors and deleting graph ---\")\n",
        "    graph_new.intermediate_tensors.clear()\n",
        "    del graph_new # Remove the strong reference to the graph itself\n",
        "    del b_new , c_new , d_new # deleting the local variable strong references\n",
        "    #gc.collect()\n",
        "\n",
        "    # Now, all non-leaf tensors should be garbage collected\n",
        "    for i, wref in enumerate(intermediate_tensors_wrefs):\n",
        "        print(f\"Intermediate tensor {i} (via weakref): {wref()}\")\n",
        "        assert wref() is None, f\"Intermediate tensor {i} should be garbage collected after graph context and intermediate_tensors are cleared.\"\n",
        "    print(\"Assertion Passed: All intermediate tensors were garbage collected.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 209,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ee8MD9T3jWsG",
        "outputId": "ff13f91f-728d-4181-9e7f-6e8e3db70690"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Starting GC Test: No Circular References (Part 1) ---\n",
            "Initial: d object: <__main__.CustomTensor object at 0x78a841a2f4c0>\n",
            "Initial: d._node_id: 3\n",
            "Initial: graph.intermediate_tensors keys: [1, 2, 3]\n",
            "Initial: refcount of d (via output_tensor_weak_ref.test_ref): 3\n",
            "\n",
            "--- After exiting 'with' block (auto_cleanup=False) ---\n",
            "After 'with' block: d object (via weakref): <__main__.CustomTensor object at 0x78a841a2f4c0>\n",
            "After 'with' block: refcount of d (via output_tensor_weak_ref.test_ref): 3\n",
            "\n",
            "--- Deleting 'd' variable ---\n",
            "After del d + gc.collect(): d object (via weakref): <__main__.CustomTensor object at 0x78a841a2f4c0>\n",
            "After del d + gc.collect(): refcount of d: 2\n",
            "\n",
            "--- Deleting strong reference from graph.intermediate_tensors for node 3 ---\n",
            "After del_non_leaf_tensor_reference: graph.intermediate_tensors keys: [1, 2]\n",
            "After del_non_leaf_tensor_reference + gc.collect(): d object (via weakref): None\n",
            "Assertion Passed: Output tensor (d) was garbage collected.\n",
            "\n",
            "--- Starting GC Test: All Intermediate Tensors ---\n",
            "After 'with' block (new graph): graph_new object: CustomAutogradGraph(nodes=4, edges=3)\n",
            "\n",
            "--- Manually clearing graph.intermediate_tensors and deleting graph ---\n",
            "Intermediate tensor 0 (via weakref): None\n",
            "Intermediate tensor 1 (via weakref): None\n",
            "Intermediate tensor 2 (via weakref): None\n",
            "Assertion Passed: All intermediate tensors were garbage collected.\n"
          ]
        }
      ],
      "source": [
        "test_no_circular_references_non_leaf_tensors_die()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# new "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w638ekDnjXKm"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'base (Python 3.13.2)' requires the ipykernel package.\n",
            "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
            "\u001b[1;31mOr install 'ipykernel' using the command: 'conda install -n base ipykernel --update-deps --force-reinstall'"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import weakref\n",
        "import numbers\n",
        "import rustworkx as rx\n",
        "import pytest\n",
        "\n",
        "# Original code (assuming it's in a file named 'autograd_system.py' or similar)\n",
        "# For the purpose of this test code, I'll include it directly.\n",
        "\n",
        "# class Operations:\n",
        "#     @torch.jit.script\n",
        "#     def add_tensor_and_scalar(tensor: torch.Tensor, scalar: float) -> torch.Tensor:\n",
        "#         return tensor + scalar\n",
        "\n",
        "#     @torch.jit.script\n",
        "#     def add_tensor_and_tensor(tensor1: torch.Tensor, tensor2: torch.Tensor) -> torch.Tensor:\n",
        "#         return tensor1 + tensor2\n",
        "\n",
        "#     @torch.jit.script\n",
        "#     def mul_tensor_and_scaler(tensor: torch.Tensor, scaler: float) -> torch.Tensor:\n",
        "#         return tensor * scaler\n",
        "\n",
        "#     @torch.jit.script\n",
        "#     def mul_tensor_and_tensor(tensor1: torch.Tensor, tensor2: torch.Tensor) -> torch.Tensor:\n",
        "#         return tensor1 * tensor2\n",
        "\n",
        "#     @torch.jit.script\n",
        "#     def sub_tensor_and_scalar(tensor: torch.Tensor, scalar: float) -> torch.Tensor:\n",
        "#         return tensor - scalar\n",
        "\n",
        "#     @torch.jit.script\n",
        "#     def sub_tensor_and_tensor(tensor1: torch.Tensor, tensor2: torch.Tensor) -> torch.Tensor:\n",
        "#         return tensor1 - tensor2\n",
        "\n",
        "#     @torch.jit.script\n",
        "#     def div_tensor_and_scalar(tensor: torch.Tensor, scalar: float) -> torch.Tensor:\n",
        "#         return tensor / scalar\n",
        "\n",
        "#     @torch.jit.script\n",
        "#     def pow_tensor_and_scalar(tensor: torch.Tensor, scalar: float) -> torch.Tensor:\n",
        "#         return tensor.pow(scalar)\n",
        "\n",
        "#     @torch.jit.script\n",
        "#     def exp_tensor(tensor: torch.Tensor) -> torch.Tensor:\n",
        "#         return torch.exp(tensor)\n",
        "\n",
        "#     @torch.jit.script\n",
        "#     def log_tensor(tensor: torch.Tensor) -> torch.Tensor:\n",
        "#         return torch.log(tensor)\n",
        "\n",
        "#     @torch.jit.script\n",
        "#     def sin_tensor(tensor: torch.Tensor) -> torch.Tensor:\n",
        "#         return torch.sin(tensor)\n",
        "\n",
        "#     @torch.jit.script\n",
        "#     def cos_tensor(tensor: torch.Tensor) -> torch.Tensor:\n",
        "#         return torch.cos(tensor)\n",
        "\n",
        "#     @torch.jit.script\n",
        "#     def dot_tensor_and_tensor(tensor1: torch.Tensor, tensor2: torch.Tensor) -> torch.Tensor:\n",
        "#         return torch.dot(tensor1, tensor2)\n",
        "\n",
        "#     @torch.jit.script\n",
        "#     def apply_binary_mask(tensor: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
        "#         return tensor * mask\n",
        "\n",
        "#     @torch.jit.script\n",
        "#     def sqrt_tensor(tensor: torch.Tensor) -> torch.Tensor:\n",
        "#         return torch.sqrt(tensor)\n",
        "\n",
        "#     @torch.jit.script\n",
        "#     def matmul_tensor_and_tensor(tensor1: torch.Tensor, tensor2: torch.Tensor) -> torch.Tensor:\n",
        "#         return torch.matmul(tensor1, tensor2)\n",
        "\n",
        "#     def einsum(equation: str, *operands) -> torch.Tensor:\n",
        "#         # torch.jit.script does not support variable arguments or string arguments well,\n",
        "#         # so einsum cannot be scripted. Leave as a static method.\n",
        "#         return torch.einsum(equation, *operands)\n",
        "\n",
        "class AutogradGraph:\n",
        "    def __init__(self, check_for_cycles=True, auto_cleanup=True):\n",
        "        self.graph = rx.PyDiGraph()\n",
        "        self.intermediate_tensors = {}\n",
        "        self._check_cycles = check_for_cycles\n",
        "        self._auto_cleanup = auto_cleanup\n",
        "\n",
        "    def __enter__(self):\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, exc_type, exc_value, traceback):\n",
        "        if self._check_cycles and not self.check_cycle():\n",
        "            raise RuntimeError(\"Cycle detected in autograd graph on context exit.\")\n",
        "        if self._auto_cleanup:\n",
        "            self.intermediate_tensors.clear()\n",
        "            self.graph.clear()\n",
        "\n",
        "    def add_tensor_graph(self, tensor, is_leaf):\n",
        "        if not tensor._custom_requires_grad:\n",
        "            raise ValueError(\"Tensor with requires_grad=False cannot be added to the graph.\")\n",
        "\n",
        "        ref = tensor if is_leaf else weakref.proxy(tensor)\n",
        "        tensor_index = self.graph.add_node(ref)\n",
        "        tensor._node_id = tensor_index\n",
        "\n",
        "    def add_non_leaf_tensor_reference(self, tensor):\n",
        "        if not tensor._custom_requires_grad:\n",
        "            raise ValueError(\"Tensor must require grad.\")\n",
        "\n",
        "        if tensor._node_id in self.intermediate_tensors:\n",
        "            raise ValueError(\"Tensor reference already exists in intermediate tensors.\")\n",
        "\n",
        "        self.intermediate_tensors[tensor._node_id] = tensor\n",
        "\n",
        "    def add_edge(self, node_from, node_to, weight=None):\n",
        "        if not all(isinstance(n, int) for n in (node_from, node_to)):\n",
        "            raise TypeError(\"Node indices must be integers.\")\n",
        "        if not self.graph.has_node(node_from) or not self.graph.has_node(node_to):\n",
        "            raise ValueError(\"Nodes must exist before adding edge.\")\n",
        "        self.graph.add_edge(node_from, node_to, weight)\n",
        "\n",
        "    def check_cycle(self):\n",
        "        return rx.is_directed_acyclic_graph(self.graph)\n",
        "\n",
        "    def reverse_toposort(self):\n",
        "        if not self.check_cycle():\n",
        "            raise RuntimeError(\"Cannot perform topological sort on a cyclic graph.\")\n",
        "        # rustworkx.topological_sort already works on node indices and dependencies\n",
        "        # The result of rx.topological_sort is a list of node indices in topological order.\n",
        "        # We then retrieve the actual tensor objects using self.graph[n].\n",
        "        return [self.graph[n] for n in reversed(rx.topological_sort(self.graph))]\n",
        "\n",
        "    def delete_node(self, node_index):\n",
        "        if not isinstance(node_index, int):\n",
        "            raise TypeError(\"Node index must be an integer.\")\n",
        "        if not self.graph.has_node(node_index):\n",
        "            raise ValueError(\"Node does not exist.\")\n",
        "        self.graph.remove_node(node_index)\n",
        "\n",
        "    def delete_edge(self, node_from, node_to):\n",
        "        if not self.graph.has_edge(node_from, node_to):\n",
        "            raise ValueError(\"Edge does not exist.\")\n",
        "        self.graph.remove_edge(node_from, node_to)\n",
        "\n",
        "    def del_non_leaf_tensor_reference(self, tensor_node_id):\n",
        "        self.intermediate_tensors.pop(tensor_node_id, None)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"CustomAutogradGraph(nodes={self.graph.num_nodes()}, edges={self.graph.num_edges()})\"\n",
        "\n",
        "class CustomTensor:\n",
        "    __slots__ = ('tensor', '_node_id', '_custom_requires_grad', '_backward', 'graph', '__weakref__')\n",
        "\n",
        "    def __new__(cls, data, *, _custom_requires_grad=False, device=None, dtype=None, graph=None, due_to_operation=False, is_leaf=False):\n",
        "        if isinstance(data, CustomTensor):\n",
        "            return data  # Don't rewrap\n",
        "        return super().__new__(cls)\n",
        "\n",
        "    def __init__(self, data, *, _custom_requires_grad=False, device=None, dtype=None, graph=None, due_to_operation=False, is_leaf=False):\n",
        "        if isinstance(data, CustomTensor):\n",
        "            return\n",
        "\n",
        "        self.tensor = data if due_to_operation else torch.as_tensor(data, dtype=dtype, device=device)\n",
        "        self.tensor.requires_grad_(False)\n",
        "        self._custom_requires_grad = _custom_requires_grad\n",
        "        self._node_id = None\n",
        "        self._backward = lambda: None\n",
        "        self.graph = None\n",
        "\n",
        "        if _custom_requires_grad:\n",
        "            self._init_graph(graph, is_leaf)\n",
        "\n",
        "    def _init_graph(self, graph, is_leaf):\n",
        "        if graph is None:\n",
        "            raise ValueError(\"Graph must be provided if requires_grad is True.\")\n",
        "        if is_leaf:\n",
        "          self.graph = weakref.proxy(graph)\n",
        "        else:\n",
        "          self.graph = graph\n",
        "        graph.add_tensor_graph(self, is_leaf=is_leaf)\n",
        "        if not is_leaf:\n",
        "            graph.add_non_leaf_tensor_reference(self)\n",
        "\n",
        "    def _zero_grad(self):\n",
        "        self.tensor.grad = torch.zeros_like(self.tensor)\n",
        "\n",
        "    def __add__(self, other):\n",
        "        if isinstance(other, numbers.Number):\n",
        "            return self._add_scalar(other)#, op=torch.add)#Operations.add_tensor_and_scalar)\n",
        "        elif isinstance(other, CustomTensor):\n",
        "            return self._add_tensor(other)#, op=torch.add)#Operations.add_tensor_and_tensor)\n",
        "        return NotImplemented\n",
        "\n",
        "    def _add_scalar(self, scalar):\n",
        "        result_tensor = torch.add(self.tensor, scalar)\n",
        "        if not self._custom_requires_grad:\n",
        "            return CustomTensor(result_tensor)\n",
        "\n",
        "        graph = self.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "        graph.add_edge(self._node_id, result._node_id)\n",
        "\n",
        "        self_ref = weakref.proxy(self)\n",
        "        result_ref = weakref.proxy(result)\n",
        "        def _backward():\n",
        "            if self_ref.tensor.grad is None:\n",
        "                self_ref._zero_grad()\n",
        "            # print(f\"Backward for scalar add: result_grad={result.tensor.grad}, self_grad_before={self_ref.tensor.grad}\") # Debugging\n",
        "            self_ref.tensor.grad.add_(result_ref.tensor.grad)\n",
        "            # print(f\"Backward for scalar add: self_grad_after={self_ref.tensor.grad}\") # Debugging\n",
        "\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "\n",
        "    def _add_tensor(self, other):\n",
        "        result_tensor = torch.add(self.tensor, other.tensor)\n",
        "        requires_grad = self._custom_requires_grad or other._custom_requires_grad\n",
        "\n",
        "        if not requires_grad:\n",
        "            return CustomTensor(result_tensor)\n",
        "\n",
        "        # Graph selection logic - assuming operations happen within a single graph context\n",
        "        graph = None\n",
        "        if self._custom_requires_grad:\n",
        "            graph = self.graph\n",
        "        elif other._custom_requires_grad:\n",
        "            graph = other.graph\n",
        "        else:\n",
        "            # This case should ideally not be reached if requires_grad is True\n",
        "            # and at least one operand has requires_grad\n",
        "            pass # Or raise an error if graph is truly missing\n",
        "\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "\n",
        "        self_ref = weakref.proxy(self)\n",
        "        other_ref = weakref.proxy(other)\n",
        "\n",
        "        if self._custom_requires_grad:\n",
        "            graph.add_edge(self._node_id, result._node_id)\n",
        "        if other._custom_requires_grad:\n",
        "            graph.add_edge(other._node_id, result._node_id)\n",
        "        result_ref = weakref.proxy(result)\n",
        "        def _backward():\n",
        "            # print(f\"Backward for tensor add: result_grad={result.tensor.grad}\") # Debugging\n",
        "            if self._custom_requires_grad:\n",
        "                if self_ref.tensor.grad is None:\n",
        "                    self_ref._zero_grad()\n",
        "                self_ref.tensor.grad.add_(result_ref.tensor.grad)\n",
        "                # print(f\"  self_grad_after={self_ref.tensor.grad}\") # Debugging\n",
        "            if other._custom_requires_grad:\n",
        "                if other_ref.tensor.grad is None:\n",
        "                    other_ref._zero_grad()\n",
        "                other_ref.tensor.grad.add_(result_ref.tensor.grad)\n",
        "                # print(f\"  other_grad_after={other_ref.tensor.grad}\") # Debugging\n",
        "\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "    def __mul__(self, other):\n",
        "        if isinstance(other, numbers.Number):\n",
        "            return self._mul_scalar(other)\n",
        "        elif isinstance(other, CustomTensor):\n",
        "            return self._mul_tensor(other)\n",
        "        return NotImplemented\n",
        "\n",
        "    def _mul_scalar(self, scalar):\n",
        "        result_tensor = torch.mul(self.tensor, scalar)\n",
        "        if not self._custom_requires_grad:\n",
        "            return CustomTensor(result_tensor)\n",
        "\n",
        "        graph = self.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "        graph.add_edge(self._node_id, result._node_id)\n",
        "\n",
        "        self_ref = weakref.proxy(self)\n",
        "        result_ref = weakref.proxy(result)\n",
        "        def _backward():\n",
        "            if self_ref.tensor.grad is None:\n",
        "                self_ref._zero_grad()\n",
        "            self_ref.tensor.grad.add_(result_ref.tensor.grad * scalar)\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "\n",
        "    def _mul_tensor(self, other):\n",
        "        result_tensor = torch.mul(self.tensor, other.tensor)\n",
        "        requires_grad = self._custom_requires_grad or other._custom_requires_grad\n",
        "        if not requires_grad:\n",
        "            return CustomTensor(result_tensor)\n",
        "\n",
        "        graph = self.graph if self._custom_requires_grad else other.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "\n",
        "        self_ref = weakref.proxy(self)\n",
        "        other_ref = weakref.proxy(other)\n",
        "        result_ref = weakref.proxy(result)\n",
        "\n",
        "        if self._custom_requires_grad:\n",
        "            graph.add_edge(self._node_id, result._node_id)\n",
        "        if other._custom_requires_grad:\n",
        "            graph.add_edge(other._node_id, result._node_id)\n",
        "\n",
        "        def _backward():\n",
        "            if self._custom_requires_grad:\n",
        "                if self_ref.tensor.grad is None:\n",
        "                    self_ref._zero_grad()\n",
        "                self_ref.tensor.grad.add_(result_ref.tensor.grad * other_ref.tensor)\n",
        "            if other._custom_requires_grad:\n",
        "                if other_ref.tensor.grad is None:\n",
        "                    other_ref._zero_grad()\n",
        "                other_ref.tensor.grad.add_(result_ref.tensor.grad * self_ref.tensor)\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "\n",
        "\n",
        "    def __sub__(self, other):\n",
        "        if isinstance(other, numbers.Number):\n",
        "            return self._sub_scalar(other)\n",
        "        elif isinstance(other, CustomTensor):\n",
        "            return self._sub_tensor(other)\n",
        "        return NotImplemented\n",
        "\n",
        "    def _sub_scalar(self, scalar):\n",
        "        result_tensor = torch.sub(self.tensor, scalar)\n",
        "        if not self._custom_requires_grad:\n",
        "            return CustomTensor(result_tensor)\n",
        "\n",
        "        graph = self.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "        graph.add_edge(self._node_id, result._node_id)\n",
        "\n",
        "        self_ref = weakref.proxy(self)\n",
        "        result_ref = weakref.proxy(result)\n",
        "        def _backward():\n",
        "            if self_ref.tensor.grad is None:\n",
        "                self_ref._zero_grad()\n",
        "            self_ref.tensor.grad.add_(result_ref.tensor.grad)\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "\n",
        "    def _sub_tensor(self, other):\n",
        "        result_tensor = torch.sub(self.tensor, other.tensor)\n",
        "        requires_grad = self._custom_requires_grad or other._custom_requires_grad\n",
        "        if not requires_grad:\n",
        "            return CustomTensor(result_tensor)\n",
        "\n",
        "        graph = self.graph if self._custom_requires_grad else other.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "\n",
        "        self_ref = weakref.proxy(self)\n",
        "        other_ref = weakref.proxy(other)\n",
        "        result_ref = weakref.proxy(result)\n",
        "\n",
        "        if self._custom_requires_grad:\n",
        "            graph.add_edge(self._node_id, result._node_id)\n",
        "        if other._custom_requires_grad:\n",
        "            graph.add_edge(other._node_id, result._node_id)\n",
        "\n",
        "        def _backward():\n",
        "            if self._custom_requires_grad:\n",
        "                if self_ref.tensor.grad is None:\n",
        "                    self_ref._zero_grad()\n",
        "                self_ref.tensor.grad.add_(result_ref.tensor.grad)\n",
        "            if other._custom_requires_grad:\n",
        "                if other_ref.tensor.grad is None:\n",
        "                    other_ref._zero_grad()\n",
        "                other_ref.tensor.grad.sub_(result_ref.tensor.grad)\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "    \n",
        "    def __truediv__(self, scalar):\n",
        "        return self._div_scalar(scalar)\n",
        "\n",
        "    def _div_scalar(self, scalar):\n",
        "        result_tensor = torch.div(self.tensor, scalar)\n",
        "        if not self._custom_requires_grad:\n",
        "            return CustomTensor(result_tensor)\n",
        "\n",
        "        graph = self.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "        graph.add_edge(self._node_id, result._node_id)\n",
        "\n",
        "        self_ref = weakref.proxy(self)\n",
        "        result_ref = weakref.proxy(result)\n",
        "        def _backward():\n",
        "            if self_ref.tensor.grad is None:\n",
        "                self_ref._zero_grad()\n",
        "            self_ref.tensor.grad.add_(result_ref.tensor.grad / scalar)\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "\n",
        "\n",
        "    def pow(self, scalar):\n",
        "        result_tensor = torch.pow(self.tensor, scalar)\n",
        "        if not self._custom_requires_grad:\n",
        "            return CustomTensor(result_tensor)\n",
        "\n",
        "        graph = self.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "        graph.add_edge(self._node_id, result._node_id)\n",
        "\n",
        "        self_ref = weakref.proxy(self)\n",
        "        result_ref = weakref.proxy(result)\n",
        "        def _backward():\n",
        "            if self_ref.tensor.grad is None:\n",
        "                self_ref._zero_grad()\n",
        "            grad_contrib = scalar * self_ref.tensor.pow(scalar - 1)\n",
        "            self_ref.tensor.grad.add_(result_ref.tensor.grad * grad_contrib)\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "    \n",
        "    def exp(self):\n",
        "        out = torch.exp(self.tensor)\n",
        "        return self._unary_op(out, lambda grad, out_tensor: grad * out_tensor)\n",
        "\n",
        "    def log(self):\n",
        "        out = torch.log(self.tensor)\n",
        "        return self._unary_op(out, lambda grad, input_tensor: grad / input_tensor)\n",
        "\n",
        "    def sin(self):\n",
        "        out = torch.sin(self.tensor)\n",
        "        return self._unary_op(out, lambda grad, input_tensor: grad * torch.cos(input_tensor))\n",
        "\n",
        "    def cos(self):\n",
        "        out = torch.cos(self.tensor)\n",
        "        return self._unary_op(out, lambda grad, input_tensor: -grad * torch.sin(input_tensor))\n",
        "\n",
        "    def sqrt(self):\n",
        "        out = torch.sqrt(self.tensor)\n",
        "        return self._unary_op(out, lambda grad, out_tensor: grad * 0.5 / out_tensor)\n",
        "\n",
        "    def _unary_op(self, result_tensor, backward_fn):\n",
        "        if not self._custom_requires_grad:\n",
        "            return CustomTensor(result_tensor)\n",
        "\n",
        "        graph = self.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "        graph.add_edge(self._node_id, result._node_id)\n",
        "\n",
        "        self_ref = weakref.proxy(self)\n",
        "        result_ref = weakref.proxy(result)\n",
        "        def _backward():\n",
        "            if self_ref.tensor.grad is None:\n",
        "                self_ref._zero_grad()\n",
        "            self_ref.tensor.grad.add_(backward_fn(result_ref.tensor.grad, self_ref.tensor))\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "\n",
        "    def matmul(self, other):\n",
        "        result_tensor = torch.matmul(self.tensor, other.tensor)\n",
        "        requires_grad = self._custom_requires_grad or other._custom_requires_grad\n",
        "        if not requires_grad:\n",
        "            return CustomTensor(result_tensor)\n",
        "\n",
        "        graph = self.graph if self._custom_requires_grad else other.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "\n",
        "        self_ref = weakref.proxy(self)\n",
        "        other_ref = weakref.proxy(other)\n",
        "        result_ref = weakref.proxy(result)\n",
        "\n",
        "        if self._custom_requires_grad:\n",
        "            graph.add_edge(self._node_id, result._node_id)\n",
        "        if other._custom_requires_grad:\n",
        "            graph.add_edge(other._node_id, result._node_id)\n",
        "\n",
        "        def _backward():\n",
        "            if self._custom_requires_grad:\n",
        "                if self_ref.tensor.grad is None:\n",
        "                    self_ref._zero_grad()\n",
        "                self_ref.tensor.grad.add_(torch.matmul(result_ref.tensor.grad, other_ref.tensor.t()))\n",
        "            if other._custom_requires_grad:\n",
        "                if other_ref.tensor.grad is None:\n",
        "                    other_ref._zero_grad()\n",
        "                other_ref.tensor.grad.add_(torch.matmul(self_ref.tensor.t(), result_ref.tensor.grad))\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "\n",
        "    def apply_mask(self, mask):\n",
        "        result_tensor = self.tensor * mask.tensor\n",
        "        requires_grad = self._custom_requires_grad or mask._custom_requires_grad\n",
        "        if not requires_grad:\n",
        "            return CustomTensor(result_tensor)\n",
        "\n",
        "        graph = self.graph if self._custom_requires_grad else mask.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "\n",
        "        self_ref = weakref.proxy(self)\n",
        "        mask_ref = weakref.proxy(mask)\n",
        "        result_ref = weakref.proxy(result)\n",
        "\n",
        "        if self._custom_requires_grad:\n",
        "            graph.add_edge(self._node_id, result._node_id)\n",
        "        if mask._custom_requires_grad:\n",
        "            graph.add_edge(mask._node_id, result._node_id)\n",
        "\n",
        "        def _backward():\n",
        "            if self._custom_requires_grad:\n",
        "                if self_ref.tensor.grad is None:\n",
        "                    self_ref._zero_grad()\n",
        "                self_ref.tensor.grad.add_(result_ref.tensor.grad * mask_ref.tensor)\n",
        "            if mask._custom_requires_grad:\n",
        "                if mask_ref.tensor.grad is None:\n",
        "                    mask_ref._zero_grad()\n",
        "                mask_ref.tensor.grad.add_(result_ref.tensor.grad * self_ref.tensor)\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "    \n",
        "    def dot(self, other):\n",
        "        result_tensor = torch.dot(self.tensor, other.tensor)\n",
        "        requires_grad = self._custom_requires_grad or other._custom_requires_grad\n",
        "        if not requires_grad:\n",
        "            return CustomTensor(result_tensor)\n",
        "\n",
        "        graph = self.graph if self._custom_requires_grad else other.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "\n",
        "        self_ref = weakref.proxy(self)\n",
        "        other_ref = weakref.proxy(other)\n",
        "        result_ref = weakref.proxy(result)\n",
        "\n",
        "        if self._custom_requires_grad:\n",
        "            graph.add_edge(self._node_id, result._node_id)\n",
        "        if other._custom_requires_grad:\n",
        "            graph.add_edge(other._node_id, result._node_id)\n",
        "\n",
        "        def _backward():\n",
        "            if self._custom_requires_grad:\n",
        "                if self_ref.tensor.grad is None:\n",
        "                    self_ref._zero_grad()\n",
        "                self_ref.tensor.grad.add_(result_ref.tensor.grad * other_ref.tensor)\n",
        "            if other._custom_requires_grad:\n",
        "                if other_ref.tensor.grad is None:\n",
        "                    other_ref._zero_grad()\n",
        "                other_ref.tensor.grad.add_(result_ref.tensor.grad * self_ref.tensor)\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "\n",
        "    def __del__(self):\n",
        "        print(\"Garbage Collector has decided that reference counts are zero so Goodbye!!\")\n",
        "\n",
        "\n",
        "# --- Test Code ---\n",
        "\n",
        "def run_backward(output_tensor: CustomTensor):\n",
        "    \"\"\"\n",
        "    Simulates the backward pass for the custom autograd system.\n",
        "    This function needs to be explicitly defined as it's not part of the original classes.\n",
        "    \"\"\"\n",
        "    if not output_tensor._custom_requires_grad:\n",
        "        raise RuntimeError(\"Output tensor does not require grad.\")\n",
        "    if output_tensor.graph is None:\n",
        "        raise RuntimeError(\"Output tensor is not part of a graph.\")\n",
        "\n",
        "    # Initialize gradient for the output tensor\n",
        "    output_tensor.tensor.grad = torch.ones_like(output_tensor.tensor)\n",
        "\n",
        "    # Perform backward pass using topological sort\n",
        "    nodes_to_process = output_tensor.graph.reverse_toposort()\n",
        "\n",
        "    # Create a strong reference to intermediate tensors needed for backward pass\n",
        "    # This simulates how a real autograd engine would keep track of them\n",
        "    # The graph context's intermediate_tensors dict already serves this purpose.\n",
        "\n",
        "    for tensor_node in nodes_to_process:\n",
        "        # Check if the weak proxy is still valid (tensor is alive)\n",
        "        if isinstance(tensor_node, weakref.ProxyTypes) and tensor_node.__slots__ is None:\n",
        "            # print(f\"Skipping dead proxy: {tensor_node}\") # Debugging\n",
        "            continue # Skip if the weak reference is dead\n",
        "\n",
        "        if tensor_node.tensor.grad is None and tensor_node is not output_tensor:\n",
        "            # This can happen if a tensor is part of the graph but its grad hasn't been set yet\n",
        "            # and it's not the root of the backward call. This typically means it's a leaf\n",
        "            # that wasn't used to compute the output or an intermediate that accumulated no grad.\n",
        "            # For simplicity in this test, we assume grads propagate.\n",
        "            # print(f\"Warning: Tensor node {tensor_node._node_id} has no grad before _backward call.\")\n",
        "            pass # A no-op for now. In a real system, you might want to handle this.\n",
        "\n",
        "        # Ensure that non-leaf tensors are still alive when their _backward is called\n",
        "        # The `intermediate_tensors` in `AutogradGraph` should keep them alive.\n",
        "        tensor_node._backward()\n",
        "\n",
        "    # Clean up intermediate tensors references after backward pass\n",
        "    # This would typically be handled by the graph context's exit, but\n",
        "    # if `_auto_cleanup` is False, you might need manual cleanup.\n",
        "    # Here, for testing GC, we'll let the context manager handle it.\n",
        "\n",
        "\n",
        "class TestCustomAutogradSystem:\n",
        "\n",
        "    def test_basic_add_scalar_grad(self):\n",
        "        with AutogradGraph() as graph:\n",
        "            a = CustomTensor(torch.tensor([2.0, 3.0]), _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            b = a + 5.0 # (a + 5)\n",
        "            c = b + 10.0 # (a + 5 + 10)\n",
        "\n",
        "            # Manually run backward pass\n",
        "            run_backward(c)\n",
        "\n",
        "            # Expected gradients:\n",
        "            # dC/dA = 1.0 (for each element)\n",
        "            assert torch.allclose(a.tensor.grad, torch.tensor([1.0, 1.0]))\n",
        "            assert b.tensor.grad is not None\n",
        "            assert torch.allclose(b.tensor.grad, torch.tensor([1.0, 1.0])) # dC/dB = 1.0\n",
        "\n",
        "            # Verify graph structure\n",
        "            assert graph.graph.num_nodes() == 3\n",
        "            assert graph.graph.num_edges() == 2\n",
        "            assert graph.graph.has_edge(a._node_id, b._node_id)\n",
        "            assert graph.graph.has_edge(b._node_id, c._node_id)\n",
        "            assert graph.check_cycle() is True\n",
        "\n",
        "    def test_basic_add_tensor_grad(self):\n",
        "        with AutogradGraph() as graph:\n",
        "            a = CustomTensor(torch.tensor([2.0, 3.0]), _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            b = CustomTensor(torch.tensor([1.0, 2.0]), _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            c = a + b # (a + b)\n",
        "            d = c + 5.0 # (a + b + 5)\n",
        "\n",
        "            run_backward(d)\n",
        "\n",
        "            # Expected gradients:\n",
        "            # dD/dA = 1.0\n",
        "            # dD/dB = 1.0\n",
        "            assert torch.allclose(a.tensor.grad, torch.tensor([1.0, 1.0]))\n",
        "            assert torch.allclose(b.tensor.grad, torch.tensor([1.0, 1.0]))\n",
        "\n",
        "            # Verify graph structure\n",
        "            assert graph.graph.num_nodes() == 4\n",
        "            assert graph.graph.num_edges() == 3\n",
        "            assert graph.graph.has_edge(a._node_id, c._node_id)\n",
        "            assert graph.graph.has_edge(b._node_id, c._node_id)\n",
        "            assert graph.graph.has_edge(c._node_id, d._node_id)\n",
        "            assert graph.check_cycle() is True\n",
        "\n",
        "    def test_mixed_requires_grad_tensor_add(self):\n",
        "        with AutogradGraph() as graph:\n",
        "            a = CustomTensor(torch.tensor([2.0, 3.0]), _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            b = CustomTensor(torch.tensor([1.0, 2.0]), _custom_requires_grad=False) # Does not require grad\n",
        "            c = a + b # c should require grad, b's grad should be None\n",
        "\n",
        "            run_backward(c)\n",
        "\n",
        "            assert torch.allclose(a.tensor.grad, torch.tensor([1.0, 1.0]))\n",
        "            assert b.tensor.grad is None # b should not have a grad\n",
        "            assert c._custom_requires_grad is True\n",
        "\n",
        "            # Verify graph structure\n",
        "            assert graph.graph.num_nodes() == 2 # Only a and c in the graph\n",
        "            assert graph.graph.num_edges() == 1\n",
        "            assert graph.graph.has_node(a._node_id)\n",
        "            assert graph.graph.has_node(c._node_id)\n",
        "            assert graph.graph.has_edge(a._node_id, c._node_id)\n",
        "            #assert not graph.graph.has_node(b._node_id) # b should not be in graph\n",
        "\n",
        "    def test_no_requires_grad(self):\n",
        "        with AutogradGraph() as graph: # Graph created, but no tensors with requires_grad=True added\n",
        "            a = CustomTensor(torch.tensor([1.0]))\n",
        "            b = CustomTensor(torch.tensor([2.0]))\n",
        "            c = a + b\n",
        "            d = c + 3.0\n",
        "\n",
        "            assert not a._custom_requires_grad\n",
        "            assert not b._custom_requires_grad\n",
        "            assert not c._custom_requires_grad\n",
        "            assert not d._custom_requires_grad\n",
        "            assert graph.graph.num_nodes() == 0 # Graph should remain empty\n",
        "            assert graph.graph.num_edges() == 0\n",
        "\n",
        "            with pytest.raises(RuntimeError, match=\"Output tensor does not require grad.\"):\n",
        "                run_backward(d)\n",
        "\n",
        "    def test_autograd_graph_context_manager(self):\n",
        "        graph = None\n",
        "        with AutogradGraph(check_for_cycles=True, auto_cleanup=True) as g:\n",
        "            graph = g\n",
        "            a = CustomTensor(torch.tensor([1.0]), _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            b = a + 1.0\n",
        "            assert graph.graph.num_nodes() == 2\n",
        "            assert graph.graph.num_edges() == 1\n",
        "            assert len(graph.intermediate_tensors) == 1 # b should be in intermediate_tensors\n",
        "\n",
        "        # After exiting the context, graph should be empty\n",
        "        assert graph.graph.num_nodes() == 0\n",
        "        assert graph.graph.num_edges() == 0\n",
        "        assert len(graph.intermediate_tensors) == 0\n",
        "\n",
        "    def test_cycle_detection(self):\n",
        "      try:\n",
        "        with AutogradGraph(check_for_cycles=True, auto_cleanup=False) as graph: # auto_cleanup=False to inspect after error\n",
        "            a = CustomTensor(torch.tensor([1.0]), _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            b = CustomTensor(torch.tensor([2.0]), _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "\n",
        "            # Manually create a cycle (a -> b -> a)\n",
        "            graph.add_edge(a._node_id, b._node_id)\n",
        "            graph.add_edge(b._node_id, a._node_id)\n",
        "      except RuntimeError as e:\n",
        "        print(f\"Raised the error of cycle detected as {e}\")\n",
        "            # with pytest.raises(RuntimeError, match=\"Cycle detected in autograd graph on context exit.\"):\n",
        "            #     pass # The __exit__ method will be called here\n",
        "\n",
        "    def test_no_circular_references_non_leaf_tensors_die(self):\n",
        "          # This test relies on the garbage collector. It's a heuristic test\n",
        "        # as Python's GC timing is not strictly deterministic.\n",
        "        # However, with weakrefs, it should work for non-leaf tensors.\n",
        "\n",
        "      print(\"\\n--- Starting GC Test: No Circular References (Part 1) ---\")\n",
        "\n",
        "      graph_ref = None\n",
        "      output_tensor_weak_ref = None\n",
        "      node_id_d = -1 # To store node_id before d is deleted\n",
        "\n",
        "      # BLOCK 1: Create graph and tensors\n",
        "      with AutogradGraph(auto_cleanup=False) as graph: # Keep graph for inspection\n",
        "          graph_ref = weakref.ref(graph)\n",
        "          a = CustomTensor(torch.tensor([1.0]), _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "          b = a + 1.0 # Intermediate tensor\n",
        "          c = b + 2.0 # Intermediate tensor\n",
        "          d = c + 3.0 # Output tensor (also intermediate from graph's perspective)\n",
        "\n",
        "          # Store weak reference to 'd' BEFORE its strong reference is potentially removed\n",
        "          output_tensor_weak_ref = weakref.ref(d)\n",
        "          node_id_d = d._node_id # Store node_id while d is alive\n",
        "\n",
        "          print(f\"Initial: d object: {d}\")\n",
        "          print(f\"Initial: d._node_id: {node_id_d}\")\n",
        "          print(f\"Initial: graph.intermediate_tensors keys: {list(graph.intermediate_tensors.keys())}\")\n",
        "          # The ref count for `d` object itself will be high here because it's in `graph.intermediate_tensors`,\n",
        "          # and held by variable `d`, and by the temporary ref in `getrefcount`.\n",
        "          print(f\"Initial: refcount of d (via output_tensor_weak_ref.test_ref): {sys.getrefcount(output_tensor_weak_ref()) if output_tensor_weak_ref() else 'N/A'}\")\n",
        "          assert len(graph.intermediate_tensors) == 3 # b, c, d should be in intermediate_tensors\n",
        "\n",
        "      # BLOCK 2: After exiting context manager (auto_cleanup=False)\n",
        "      print(\"\\n--- After exiting 'with' block (auto_cleanup=False) ---\")\n",
        "      # The 'graph' variable still holds a strong reference to the AutogradGraph instance.\n",
        "      # graph_ref() should return the graph object.\n",
        "      assert graph_ref() is not None, \"Graph object should still be alive.\"\n",
        "      assert len(graph_ref().intermediate_tensors) == 3, \"Intermediate tensors should still be referenced by the graph.\"\n",
        "      print(f\"After 'with' block: d object (via weakref): {output_tensor_weak_ref()}\")\n",
        "      print(f\"After 'with' block: refcount of d (via output_tensor_weak_ref.test_ref): {sys.getrefcount(output_tensor_weak_ref())}\")\n",
        "\n",
        "      # BLOCK 3: Remove strong reference 'd' from local scope\n",
        "      print(\"\\n--- Deleting 'd' variable ---\")\n",
        "      del d # Remove the local strong reference to the CustomTensor object.\n",
        "      gc.collect() # Force garbage collection\n",
        "\n",
        "      # Now, output_tensor_weak_ref() *still* shouldn't be None because `graph_ref().intermediate_tensors`\n",
        "      # holds the strong reference.\n",
        "      print(f\"After del d + gc.collect(): d object (via weakref): {output_tensor_weak_ref()}\")\n",
        "      # We expect this to *not* be None yet, and to still show a refcount reflecting intermediate_tensors.\n",
        "      assert output_tensor_weak_ref() is not None, \"d should still be alive due to intermediate_tensors.\"\n",
        "      current_d_refcount_after_del_d = sys.getrefcount(output_tensor_weak_ref()) if output_tensor_weak_ref() else 'N/A'\n",
        "      print(f\"After del d + gc.collect(): refcount of d: {current_d_refcount_after_del_d}\")\n",
        "      # Expected refcount should be 2: one from intermediate_tensors, one from getrefcount()\n",
        "      assert current_d_refcount_after_del_d == 2, f\"Expected refcount 2, got {current_d_refcount_after_del_d}\"\n",
        "\n",
        "      # BLOCK 4: Remove strong reference from intermediate_tensors\n",
        "      print(f\"\\n--- Deleting strong reference from graph.intermediate_tensors for node {node_id_d} ---\")\n",
        "      graph_ref().del_non_leaf_tensor_reference(node_id_d) # THIS IS THE CRUCIAL STEP\n",
        "      print(f\"After del_non_leaf_tensor_reference: graph.intermediate_tensors keys: {list(graph_ref().intermediate_tensors.keys())}\")\n",
        "      #gc.collect() # Force garbage collection again\n",
        "\n",
        "      # Now, with the last strong reference gone, 'd' should be garbage collected.\n",
        "      print(f\"After del_non_leaf_tensor_reference + gc.collect(): d object (via weakref): {output_tensor_weak_ref()}\")\n",
        "      # This is where your original assertion was. It *should* pass now.\n",
        "      assert output_tensor_weak_ref() is None, \"Output tensor (non-leaf) should be garbage collected after its strong reference is deleted from intermediate_tensors.\"\n",
        "      print(\"Assertion Passed: Output tensor (d) was garbage collected.\")\n",
        "\n",
        "      # BLOCK 5: Verify other intermediate tensors are collected when graph is cleared\n",
        "      print(\"\\n--- Starting GC Test: All Intermediate Tensors ---\")\n",
        "      intermediate_tensors_wrefs = []\n",
        "      # Create a new graph and new tensors to avoid interference from previous block\n",
        "      with AutogradGraph(auto_cleanup=False) as graph_new:\n",
        "          a_new = CustomTensor(torch.tensor([1.0]), _custom_requires_grad=True, graph=graph_new, is_leaf=True)\n",
        "          b_new = a_new + 1.0 # Intermediate\n",
        "          c_new = b_new + 2.0 # Intermediate\n",
        "          d_new = c_new + 3.0 # Intermediate (output of a chain)\n",
        "\n",
        "          # Store weak references to the intermediate tensors\n",
        "          intermediate_tensors_wrefs.append(weakref.ref(b_new))\n",
        "          intermediate_tensors_wrefs.append(weakref.ref(c_new))\n",
        "          intermediate_tensors_wrefs.append(weakref.ref(d_new))\n",
        "\n",
        "          # Verify they are initially alive\n",
        "          assert all(wref() is not None for wref in intermediate_tensors_wrefs)\n",
        "          assert len(graph_new.intermediate_tensors) == 3\n",
        "\n",
        "      print(f\"After 'with' block (new graph): graph_new object: {graph_new}\")\n",
        "      assert graph_new is not None, \"New graph object should still be alive after 'with' block.\"\n",
        "      assert len(graph_new.intermediate_tensors) == 3, \"New graph intermediate_tensors should still hold refs.\"\n",
        "\n",
        "      # Manually clear the intermediate_tensors dictionary and remove graph reference\n",
        "      print(\"\\n--- Manually clearing graph.intermediate_tensors and deleting graph ---\")\n",
        "      graph_new.intermediate_tensors.clear()\n",
        "      del graph_new # Remove the strong reference to the graph itself\n",
        "      del b_new , c_new , d_new # deleting the local variable strong references\n",
        "      #gc.collect()\n",
        "\n",
        "      # Now, all non-leaf tensors should be garbage collected\n",
        "      for i, wref in enumerate(intermediate_tensors_wrefs):\n",
        "          print(f\"Intermediate tensor {i} (via weakref): {wref()}\")\n",
        "          assert wref() is None, f\"Intermediate tensor {i} should be garbage collected after graph context and intermediate_tensors are cleared.\"\n",
        "      print(\"Assertion Passed: All intermediate tensors were garbage collected.\")\n",
        "\n",
        "    def test_topological_sort_order(self):\n",
        "        with AutogradGraph() as graph:\n",
        "            t1 = CustomTensor(torch.tensor([1.0]), _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            t2 = CustomTensor(torch.tensor([2.0]), _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            t3 = t1 + t2\n",
        "            t4 = t3 + 5.0\n",
        "            t5 = t2 + 10.0 # Another branch\n",
        "            t6 = t4 + t5\n",
        "\n",
        "            # The topological sort should produce an order where dependencies come before their dependents.\n",
        "            # Reversed topological sort should produce an order where outputs come before their inputs.\n",
        "            # Example expected order: t6, t4, t5, t3, t2, t1 (or variations respecting dependencies)\n",
        "            sorted_tensors = graph.reverse_toposort()\n",
        "\n",
        "            # Check if dependencies are respected in reverse order\n",
        "            # If A -> B, then B should appear before A in reverse topological sort.\n",
        "            # t6 depends on t4, t5. So t6 should be before t4 and t5.\n",
        "            # t4 depends on t3. So t4 should be before t3.\n",
        "            # t5 depends on t2. So t5 should be before t2.\n",
        "            # t3 depends on t1, t2. So t3 should be before t1 and t2.\n",
        "\n",
        "            # Simple check: The first element should be t6 (the ultimate output).\n",
        "            assert sorted_tensors[0] is t6\n",
        "\n",
        "            # Check positions:\n",
        "            pos = {t: i for i, t in enumerate(sorted_tensors)}\n",
        "\n",
        "            assert pos[t6] < pos[t4]\n",
        "            assert pos[t6] < pos[t5]\n",
        "            assert pos[t4] < pos[t3]\n",
        "            assert pos[t5] < pos[t2]\n",
        "            assert pos[t3] < pos[t1]\n",
        "            assert pos[t3] < pos[t2] # t3 also depends on t2\n",
        "\n",
        "            # Additional check: t2 is a dependency for both t3 and t5.\n",
        "            # In reverse topo sort, t3 and t5 must appear before t2.\n",
        "            assert pos[t3] < pos[t2]\n",
        "            assert pos[t5] < pos[t2]\n",
        "\n",
        "            # t1 is only a dependency for t3.\n",
        "            assert pos[t3] < pos[t1]\n",
        "\n",
        "            # Check if all 6 tensors are in the sorted list\n",
        "            assert len(sorted_tensors) == 6\n",
        "            assert set(sorted_tensors) == {t1, t2, t3, t4, t5, t6}\n",
        "\n",
        "# To run these tests, save the code as a Python file (e.g., `test_autograd.py`)\n",
        "# and run `pytest` from your terminal in the same directory.\n",
        "# `pip install pytest torch rustworkx` if you don't have them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "! pip install rustworkx\n",
        "import torch\n",
        "import weakref\n",
        "import numbers\n",
        "import rustworkx as rx\n",
        "import pytest\n",
        "\n",
        "class AutogradGraph:\n",
        "    def __init__(self, check_for_cycles=True, auto_cleanup=True):\n",
        "        self.graph = rx.PyDiGraph()\n",
        "        self.intermediate_tensors = {}\n",
        "        self._check_cycles = check_for_cycles\n",
        "        self._auto_cleanup = auto_cleanup\n",
        "\n",
        "    def __enter__(self):\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, exc_type, exc_value, traceback):\n",
        "        if self._check_cycles and not self.check_cycle():\n",
        "            raise RuntimeError(\"Cycle detected in autograd graph on context exit.\")\n",
        "        if self._auto_cleanup:\n",
        "            self.intermediate_tensors.clear()\n",
        "            self.graph.clear()\n",
        "\n",
        "    def add_tensor_graph(self, tensor, is_leaf):\n",
        "        if not tensor._custom_requires_grad:\n",
        "            raise ValueError(\"Tensor with requires_grad=False cannot be added to the graph.\")\n",
        "\n",
        "        ref = tensor if is_leaf else weakref.proxy(tensor)\n",
        "        tensor_index = self.graph.add_node(ref)\n",
        "        tensor._node_id = tensor_index\n",
        "\n",
        "    def add_non_leaf_tensor_reference(self, tensor):\n",
        "        if not tensor._custom_requires_grad:\n",
        "            raise ValueError(\"Tensor must require grad.\")\n",
        "\n",
        "        if tensor._node_id in self.intermediate_tensors:\n",
        "            raise ValueError(\"Tensor reference already exists in intermediate tensors.\")\n",
        "\n",
        "        self.intermediate_tensors[tensor._node_id] = tensor\n",
        "\n",
        "    def add_edge(self, node_from, node_to, weight=None):\n",
        "        if not all(isinstance(n, int) for n in (node_from, node_to)):\n",
        "            raise TypeError(\"Node indices must be integers.\")\n",
        "        if not self.graph.has_node(node_from) or not self.graph.has_node(node_to):\n",
        "            raise ValueError(\"Nodes must exist before adding edge.\")\n",
        "        self.graph.add_edge(node_from, node_to, weight)\n",
        "\n",
        "    def check_cycle(self):\n",
        "        return rx.is_directed_acyclic_graph(self.graph)\n",
        "\n",
        "    def reverse_toposort(self):\n",
        "        if not self.check_cycle():\n",
        "            raise RuntimeError(\"Cannot perform topological sort on a cyclic graph.\")\n",
        "        # rustworkx.topological_sort already works on node indices and dependencies\n",
        "        # The result of rx.topological_sort is a list of node indices in topological order.\n",
        "        # We then retrieve the actual tensor objects using self.graph[n].\n",
        "        return [self.graph[n] for n in reversed(rx.topological_sort(self.graph))]\n",
        "\n",
        "    def delete_node(self, node_index):\n",
        "        if not isinstance(node_index, int):\n",
        "            raise TypeError(\"Node index must be an integer.\")\n",
        "        if not self.graph.has_node(node_index):\n",
        "            raise ValueError(\"Node does not exist.\")\n",
        "        self.graph.remove_node(node_index)\n",
        "\n",
        "    def delete_edge(self, node_from, node_to):\n",
        "        if not self.graph.has_edge(node_from, node_to):\n",
        "            raise ValueError(\"Edge does not exist.\")\n",
        "        self.graph.remove_edge(node_from, node_to)\n",
        "\n",
        "    def del_non_leaf_tensor_reference(self, tensor_node_id):\n",
        "        self.intermediate_tensors.pop(tensor_node_id, None)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"CustomAutogradGraph(nodes={self.graph.num_nodes()}, edges={self.graph.num_edges()})\"\n",
        "\n",
        "class CustomTensor:\n",
        "    __slots__ = ('tensor', '_node_id', '_custom_requires_grad', '_backward', 'graph', '__weakref__')\n",
        "\n",
        "    def __new__(cls, data, *, _custom_requires_grad=False, device=None, dtype=None, graph=None, due_to_operation=False, is_leaf=False):\n",
        "        if isinstance(data, CustomTensor):\n",
        "            return data  # Don't rewrap\n",
        "        return super().__new__(cls)\n",
        "\n",
        "    def __init__(self, data, *, _custom_requires_grad=False, device=None, dtype=None, graph=None, due_to_operation=False, is_leaf=False):\n",
        "        if isinstance(data, CustomTensor):\n",
        "            return\n",
        "\n",
        "        self.tensor = data if due_to_operation else torch.as_tensor(data, dtype=dtype, device=device)\n",
        "        self.tensor.requires_grad_(False)\n",
        "        self._custom_requires_grad = _custom_requires_grad\n",
        "        self._node_id = None\n",
        "        self._backward = lambda: None\n",
        "        self.graph = None\n",
        "\n",
        "        if _custom_requires_grad:\n",
        "            self._init_graph(graph, is_leaf)\n",
        "\n",
        "    def _init_graph(self, graph, is_leaf):\n",
        "        if graph is None:\n",
        "            raise ValueError(\"Graph must be provided if requires_grad is True.\")\n",
        "        if is_leaf:\n",
        "          self.graph = weakref.proxy(graph)\n",
        "        else:\n",
        "          self.graph = graph\n",
        "        graph.add_tensor_graph(self, is_leaf=is_leaf)\n",
        "        if not is_leaf:\n",
        "            graph.add_non_leaf_tensor_reference(self)\n",
        "\n",
        "    def _zero_grad(self):\n",
        "        self.tensor.grad = torch.zeros_like(self.tensor)\n",
        "\n",
        "    def __add__(self, other):\n",
        "        if isinstance(other, numbers.Number):\n",
        "            return self._add_scalar(other)#, op=torch.add)#Operations.add_tensor_and_scalar)\n",
        "        elif isinstance(other, CustomTensor):\n",
        "            return self._add_tensor(other)#, op=torch.add)#Operations.add_tensor_and_tensor)\n",
        "        return NotImplemented\n",
        "\n",
        "    def _add_scalar(self, scalar):\n",
        "        result_tensor = torch.add(self.tensor, scalar)\n",
        "        if not self._custom_requires_grad:\n",
        "            return CustomTensor(result_tensor)\n",
        "\n",
        "        graph = self.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "        graph.add_edge(self._node_id, result._node_id)\n",
        "\n",
        "        self_ref = weakref.proxy(self)\n",
        "        result_ref = weakref.proxy(result)\n",
        "        def _backward():\n",
        "            if self_ref.tensor.grad is None:\n",
        "                self_ref._zero_grad()\n",
        "            # print(f\"Backward for scalar add: result_grad={result.tensor.grad}, self_grad_before={self_ref.tensor.grad}\") # Debugging\n",
        "            self_ref.tensor.grad.add_(result_ref.tensor.grad)\n",
        "            # print(f\"Backward for scalar add: self_grad_after={self_ref.tensor.grad}\") # Debugging\n",
        "\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "\n",
        "    def _add_tensor(self, other):\n",
        "        result_tensor = torch.add(self.tensor, other.tensor)\n",
        "        requires_grad = self._custom_requires_grad or other._custom_requires_grad\n",
        "\n",
        "        if not requires_grad:\n",
        "            return CustomTensor(result_tensor)\n",
        "\n",
        "        # Graph selection logic - assuming operations happen within a single graph context\n",
        "        graph = None\n",
        "        if self._custom_requires_grad:\n",
        "            graph = self.graph\n",
        "        elif other._custom_requires_grad:\n",
        "            graph = other.graph\n",
        "        else:\n",
        "            # This case should ideally not be reached if requires_grad is True\n",
        "            # and at least one operand has requires_grad\n",
        "            pass # Or raise an error if graph is truly missing\n",
        "\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "\n",
        "        self_ref = weakref.proxy(self)\n",
        "        other_ref = weakref.proxy(other)\n",
        "\n",
        "        if self._custom_requires_grad:\n",
        "            graph.add_edge(self._node_id, result._node_id)\n",
        "        if other._custom_requires_grad:\n",
        "            graph.add_edge(other._node_id, result._node_id)\n",
        "        result_ref = weakref.proxy(result)\n",
        "        def _backward():\n",
        "            # print(f\"Backward for tensor add: result_grad={result.tensor.grad}\") # Debugging\n",
        "            if self._custom_requires_grad:\n",
        "                if self_ref.tensor.grad is None:\n",
        "                    self_ref._zero_grad()\n",
        "                self_ref.tensor.grad.add_(result_ref.tensor.grad)\n",
        "                # print(f\"  self_grad_after={self_ref.tensor.grad}\") # Debugging\n",
        "            if other._custom_requires_grad:\n",
        "                if other_ref.tensor.grad is None:\n",
        "                    other_ref._zero_grad()\n",
        "                other_ref.tensor.grad.add_(result_ref.tensor.grad)\n",
        "                # print(f\"  other_grad_after={other_ref.tensor.grad}\") # Debugging\n",
        "\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "    def __mul__(self, other):\n",
        "        if isinstance(other, numbers.Number):\n",
        "            return self._mul_scalar(other)\n",
        "        elif isinstance(other, CustomTensor):\n",
        "            return self._mul_tensor(other)\n",
        "        return NotImplemented\n",
        "\n",
        "    def _mul_scalar(self, scalar):\n",
        "        result_tensor = torch.mul(self.tensor, scalar)\n",
        "        if not self._custom_requires_grad:\n",
        "            return CustomTensor(result_tensor)\n",
        "\n",
        "        graph = self.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "        graph.add_edge(self._node_id, result._node_id)\n",
        "\n",
        "        self_ref = weakref.proxy(self)\n",
        "        result_ref = weakref.proxy(result)\n",
        "        def _backward():\n",
        "            if self_ref.tensor.grad is None:\n",
        "                self_ref._zero_grad()\n",
        "            self_ref.tensor.grad.add_(result_ref.tensor.grad * scalar)\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "\n",
        "    def _mul_tensor(self, other):\n",
        "        result_tensor = torch.mul(self.tensor, other.tensor)\n",
        "        requires_grad = self._custom_requires_grad or other._custom_requires_grad\n",
        "        if not requires_grad:\n",
        "            return CustomTensor(result_tensor)\n",
        "\n",
        "        graph = self.graph if self._custom_requires_grad else other.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "\n",
        "        self_ref = weakref.proxy(self)\n",
        "        other_ref = weakref.proxy(other)\n",
        "        result_ref = weakref.proxy(result)\n",
        "\n",
        "        if self._custom_requires_grad:\n",
        "            graph.add_edge(self._node_id, result._node_id)\n",
        "        if other._custom_requires_grad:\n",
        "            graph.add_edge(other._node_id, result._node_id)\n",
        "\n",
        "        def _backward():\n",
        "            if self._custom_requires_grad:\n",
        "                if self_ref.tensor.grad is None:\n",
        "                    self_ref._zero_grad()\n",
        "                self_ref.tensor.grad.add_(result_ref.tensor.grad * other_ref.tensor)\n",
        "            if other._custom_requires_grad:\n",
        "                if other_ref.tensor.grad is None:\n",
        "                    other_ref._zero_grad()\n",
        "                other_ref.tensor.grad.add_(result_ref.tensor.grad * self_ref.tensor)\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "\n",
        "\n",
        "    def __sub__(self, other):\n",
        "        if isinstance(other, numbers.Number):\n",
        "            return self._sub_scalar(other)\n",
        "        elif isinstance(other, CustomTensor):\n",
        "            return self._sub_tensor(other)\n",
        "        return NotImplemented\n",
        "\n",
        "    def _sub_scalar(self, scalar):\n",
        "        result_tensor = torch.sub(self.tensor, scalar)\n",
        "        if not self._custom_requires_grad:\n",
        "            return CustomTensor(result_tensor)\n",
        "\n",
        "        graph = self.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "        graph.add_edge(self._node_id, result._node_id)\n",
        "\n",
        "        self_ref = weakref.proxy(self)\n",
        "        result_ref = weakref.proxy(result)\n",
        "        def _backward():\n",
        "            if self_ref.tensor.grad is None:\n",
        "                self_ref._zero_grad()\n",
        "            self_ref.tensor.grad.add_(result_ref.tensor.grad)\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "\n",
        "    def _sub_tensor(self, other):\n",
        "        result_tensor = torch.sub(self.tensor, other.tensor)\n",
        "        requires_grad = self._custom_requires_grad or other._custom_requires_grad\n",
        "        if not requires_grad:\n",
        "            return CustomTensor(result_tensor)\n",
        "\n",
        "        graph = self.graph if self._custom_requires_grad else other.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "\n",
        "        self_ref = weakref.proxy(self)\n",
        "        other_ref = weakref.proxy(other)\n",
        "        result_ref = weakref.proxy(result)\n",
        "\n",
        "        if self._custom_requires_grad:\n",
        "            graph.add_edge(self._node_id, result._node_id)\n",
        "        if other._custom_requires_grad:\n",
        "            graph.add_edge(other._node_id, result._node_id)\n",
        "\n",
        "        def _backward():\n",
        "            if self._custom_requires_grad:\n",
        "                if self_ref.tensor.grad is None:\n",
        "                    self_ref._zero_grad()\n",
        "                self_ref.tensor.grad.add_(result_ref.tensor.grad)\n",
        "            if other._custom_requires_grad:\n",
        "                if other_ref.tensor.grad is None:\n",
        "                    other_ref._zero_grad()\n",
        "                other_ref.tensor.grad.sub_(result_ref.tensor.grad)\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "    \n",
        "    def __truediv__(self, scalar):\n",
        "        return self._div_scalar(scalar)\n",
        "\n",
        "    def _div_scalar(self, scalar):\n",
        "        result_tensor = torch.div(self.tensor, scalar)\n",
        "        if not self._custom_requires_grad:\n",
        "            return CustomTensor(result_tensor)\n",
        "\n",
        "        graph = self.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "        graph.add_edge(self._node_id, result._node_id)\n",
        "\n",
        "        self_ref = weakref.proxy(self)\n",
        "        result_ref = weakref.proxy(result)\n",
        "        def _backward():\n",
        "            if self_ref.tensor.grad is None:\n",
        "                self_ref._zero_grad()\n",
        "            self_ref.tensor.grad.add_(result_ref.tensor.grad / scalar)\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "\n",
        "\n",
        "    def pow(self, scalar):\n",
        "        result_tensor = torch.pow(self.tensor, scalar)\n",
        "        if not self._custom_requires_grad:\n",
        "            return CustomTensor(result_tensor)\n",
        "\n",
        "        graph = self.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "        graph.add_edge(self._node_id, result._node_id)\n",
        "\n",
        "        self_ref = weakref.proxy(self)\n",
        "        result_ref = weakref.proxy(result)\n",
        "        def _backward():\n",
        "            if self_ref.tensor.grad is None:\n",
        "                self_ref._zero_grad()\n",
        "            grad_contrib = scalar * self_ref.tensor.pow(scalar - 1)\n",
        "            self_ref.tensor.grad.add_(result_ref.tensor.grad * grad_contrib)\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "    \n",
        "    def exp(self):\n",
        "        out = torch.exp(self.tensor)\n",
        "        return self._unary_op(out, lambda grad, out_tensor: grad * out_tensor)\n",
        "\n",
        "    def log(self):\n",
        "        out = torch.log(self.tensor)\n",
        "        return self._unary_op(out, lambda grad, input_tensor: grad / input_tensor)\n",
        "\n",
        "    def sin(self):\n",
        "        out = torch.sin(self.tensor)\n",
        "        return self._unary_op(out, lambda grad, input_tensor: grad * torch.cos(input_tensor))\n",
        "\n",
        "    def cos(self):\n",
        "        out = torch.cos(self.tensor)\n",
        "        return self._unary_op(out, lambda grad, input_tensor: -grad * torch.sin(input_tensor))\n",
        "\n",
        "    def sqrt(self):\n",
        "        out = torch.sqrt(self.tensor)\n",
        "        return self._unary_op(out, lambda grad, out_tensor: grad * 0.5 / out_tensor)\n",
        "\n",
        "    def _unary_op(self, result_tensor, backward_fn):\n",
        "        if not self._custom_requires_grad:\n",
        "            return CustomTensor(result_tensor)\n",
        "\n",
        "        graph = self.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "        graph.add_edge(self._node_id, result._node_id)\n",
        "\n",
        "        self_ref = weakref.proxy(self)\n",
        "        result_ref = weakref.proxy(result)\n",
        "        def _backward():\n",
        "            if self_ref.tensor.grad is None:\n",
        "                self_ref._zero_grad()\n",
        "            self_ref.tensor.grad.add_(backward_fn(result_ref.tensor.grad, self_ref.tensor))\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "\n",
        "    def matmul(self, other):\n",
        "        result_tensor = torch.matmul(self.tensor, other.tensor)\n",
        "        requires_grad = self._custom_requires_grad or other._custom_requires_grad\n",
        "        if not requires_grad:\n",
        "            return CustomTensor(result_tensor)\n",
        "\n",
        "        graph = self.graph if self._custom_requires_grad else other.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "\n",
        "        self_ref = weakref.proxy(self)\n",
        "        other_ref = weakref.proxy(other)\n",
        "        result_ref = weakref.proxy(result)\n",
        "\n",
        "        if self._custom_requires_grad:\n",
        "            graph.add_edge(self._node_id, result._node_id)\n",
        "        if other._custom_requires_grad:\n",
        "            graph.add_edge(other._node_id, result._node_id)\n",
        "\n",
        "        def _backward():\n",
        "            if self._custom_requires_grad:\n",
        "                if self_ref.tensor.grad is None:\n",
        "                    self_ref._zero_grad()\n",
        "                self_ref.tensor.grad.add_(torch.matmul(result_ref.tensor.grad, other_ref.tensor.t()))\n",
        "            if other._custom_requires_grad:\n",
        "                if other_ref.tensor.grad is None:\n",
        "                    other_ref._zero_grad()\n",
        "                other_ref.tensor.grad.add_(torch.matmul(self_ref.tensor.t(), result_ref.tensor.grad))\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "\n",
        "    def apply_mask(self, mask):\n",
        "        result_tensor = self.tensor * mask.tensor\n",
        "        requires_grad = self._custom_requires_grad or mask._custom_requires_grad\n",
        "        if not requires_grad:\n",
        "            return CustomTensor(result_tensor)\n",
        "\n",
        "        graph = self.graph if self._custom_requires_grad else mask.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "\n",
        "        self_ref = weakref.proxy(self)\n",
        "        mask_ref = weakref.proxy(mask)\n",
        "        result_ref = weakref.proxy(result)\n",
        "\n",
        "        if self._custom_requires_grad:\n",
        "            graph.add_edge(self._node_id, result._node_id)\n",
        "        if mask._custom_requires_grad:\n",
        "            graph.add_edge(mask._node_id, result._node_id)\n",
        "\n",
        "        def _backward():\n",
        "            if self._custom_requires_grad:\n",
        "                if self_ref.tensor.grad is None:\n",
        "                    self_ref._zero_grad()\n",
        "                self_ref.tensor.grad.add_(result_ref.tensor.grad * mask_ref.tensor)\n",
        "            if mask._custom_requires_grad:\n",
        "                if mask_ref.tensor.grad is None:\n",
        "                    mask_ref._zero_grad()\n",
        "                mask_ref.tensor.grad.add_(result_ref.tensor.grad * self_ref.tensor)\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "    \n",
        "    def dot(self, other):\n",
        "        result_tensor = torch.dot(self.tensor, other.tensor)\n",
        "        requires_grad = self._custom_requires_grad or other._custom_requires_grad\n",
        "        if not requires_grad:\n",
        "            return CustomTensor(result_tensor)\n",
        "\n",
        "        graph = self.graph if self._custom_requires_grad else other.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "\n",
        "        self_ref = weakref.proxy(self)\n",
        "        other_ref = weakref.proxy(other)\n",
        "        result_ref = weakref.proxy(result)\n",
        "\n",
        "        if self._custom_requires_grad:\n",
        "            graph.add_edge(self._node_id, result._node_id)\n",
        "        if other._custom_requires_grad:\n",
        "            graph.add_edge(other._node_id, result._node_id)\n",
        "\n",
        "        def _backward():\n",
        "            if self._custom_requires_grad:\n",
        "                if self_ref.tensor.grad is None:\n",
        "                    self_ref._zero_grad()\n",
        "                self_ref.tensor.grad.add_(result_ref.tensor.grad * other_ref.tensor)\n",
        "            if other._custom_requires_grad:\n",
        "                if other_ref.tensor.grad is None:\n",
        "                    other_ref._zero_grad()\n",
        "                other_ref.tensor.grad.add_(result_ref.tensor.grad * self_ref.tensor)\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "    #def backward():\n",
        "        \n",
        "\n",
        "    def __del__(self):\n",
        "        print(\"Garbage Collector has decided that reference counts are zero so Goodbye!!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import numbers\n",
        "import weakref\n",
        "import rustworkx as rx\n",
        "from typing import Optional, Any\n",
        "\n",
        "\n",
        "class AutogradTester:\n",
        "    \"\"\"Test suite to verify custom autograd against PyTorch's autograd\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.passed_tests = 0\n",
        "        self.failed_tests = 0\n",
        "        self.tolerance = 1e-6\n",
        "    \n",
        "    def assert_tensors_close(self, custom_tensor, pytorch_tensor, test_name, check_grad=True):\n",
        "        \"\"\"Compare custom tensor with PyTorch tensor\"\"\"\n",
        "        try:\n",
        "            # Check values\n",
        "            np.testing.assert_allclose(\n",
        "                custom_tensor.tensor.detach().numpy(),\n",
        "                pytorch_tensor.detach().numpy(),\n",
        "                rtol=self.tolerance,\n",
        "                atol=self.tolerance\n",
        "            )\n",
        "            \n",
        "            # Check gradients if requested\n",
        "            if check_grad and pytorch_tensor.grad is not None:\n",
        "                if custom_tensor.tensor.grad is None:\n",
        "                    raise AssertionError(f\"Custom tensor has no gradient in {test_name}\")\n",
        "                \n",
        "                np.testing.assert_allclose(\n",
        "                    custom_tensor.tensor.grad.detach().numpy(),\n",
        "                    pytorch_tensor.grad.detach().numpy(),\n",
        "                    rtol=self.tolerance,\n",
        "                    atol=self.tolerance\n",
        "                )\n",
        "            \n",
        "            print(f\"✓ {test_name}\")\n",
        "            self.passed_tests += 1\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"✗ {test_name}: {str(e)}\")\n",
        "            self.failed_tests += 1\n",
        "    \n",
        "    def test_basic_operations(self):\n",
        "        \"\"\"Test basic arithmetic operations\"\"\"\n",
        "        print(\"\\n=== Testing Basic Operations ===\")\n",
        "        \n",
        "        with AutogradGraph() as graph:\n",
        "            # Test scalar addition\n",
        "            x_custom = CustomTensor([2.0, 3.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = x_custom + 5.0\n",
        "            y_custom.tensor.backward(torch.ones_like(y_custom.tensor))\n",
        "            \n",
        "            x_pytorch = torch.tensor([2.0, 3.0], requires_grad=True)\n",
        "            y_pytorch = x_pytorch + 5.0\n",
        "            y_pytorch.backward(torch.ones_like(y_pytorch))\n",
        "            \n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Scalar Addition\")\n",
        "            self.assert_tensors_close(y_custom, y_pytorch, \"Scalar Addition Result\", check_grad=False)\n",
        "        \n",
        "        with AutogradGraph() as graph:\n",
        "            # Test tensor addition\n",
        "            x_custom = CustomTensor([1.0, 2.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = CustomTensor([3.0, 4.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            z_custom = x_custom + y_custom\n",
        "            z_custom.tensor.backward(torch.ones_like(z_custom.tensor))\n",
        "            \n",
        "            x_pytorch = torch.tensor([1.0, 2.0], requires_grad=True)\n",
        "            y_pytorch = torch.tensor([3.0, 4.0], requires_grad=True)\n",
        "            z_pytorch = x_pytorch + y_pytorch\n",
        "            z_pytorch.backward(torch.ones_like(z_pytorch))\n",
        "            \n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Tensor Addition - x\")\n",
        "            self.assert_tensors_close(y_custom, y_pytorch, \"Tensor Addition - y\")\n",
        "    \n",
        "    def test_multiplication(self):\n",
        "        \"\"\"Test multiplication operations\"\"\"\n",
        "        print(\"\\n=== Testing Multiplication ===\")\n",
        "        \n",
        "        with AutogradGraph() as graph:\n",
        "            # Test scalar multiplication\n",
        "            x_custom = CustomTensor([2.0, 3.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = x_custom * 4.0\n",
        "            y_custom.tensor.backward(torch.ones_like(y_custom.tensor))\n",
        "            \n",
        "            x_pytorch = torch.tensor([2.0, 3.0], requires_grad=True)\n",
        "            y_pytorch = x_pytorch * 4.0\n",
        "            y_pytorch.backward(torch.ones_like(y_pytorch))\n",
        "            \n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Scalar Multiplication\")\n",
        "        \n",
        "        with AutogradGraph() as graph:\n",
        "            # Test tensor multiplication\n",
        "            x_custom = CustomTensor([2.0, 3.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = CustomTensor([4.0, 5.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            z_custom = x_custom * y_custom\n",
        "            z_custom.tensor.backward(torch.ones_like(z_custom.tensor))\n",
        "            \n",
        "            x_pytorch = torch.tensor([2.0, 3.0], requires_grad=True)\n",
        "            y_pytorch = torch.tensor([4.0, 5.0], requires_grad=True)\n",
        "            z_pytorch = x_pytorch * y_pytorch\n",
        "            z_pytorch.backward(torch.ones_like(z_pytorch))\n",
        "            \n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Tensor Multiplication - x\")\n",
        "            self.assert_tensors_close(y_custom, y_pytorch, \"Tensor Multiplication - y\")\n",
        "    \n",
        "    def test_subtraction_division(self):\n",
        "        \"\"\"Test subtraction and division\"\"\"\n",
        "        print(\"\\n=== Testing Subtraction and Division ===\")\n",
        "        \n",
        "        with AutogradGraph() as graph:\n",
        "            # Test subtraction\n",
        "            x_custom = CustomTensor([5.0, 6.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = x_custom - 2.0\n",
        "            y_custom.tensor.backward(torch.ones_like(y_custom.tensor))\n",
        "            \n",
        "            x_pytorch = torch.tensor([5.0, 6.0], requires_grad=True)\n",
        "            y_pytorch = x_pytorch - 2.0\n",
        "            y_pytorch.backward(torch.ones_like(y_pytorch))\n",
        "            \n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Scalar Subtraction\")\n",
        "        \n",
        "        with AutogradGraph() as graph:\n",
        "            # Test division\n",
        "            x_custom = CustomTensor([8.0, 12.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = x_custom / 4.0\n",
        "            y_custom.tensor.backward(torch.ones_like(y_custom.tensor))\n",
        "            \n",
        "            x_pytorch = torch.tensor([8.0, 12.0], requires_grad=True)\n",
        "            y_pytorch = x_pytorch / 4.0\n",
        "            y_pytorch.backward(torch.ones_like(y_pytorch))\n",
        "            \n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Scalar Division\")\n",
        "    \n",
        "    def test_power_function(self):\n",
        "        \"\"\"Test power operation\"\"\"\n",
        "        print(\"\\n=== Testing Power Function ===\")\n",
        "        \n",
        "        with AutogradGraph() as graph:\n",
        "            x_custom = CustomTensor([2.0, 3.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = x_custom.pow(3.0)\n",
        "            y_custom.tensor.backward(torch.ones_like(y_custom.tensor))\n",
        "            \n",
        "            x_pytorch = torch.tensor([2.0, 3.0], requires_grad=True)\n",
        "            y_pytorch = torch.pow(x_pytorch, 3.0)\n",
        "            y_pytorch.backward(torch.ones_like(y_pytorch))\n",
        "            \n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Power Function\")\n",
        "    \n",
        "    def test_unary_functions(self):\n",
        "        \"\"\"Test unary mathematical functions\"\"\"\n",
        "        print(\"\\n=== Testing Unary Functions ===\")\n",
        "        \n",
        "        # Test exp\n",
        "        with AutogradGraph() as graph:\n",
        "            x_custom = CustomTensor([1.0, 2.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = x_custom.exp()\n",
        "            y_custom.tensor.backward(torch.ones_like(y_custom.tensor))\n",
        "            \n",
        "            x_pytorch = torch.tensor([1.0, 2.0], requires_grad=True)\n",
        "            y_pytorch = torch.exp(x_pytorch)\n",
        "            y_pytorch.backward(torch.ones_like(y_pytorch))\n",
        "            \n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Exponential Function\")\n",
        "        \n",
        "        # Test log\n",
        "        with AutogradGraph() as graph:\n",
        "            x_custom = CustomTensor([1.0, 2.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = x_custom.log()\n",
        "            y_custom.tensor.backward(torch.ones_like(y_custom.tensor))\n",
        "            \n",
        "            x_pytorch = torch.tensor([1.0, 2.0], requires_grad=True)\n",
        "            y_pytorch = torch.log(x_pytorch)\n",
        "            y_pytorch.backward(torch.ones_like(y_pytorch))\n",
        "            \n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Logarithm Function\")\n",
        "        \n",
        "        # Test sin\n",
        "        with AutogradGraph() as graph:\n",
        "            x_custom = CustomTensor([0.5, 1.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = x_custom.sin()\n",
        "            y_custom.tensor.backward(torch.ones_like(y_custom.tensor))\n",
        "            \n",
        "            x_pytorch = torch.tensor([0.5, 1.0], requires_grad=True)\n",
        "            y_pytorch = torch.sin(x_pytorch)\n",
        "            y_pytorch.backward(torch.ones_like(y_pytorch))\n",
        "            \n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Sine Function\")\n",
        "        \n",
        "        # Test sqrt\n",
        "        with AutogradGraph() as graph:\n",
        "            x_custom = CustomTensor([4.0, 9.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = x_custom.sqrt()\n",
        "            y_custom.tensor.backward(torch.ones_like(y_custom.tensor))\n",
        "            \n",
        "            x_pytorch = torch.tensor([4.0, 9.0], requires_grad=True)\n",
        "            y_pytorch = torch.sqrt(x_pytorch)\n",
        "            y_pytorch.backward(torch.ones_like(y_pytorch))\n",
        "            \n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Square Root Function\")\n",
        "    \n",
        "    def test_matrix_operations(self):\n",
        "        \"\"\"Test matrix operations\"\"\"\n",
        "        print(\"\\n=== Testing Matrix Operations ===\")\n",
        "        \n",
        "        # Test matrix multiplication\n",
        "        with AutogradGraph() as graph:\n",
        "            x_custom = CustomTensor([[1.0, 2.0], [3.0, 4.0]], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = CustomTensor([[5.0, 6.0], [7.0, 8.0]], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            z_custom = x_custom.matmul(y_custom)\n",
        "            z_custom.tensor.backward(torch.ones_like(z_custom.tensor))\n",
        "            \n",
        "            x_pytorch = torch.tensor([[1.0, 2.0], [3.0, 4.0]], requires_grad=True)\n",
        "            y_pytorch = torch.tensor([[5.0, 6.0], [7.0, 8.0]], requires_grad=True)\n",
        "            z_pytorch = torch.matmul(x_pytorch, y_pytorch)\n",
        "            z_pytorch.backward(torch.ones_like(z_pytorch))\n",
        "            \n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Matrix Multiplication - x\")\n",
        "            self.assert_tensors_close(y_custom, y_pytorch, \"Matrix Multiplication - y\")\n",
        "        \n",
        "        # Test dot product\n",
        "        with AutogradGraph() as graph:\n",
        "            x_custom = CustomTensor([1.0, 2.0, 3.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = CustomTensor([4.0, 5.0, 6.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            z_custom = x_custom.dot(y_custom)\n",
        "            z_custom.tensor.backward()\n",
        "            \n",
        "            x_pytorch = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
        "            y_pytorch = torch.tensor([4.0, 5.0, 6.0], requires_grad=True)\n",
        "            z_pytorch = torch.dot(x_pytorch, y_pytorch)\n",
        "            z_pytorch.backward()\n",
        "            \n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Dot Product - x\")\n",
        "            self.assert_tensors_close(y_custom, y_pytorch, \"Dot Product - y\")\n",
        "    \n",
        "    def test_complex_chain(self):\n",
        "        \"\"\"Test complex computational chains\"\"\"\n",
        "        print(\"\\n=== Testing Complex Chains ===\")\n",
        "        \n",
        "        with AutogradGraph() as graph:\n",
        "            # Test: z = (x + y) * (x - y) + x^2\n",
        "            x_custom = CustomTensor([3.0, 4.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = CustomTensor([1.0, 2.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            \n",
        "            sum_custom = x_custom + y_custom\n",
        "            diff_custom = x_custom - y_custom\n",
        "            prod_custom = sum_custom * diff_custom\n",
        "            x_squared_custom = x_custom.pow(2.0)\n",
        "            z_custom = prod_custom + x_squared_custom\n",
        "            \n",
        "            z_custom.tensor.backward(torch.ones_like(z_custom.tensor))\n",
        "            \n",
        "            x_pytorch = torch.tensor([3.0, 4.0], requires_grad=True)\n",
        "            y_pytorch = torch.tensor([1.0, 2.0], requires_grad=True)\n",
        "            \n",
        "            sum_pytorch = x_pytorch + y_pytorch\n",
        "            diff_pytorch = x_pytorch - y_pytorch\n",
        "            prod_pytorch = sum_pytorch * diff_pytorch\n",
        "            x_squared_pytorch = torch.pow(x_pytorch, 2.0)\n",
        "            z_pytorch = prod_pytorch + x_squared_pytorch\n",
        "            \n",
        "            z_pytorch.backward(torch.ones_like(z_pytorch))\n",
        "            \n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Complex Chain - x\")\n",
        "            self.assert_tensors_close(y_custom, y_pytorch, \"Complex Chain - y\")\n",
        "    \n",
        "    def test_mixed_operations(self):\n",
        "        \"\"\"Test mixing operations with and without gradients\"\"\"\n",
        "        print(\"\\n=== Testing Mixed Operations ===\")\n",
        "        \n",
        "        with AutogradGraph() as graph:\n",
        "            # One tensor requires grad, other doesn't\n",
        "            x_custom = CustomTensor([2.0, 3.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = CustomTensor([4.0, 5.0])  # No grad\n",
        "            z_custom = x_custom * y_custom\n",
        "            z_custom.tensor.backward(torch.ones_like(z_custom.tensor))\n",
        "            \n",
        "            x_pytorch = torch.tensor([2.0, 3.0], requires_grad=True)\n",
        "            y_pytorch = torch.tensor([4.0, 5.0])  # No grad\n",
        "            z_pytorch = x_pytorch * y_pytorch\n",
        "            z_pytorch.backward(torch.ones_like(z_pytorch))\n",
        "            \n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Mixed Operations - x\")\n",
        "    \n",
        "    def run_all_tests(self):\n",
        "        \"\"\"Run all tests\"\"\"\n",
        "        print(\"Running Custom Autograd Correctness Tests\")\n",
        "        print(\"=\" * 50)\n",
        "        \n",
        "        self.test_basic_operations()\n",
        "        self.test_multiplication()\n",
        "        self.test_subtraction_division()\n",
        "        self.test_power_function()\n",
        "        self.test_unary_functions()\n",
        "        self.test_matrix_operations()\n",
        "        self.test_complex_chain()\n",
        "        self.test_mixed_operations()\n",
        "        \n",
        "        print(f\"\\n\" + \"=\" * 50)\n",
        "        print(f\"Test Results: {self.passed_tests} passed, {self.failed_tests} failed\")\n",
        "        \n",
        "        if self.failed_tests == 0:\n",
        "            print(\"🎉 All tests passed! Your autograd implementation is correct.\")\n",
        "        else:\n",
        "            print(\"❌ Some tests failed. Check the implementation.\")\n",
        "        \n",
        "        return self.failed_tests == 0\n",
        "\n",
        "# Usage example:\n",
        "if __name__ == \"__main__\":\n",
        "    # Insert your AutogradGraph and CustomTensor classes here\n",
        "    # Then run the tests\n",
        "    \n",
        "    tester = AutogradTester()\n",
        "    success = tester.run_all_tests()\n",
        "    \n",
        "    # Additional manual verification\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"Manual Verification Example:\")\n",
        "    \n",
        "    with AutogradGraph() as graph:\n",
        "        x = CustomTensor([1.0, 2.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "        y = CustomTensor([3.0, 4.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "        \n",
        "        # Compute z = x^2 + 2*x*y + y^2 = (x + y)^2\n",
        "        z = x.pow(2.0) + (x * y * 2.0) + y.pow(2.0)\n",
        "        print(f\"z = {z.tensor}\")\n",
        "        \n",
        "        # Backward pass\n",
        "        z.tensor.backward(torch.ones_like(z.tensor))\n",
        "        \n",
        "        print(f\"dz/dx = {x.tensor.grad}\")  # Should be 2*(x + y)\n",
        "        print(f\"dz/dy = {y.tensor.grad}\")  # Should be 2*(x + y)\n",
        "        \n",
        "        # Expected: dz/dx = dz/dy = 2*(x + y) = 2*[4, 6] = [8, 12]\n",
        "        expected_grad = 2 * (x.tensor + y.tensor)\n",
        "        print(f\"Expected gradient: {expected_grad}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
