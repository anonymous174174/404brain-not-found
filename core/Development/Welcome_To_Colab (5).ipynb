{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1RPHatcftQx"
      },
      "source": [
        "# Tensor defination and AutoGradGraph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ALqKbplsyymO"
      },
      "outputs": [],
      "source": [
        "! pip install rustworkx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "7al82ATvfn7n"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "import weakref\n",
        "import numbers\n",
        "import rustworkx as rx\n",
        "import pytest\n",
        "\n",
        "class AutogradGraph:\n",
        "    __slots__ = ('graph', 'intermediate_tensors', '_check_cycles', '_auto_cleanup', '__weakref__')\n",
        "    def __init__(self, check_for_cycles=True, auto_cleanup=True):\n",
        "\n",
        "        self.graph = rx.PyDiGraph()\n",
        "        self.intermediate_tensors = {}\n",
        "        self._check_cycles = check_for_cycles\n",
        "        self._auto_cleanup = auto_cleanup\n",
        "\n",
        "    def __enter__(self):\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, exc_type, exc_value, traceback):\n",
        "        if self._check_cycles and self.check_cycle():\n",
        "            raise RuntimeError(\"Cycle detected in autograd graph on context exit.\")\n",
        "        if self._auto_cleanup:\n",
        "            self.intermediate_tensors.clear()\n",
        "            self.graph.clear()\n",
        "\n",
        "    def add_tensor_graph(self, tensor, is_leaf):\n",
        "        if not tensor._custom_requires_grad:\n",
        "            raise ValueError(\"Tensor with requires_grad=False cannot be added to the graph.\")\n",
        "\n",
        "        ref = tensor if is_leaf else weakref.proxy(tensor)\n",
        "        tensor_index = self.graph.add_node(ref)\n",
        "        tensor._node_id = tensor_index\n",
        "\n",
        "    def add_non_leaf_tensor_reference(self, tensor):\n",
        "        if not tensor._custom_requires_grad:\n",
        "            raise ValueError(\"Tensor must require grad.\")\n",
        "\n",
        "        if tensor._node_id in self.intermediate_tensors:\n",
        "            raise ValueError(\"Tensor reference already exists in intermediate tensors.\")\n",
        "\n",
        "        self.intermediate_tensors[tensor._node_id] = tensor\n",
        "\n",
        "    def add_edge(self, node_from, node_to, weight=None):\n",
        "        if not all(isinstance(n, int) for n in (node_from, node_to)):\n",
        "            raise TypeError(\"Node indices must be integers.\")\n",
        "        if not self.graph.has_node(node_from) or not self.graph.has_node(node_to):\n",
        "            raise ValueError(\"Nodes must exist before adding edge.\")\n",
        "        self.graph.add_edge(node_from, node_to, weight)\n",
        "\n",
        "    def check_cycle(self):\n",
        "        return not rx.is_directed_acyclic_graph(self.graph)\n",
        "\n",
        "    def reverse_toposort(self):\n",
        "        return [self.graph[n] for n in reversed(rx.topological_sort(self.graph))]\n",
        "\n",
        "    def reverse_toposort_from_tensor(self, tensor_index):\n",
        "        graph=self.graph\n",
        "        predecessors = list(rx.ancestors(graph, tensor_index))\n",
        "        predecessors.append(tensor_index)\n",
        "        sub_graph = graph.subgraph(predecessors)\n",
        "        return [sub_graph[i] for i in reversed(rx.topological_sort(sub_graph))]\n",
        "\n",
        "    # def alternative_reverse_toposort_from_tensor(self, tensor_index):\n",
        "    #     graph = self.graph\n",
        "    #     relevant_nodes = rx.ancestors(graph, tensor_index)\n",
        "    #     relevant_nodes.add(tensor_index)\n",
        "    #     full_topo = rx.topological_sort(graph)\n",
        "    #     relevant_topo = [graph[_node_id] for _node_id in reversed(full_topo) if _node_id in relevant_nodes]\n",
        "    #     return relevant_topo\n",
        "\n",
        "    def delete_node(self, node_index):\n",
        "        if not isinstance(node_index, int):\n",
        "            raise TypeError(\"Node index must be an integer.\")\n",
        "        if not self.graph.has_node(node_index):\n",
        "            raise ValueError(\"Node does not exist.\")\n",
        "        self.graph.remove_node(node_index)\n",
        "\n",
        "    def delete_edge(self, node_from, node_to):\n",
        "        if not self.graph.has_edge(node_from, node_to):\n",
        "            raise ValueError(\"Edge does not exist.\")\n",
        "        self.graph.remove_edge(node_from, node_to)\n",
        "\n",
        "    def del_non_leaf_tensor_reference(self, tensor_node_id):\n",
        "        self.intermediate_tensors.pop(tensor_node_id, None)\n",
        "    def delete_all_non_leaf_nodes(self):\n",
        "        # removes non leaf nodes from graph and clears the intermediate_tensors dict\n",
        "        self.graph.remove_nodes_from(list(self.intermediate_tensors.keys()))\n",
        "        self.intermediate_tensors.clear()\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"CustomAutogradGraph(nodes={self.graph.num_nodes()}, edges={self.graph.num_edges()})\"\n",
        "\n",
        "class CustomTensor:\n",
        "    __slots__ = ('tensor', '_node_id', '_custom_requires_grad', '_backward', 'graph', '__weakref__')\n",
        "\n",
        "    def __new__(cls, data, *, _custom_requires_grad=False, device=None, dtype=None, graph=None, due_to_operation=False, is_leaf=False):\n",
        "        if isinstance(data, CustomTensor):\n",
        "            return data  # Don't rewrap\n",
        "        return super().__new__(cls)\n",
        "\n",
        "    def __init__(self, data, *, _custom_requires_grad=False, device=None, dtype=None, graph=None, due_to_operation=False, is_leaf=False):\n",
        "        if isinstance(data, CustomTensor):\n",
        "            return\n",
        "\n",
        "        self.tensor = data if due_to_operation else torch.as_tensor(data, dtype=dtype, device=device)\n",
        "        self.tensor.requires_grad_(False)\n",
        "        self._custom_requires_grad = _custom_requires_grad\n",
        "        self._node_id = None\n",
        "        self._backward = lambda: None\n",
        "        self.graph = None\n",
        "\n",
        "        if _custom_requires_grad:\n",
        "            self._init_graph(graph, is_leaf)\n",
        "\n",
        "    def _init_graph(self, graph, is_leaf):\n",
        "        if graph is None:\n",
        "            raise ValueError(\"Graph must be provided if requires_grad is True.\")\n",
        "        if is_leaf:\n",
        "          self.graph = weakref.proxy(graph)\n",
        "        else:\n",
        "          self.graph = graph # this line is only reached for tensors which are created by operations and graph passed is already a weakreference hence no need for wrapping\n",
        "        graph.add_tensor_graph(self, is_leaf=is_leaf)\n",
        "        if not is_leaf:\n",
        "            graph.add_non_leaf_tensor_reference(self)\n",
        "\n",
        "    def _zero_grad(self):\n",
        "        self.tensor.grad = torch.zeros_like(self.tensor)\n",
        "\n",
        "    def __add__(self, other):\n",
        "        if isinstance(other, numbers.Number):\n",
        "            return self._add_scalar(other)#, op=torch.add)#Operations.add_tensor_and_scalar)\n",
        "        elif isinstance(other, CustomTensor):\n",
        "            return self._add_tensor(other)#, op=torch.add)#Operations.add_tensor_and_tensor)\n",
        "        return NotImplemented\n",
        "\n",
        "    def _add_scalar(self, scalar):\n",
        "        result_tensor = torch.add(self.tensor, scalar)\n",
        "        if not self._custom_requires_grad:\n",
        "            return CustomTensor(result_tensor)\n",
        "\n",
        "        graph = self.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "        graph.add_edge(self._node_id, result._node_id)\n",
        "\n",
        "        self_ref = weakref.proxy(self)\n",
        "        result_ref = weakref.proxy(result)\n",
        "        def _backward():\n",
        "            if self_ref.tensor.grad is None:\n",
        "                self_ref._zero_grad()\n",
        "            # print(f\"Backward for scalar add: result_grad={result.tensor.grad}, self_grad_before={self_ref.tensor.grad}\") # Debugging\n",
        "            self_ref.tensor.grad.add_(result_ref.tensor.grad)\n",
        "            # print(f\"Backward for scalar add: self_grad_after={self_ref.tensor.grad}\") # Debugging\n",
        "\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "\n",
        "    def _add_tensor(self, other):\n",
        "        result_tensor = torch.add(self.tensor, other.tensor)\n",
        "        requires_grad = self._custom_requires_grad or other._custom_requires_grad\n",
        "\n",
        "        if not requires_grad:\n",
        "            return CustomTensor(result_tensor)\n",
        "\n",
        "        # Graph selection logic - assuming operations happen within a single graph context\n",
        "        graph = None\n",
        "        if self._custom_requires_grad:\n",
        "            graph = self.graph\n",
        "        elif other._custom_requires_grad:\n",
        "            graph = other.graph\n",
        "        else:\n",
        "            # This case should ideally not be reached if requires_grad is True\n",
        "            # and at least one operand has requires_grad\n",
        "            pass # Or raise an error if graph is truly missing\n",
        "\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "\n",
        "        self_ref = weakref.proxy(self)\n",
        "        other_ref = weakref.proxy(other)\n",
        "\n",
        "        if self._custom_requires_grad:\n",
        "            graph.add_edge(self._node_id, result._node_id)\n",
        "        if other._custom_requires_grad:\n",
        "            graph.add_edge(other._node_id, result._node_id)\n",
        "        result_ref = weakref.proxy(result)\n",
        "        def _backward():\n",
        "            # print(f\"Backward for tensor add: result_grad={result.tensor.grad}\") # Debugging\n",
        "            if self._custom_requires_grad:\n",
        "                if self_ref.tensor.grad is None:\n",
        "                    self_ref._zero_grad()\n",
        "                self_ref.tensor.grad.add_(result_ref.tensor.grad)\n",
        "                # print(f\"  self_grad_after={self_ref.tensor.grad}\") # Debugging\n",
        "            if other._custom_requires_grad:\n",
        "                if other_ref.tensor.grad is None:\n",
        "                    other_ref._zero_grad()\n",
        "                other_ref.tensor.grad.add_(result_ref.tensor.grad)\n",
        "                # print(f\"  other_grad_after={other_ref.tensor.grad}\") # Debugging\n",
        "\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "    def __mul__(self, other):\n",
        "        if isinstance(other, numbers.Number):\n",
        "            return self._mul_scalar(other)\n",
        "        elif isinstance(other, CustomTensor):\n",
        "            return self._mul_tensor(other)\n",
        "        return NotImplemented\n",
        "\n",
        "    def _mul_scalar(self, scalar):\n",
        "        result_tensor = torch.mul(self.tensor, scalar)\n",
        "        if not self._custom_requires_grad:\n",
        "            return CustomTensor(result_tensor)\n",
        "\n",
        "        graph = self.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "        graph.add_edge(self._node_id, result._node_id)\n",
        "\n",
        "        self_ref = weakref.proxy(self)\n",
        "        result_ref = weakref.proxy(result)\n",
        "        def _backward():\n",
        "            if self_ref.tensor.grad is None:\n",
        "                self_ref._zero_grad()\n",
        "            self_ref.tensor.grad.add_(result_ref.tensor.grad * scalar)\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "\n",
        "    def _mul_tensor(self, other):\n",
        "        result_tensor = torch.mul(self.tensor, other.tensor)\n",
        "        requires_grad = self._custom_requires_grad or other._custom_requires_grad\n",
        "        if not requires_grad:\n",
        "            return CustomTensor(result_tensor)\n",
        "\n",
        "        graph = self.graph if self._custom_requires_grad else other.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "\n",
        "        self_ref = weakref.proxy(self)\n",
        "        other_ref = weakref.proxy(other)\n",
        "        result_ref = weakref.proxy(result)\n",
        "\n",
        "        if self._custom_requires_grad:\n",
        "            graph.add_edge(self._node_id, result._node_id)\n",
        "        if other._custom_requires_grad:\n",
        "            graph.add_edge(other._node_id, result._node_id)\n",
        "\n",
        "        def _backward():\n",
        "            if self._custom_requires_grad:\n",
        "                if self_ref.tensor.grad is None:\n",
        "                    self_ref._zero_grad()\n",
        "                self_ref.tensor.grad.add_(result_ref.tensor.grad * other_ref.tensor)\n",
        "            if other._custom_requires_grad:\n",
        "                if other_ref.tensor.grad is None:\n",
        "                    other_ref._zero_grad()\n",
        "                other_ref.tensor.grad.add_(result_ref.tensor.grad * self_ref.tensor)\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "\n",
        "\n",
        "    def __sub__(self, other):\n",
        "        if isinstance(other, numbers.Number):\n",
        "            return self._sub_scalar(other)\n",
        "        elif isinstance(other, CustomTensor):\n",
        "            return self._sub_tensor(other)\n",
        "        return NotImplemented\n",
        "\n",
        "    def _sub_scalar(self, scalar):\n",
        "        result_tensor = torch.sub(self.tensor, scalar)\n",
        "        if not self._custom_requires_grad:\n",
        "            return CustomTensor(result_tensor)\n",
        "\n",
        "        graph = self.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "        graph.add_edge(self._node_id, result._node_id)\n",
        "\n",
        "        self_ref = weakref.proxy(self)\n",
        "        result_ref = weakref.proxy(result)\n",
        "        def _backward():\n",
        "            if self_ref.tensor.grad is None:\n",
        "                self_ref._zero_grad()\n",
        "            self_ref.tensor.grad.add_(result_ref.tensor.grad)\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "\n",
        "    def _sub_tensor(self, other):\n",
        "        result_tensor = torch.sub(self.tensor, other.tensor)\n",
        "        requires_grad = self._custom_requires_grad or other._custom_requires_grad\n",
        "        if not requires_grad:\n",
        "            return CustomTensor(result_tensor)\n",
        "\n",
        "        graph = self.graph if self._custom_requires_grad else other.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "\n",
        "        self_ref = weakref.proxy(self)\n",
        "        other_ref = weakref.proxy(other)\n",
        "        result_ref = weakref.proxy(result)\n",
        "\n",
        "        if self._custom_requires_grad:\n",
        "            graph.add_edge(self._node_id, result._node_id)\n",
        "        if other._custom_requires_grad:\n",
        "            graph.add_edge(other._node_id, result._node_id)\n",
        "\n",
        "        def _backward():\n",
        "            if self._custom_requires_grad:\n",
        "                if self_ref.tensor.grad is None:\n",
        "                    self_ref._zero_grad()\n",
        "                self_ref.tensor.grad.add_(result_ref.tensor.grad)\n",
        "            if other._custom_requires_grad:\n",
        "                if other_ref.tensor.grad is None:\n",
        "                    other_ref._zero_grad()\n",
        "                other_ref.tensor.grad.sub_(result_ref.tensor.grad)\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "\n",
        "    def __truediv__(self, scalar):\n",
        "        return self._div_scalar(scalar)\n",
        "\n",
        "    def _div_scalar(self, scalar):\n",
        "        result_tensor = torch.div(self.tensor, scalar)\n",
        "        if not self._custom_requires_grad:\n",
        "            return CustomTensor(result_tensor)\n",
        "\n",
        "        graph = self.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "        graph.add_edge(self._node_id, result._node_id)\n",
        "\n",
        "        self_ref = weakref.proxy(self)\n",
        "        result_ref = weakref.proxy(result)\n",
        "        def _backward():\n",
        "            if self_ref.tensor.grad is None:\n",
        "                self_ref._zero_grad()\n",
        "            self_ref.tensor.grad.add_(result_ref.tensor.grad / scalar)\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "\n",
        "\n",
        "    def pow(self, scalar):\n",
        "        result_tensor = torch.pow(self.tensor, scalar)\n",
        "        if not self._custom_requires_grad:\n",
        "            return CustomTensor(result_tensor)\n",
        "\n",
        "        graph = self.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "        graph.add_edge(self._node_id, result._node_id)\n",
        "\n",
        "        self_ref = weakref.proxy(self)\n",
        "        result_ref = weakref.proxy(result)\n",
        "        def _backward():\n",
        "            if self_ref.tensor.grad is None:\n",
        "                self_ref._zero_grad()\n",
        "            grad_contrib = scalar * self_ref.tensor.pow(scalar - 1)\n",
        "            self_ref.tensor.grad.add_(result_ref.tensor.grad * grad_contrib)\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "\n",
        "    def exp(self):\n",
        "        out = torch.exp(self.tensor)\n",
        "        return self._unary_op(out, lambda grad, out_tensor: grad * out_tensor)\n",
        "\n",
        "    def log(self):\n",
        "        out = torch.log(self.tensor)\n",
        "        return self._unary_op(out, lambda grad, input_tensor: grad / input_tensor)\n",
        "\n",
        "    def sin(self):\n",
        "        out = torch.sin(self.tensor)\n",
        "        return self._unary_op(out, lambda grad, input_tensor: grad * torch.cos(input_tensor))\n",
        "\n",
        "    def cos(self):\n",
        "        out = torch.cos(self.tensor)\n",
        "        return self._unary_op(out, lambda grad, input_tensor: -grad * torch.sin(input_tensor))\n",
        "\n",
        "    def sqrt(self):\n",
        "        out = torch.sqrt(self.tensor)\n",
        "        return self._unary_op(out, lambda grad, out_tensor: grad * 0.5 / out_tensor)\n",
        "\n",
        "    def _unary_op(self, result_tensor, backward_fn):\n",
        "        if not self._custom_requires_grad:\n",
        "            return CustomTensor(result_tensor)\n",
        "\n",
        "        graph = self.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "        graph.add_edge(self._node_id, result._node_id)\n",
        "\n",
        "        self_ref = weakref.proxy(self)\n",
        "        result_ref = weakref.proxy(result)\n",
        "        def _backward():\n",
        "            if self_ref.tensor.grad is None:\n",
        "                self_ref._zero_grad()\n",
        "            self_ref.tensor.grad.add_(backward_fn(result_ref.tensor.grad, self_ref.tensor))\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "\n",
        "    def matmul(self, other):\n",
        "        result_tensor = torch.matmul(self.tensor, other.tensor)\n",
        "        requires_grad = self._custom_requires_grad or other._custom_requires_grad\n",
        "        if not requires_grad:\n",
        "            return CustomTensor(result_tensor)\n",
        "\n",
        "        graph = self.graph if self._custom_requires_grad else other.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "\n",
        "        self_ref = weakref.proxy(self)\n",
        "        other_ref = weakref.proxy(other)\n",
        "        result_ref = weakref.proxy(result)\n",
        "\n",
        "        if self._custom_requires_grad:\n",
        "            graph.add_edge(self._node_id, result._node_id)\n",
        "        if other._custom_requires_grad:\n",
        "            graph.add_edge(other._node_id, result._node_id)\n",
        "\n",
        "        def _backward():\n",
        "            if self._custom_requires_grad:\n",
        "                if self_ref.tensor.grad is None:\n",
        "                    self_ref._zero_grad()\n",
        "                self_ref.tensor.grad.add_(torch.matmul(result_ref.tensor.grad, other_ref.tensor.t()))\n",
        "            if other._custom_requires_grad:\n",
        "                if other_ref.tensor.grad is None:\n",
        "                    other_ref._zero_grad()\n",
        "                other_ref.tensor.grad.add_(torch.matmul(self_ref.tensor.t(), result_ref.tensor.grad))\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "\n",
        "    def apply_mask(self, mask):\n",
        "        result_tensor = self.tensor * mask.tensor\n",
        "        requires_grad = self._custom_requires_grad or mask._custom_requires_grad\n",
        "        if not requires_grad:\n",
        "            return CustomTensor(result_tensor)\n",
        "\n",
        "        graph = self.graph if self._custom_requires_grad else mask.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "\n",
        "        self_ref = weakref.proxy(self)\n",
        "        mask_ref = weakref.proxy(mask)\n",
        "        result_ref = weakref.proxy(result)\n",
        "\n",
        "        if self._custom_requires_grad:\n",
        "            graph.add_edge(self._node_id, result._node_id)\n",
        "        if mask._custom_requires_grad:\n",
        "            graph.add_edge(mask._node_id, result._node_id)\n",
        "\n",
        "        def _backward():\n",
        "            if self._custom_requires_grad:\n",
        "                if self_ref.tensor.grad is None:\n",
        "                    self_ref._zero_grad()\n",
        "                self_ref.tensor.grad.add_(result_ref.tensor.grad * mask_ref.tensor)\n",
        "            if mask._custom_requires_grad:\n",
        "                if mask_ref.tensor.grad is None:\n",
        "                    mask_ref._zero_grad()\n",
        "                mask_ref.tensor.grad.add_(result_ref.tensor.grad * self_ref.tensor)\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "\n",
        "    def dot(self, other):\n",
        "        result_tensor = torch.dot(self.tensor, other.tensor)\n",
        "        requires_grad = self._custom_requires_grad or other._custom_requires_grad\n",
        "        if not requires_grad:\n",
        "            return CustomTensor(result_tensor)\n",
        "\n",
        "        graph = self.graph if self._custom_requires_grad else other.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "\n",
        "        self_ref = weakref.proxy(self)\n",
        "        other_ref = weakref.proxy(other)\n",
        "        result_ref = weakref.proxy(result)\n",
        "\n",
        "        if self._custom_requires_grad:\n",
        "            graph.add_edge(self._node_id, result._node_id)\n",
        "        if other._custom_requires_grad:\n",
        "            graph.add_edge(other._node_id, result._node_id)\n",
        "\n",
        "        def _backward():\n",
        "            if self._custom_requires_grad:\n",
        "                if self_ref.tensor.grad is None:\n",
        "                    self_ref._zero_grad()\n",
        "                self_ref.tensor.grad.add_(result_ref.tensor.grad * other_ref.tensor)\n",
        "            if other._custom_requires_grad:\n",
        "                if other_ref.tensor.grad is None:\n",
        "                    other_ref._zero_grad()\n",
        "                other_ref.tensor.grad.add_(result_ref.tensor.grad * self_ref.tensor)\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "    def backward(self,weightage_tensor=1):\n",
        "        if not self._custom_requires_grad:\n",
        "            raise RuntimeError(\"Output tensor does not require grad.\")\n",
        "        if self.graph is None:\n",
        "            raise RuntimeError(\"Output tensor is not part of a graph.\")\n",
        "        graph = self.graph\n",
        "\n",
        "        # Initialize gradient for the output tensor\n",
        "        if isinstance(weightage_tensor,numbers.Number):\n",
        "            self.tensor.grad = torch.full_like(self.tensor, fill_value=weightage_tensor)\n",
        "        elif isinstance(weightage_tensor,torch.Tensor):\n",
        "            self.tensor.grad = weightage_tensor.clone() # we don't want to modify the original tensor data\n",
        "\n",
        "        # Perform backward pass using topological sort\n",
        "\n",
        "        nodes_to_process = graph.reverse_toposort_from_tensor(self._node_id)\n",
        "\n",
        "        # Create a strong reference to intermediate tensors needed for backward pass\n",
        "        # This simulates how a real autograd engine would keep track of them\n",
        "        # The graph context's intermediate_tensors dict already serves this purpose.\n",
        "\n",
        "        for tensor_node in nodes_to_process:\n",
        "            # Check if the weak proxy is still valid (tensor is alive)\n",
        "            if isinstance(tensor_node, weakref.ProxyTypes) and tensor_node.__slots__ is None:\n",
        "                # print(f\"Skipping dead proxy: {tensor_node}\") # Debugging\n",
        "                continue # Skip if the weak reference is dead\n",
        "\n",
        "            if tensor_node.tensor.grad is None and tensor_node is not self.tensor:\n",
        "                # This can happen if a tensor is part of the graph but its grad hasn't been set yet\n",
        "                # and it's not the root of the backward call. This typically means it's a leaf\n",
        "                # that wasn't used to compute the output or an intermediate that accumulated no grad.\n",
        "                # For simplicity in this test, we assume grads propagate.\n",
        "                # print(f\"Warning: Tensor node {tensor_node._node_id} has no grad before _backward call.\")\n",
        "                pass # A no-op for now. In a real system, you might want to handle this.\n",
        "\n",
        "            # Ensure that non-leaf tensors are still alive when their _backward is called\n",
        "            # The `intermediate_tensors` in `AutogradGraph` should keep them alive.\n",
        "            tensor_node._backward()\n",
        "\n",
        "        # Clean up intermediate tensors references after backward pass\n",
        "        # This would typically be handled by the graph context's exit, but\n",
        "        # if `_auto_cleanup` is False, you might need manual cleanup.\n",
        "        # Here, for testing GC, we'll let the context manager handle it.\n",
        "\n",
        "\n",
        "\n",
        "    def __del__(self):\n",
        "      print(\"Garbage Collector has decided that reference counts are zero so Goodbye!!\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OdyuwaBf0FK"
      },
      "source": [
        "# testing strong/ weak ref memory leakage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "yoc_uwwwf4Xt"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import gc\n",
        "class TestCustomAutogradSystem:\n",
        "\n",
        "    def test_basic_add_scalar_grad(self):\n",
        "        with AutogradGraph() as graph:\n",
        "            a = CustomTensor(torch.tensor([2.0, 3.0]), _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            b = a + 5.0 # (a + 5)\n",
        "            c = b + 10.0 # (a + 5 + 10)\n",
        "\n",
        "            # Manually run backward pass\n",
        "            c.backward(weightage_tensor=1)\n",
        "\n",
        "            # Expected gradients:\n",
        "            # dC/dA = 1.0 (for each element)\n",
        "            assert torch.allclose(a.tensor.grad, torch.tensor([1.0, 1.0]))\n",
        "            assert b.tensor.grad is not None\n",
        "            assert torch.allclose(b.tensor.grad, torch.tensor([1.0, 1.0])) # dC/dB = 1.0\n",
        "\n",
        "            # Verify graph structure\n",
        "            assert graph.graph.num_nodes() == 3\n",
        "            assert graph.graph.num_edges() == 2\n",
        "            assert graph.graph.has_edge(a._node_id, b._node_id)\n",
        "            assert graph.graph.has_edge(b._node_id, c._node_id)\n",
        "            assert graph.check_cycle() is False\n",
        "\n",
        "    def test_basic_add_tensor_grad(self):\n",
        "        with AutogradGraph() as graph:\n",
        "            a = CustomTensor(torch.tensor([2.0, 3.0]), _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            b = CustomTensor(torch.tensor([1.0, 2.0]), _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            c = a + b # (a + b)\n",
        "            d = c + 5.0 # (a + b + 5)\n",
        "\n",
        "            d.backward(weightage_tensor=1)\n",
        "\n",
        "            # Expected gradients:\n",
        "            # dD/dA = 1.0\n",
        "            # dD/dB = 1.0\n",
        "            assert torch.allclose(a.tensor.grad, torch.tensor([1.0, 1.0]))\n",
        "            assert torch.allclose(b.tensor.grad, torch.tensor([1.0, 1.0]))\n",
        "\n",
        "            # Verify graph structure\n",
        "            assert graph.graph.num_nodes() == 4\n",
        "            assert graph.graph.num_edges() == 3\n",
        "            assert graph.graph.has_edge(a._node_id, c._node_id)\n",
        "            assert graph.graph.has_edge(b._node_id, c._node_id)\n",
        "            assert graph.graph.has_edge(c._node_id, d._node_id)\n",
        "            assert graph.check_cycle() is False\n",
        "\n",
        "    def test_mixed_requires_grad_tensor_add(self):\n",
        "        with AutogradGraph() as graph:\n",
        "            a = CustomTensor(torch.tensor([2.0, 3.0]), _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            b = CustomTensor(torch.tensor([1.0, 2.0]), _custom_requires_grad=False) # Does not require grad\n",
        "            c = a + b # c should require grad, b's grad should be None\n",
        "\n",
        "            c.backward(weightage_tensor=1)\n",
        "\n",
        "            assert torch.allclose(a.tensor.grad, torch.tensor([1.0, 1.0]))\n",
        "            assert b.tensor.grad is None # b should not have a grad\n",
        "            assert c._custom_requires_grad is True\n",
        "\n",
        "            # Verify graph structure\n",
        "            assert graph.graph.num_nodes() == 2 # Only a and c in the graph\n",
        "            assert graph.graph.num_edges() == 1\n",
        "            assert graph.graph.has_node(a._node_id)\n",
        "            assert graph.graph.has_node(c._node_id)\n",
        "            assert graph.graph.has_edge(a._node_id, c._node_id)\n",
        "            #assert not graph.graph.has_node(b._node_id) # b should not be in graph\n",
        "\n",
        "    def test_no_requires_grad(self):\n",
        "        with AutogradGraph() as graph: # Graph created, but no tensors with requires_grad=True added\n",
        "            a = CustomTensor(torch.tensor([1.0]))\n",
        "            b = CustomTensor(torch.tensor([2.0]))\n",
        "            c = a + b\n",
        "            d = c + 3.0\n",
        "\n",
        "            assert not a._custom_requires_grad\n",
        "            assert not b._custom_requires_grad\n",
        "            assert not c._custom_requires_grad\n",
        "            assert not d._custom_requires_grad\n",
        "            assert graph.graph.num_nodes() == 0 # Graph should remain empty\n",
        "            assert graph.graph.num_edges() == 0\n",
        "\n",
        "            with pytest.raises(RuntimeError, match=\"Output tensor does not require grad.\"):\n",
        "                d.backward(weightage_tensor=1)\n",
        "\n",
        "    def test_autograd_graph_context_manager(self):\n",
        "        graph = None\n",
        "        with AutogradGraph(check_for_cycles=True, auto_cleanup=True) as g:\n",
        "            graph = g\n",
        "            a = CustomTensor(torch.tensor([1.0]), _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            b = a + 1.0\n",
        "            assert graph.graph.num_nodes() == 2\n",
        "            assert graph.graph.num_edges() == 1\n",
        "            assert len(graph.intermediate_tensors) == 1 # b should be in intermediate_tensors\n",
        "\n",
        "        # After exiting the context, graph should be empty\n",
        "        assert graph.graph.num_nodes() == 0\n",
        "        assert graph.graph.num_edges() == 0\n",
        "        assert len(graph.intermediate_tensors) == 0\n",
        "\n",
        "    def test_cycle_detection(self):\n",
        "      try:\n",
        "        with AutogradGraph(check_for_cycles=True, auto_cleanup=False) as graph: # auto_cleanup=False to inspect after error\n",
        "            a = CustomTensor(torch.tensor([1.0]), _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            b = CustomTensor(torch.tensor([2.0]), _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "\n",
        "            # Manually create a cycle (a -> b -> a)\n",
        "            graph.add_edge(a._node_id, b._node_id)\n",
        "            graph.add_edge(b._node_id, a._node_id)\n",
        "      except RuntimeError as e:\n",
        "        print(f\"Raised the error of cycle detected as {e}\")\n",
        "            # with pytest.raises(RuntimeError, match=\"Cycle detected in autograd graph on context exit.\"):\n",
        "            #     pass # The __exit__ method will be called here\n",
        "\n",
        "    def test_no_circular_references_non_leaf_tensors_die(self):\n",
        "          # This test relies on the garbage collector. It's a heuristic test\n",
        "        # as Python's GC timing is not strictly deterministic.\n",
        "        # However, with weakrefs, it should work for non-leaf tensors.\n",
        "\n",
        "      print(\"\\n--- Starting GC Test: No Circular References (Part 1) ---\")\n",
        "\n",
        "      graph_ref = None\n",
        "      output_tensor_weak_ref = None\n",
        "      node_id_d = -1 # To store node_id before d is deleted\n",
        "\n",
        "      # BLOCK 1: Create graph and tensors\n",
        "      with AutogradGraph(auto_cleanup=False) as graph: # Keep graph for inspection\n",
        "          graph_ref = weakref.ref(graph)\n",
        "          a = CustomTensor(torch.tensor([1.0]), _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "          b = a + 1.0 # Intermediate tensor\n",
        "          c = b + 2.0 # Intermediate tensor\n",
        "          d = c + 3.0 # Output tensor (also intermediate from graph's perspective)\n",
        "\n",
        "          # Store weak reference to 'd' BEFORE its strong reference is potentially removed\n",
        "          output_tensor_weak_ref = weakref.ref(d)\n",
        "          node_id_d = d._node_id # Store node_id while d is alive\n",
        "\n",
        "          print(f\"Initial: d object: {d}\")\n",
        "          print(f\"Initial: d._node_id: {node_id_d}\")\n",
        "          print(f\"Initial: graph.intermediate_tensors keys: {list(graph.intermediate_tensors.keys())}\")\n",
        "          # The ref count for `d` object itself will be high here because it's in `graph.intermediate_tensors`,\n",
        "          # and held by variable `d`, and by the temporary ref in `getrefcount`.\n",
        "          print(f\"Initial: refcount of d (via output_tensor_weak_ref.test_ref): {sys.getrefcount(output_tensor_weak_ref()) if output_tensor_weak_ref() else 'N/A'}\")\n",
        "          assert len(graph.intermediate_tensors) == 3 # b, c, d should be in intermediate_tensors\n",
        "\n",
        "      # BLOCK 2: After exiting context manager (auto_cleanup=False)\n",
        "      print(\"\\n--- After exiting 'with' block (auto_cleanup=False) ---\")\n",
        "      # The 'graph' variable still holds a strong reference to the AutogradGraph instance.\n",
        "      # graph_ref() should return the graph object.\n",
        "      assert graph_ref() is not None, \"Graph object should still be alive.\"\n",
        "      assert len(graph_ref().intermediate_tensors) == 3, \"Intermediate tensors should still be referenced by the graph.\"\n",
        "      print(f\"After 'with' block: d object (via weakref): {output_tensor_weak_ref()}\")\n",
        "      print(f\"After 'with' block: refcount of d (via output_tensor_weak_ref.test_ref): {sys.getrefcount(output_tensor_weak_ref())}\")\n",
        "\n",
        "      # BLOCK 3: Remove strong reference 'd' from local scope\n",
        "      print(\"\\n--- Deleting 'd' variable ---\")\n",
        "      del d # Remove the local strong reference to the CustomTensor object.\n",
        "      gc.collect() # Force garbage collection\n",
        "\n",
        "      # Now, output_tensor_weak_ref() *still* shouldn't be None because `graph_ref().intermediate_tensors`\n",
        "      # holds the strong reference.\n",
        "      print(f\"After del d + gc.collect(): d object (via weakref): {output_tensor_weak_ref()}\")\n",
        "      # We expect this to *not* be None yet, and to still show a refcount reflecting intermediate_tensors.\n",
        "      assert output_tensor_weak_ref() is not None, \"d should still be alive due to intermediate_tensors.\"\n",
        "      current_d_refcount_after_del_d = sys.getrefcount(output_tensor_weak_ref()) if output_tensor_weak_ref() else 'N/A'\n",
        "      print(f\"After del d + gc.collect(): refcount of d: {current_d_refcount_after_del_d}\")\n",
        "      # Expected refcount should be 2: one from intermediate_tensors, one from getrefcount()\n",
        "      assert current_d_refcount_after_del_d == 2, f\"Expected refcount 2, got {current_d_refcount_after_del_d}\"\n",
        "\n",
        "      # BLOCK 4: Remove strong reference from intermediate_tensors\n",
        "      print(f\"\\n--- Deleting strong reference from graph.intermediate_tensors for node {node_id_d} ---\")\n",
        "      graph_ref().del_non_leaf_tensor_reference(node_id_d) # THIS IS THE CRUCIAL STEP\n",
        "      print(f\"After del_non_leaf_tensor_reference: graph.intermediate_tensors keys: {list(graph_ref().intermediate_tensors.keys())}\")\n",
        "      #gc.collect() # Force garbage collection again\n",
        "\n",
        "      # Now, with the last strong reference gone, 'd' should be garbage collected.\n",
        "      print(f\"After del_non_leaf_tensor_reference + gc.collect(): d object (via weakref): {output_tensor_weak_ref()}\")\n",
        "      # This is where your original assertion was. It *should* pass now.\n",
        "      assert output_tensor_weak_ref() is None, \"Output tensor (non-leaf) should be garbage collected after its strong reference is deleted from intermediate_tensors.\"\n",
        "      print(\"Assertion Passed: Output tensor (d) was garbage collected.\")\n",
        "\n",
        "      # BLOCK 5: Verify other intermediate tensors are collected when graph is cleared\n",
        "      print(\"\\n--- Starting GC Test: All Intermediate Tensors ---\")\n",
        "      intermediate_tensors_wrefs = []\n",
        "      # Create a new graph and new tensors to avoid interference from previous block\n",
        "      with AutogradGraph(auto_cleanup=False) as graph_new:\n",
        "          a_new = CustomTensor(torch.tensor([1.0]), _custom_requires_grad=True, graph=graph_new, is_leaf=True)\n",
        "          b_new = a_new + 1.0 # Intermediate\n",
        "          c_new = b_new + 2.0 # Intermediate\n",
        "          d_new = c_new + 3.0 # Intermediate (output of a chain)\n",
        "\n",
        "          # Store weak references to the intermediate tensors\n",
        "          intermediate_tensors_wrefs.append(weakref.ref(b_new))\n",
        "          intermediate_tensors_wrefs.append(weakref.ref(c_new))\n",
        "          intermediate_tensors_wrefs.append(weakref.ref(d_new))\n",
        "\n",
        "          # Verify they are initially alive\n",
        "          assert all(wref() is not None for wref in intermediate_tensors_wrefs)\n",
        "          assert len(graph_new.intermediate_tensors) == 3\n",
        "\n",
        "      print(f\"After 'with' block (new graph): graph_new object: {graph_new}\")\n",
        "      assert graph_new is not None, \"New graph object should still be alive after 'with' block.\"\n",
        "      assert len(graph_new.intermediate_tensors) == 3, \"New graph intermediate_tensors should still hold refs.\"\n",
        "\n",
        "      # Manually clear the intermediate_tensors dictionary and remove graph reference\n",
        "      print(\"\\n--- Manually clearing graph.intermediate_tensors and deleting graph ---\")\n",
        "      graph_new.intermediate_tensors.clear()\n",
        "      del graph_new # Remove the strong reference to the graph itself\n",
        "      del b_new , c_new , d_new # deleting the local variable strong references\n",
        "      #gc.collect()\n",
        "\n",
        "      # Now, all non-leaf tensors should be garbage collected\n",
        "      for i, wref in enumerate(intermediate_tensors_wrefs):\n",
        "          print(f\"Intermediate tensor {i} (via weakref): {wref()}\")\n",
        "          assert wref() is None, f\"Intermediate tensor {i} should be garbage collected after graph context and intermediate_tensors are cleared.\"\n",
        "      print(\"Assertion Passed: All intermediate tensors were garbage collected.\")\n",
        "\n",
        "    def test_topological_sort_order(self):\n",
        "        with AutogradGraph() as graph:\n",
        "            t1 = CustomTensor(torch.tensor([1.0]), _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            t2 = CustomTensor(torch.tensor([2.0]), _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            t3 = t1 + t2\n",
        "            t4 = t3 + 5.0\n",
        "            t5 = t2 + 10.0 # Another branch\n",
        "            t6 = t4 + t5\n",
        "\n",
        "            # The topological sort should produce an order where dependencies come before their dependents.\n",
        "            # Reversed topological sort should produce an order where outputs come before their inputs.\n",
        "            # Example expected order: t6, t4, t5, t3, t2, t1 (or variations respecting dependencies)\n",
        "            sorted_tensors = graph.reverse_toposort()\n",
        "\n",
        "            # Check if dependencies are respected in reverse order\n",
        "            # If A -> B, then B should appear before A in reverse topological sort.\n",
        "            # t6 depends on t4, t5. So t6 should be before t4 and t5.\n",
        "            # t4 depends on t3. So t4 should be before t3.\n",
        "            # t5 depends on t2. So t5 should be before t2.\n",
        "            # t3 depends on t1, t2. So t3 should be before t1 and t2.\n",
        "\n",
        "            # Simple check: The first element should be t6 (the ultimate output).\n",
        "            assert sorted_tensors[0] is t6\n",
        "\n",
        "            # Check positions:\n",
        "            pos = {t: i for i, t in enumerate(sorted_tensors)}\n",
        "\n",
        "            assert pos[t6] < pos[t4]\n",
        "            assert pos[t6] < pos[t5]\n",
        "            assert pos[t4] < pos[t3]\n",
        "            assert pos[t5] < pos[t2]\n",
        "            assert pos[t3] < pos[t1]\n",
        "            assert pos[t3] < pos[t2] # t3 also depends on t2\n",
        "\n",
        "            # Additional check: t2 is a dependency for both t3 and t5.\n",
        "            # In reverse topo sort, t3 and t5 must appear before t2.\n",
        "            assert pos[t3] < pos[t2]\n",
        "            assert pos[t5] < pos[t2]\n",
        "\n",
        "            # t1 is only a dependency for t3.\n",
        "            assert pos[t3] < pos[t1]\n",
        "\n",
        "            # Check if all 6 tensors are in the sorted list\n",
        "            assert len(sorted_tensors) == 6\n",
        "            assert set(sorted_tensors) == {t1, t2, t3, t4, t5, t6}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "S3VwFluwySi9"
      },
      "outputs": [],
      "source": [
        "k=TestCustomAutogradSystem()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dg5jv4GSyUM-",
        "outputId": "e8340cf4-59a6-4014-d9a2-06e59e2544fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Garbage Collector has decided that reference counts are zero so Goodbye!!\n",
            "Garbage Collector has decided that reference counts are zero so Goodbye!!\n",
            "Garbage Collector has decided that reference counts are zero so Goodbye!!\n"
          ]
        }
      ],
      "source": [
        "k.test_basic_add_scalar_grad()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aVnpNgeyzg49",
        "outputId": "db8e835d-e60a-443f-8e09-5a38e105fe33"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Garbage Collector has decided that reference counts are zero so Goodbye!!\n",
            "Garbage Collector has decided that reference counts are zero so Goodbye!!\n",
            "Garbage Collector has decided that reference counts are zero so Goodbye!!\n",
            "Garbage Collector has decided that reference counts are zero so Goodbye!!\n"
          ]
        }
      ],
      "source": [
        "k.test_basic_add_tensor_grad()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Qpn8ERgzit0",
        "outputId": "6b6b359c-84c7-4a64-f8a7-08a7feae5a92"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Garbage Collector has decided that reference counts are zero so Goodbye!!\n",
            "Garbage Collector has decided that reference counts are zero so Goodbye!!\n",
            "Garbage Collector has decided that reference counts are zero so Goodbye!!\n"
          ]
        }
      ],
      "source": [
        "k.test_mixed_requires_grad_tensor_add()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TB1W2cUZzkWV",
        "outputId": "cc6d1b6f-3184-4359-df2b-e3b29e8f0ab5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Garbage Collector has decided that reference counts are zero so Goodbye!!\n",
            "Garbage Collector has decided that reference counts are zero so Goodbye!!\n",
            "Garbage Collector has decided that reference counts are zero so Goodbye!!\n",
            "Garbage Collector has decided that reference counts are zero so Goodbye!!\n"
          ]
        }
      ],
      "source": [
        "k.test_no_requires_grad()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FWSiM7BBzl0k",
        "outputId": "6ef6ef97-0b54-4043-9b77-739994e00645"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Garbage Collector has decided that reference counts are zero so Goodbye!!\n",
            "Garbage Collector has decided that reference counts are zero so Goodbye!!\n"
          ]
        }
      ],
      "source": [
        "k.test_autograd_graph_context_manager()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0tEIQY73zndG",
        "outputId": "6d7029ee-9612-44dd-e5fd-db53de4676f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Raised the error of cycle detected as Cycle detected in autograd graph on context exit.\n",
            "Garbage Collector has decided that reference counts are zero so Goodbye!!\n",
            "Garbage Collector has decided that reference counts are zero so Goodbye!!\n"
          ]
        }
      ],
      "source": [
        "k.test_cycle_detection()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zcT5elQhzwPW",
        "outputId": "5e47c355-c88f-4e44-e26a-d17037c71872"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Starting GC Test: No Circular References (Part 1) ---\n",
            "Initial: d object: <__main__.CustomTensor object at 0x7e0af05bf740>\n",
            "Initial: d._node_id: 3\n",
            "Initial: graph.intermediate_tensors keys: [1, 2, 3]\n",
            "Initial: refcount of d (via output_tensor_weak_ref.test_ref): 3\n",
            "\n",
            "--- After exiting 'with' block (auto_cleanup=False) ---\n",
            "After 'with' block: d object (via weakref): <__main__.CustomTensor object at 0x7e0af05bf740>\n",
            "After 'with' block: refcount of d (via output_tensor_weak_ref.test_ref): 3\n",
            "\n",
            "--- Deleting 'd' variable ---\n",
            "Garbage Collector has decided that reference counts are zero so Goodbye!!\n",
            "Garbage Collector has decided that reference counts are zero so Goodbye!!\n",
            "Garbage Collector has decided that reference counts are zero so Goodbye!!\n",
            "Garbage Collector has decided that reference counts are zero so Goodbye!!\n",
            "After del d + gc.collect(): d object (via weakref): <__main__.CustomTensor object at 0x7e0af05bf740>\n",
            "After del d + gc.collect(): refcount of d: 2\n",
            "\n",
            "--- Deleting strong reference from graph.intermediate_tensors for node 3 ---\n",
            "Garbage Collector has decided that reference counts are zero so Goodbye!!\n",
            "After del_non_leaf_tensor_reference: graph.intermediate_tensors keys: [1, 2]\n",
            "After del_non_leaf_tensor_reference + gc.collect(): d object (via weakref): None\n",
            "Assertion Passed: Output tensor (d) was garbage collected.\n",
            "\n",
            "--- Starting GC Test: All Intermediate Tensors ---\n",
            "After 'with' block (new graph): graph_new object: CustomAutogradGraph(nodes=4, edges=3)\n",
            "\n",
            "--- Manually clearing graph.intermediate_tensors and deleting graph ---\n",
            "Garbage Collector has decided that reference counts are zero so Goodbye!!\n",
            "Garbage Collector has decided that reference counts are zero so Goodbye!!\n",
            "Garbage Collector has decided that reference counts are zero so Goodbye!!\n",
            "Intermediate tensor 0 (via weakref): None\n",
            "Intermediate tensor 1 (via weakref): None\n",
            "Intermediate tensor 2 (via weakref): None\n",
            "Assertion Passed: All intermediate tensors were garbage collected.\n",
            "Garbage Collector has decided that reference counts are zero so Goodbye!!\n",
            "Garbage Collector has decided that reference counts are zero so Goodbye!!\n",
            "Garbage Collector has decided that reference counts are zero so Goodbye!!\n",
            "Garbage Collector has decided that reference counts are zero so Goodbye!!\n"
          ]
        }
      ],
      "source": [
        "k.test_no_circular_references_non_leaf_tensors_die()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8nEVQKzhQXU",
        "outputId": "6456ebbc-dc86-4a79-d7ff-5c0703da5e05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Garbage Collector has decided that reference counts are zero so Goodbye!!\n",
            "Garbage Collector has decided that reference counts are zero so Goodbye!!\n",
            "Garbage Collector has decided that reference counts are zero so Goodbye!!\n",
            "Garbage Collector has decided that reference counts are zero so Goodbye!!\n",
            "Garbage Collector has decided that reference counts are zero so Goodbye!!\n",
            "Garbage Collector has decided that reference counts are zero so Goodbye!!\n",
            "Garbage Collector has decided that reference counts are zero so Goodbye!!\n",
            "Garbage Collector has decided that reference counts are zero so Goodbye!!\n",
            "Garbage Collector has decided that reference counts are zero so Goodbye!!\n",
            "Garbage Collector has decided that reference counts are zero so Goodbye!!\n",
            "Garbage Collector has decided that reference counts are zero so Goodbye!!\n",
            "Garbage Collector has decided that reference counts are zero so Goodbye!!\n",
            "Garbage Collector has decided that reference counts are zero so Goodbye!!\n",
            "Garbage Collector has decided that reference counts are zero so Goodbye!!\n",
            "Garbage Collector has decided that reference counts are zero so Goodbye!!\n",
            "Garbage Collector has decided that reference counts are zero so Goodbye!!\n",
            "Raised the error of cycle detected as Cycle detected in autograd graph on context exit.\n",
            "Garbage Collector has decided that reference counts are zero so Goodbye!!\n",
            "Garbage Collector has decided that reference counts are zero so Goodbye!!\n",
            "\n",
            "--- Starting GC Test: No Circular References (Part 1) ---\n",
            "Initial: d object: <__main__.CustomTensor object at 0x7e0af0369e90>\n",
            "Initial: d._node_id: 3\n",
            "Initial: graph.intermediate_tensors keys: [1, 2, 3]\n",
            "Initial: refcount of d (via output_tensor_weak_ref.test_ref): 3\n",
            "\n",
            "--- After exiting 'with' block (auto_cleanup=False) ---\n",
            "After 'with' block: d object (via weakref): <__main__.CustomTensor object at 0x7e0af0369e90>\n",
            "After 'with' block: refcount of d (via output_tensor_weak_ref.test_ref): 3\n",
            "\n",
            "--- Deleting 'd' variable ---\n",
            "After del d + gc.collect(): d object (via weakref): <__main__.CustomTensor object at 0x7e0af0369e90>\n",
            "After del d + gc.collect(): refcount of d: 2\n",
            "\n",
            "--- Deleting strong reference from graph.intermediate_tensors for node 3 ---\n",
            "Garbage Collector has decided that reference counts are zero so Goodbye!!\n",
            "After del_non_leaf_tensor_reference: graph.intermediate_tensors keys: [1, 2]\n",
            "After del_non_leaf_tensor_reference + gc.collect(): d object (via weakref): None\n",
            "Assertion Passed: Output tensor (d) was garbage collected.\n",
            "\n",
            "--- Starting GC Test: All Intermediate Tensors ---\n",
            "After 'with' block (new graph): graph_new object: CustomAutogradGraph(nodes=4, edges=3)\n",
            "\n",
            "--- Manually clearing graph.intermediate_tensors and deleting graph ---\n",
            "Garbage Collector has decided that reference counts are zero so Goodbye!!\n",
            "Garbage Collector has decided that reference counts are zero so Goodbye!!\n",
            "Garbage Collector has decided that reference counts are zero so Goodbye!!\n",
            "Intermediate tensor 0 (via weakref): None\n",
            "Intermediate tensor 1 (via weakref): None\n",
            "Intermediate tensor 2 (via weakref): None\n",
            "Assertion Passed: All intermediate tensors were garbage collected.\n",
            "Garbage Collector has decided that reference counts are zero so Goodbye!!\n",
            "Garbage Collector has decided that reference counts are zero so Goodbye!!\n",
            "Garbage Collector has decided that reference counts are zero so Goodbye!!\n",
            "Garbage Collector has decided that reference counts are zero so Goodbye!!\n",
            "Raised the error of cycle detected as Cycle detected in autograd graph on context exit.\n",
            "Garbage Collector has decided that reference counts are zero so Goodbye!!\n",
            "Garbage Collector has decided that reference counts are zero so Goodbye!!\n"
          ]
        }
      ],
      "source": [
        "k=TestCustomAutogradSystem()\n",
        "k.test_basic_add_scalar_grad()\n",
        "k.test_basic_add_tensor_grad()\n",
        "k.test_mixed_requires_grad_tensor_add()\n",
        "k.test_no_requires_grad()\n",
        "k.test_autograd_graph_context_manager()\n",
        "k.test_cycle_detection()\n",
        "k.test_no_circular_references_non_leaf_tensors_die()\n",
        "k.test_cycle_detection()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JViyNlfgj26"
      },
      "source": [
        "# Autograd Checker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AQbOlwvKglYP",
        "outputId": "227cd3fa-f25d-43e9-b2e8-99a89fb84d16"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running Custom Autograd Correctness Tests\n",
            "==================================================\n",
            "\n",
            "=== Testing Basic Operations ===\n",
            " Scalar Addition\n",
            " Scalar Addition Result\n",
            "Garbage Collector has decided that reference counts are zero so Goodbye!!\n",
            "Garbage Collector has decided that reference counts are zero so Goodbye!!\n",
            " Tensor Addition - x\n",
            " Tensor Addition - y\n",
            "Garbage Collector has decided that reference counts are zero so Goodbye!!\n",
            "Garbage Collector has decided that reference counts are zero so Goodbye!!\n",
            "Garbage Collector has decided that reference counts are zero so Goodbye!!\n",
            "\n",
            "=== Testing Multiplication ===\n",
            " Scalar Multiplication\n",
            "Garbage Collector has decided that reference counts are zero so Goodbye!!\n",
            "Garbage Collector has decided that reference counts are zero so Goodbye!!\n",
            " Tensor Multiplication - x\n",
            " Tensor Multiplication - y\n",
            "Garbage Collector has decided that reference counts are zero so Goodbye!!\n",
            "Garbage Collector has decided that reference counts are zero so Goodbye!!\n",
            "Garbage Collector has decided that reference counts are zero so Goodbye!!\n",
            "\n",
            "=== Testing Subtraction and Division ===\n",
            " Scalar Subtraction\n",
            "Garbage Collector has decided that reference counts are zero so Goodbye!!\n",
            "Garbage Collector has decided that reference counts are zero so Goodbye!!\n",
            " Scalar Division\n",
            "Garbage Collector has decided that reference counts are zero so Goodbye!!\n",
            "Garbage Collector has decided that reference counts are zero so Goodbye!!\n",
            "\n",
            "=== Testing Power Function ===\n",
            " Power Function\n",
            "Garbage Collector has decided that reference counts are zero so Goodbye!!\n",
            "Garbage Collector has decided that reference counts are zero so Goodbye!!\n",
            "\n",
            "=== Testing Unary Functions ===\n",
            " Exponential Function: \n",
            "Not equal to tolerance rtol=1e-06, atol=1e-06\n",
            "\n",
            "Mismatched elements: 2 / 2 (100%)\n",
            "Max absolute difference among violations: 5.389056\n",
            "Max relative difference among violations: 0.72932947\n",
            " ACTUAL: array([1., 2.], dtype=float32)\n",
            " DESIRED: array([2.718282, 7.389056], dtype=float32)\n",
            "Garbage Collector has decided that reference counts are zero so Goodbye!!\n",
            "Garbage Collector has decided that reference counts are zero so Goodbye!!\n",
            " Logarithm Function\n",
            "Garbage Collector has decided that reference counts are zero so Goodbye!!\n",
            "Garbage Collector has decided that reference counts are zero so Goodbye!!\n",
            " Sine Function\n",
            "Garbage Collector has decided that reference counts are zero so Goodbye!!\n",
            "Garbage Collector has decided that reference counts are zero so Goodbye!!\n",
            " Square Root Function: \n",
            "Not equal to tolerance rtol=1e-06, atol=1e-06\n",
            "\n",
            "Mismatched elements: 2 / 2 (100%)\n",
            "Max absolute difference among violations: 0.125\n",
            "Max relative difference among violations: 0.6666667\n",
            " ACTUAL: array([0.125   , 0.055556], dtype=float32)\n",
            " DESIRED: array([0.25    , 0.166667], dtype=float32)\n",
            "Garbage Collector has decided that reference counts are zero so Goodbye!!\n",
            "Garbage Collector has decided that reference counts are zero so Goodbye!!\n",
            "\n",
            "=== Testing Matrix Operations ===\n",
            " Matrix Multiplication - x\n",
            " Matrix Multiplication - y\n",
            "Garbage Collector has decided that reference counts are zero so Goodbye!!\n",
            "Garbage Collector has decided that reference counts are zero so Goodbye!!\n",
            "Garbage Collector has decided that reference counts are zero so Goodbye!!\n",
            " Dot Product - x\n",
            " Dot Product - y\n",
            "Garbage Collector has decided that reference counts are zero so Goodbye!!\n",
            "Garbage Collector has decided that reference counts are zero so Goodbye!!\n",
            "Garbage Collector has decided that reference counts are zero so Goodbye!!\n",
            "\n",
            "=== Testing Complex Chains ===\n",
            " Complex Chain - x\n",
            " Complex Chain - y\n",
            "Garbage Collector has decided that reference counts are zero so Goodbye!!\n",
            "Garbage Collector has decided that reference counts are zero so Goodbye!!\n",
            "Garbage Collector has decided that reference counts are zero so Goodbye!!\n",
            "Garbage Collector has decided that reference counts are zero so Goodbye!!\n",
            "Garbage Collector has decided that reference counts are zero so Goodbye!!\n",
            "Garbage Collector has decided that reference counts are zero so Goodbye!!\n",
            "Garbage Collector has decided that reference counts are zero so Goodbye!!\n",
            "\n",
            "=== Testing Mixed Operations ===\n",
            " Mixed Operations - x\n",
            "Garbage Collector has decided that reference counts are zero so Goodbye!!\n",
            "Garbage Collector has decided that reference counts are zero so Goodbye!!\n",
            "Garbage Collector has decided that reference counts are zero so Goodbye!!\n",
            "\n",
            "==================================================\n",
            "Test Results: 19 passed, 2 failed\n",
            " Some tests failed. Check the implementation.\n",
            "\n",
            "==================================================\n",
            "Manual Verification Example:\n",
            "z = tensor([16., 36.])\n",
            "dz/dx = tensor([ 8., 12.])\n",
            "dz/dy = tensor([ 8., 12.])\n",
            "Expected gradient: tensor([ 8., 12.])\n",
            "Garbage Collector has decided that reference counts are zero so Goodbye!!\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import numbers\n",
        "import weakref\n",
        "import rustworkx as rx\n",
        "from typing import Optional, Any\n",
        "\n",
        "\n",
        "class AutogradTester:\n",
        "    \"\"\"Test suite to verify custom autograd against PyTorch's autograd\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.passed_tests = 0\n",
        "        self.failed_tests = 0\n",
        "        self.tolerance = 1e-6\n",
        "\n",
        "    def assert_tensors_close(self, custom_tensor, pytorch_tensor, test_name, check_grad=True):\n",
        "        \"\"\"Compare custom tensor with PyTorch tensor\"\"\"\n",
        "        try:\n",
        "            # Check values\n",
        "            np.testing.assert_allclose(\n",
        "                custom_tensor.tensor.detach().numpy(),\n",
        "                pytorch_tensor.detach().numpy(),\n",
        "                rtol=self.tolerance,\n",
        "                atol=self.tolerance\n",
        "            )\n",
        "\n",
        "            # Check gradients if requested\n",
        "            if check_grad and pytorch_tensor.grad is not None:\n",
        "                if custom_tensor.tensor.grad is None:\n",
        "                    raise AssertionError(f\"Custom tensor has no gradient in {test_name}\")\n",
        "\n",
        "                np.testing.assert_allclose(\n",
        "                    custom_tensor.tensor.grad.detach().numpy(),\n",
        "                    pytorch_tensor.grad.detach().numpy(),\n",
        "                    rtol=self.tolerance,\n",
        "                    atol=self.tolerance\n",
        "                )\n",
        "\n",
        "            print(f\" {test_name}\")\n",
        "            self.passed_tests += 1\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\" {test_name}: {str(e)}\")\n",
        "            self.failed_tests += 1\n",
        "\n",
        "    def test_basic_operations(self):\n",
        "        \"\"\"Test basic arithmetic operations\"\"\"\n",
        "        print(\"\\n=== Testing Basic Operations ===\")\n",
        "\n",
        "        with AutogradGraph() as graph:\n",
        "            # Test scalar addition\n",
        "            x_custom = CustomTensor([2.0, 3.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = x_custom + 5.0\n",
        "            y_custom.backward(torch.ones_like(y_custom.tensor))\n",
        "\n",
        "            x_pytorch = torch.tensor([2.0, 3.0], requires_grad=True)\n",
        "            y_pytorch = x_pytorch + 5.0\n",
        "            y_pytorch.backward(torch.ones_like(y_pytorch))\n",
        "\n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Scalar Addition\")\n",
        "            self.assert_tensors_close(y_custom, y_pytorch, \"Scalar Addition Result\", check_grad=False)\n",
        "\n",
        "        with AutogradGraph() as graph:\n",
        "            # Test tensor addition\n",
        "            x_custom = CustomTensor([1.0, 2.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = CustomTensor([3.0, 4.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            z_custom = x_custom + y_custom\n",
        "            z_custom.backward(torch.ones_like(z_custom.tensor))\n",
        "\n",
        "            x_pytorch = torch.tensor([1.0, 2.0], requires_grad=True)\n",
        "            y_pytorch = torch.tensor([3.0, 4.0], requires_grad=True)\n",
        "            z_pytorch = x_pytorch + y_pytorch\n",
        "            z_pytorch.backward(torch.ones_like(z_pytorch))\n",
        "\n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Tensor Addition - x\")\n",
        "            self.assert_tensors_close(y_custom, y_pytorch, \"Tensor Addition - y\")\n",
        "\n",
        "    def test_multiplication(self):\n",
        "        \"\"\"Test multiplication operations\"\"\"\n",
        "        print(\"\\n=== Testing Multiplication ===\")\n",
        "\n",
        "        with AutogradGraph() as graph:\n",
        "            # Test scalar multiplication\n",
        "            x_custom = CustomTensor([2.0, 3.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = x_custom * 4.0\n",
        "            y_custom.backward(torch.ones_like(y_custom.tensor))\n",
        "\n",
        "            x_pytorch = torch.tensor([2.0, 3.0], requires_grad=True)\n",
        "            y_pytorch = x_pytorch * 4.0\n",
        "            y_pytorch.backward(torch.ones_like(y_pytorch))\n",
        "\n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Scalar Multiplication\")\n",
        "\n",
        "        with AutogradGraph() as graph:\n",
        "            # Test tensor multiplication\n",
        "            x_custom = CustomTensor([2.0, 3.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = CustomTensor([4.0, 5.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            z_custom = x_custom * y_custom\n",
        "            z_custom.backward(torch.ones_like(z_custom.tensor))\n",
        "\n",
        "            x_pytorch = torch.tensor([2.0, 3.0], requires_grad=True)\n",
        "            y_pytorch = torch.tensor([4.0, 5.0], requires_grad=True)\n",
        "            z_pytorch = x_pytorch * y_pytorch\n",
        "            z_pytorch.backward(torch.ones_like(z_pytorch))\n",
        "\n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Tensor Multiplication - x\")\n",
        "            self.assert_tensors_close(y_custom, y_pytorch, \"Tensor Multiplication - y\")\n",
        "\n",
        "    def test_subtraction_division(self):\n",
        "        \"\"\"Test subtraction and division\"\"\"\n",
        "        print(\"\\n=== Testing Subtraction and Division ===\")\n",
        "\n",
        "        with AutogradGraph() as graph:\n",
        "            # Test subtraction\n",
        "            x_custom = CustomTensor([5.0, 6.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = x_custom - 2.0\n",
        "            y_custom.backward(torch.ones_like(y_custom.tensor))\n",
        "\n",
        "            x_pytorch = torch.tensor([5.0, 6.0], requires_grad=True)\n",
        "            y_pytorch = x_pytorch - 2.0\n",
        "            y_pytorch.backward(torch.ones_like(y_pytorch))\n",
        "\n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Scalar Subtraction\")\n",
        "\n",
        "        with AutogradGraph() as graph:\n",
        "            # Test division\n",
        "            x_custom = CustomTensor([8.0, 12.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = x_custom / 4.0\n",
        "            y_custom.backward(torch.ones_like(y_custom.tensor))\n",
        "\n",
        "            x_pytorch = torch.tensor([8.0, 12.0], requires_grad=True)\n",
        "            y_pytorch = x_pytorch / 4.0\n",
        "            y_pytorch.backward(torch.ones_like(y_pytorch))\n",
        "\n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Scalar Division\")\n",
        "\n",
        "    def test_power_function(self):\n",
        "        \"\"\"Test power operation\"\"\"\n",
        "        print(\"\\n=== Testing Power Function ===\")\n",
        "\n",
        "        with AutogradGraph() as graph:\n",
        "            x_custom = CustomTensor([2.0, 3.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = x_custom.pow(3.0)\n",
        "            y_custom.backward(torch.ones_like(y_custom.tensor))\n",
        "\n",
        "            x_pytorch = torch.tensor([2.0, 3.0], requires_grad=True)\n",
        "            y_pytorch = torch.pow(x_pytorch, 3.0)\n",
        "            y_pytorch.backward(torch.ones_like(y_pytorch))\n",
        "\n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Power Function\")\n",
        "\n",
        "    def test_unary_functions(self):\n",
        "        \"\"\"Test unary mathematical functions\"\"\"\n",
        "        print(\"\\n=== Testing Unary Functions ===\")\n",
        "\n",
        "        # Test exp\n",
        "        with AutogradGraph() as graph:\n",
        "            x_custom = CustomTensor([1.0, 2.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = x_custom.exp()\n",
        "            y_custom.backward(torch.ones_like(y_custom.tensor))\n",
        "\n",
        "            x_pytorch = torch.tensor([1.0, 2.0], requires_grad=True)\n",
        "            y_pytorch = torch.exp(x_pytorch)\n",
        "            y_pytorch.backward(torch.ones_like(y_pytorch))\n",
        "\n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Exponential Function\")\n",
        "\n",
        "        # Test log\n",
        "        with AutogradGraph() as graph:\n",
        "            x_custom = CustomTensor([1.0, 2.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = x_custom.log()\n",
        "            y_custom.backward(torch.ones_like(y_custom.tensor))\n",
        "\n",
        "            x_pytorch = torch.tensor([1.0, 2.0], requires_grad=True)\n",
        "            y_pytorch = torch.log(x_pytorch)\n",
        "            y_pytorch.backward(torch.ones_like(y_pytorch))\n",
        "\n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Logarithm Function\")\n",
        "\n",
        "        # Test sin\n",
        "        with AutogradGraph() as graph:\n",
        "            x_custom = CustomTensor([0.5, 1.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = x_custom.sin()\n",
        "            y_custom.backward(torch.ones_like(y_custom.tensor))\n",
        "\n",
        "            x_pytorch = torch.tensor([0.5, 1.0], requires_grad=True)\n",
        "            y_pytorch = torch.sin(x_pytorch)\n",
        "            y_pytorch.backward(torch.ones_like(y_pytorch))\n",
        "\n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Sine Function\")\n",
        "\n",
        "        # Test sqrt\n",
        "        with AutogradGraph() as graph:\n",
        "            x_custom = CustomTensor([4.0, 9.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = x_custom.sqrt()\n",
        "            y_custom.backward(torch.ones_like(y_custom.tensor))\n",
        "\n",
        "            x_pytorch = torch.tensor([4.0, 9.0], requires_grad=True)\n",
        "            y_pytorch = torch.sqrt(x_pytorch)\n",
        "            y_pytorch.backward(torch.ones_like(y_pytorch))\n",
        "\n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Square Root Function\")\n",
        "\n",
        "    def test_matrix_operations(self):\n",
        "        \"\"\"Test matrix operations\"\"\"\n",
        "        print(\"\\n=== Testing Matrix Operations ===\")\n",
        "\n",
        "        # Test matrix multiplication\n",
        "        with AutogradGraph() as graph:\n",
        "            x_custom = CustomTensor([[1.0, 2.0], [3.0, 4.0]], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = CustomTensor([[5.0, 6.0], [7.0, 8.0]], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            z_custom = x_custom.matmul(y_custom)\n",
        "            z_custom.backward(torch.ones_like(z_custom.tensor))\n",
        "\n",
        "            x_pytorch = torch.tensor([[1.0, 2.0], [3.0, 4.0]], requires_grad=True)\n",
        "            y_pytorch = torch.tensor([[5.0, 6.0], [7.0, 8.0]], requires_grad=True)\n",
        "            z_pytorch = torch.matmul(x_pytorch, y_pytorch)\n",
        "            z_pytorch.backward(torch.ones_like(z_pytorch))\n",
        "\n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Matrix Multiplication - x\")\n",
        "            self.assert_tensors_close(y_custom, y_pytorch, \"Matrix Multiplication - y\")\n",
        "\n",
        "        # Test dot product\n",
        "        with AutogradGraph() as graph:\n",
        "            x_custom = CustomTensor([1.0, 2.0, 3.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = CustomTensor([4.0, 5.0, 6.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            z_custom = x_custom.dot(y_custom)\n",
        "            z_custom.backward()\n",
        "\n",
        "            x_pytorch = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
        "            y_pytorch = torch.tensor([4.0, 5.0, 6.0], requires_grad=True)\n",
        "            z_pytorch = torch.dot(x_pytorch, y_pytorch)\n",
        "            z_pytorch.backward()\n",
        "\n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Dot Product - x\")\n",
        "            self.assert_tensors_close(y_custom, y_pytorch, \"Dot Product - y\")\n",
        "\n",
        "    def test_complex_chain(self):\n",
        "        \"\"\"Test complex computational chains\"\"\"\n",
        "        print(\"\\n=== Testing Complex Chains ===\")\n",
        "\n",
        "        with AutogradGraph() as graph:\n",
        "            # Test: z = (x + y) * (x - y) + x^2\n",
        "            x_custom = CustomTensor([3.0, 4.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = CustomTensor([1.0, 2.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "\n",
        "            sum_custom = x_custom + y_custom\n",
        "            diff_custom = x_custom - y_custom\n",
        "            prod_custom = sum_custom * diff_custom\n",
        "            x_squared_custom = x_custom.pow(2.0)\n",
        "            z_custom = prod_custom + x_squared_custom\n",
        "\n",
        "            z_custom.backward(torch.ones_like(z_custom.tensor))\n",
        "\n",
        "            x_pytorch = torch.tensor([3.0, 4.0], requires_grad=True)\n",
        "            y_pytorch = torch.tensor([1.0, 2.0], requires_grad=True)\n",
        "\n",
        "            sum_pytorch = x_pytorch + y_pytorch\n",
        "            diff_pytorch = x_pytorch - y_pytorch\n",
        "            prod_pytorch = sum_pytorch * diff_pytorch\n",
        "            x_squared_pytorch = torch.pow(x_pytorch, 2.0)\n",
        "            z_pytorch = prod_pytorch + x_squared_pytorch\n",
        "\n",
        "            z_pytorch.backward(torch.ones_like(z_pytorch))\n",
        "\n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Complex Chain - x\")\n",
        "            self.assert_tensors_close(y_custom, y_pytorch, \"Complex Chain - y\")\n",
        "\n",
        "    def test_mixed_operations(self):\n",
        "        \"\"\"Test mixing operations with and without gradients\"\"\"\n",
        "        print(\"\\n=== Testing Mixed Operations ===\")\n",
        "\n",
        "        with AutogradGraph() as graph:\n",
        "            # One tensor requires grad, other doesn't\n",
        "            x_custom = CustomTensor([2.0, 3.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = CustomTensor([4.0, 5.0])  # No grad\n",
        "            z_custom = x_custom * y_custom\n",
        "            z_custom.backward(torch.ones_like(z_custom.tensor))\n",
        "\n",
        "            x_pytorch = torch.tensor([2.0, 3.0], requires_grad=True)\n",
        "            y_pytorch = torch.tensor([4.0, 5.0])  # No grad\n",
        "            z_pytorch = x_pytorch * y_pytorch\n",
        "            z_pytorch.backward(torch.ones_like(z_pytorch))\n",
        "\n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Mixed Operations - x\")\n",
        "\n",
        "    def run_all_tests(self):\n",
        "        \"\"\"Run all tests\"\"\"\n",
        "        print(\"Running Custom Autograd Correctness Tests\")\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "        self.test_basic_operations()\n",
        "        self.test_multiplication()\n",
        "        self.test_subtraction_division()\n",
        "        self.test_power_function()\n",
        "        self.test_unary_functions()\n",
        "        self.test_matrix_operations()\n",
        "        self.test_complex_chain()\n",
        "        self.test_mixed_operations()\n",
        "\n",
        "        print(f\"\\n\" + \"=\" * 50)\n",
        "        print(f\"Test Results: {self.passed_tests} passed, {self.failed_tests} failed\")\n",
        "\n",
        "        if self.failed_tests == 0:\n",
        "            print(\" All tests passed! Your autograd implementation is correct.\")\n",
        "        else:\n",
        "            print(\" Some tests failed. Check the implementation.\")\n",
        "\n",
        "        return self.failed_tests == 0\n",
        "\n",
        "# Usage example:\n",
        "if __name__ == \"__main__\":\n",
        "    # Insert your AutogradGraph and CustomTensor classes here\n",
        "    # Then run the tests\n",
        "\n",
        "    tester = AutogradTester()\n",
        "    success = tester.run_all_tests()\n",
        "\n",
        "    # Additional manual verification\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"Manual Verification Example:\")\n",
        "\n",
        "    with AutogradGraph() as graph:\n",
        "        x = CustomTensor([1.0, 2.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "        y = CustomTensor([3.0, 4.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "\n",
        "        # Compute z = x^2 + 2*x*y + y^2 = (x + y)^2\n",
        "        z = x.pow(2.0) + (x * y * 2.0) + y.pow(2.0)\n",
        "        print(f\"z = {z.tensor}\")\n",
        "\n",
        "        # Backward pass\n",
        "        z.backward(torch.ones_like(z.tensor))\n",
        "\n",
        "        print(f\"dz/dx = {x.tensor.grad}\")  # Should be 2*(x + y)\n",
        "        print(f\"dz/dy = {y.tensor.grad}\")  # Should be 2*(x + y)\n",
        "\n",
        "        # Expected: dz/dx = dz/dy = 2*(x + y) = 2*[4, 6] = [8, 12]\n",
        "        expected_grad = 2 * (x.tensor + y.tensor)\n",
        "        print(f\"Expected gradient: {expected_grad}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGbfpfLBmxne"
      },
      "source": [
        "# torch checking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "x7vW6DzJnPzG"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "iAr7rlHimz5-"
      },
      "outputs": [],
      "source": [
        "x = torch.tensor([1, 2, 3], requires_grad=True, dtype=torch.float32)\n",
        "y = torch.tensor([6, 7, 8], requires_grad=True, dtype=torch.float32)\n",
        "\n",
        "z = x + y\n",
        "w = z * x - y\n",
        "x = w + y + x\n",
        "e = x + w + y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gSNqG8uOoXSW",
        "outputId": "7a3d16fb-c17e-46c7-ae2b-719ba077cd18"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x.grad: tensor([17., 23., 29.])\n",
            "y.grad: tensor([2., 4., 6.])\n",
            "z.grad: tensor([2., 4., 6.])\n",
            "w.grad: tensor([2., 2., 2.])\n",
            "x.grad: tensor([1., 1., 1.])\n",
            "e.grad: tensor([1., 1., 1.])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Leaf tensors\n",
        "x = torch.tensor([1, 2, 3], requires_grad=True, dtype=torch.float32)\n",
        "y = torch.tensor([6, 7, 8], requires_grad=True, dtype=torch.float32)\n",
        "\n",
        "# Intermediate tensors\n",
        "z = x + y\n",
        "z.retain_grad()\n",
        "\n",
        "w = z * x - y\n",
        "w.retain_grad()\n",
        "\n",
        "x1 = w + y + x\n",
        "x1.retain_grad()\n",
        "\n",
        "e = x1 + w + y\n",
        "e.retain_grad()\n",
        "\n",
        "# Backward pass\n",
        "e.backward(torch.ones_like(e))\n",
        "\n",
        "# Print gradients\n",
        "print(\"x.grad:\", x.grad)     # leaf\n",
        "print(\"y.grad:\", y.grad)     # leaf\n",
        "print(\"z.grad:\", z.grad)     # non-leaf, manually retained\n",
        "print(\"w.grad:\", w.grad)     # non-leaf, manually retained\n",
        "print(\"x.grad:\", x1.grad)    # non-leaf, manually retained\n",
        "print(\"e.grad:\", e.grad)     # non-leaf, manually retained\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "hdUq6QGdnOAr"
      },
      "outputs": [],
      "source": [
        "e.backward(torch.ones_like(e))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jh_rZJGJnyKx",
        "outputId": "3f1df224-e520-433b-b851-058686cd6428"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-6-2814570312.py:1: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /pytorch/build/aten/src/ATen/core/TensorBody.h:489.)\n",
            "  e.grad\n"
          ]
        }
      ],
      "source": [
        "e.grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7s5eGRDKnzVh"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
