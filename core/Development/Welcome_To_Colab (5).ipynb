{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 19 july\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tensor defination and AutoGradGraph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import weakref\n",
        "import numbers\n",
        "import rustworkx as rx\n",
        "import math\n",
        "\n",
        "# Your existing AutogradGraph class (with minor improvements)\n",
        "class AutogradGraph:\n",
        "    \"\"\"\n",
        "    Manages the computation graph for automatic differentiation.\n",
        "    It uses a directed acyclic graph to track dependencies between tensors.\n",
        "    \"\"\"\n",
        "    __slots__ = ('graph', 'intermediate_tensors', '_check_cycles', '_auto_cleanup', '__weakref__')\n",
        "\n",
        "    def __init__(self, check_for_cycles=True, auto_cleanup=True):\n",
        "        self.graph = rx.PyDiGraph()\n",
        "        self.intermediate_tensors = {}\n",
        "        self._check_cycles = check_for_cycles\n",
        "        self._auto_cleanup = auto_cleanup\n",
        "\n",
        "    def __enter__(self):\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, exc_type, exc_value, traceback):\n",
        "        if self._check_cycles and self.check_cycle():\n",
        "            raise RuntimeError(\"Cycle detected in autograd graph on context exit.\")\n",
        "        if self._auto_cleanup:\n",
        "            self.intermediate_tensors.clear()\n",
        "            self.graph.clear()\n",
        "\n",
        "    def add_tensor_graph(self, tensor):\n",
        "        if not tensor._custom_requires_grad:\n",
        "            raise ValueError(\"Tensor with requires_grad=False cannot be added to the graph.\")\n",
        "        ref = weakref.proxy(tensor)\n",
        "        tensor_index = self.graph.add_node(ref)\n",
        "        tensor._node_id = tensor_index\n",
        "\n",
        "    def add_non_leaf_tensor_reference(self, tensor):\n",
        "        if not tensor._custom_requires_grad:\n",
        "            raise ValueError(\"Tensor must require grad.\")\n",
        "        if tensor._node_id in self.intermediate_tensors:\n",
        "            raise ValueError(\"Tensor reference already exists in intermediate tensors.\")\n",
        "        self.intermediate_tensors[tensor._node_id] = tensor\n",
        "\n",
        "    def add_edge(self, node_from, node_to, weight=None):\n",
        "        if not all(isinstance(n, int) for n in (node_from, node_to)):\n",
        "            raise TypeError(\"Node indices must be integers.\")\n",
        "        if not self.graph.has_node(node_from) or not self.graph.has_node(node_to):\n",
        "            raise ValueError(\"Nodes must exist before adding edge.\")\n",
        "        self.graph.add_edge(node_from, node_to, weight)\n",
        "\n",
        "    def check_cycle(self):\n",
        "        return not rx.is_directed_acyclic_graph(self.graph)\n",
        "\n",
        "    def reverse_toposort_from_tensor(self, tensor_index):\n",
        "        graph=self.graph\n",
        "        predecessors = list(rx.ancestors(graph, tensor_index))\n",
        "        predecessors.append(tensor_index)\n",
        "        sub_graph = graph.subgraph(predecessors)\n",
        "        return [sub_graph[i] for i in reversed(rx.topological_sort(sub_graph))]\n",
        "    # def alternative_reverse_toposort_from_tensor(self, tensor_index):\n",
        "    #     graph = self.graph\n",
        "    #     relevant_nodes = rx.ancestors(graph, tensor_index)\n",
        "    #     relevant_nodes.add(tensor_index)\n",
        "    #     full_topo = rx.topological_sort(graph)\n",
        "    #     relevant_topo = [graph[_node_id] for _node_id in reversed(full_topo) if _node_id in relevant_nodes]\n",
        "    #     return relevant_topo\n",
        "\n",
        "    def delete_node(self, node_index):\n",
        "        if not isinstance(node_index, int):\n",
        "            raise TypeError(\"Node index must be an integer.\")\n",
        "        if self.graph.has_node(node_index):\n",
        "             self.graph.remove_node(node_index)\n",
        "    def delete_edge(self, node_from, node_to):\n",
        "        if not self.graph.has_edge(node_from, node_to):\n",
        "            raise ValueError(\"Edge does not exist.\")\n",
        "        self.graph.remove_edge(node_from, node_to)\n",
        "\n",
        "    def del_non_leaf_tensor_reference(self, tensor_node_id):\n",
        "        self.intermediate_tensors.pop(tensor_node_id, None)\n",
        "\n",
        "    def delete_all_non_leaf_nodes(self):\n",
        "        # removes non leaf nodes from graph and clears the intermediate_tensors dict\n",
        "        self.graph.remove_nodes_from(list(self.intermediate_tensors.keys()))\n",
        "        self.intermediate_tensors.clear()\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"CustomAutogradGraph(nodes={self.graph.num_nodes()}, edges={self.graph.num_edges()})\"\n",
        "\n",
        "# Your existing CustomTensor class, now enhanced with new methods\n",
        "class CustomTensor:\n",
        "    \"\"\"\n",
        "    A custom tensor class that wraps a PyTorch tensor to enable a custom\n",
        "    autograd engine. It tracks operations to build a computation graph.\n",
        "    \"\"\"\n",
        "    __slots__ = ('tensor', '_node_id', '_custom_requires_grad', '_backward', 'graph', '__weakref__','_is_leaf')\n",
        "\n",
        "    def __new__(cls, data, *, _custom_requires_grad=False, device=\"cpu\", dtype=torch.float32, graph=None, due_to_operation=False, is_leaf=False):\n",
        "        if isinstance(data, CustomTensor):\n",
        "            return data  # Don't rewrap\n",
        "        return super().__new__(cls)\n",
        "\n",
        "    def __init__(self, data, *, _custom_requires_grad=False, device=\"cpu\", dtype=torch.float32, graph=None, due_to_operation=False, is_leaf=False):\n",
        "        if isinstance(data, CustomTensor):\n",
        "            return\n",
        "\n",
        "        self.tensor = data if due_to_operation else torch.as_tensor(data, dtype=dtype, device=device)\n",
        "        self.tensor.requires_grad_(False)\n",
        "        self._custom_requires_grad = _custom_requires_grad\n",
        "        self._node_id = None\n",
        "        self._backward = lambda: None\n",
        "        self.graph = None\n",
        "        self._is_leaf = is_leaf\n",
        "\n",
        "        if _custom_requires_grad:\n",
        "            self._init_graph(graph)\n",
        "\n",
        "    def _init_graph(self, graph):\n",
        "        if graph is None:\n",
        "            raise ValueError(\"Graph must be provided if requires_grad is True.\")\n",
        "        is_leaf=self._is_leaf\n",
        "        if is_leaf:\n",
        "            self.graph = weakref.proxy(graph)\n",
        "        else:\n",
        "            self.graph = graph # this line is only reached for tensors which are created by operations and graph passed is already a weakreference hence no need for wrapping\n",
        "        graph.add_tensor_graph(self)\n",
        "        if not is_leaf:\n",
        "            graph.add_non_leaf_tensor_reference(self)\n",
        "\n",
        "    def _zero_grad(self):\n",
        "        \"\"\"Sets the gradient of the underlying tensor to zero.\"\"\"\n",
        "        self.tensor.grad = torch.zeros_like(self.tensor)\n",
        "\n",
        "    def zero_(self):\n",
        "        \"\"\"Sets the gradient of the underlying tensor to zero.\"\"\"\n",
        "        if self.tensor.grad is not None:\n",
        "            self.tensor.grad.zero_()\n",
        "\n",
        "\n",
        "    # --- Broadcasting Helper ---\n",
        "    def _reduce_grad_for_broadcast(self, grad, target_shape):\n",
        "        \"\"\"Reduces a gradient to match the shape of a tensor that was broadcasted.\"\"\"\n",
        "        if grad.shape == target_shape:\n",
        "            return grad\n",
        "        \n",
        "        # Add singleton dimensions to the front of target_shape to match grad's ndim\n",
        "        padded_target_shape = (1,) * (grad.ndim - len(target_shape)) + target_shape\n",
        "        \n",
        "        # Identify dimensions that were broadcasted\n",
        "        sum_dims = [i for i, (grad_dim, target_dim) in enumerate(zip(grad.shape, padded_target_shape)) if target_dim == 1 and grad_dim > 1]\n",
        "\n",
        "        if sum_dims:\n",
        "            grad = grad.sum(dim=sum_dims, keepdim=True)\n",
        "        \n",
        "        # Remove singleton dimensions to match the final target shape\n",
        "        return grad.reshape(target_shape)\n",
        "\n",
        "\n",
        "\n",
        "    # --- Basic Operators (from your original code, now compatible with new features) ---\n",
        "    def __add__(self, other):\n",
        "        # ... [Your original implementation]\n",
        "        if isinstance(other, numbers.Number):\n",
        "            return self._add_scalar(other)\n",
        "        elif isinstance(other, CustomTensor):\n",
        "            return self._add_tensor(other)\n",
        "        return NotImplemented\n",
        "    def __radd__(self,other):\n",
        "        return self + other\n",
        "    def __iadd__(self,other):\n",
        "        if isinstance(other, numbers.Number):\n",
        "            self.tensor.add_(other)\n",
        "        elif isinstance(other,CustomTensor):\n",
        "            self.tensor.add_(other.tensor)\n",
        "    def _add_scalar(self, scalar):\n",
        "        result_tensor = torch.add(self.tensor, scalar)\n",
        "        if not self._custom_requires_grad:\n",
        "            return CustomTensor(result_tensor,due_to_operation=True)\n",
        "        graph = self.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "        graph.add_edge(self._node_id, result._node_id)\n",
        "        self_ref = weakref.proxy(self)\n",
        "        result_ref = weakref.proxy(result)\n",
        "        def _backward():\n",
        "            if self_ref.tensor.grad is None: self_ref._zero_grad()\n",
        "            self_ref.tensor.grad.add_(result_ref.tensor.grad)\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "    def _add_tensor(self, other):\n",
        "        result_tensor = torch.add(self.tensor, other.tensor)\n",
        "        requires_grad = self._custom_requires_grad or other._custom_requires_grad\n",
        "        if not requires_grad:\n",
        "            return CustomTensor(result_tensor,due_to_operation=True)\n",
        "        graph = self.graph if self._custom_requires_grad else other.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "        self_ref = weakref.proxy(self)\n",
        "        other_ref = weakref.proxy(other)\n",
        "        if self._custom_requires_grad:\n",
        "            graph.add_edge(self._node_id, result._node_id)\n",
        "        if other._custom_requires_grad:\n",
        "            graph.add_edge(other._node_id, result._node_id)\n",
        "        result_ref = weakref.proxy(result)\n",
        "        def _backward():\n",
        "            if self_ref._custom_requires_grad:\n",
        "                if self_ref.tensor.grad is None: self_ref._zero_grad()\n",
        "                grad_for_self = self_ref._reduce_grad_for_broadcast(result_ref.tensor.grad, self_ref.tensor.shape)\n",
        "                self_ref.tensor.grad.add_(grad_for_self)\n",
        "            if other_ref._custom_requires_grad:\n",
        "                if other_ref.tensor.grad is None: other_ref._zero_grad()\n",
        "                grad_for_other = other_ref._reduce_grad_for_broadcast(result_ref.tensor.grad, other_ref.tensor.shape)\n",
        "                other_ref.tensor.grad.add_(grad_for_other)\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "\n",
        "    def __mul__(self, other):\n",
        "        # ... [Your original implementation]\n",
        "        if isinstance(other, numbers.Number):\n",
        "            return self._mul_scalar(other)\n",
        "        elif isinstance(other, CustomTensor):\n",
        "            return self._mul_tensor(other)\n",
        "        return NotImplemented\n",
        "    def __rmul__(self,other):\n",
        "        return self*other\n",
        "    def __imul__(self,other):\n",
        "        if isinstance(other, numbers.Number):\n",
        "            self.tensor.mul_(other)\n",
        "        elif isinstance(other,CustomTensor):\n",
        "            self.tensor.mul_(other.tensor)\n",
        "    def _mul_scalar(self, scalar):\n",
        "        result_tensor = torch.mul(self.tensor, scalar)\n",
        "        if not self._custom_requires_grad:\n",
        "            return CustomTensor(result_tensor,due_to_operation=True)\n",
        "        graph = self.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "        graph.add_edge(self._node_id, result._node_id)\n",
        "        self_ref = weakref.proxy(self)\n",
        "        result_ref = weakref.proxy(result)\n",
        "        def _backward():\n",
        "            if self_ref.tensor.grad is None:\n",
        "                self_ref._zero_grad()\n",
        "            self_ref.tensor.grad.add_(result_ref.tensor.grad * scalar)\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "    def _mul_tensor(self, other):\n",
        "        result_tensor = torch.mul(self.tensor, other.tensor)\n",
        "        requires_grad = self._custom_requires_grad or other._custom_requires_grad\n",
        "        if not requires_grad:\n",
        "            return CustomTensor(result_tensor,due_to_operation=True)\n",
        "        graph = self.graph if self._custom_requires_grad else other.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "        self_ref = weakref.proxy(self)\n",
        "        other_ref = weakref.proxy(other)\n",
        "        result_ref = weakref.proxy(result)\n",
        "        if self._custom_requires_grad:\n",
        "            graph.add_edge(self._node_id, result._node_id)\n",
        "        if other._custom_requires_grad:\n",
        "            graph.add_edge(other._node_id, result._node_id)\n",
        "        def _backward():\n",
        "            if self_ref._custom_requires_grad:\n",
        "                if self_ref.tensor.grad is None: self_ref._zero_grad()\n",
        "                grad_for_self = self_ref._reduce_grad_for_broadcast(result_ref.tensor.grad * other_ref.tensor, self_ref.tensor.shape)\n",
        "                self_ref.tensor.grad.add_(grad_for_self)\n",
        "            if other_ref._custom_requires_grad:\n",
        "                if other_ref.tensor.grad is None: other_ref._zero_grad()\n",
        "                grad_for_other = other_ref._reduce_grad_for_broadcast(result_ref.tensor.grad * self_ref.tensor, other_ref.tensor.shape)\n",
        "                other_ref.tensor.grad.add_(grad_for_other)\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "\n",
        "    def __sub__(self, other):\n",
        "        if isinstance(other, numbers.Number):\n",
        "            return self._sub_scalar(other)\n",
        "        elif isinstance(other, CustomTensor):\n",
        "            return self._sub_tensor(other)\n",
        "        return NotImplemented\n",
        "    \n",
        "    def __rsub__(self, other):\n",
        "        if isinstance(other, numbers.Number):\n",
        "            return self._rsub_scalar(other)\n",
        "        \n",
        "    def __isub__(self,other):\n",
        "        if isinstance(other, numbers.Number):\n",
        "            self.tensor.sub_(other)\n",
        "        elif isinstance(other,CustomTensor):\n",
        "            self.tensor.sub_(other.tensor)\n",
        "        \n",
        "    def _rsub_scalar(self, scalar):\n",
        "        result_tensor = torch.sub(scalar, self.tensor)\n",
        "        if not self._custom_requires_grad:\n",
        "            return CustomTensor(result_tensor,due_to_operation=True)\n",
        "\n",
        "        graph = self.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "        graph.add_edge(self._node_id, result._node_id)\n",
        "\n",
        "        self_ref = weakref.proxy(self)\n",
        "        result_ref = weakref.proxy(result)\n",
        "        def _backward():\n",
        "            if self_ref.tensor.grad is None:\n",
        "                self_ref._zero_grad()\n",
        "            # Derivative of scalar - x is -1\n",
        "            self_ref.tensor.grad.sub_(result_ref.tensor.grad) # No broadcasting specific logic for scalar op\n",
        "\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "\n",
        "    \n",
        "    def _sub_scalar(self, scalar):\n",
        "        result_tensor = torch.sub(self.tensor, scalar)\n",
        "        if not self._custom_requires_grad:\n",
        "            return CustomTensor(result_tensor,due_to_operation=True)\n",
        "\n",
        "        graph = self.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "        graph.add_edge(self._node_id, result._node_id)\n",
        "\n",
        "        self_ref = weakref.proxy(self)\n",
        "        result_ref = weakref.proxy(result)\n",
        "        def _backward():\n",
        "            if self_ref.tensor.grad is None:\n",
        "                self_ref._zero_grad()\n",
        "            self_ref.tensor.grad.add_(result_ref.tensor.grad) # No broadcasting specific logic for scalar op\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "\n",
        "    def _sub_tensor(self, other):\n",
        "        result_tensor = torch.sub(self.tensor, other.tensor)\n",
        "        requires_grad = self._custom_requires_grad or other._custom_requires_grad\n",
        "        if not requires_grad:\n",
        "            return CustomTensor(result_tensor,due_to_operation=True)\n",
        "\n",
        "        graph = self.graph if self._custom_requires_grad else other.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "\n",
        "        self_ref = weakref.proxy(self)\n",
        "        other_ref = weakref.proxy(other)\n",
        "        result_ref = weakref.proxy(result)\n",
        "\n",
        "        if self._custom_requires_grad:\n",
        "            graph.add_edge(self._node_id, result._node_id)\n",
        "        if other._custom_requires_grad:\n",
        "            graph.add_edge(other._node_id, result._node_id)\n",
        "\n",
        "        def _backward():\n",
        "            if self_ref._custom_requires_grad:\n",
        "                if self_ref.tensor.grad is None:\n",
        "                    self_ref._zero_grad()\n",
        "                grad_for_self = self_ref._reduce_grad_for_broadcast(result_ref.tensor.grad, self_ref.tensor.shape)\n",
        "                self_ref.tensor.grad.add_(grad_for_self)\n",
        "            if other_ref._custom_requires_grad:\n",
        "                if other_ref.tensor.grad is None:\n",
        "                    other_ref._zero_grad()\n",
        "                grad_for_other = other_ref._reduce_grad_for_broadcast(-result_ref.tensor.grad, other_ref.tensor.shape)\n",
        "                other_ref.tensor.grad.add_(grad_for_other)\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "\n",
        "    def __truediv__(self, other):\n",
        "        if isinstance(other, numbers.Number):\n",
        "            return self._div_scalar(other)\n",
        "        elif isinstance(other, CustomTensor):\n",
        "            return self._div_tensor(other)\n",
        "        return NotImplemented\n",
        "    def __itruediv__(self,other):\n",
        "        if isinstance(other, numbers.Number):\n",
        "            self.tensor.div_(other)\n",
        "        elif isinstance(other,CustomTensor):\n",
        "            self.tensor.div_(other.tensor)\n",
        "    def _div_scalar(self, scalar):\n",
        "        result_tensor = torch.div(self.tensor, scalar)\n",
        "        if not self._custom_requires_grad:\n",
        "            return CustomTensor(result_tensor,due_to_operation=True)\n",
        "\n",
        "        graph = self.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "        graph.add_edge(self._node_id, result._node_id)\n",
        "\n",
        "        self_ref = weakref.proxy(self)\n",
        "        result_ref = weakref.proxy(result)\n",
        "        def _backward():\n",
        "            if self_ref.tensor.grad is None:\n",
        "                self_ref._zero_grad()\n",
        "            self_ref.tensor.grad.add_(result_ref.tensor.grad / scalar)\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "\n",
        "    def _div_tensor(self,other):\n",
        "        result_tensor = torch.div(self.tensor, other.tensor)\n",
        "        requires_grad = self._custom_requires_grad or other._custom_requires_grad\n",
        "        if not requires_grad:\n",
        "            return CustomTensor(result_tensor,due_to_operation=True)\n",
        "\n",
        "        graph = self.graph if self._custom_requires_grad else other.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "\n",
        "        self_ref = weakref.proxy(self)\n",
        "        other_ref = weakref.proxy(other)\n",
        "        result_ref = weakref.proxy(result)\n",
        "\n",
        "        if self._custom_requires_grad:\n",
        "            graph.add_edge(self._node_id, result._node_id)\n",
        "        if other._custom_requires_grad:\n",
        "            graph.add_edge(other._node_id, result._node_id)\n",
        "\n",
        "        def _backward():\n",
        "            if self_ref._custom_requires_grad:\n",
        "                if self_ref.tensor.grad is None:\n",
        "                    self_ref._zero_grad()\n",
        "                grad_for_self = self_ref._reduce_grad_for_broadcast(result_ref.tensor.grad / other_ref.tensor, self_ref.tensor.shape)\n",
        "                self_ref.tensor.grad.add_(grad_for_self)\n",
        "            if other_ref._custom_requires_grad:\n",
        "                if other_ref.tensor.grad is None:\n",
        "                    other_ref._zero_grad()\n",
        "                grad_for_other = other_ref._reduce_grad_for_broadcast(-result_ref.tensor.grad * self_ref.tensor / other_ref.tensor.pow(2), other_ref.tensor.shape)\n",
        "                other_ref.tensor.grad.add_(grad_for_other)\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "\n",
        "    def pow(self, scalar):\n",
        "        result_tensor = torch.pow(self.tensor, scalar)\n",
        "        if not self._custom_requires_grad:\n",
        "            return CustomTensor(result_tensor,due_to_operation=True)\n",
        "\n",
        "        graph = self.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "        graph.add_edge(self._node_id, result._node_id)\n",
        "\n",
        "        self_ref = weakref.proxy(self)\n",
        "        result_ref = weakref.proxy(result)\n",
        "        def _backward():\n",
        "            if self_ref.tensor.grad is None:\n",
        "                self_ref._zero_grad()\n",
        "            grad_contrib = scalar * self_ref.tensor.pow(scalar - 1)\n",
        "            self_ref.tensor.grad.add_(result_ref.tensor.grad * grad_contrib)\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "    def __ipow__(self,other):\n",
        "        self.tensor.pow_(other)\n",
        "\n",
        "    def exp(self):\n",
        "        out = torch.exp(self.tensor)\n",
        "        if not self._custom_requires_grad:\n",
        "            return CustomTensor(out,due_to_operation=True)\n",
        "        \n",
        "        graph = self.graph\n",
        "        result = CustomTensor(out, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "        graph.add_edge(self._node_id, result._node_id)\n",
        "        self_ref = weakref.proxy(self)\n",
        "        result_ref = weakref.proxy(result)\n",
        "        def _backward():\n",
        "            if self_ref.tensor.grad is None:\n",
        "                self_ref._zero_grad()\n",
        "            self_ref.tensor.grad.add_(result_ref.tensor.grad * out)\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "\n",
        "    def log(self):\n",
        "        out = torch.log(self.tensor)\n",
        "        if not self._custom_requires_grad:\n",
        "            return CustomTensor(out,due_to_operation=True)\n",
        "        \n",
        "        graph = self.graph\n",
        "        result = CustomTensor(out, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "        graph.add_edge(self._node_id, result._node_id)\n",
        "        self_ref = weakref.proxy(self)\n",
        "        result_ref = weakref.proxy(result)\n",
        "        def _backward():\n",
        "            if self_ref.tensor.grad is None:\n",
        "                self_ref._zero_grad()\n",
        "            self_ref.tensor.grad.add_(result_ref.tensor.grad / self_ref.tensor)\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "\n",
        "    def sin(self):\n",
        "        out = torch.sin(self.tensor)\n",
        "        if not self._custom_requires_grad:\n",
        "            return CustomTensor(out,due_to_operation=True)\n",
        "        \n",
        "        graph = self.graph\n",
        "        result = CustomTensor(out, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "        graph.add_edge(self._node_id, result._node_id)\n",
        "        self_ref = weakref.proxy(self)\n",
        "        result_ref = weakref.proxy(result)\n",
        "        def _backward():\n",
        "            if self_ref.tensor.grad is None:\n",
        "                self_ref._zero_grad()\n",
        "            self_ref.tensor.grad.add_(result_ref.tensor.grad * torch.cos(self_ref.tensor))\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "\n",
        "    def cos(self):\n",
        "        out = torch.cos(self.tensor)\n",
        "        if not self._custom_requires_grad:\n",
        "            return CustomTensor(out,due_to_operation=True)\n",
        "        \n",
        "        graph = self.graph\n",
        "        result = CustomTensor(out, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "        graph.add_edge(self._node_id, result._node_id)\n",
        "        self_ref = weakref.proxy(self)\n",
        "        result_ref = weakref.proxy(result)\n",
        "        def _backward():\n",
        "            if self_ref.tensor.grad is None:\n",
        "                self_ref._zero_grad()\n",
        "            self_ref.tensor.grad.add_(-result_ref.tensor.grad*torch.sin(self_ref.tensor))\n",
        "        result._backward = _backward\n",
        "        return result \n",
        "\n",
        "    def sqrt(self):\n",
        "        out = torch.sqrt(self.tensor)\n",
        "        if not self._custom_requires_grad:\n",
        "            return CustomTensor(out,due_to_operation=True)\n",
        "        \n",
        "        graph = self.graph\n",
        "        result = CustomTensor(out, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "        graph.add_edge(self._node_id, result._node_id)\n",
        "        self_ref = weakref.proxy(self)\n",
        "        result_ref = weakref.proxy(result)\n",
        "        def _backward():\n",
        "            if self_ref.tensor.grad is None:\n",
        "                self_ref._zero_grad()\n",
        "            self_ref.tensor.grad.add_(result_ref.tensor.grad*0.5*self_ref.tensor.pow(-0.5))\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "\n",
        "    def matmul(self, other):\n",
        "        result_tensor = torch.matmul(self.tensor, other.tensor)\n",
        "        requires_grad = self._custom_requires_grad or other._custom_requires_grad\n",
        "        if not requires_grad:\n",
        "            return CustomTensor(result_tensor,due_to_operation=True)\n",
        "\n",
        "        graph = self.graph if self._custom_requires_grad else other.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "\n",
        "        self_ref = weakref.proxy(self)\n",
        "        other_ref = weakref.proxy(other)\n",
        "        result_ref = weakref.proxy(result)\n",
        "\n",
        "        if self._custom_requires_grad:\n",
        "            graph.add_edge(self._node_id, result._node_id)\n",
        "        if other._custom_requires_grad:\n",
        "            graph.add_edge(other._node_id, result._node_id)\n",
        "\n",
        "        def _backward():\n",
        "            if self_ref._custom_requires_grad:\n",
        "                if self_ref.tensor.grad is None: self_ref._zero_grad()\n",
        "                # Use robust broadcasting for matmul gradient\n",
        "                grad_for_self = torch.matmul(result_ref.tensor.grad, other_ref.tensor.transpose(-2, -1))\n",
        "                self_ref.tensor.grad.add_(self_ref._reduce_grad_for_broadcast(grad_for_self, self_ref.tensor.shape))\n",
        "            if other_ref._custom_requires_grad:\n",
        "                if other_ref.tensor.grad is None: other_ref._zero_grad()\n",
        "                grad_for_other = torch.matmul(self_ref.tensor.transpose(-2, -1), result_ref.tensor.grad)\n",
        "                other_ref.tensor.grad.add_(other_ref._reduce_grad_for_broadcast(grad_for_other, other_ref.tensor.shape))\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "    def dot(self, other):\n",
        "        # torch.dot only works for 1D tensors, or for higher-D tensors,\n",
        "        # it flattens them to 1D and then computes the dot product.\n",
        "        # This means the gradients will also be 1D, so no complex broadcasting\n",
        "        # reduction is needed on the output gradient itself.\n",
        "        # However, the input tensors themselves could have been results of broadcasting ops.\n",
        "        # For a truly general dot product, you'd use torch.matmul.\n",
        "        result_tensor = torch.dot(self.tensor.reshape(-1), other.tensor.reshape(-1))\n",
        "        requires_grad = self._custom_requires_grad or other._custom_requires_grad\n",
        "        if not requires_grad:\n",
        "            return CustomTensor(result_tensor,due_to_operation=True)\n",
        "\n",
        "        graph = self.graph if self._custom_requires_grad else other.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "\n",
        "        self_ref = weakref.proxy(self)\n",
        "        other_ref = weakref.proxy(other)\n",
        "        result_ref = weakref.proxy(result)\n",
        "\n",
        "        if self._custom_requires_grad:\n",
        "            graph.add_edge(self._node_id, result._node_id)\n",
        "        if other._custom_requires_grad:\n",
        "            graph.add_edge(other._node_id, result._node_id)\n",
        "\n",
        "        def _backward():\n",
        "            if self_ref._custom_requires_grad:\n",
        "                if self_ref.tensor.grad is None:\n",
        "                    self_ref._zero_grad()\n",
        "                # The grad from result_ref.tensor.grad will be a scalar.\n",
        "                # It needs to be multiplied by the other_ref.tensor (original shape)\n",
        "                # and then potentially re-shaped if original was >1D\n",
        "                grad_contrib = result_ref.tensor.grad * other_ref.tensor\n",
        "                self_ref.tensor.grad.add_(grad_contrib)\n",
        "            if other_ref._custom_requires_grad:\n",
        "                if other_ref.tensor.grad is None:\n",
        "                    other_ref._zero_grad()\n",
        "                grad_contrib = result_ref.tensor.grad * self_ref.tensor\n",
        "                other_ref.tensor.grad.add_(grad_contrib)\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "\n",
        "\n",
        "    \n",
        "    # --- New Unary Operations ---\n",
        "    \n",
        "    def sum(self, dim=None, keepdim=False):\n",
        "        \"\"\"Computes the sum of elements along given dimensions.\"\"\"\n",
        "        result_tensor = self.tensor.sum(dim=dim, keepdim=keepdim)\n",
        "        if not self._custom_requires_grad:\n",
        "            return CustomTensor(result_tensor, due_to_operation=True)\n",
        "            \n",
        "        graph = self.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "        graph.add_edge(self._node_id, result._node_id)\n",
        "\n",
        "        self_ref = weakref.proxy(self)\n",
        "        result_ref = weakref.proxy(result)\n",
        "\n",
        "        def _backward():\n",
        "            if self_ref.tensor.grad is None:\n",
        "                self_ref._zero_grad()\n",
        "                \n",
        "            grad = result_ref.tensor.grad\n",
        "            # If keepdim was false, the summed dim was squeezed. We need to unsqueeze it back for broadcasting.\n",
        "            if not keepdim and dim is not None:\n",
        "                grad = grad.unsqueeze(dim)\n",
        "            \n",
        "            self_ref.tensor.grad.add_(grad)\n",
        "\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "\n",
        "    def mean(self, dim=None, keepdim=False):\n",
        "        \"\"\"Computes the mean of elements along given dimensions.\"\"\"\n",
        "        result_tensor = self.tensor.mean(dim=dim, keepdim=keepdim)\n",
        "        if not self._custom_requires_grad:\n",
        "            return CustomTensor(result_tensor, due_to_operation=True)\n",
        "\n",
        "        graph = self.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "        graph.add_edge(self._node_id, result._node_id)\n",
        "\n",
        "        self_ref = weakref.proxy(self)\n",
        "        result_ref = weakref.proxy(result)\n",
        "        \n",
        "        # Determine the number of elements that were averaged\n",
        "        if dim is None:\n",
        "            n = self.tensor.numel()\n",
        "        else:\n",
        "            n = self.tensor.shape[dim]\n",
        "\n",
        "        def _backward():\n",
        "            if self_ref.tensor.grad is None:\n",
        "                self_ref._zero_grad()\n",
        "            \n",
        "            grad = result_ref.tensor.grad\n",
        "            if not keepdim and dim is not None:\n",
        "                grad = grad.unsqueeze(dim)\n",
        "            \n",
        "            # Distribute gradient evenly\n",
        "            self_ref.tensor.grad.add_(grad / n)\n",
        "\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "\n",
        "    def reshape(self, *shape):\n",
        "        \"\"\"Reshapes the tensor to the given shape.\"\"\"\n",
        "        original_shape = self.shape\n",
        "        result_tensor = self.tensor.reshape(*shape)\n",
        "        if not self._custom_requires_grad:\n",
        "            return CustomTensor(result_tensor, due_to_operation=True)\n",
        "\n",
        "        graph = self.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "        graph.add_edge(self._node_id, result._node_id)\n",
        "        \n",
        "        self_ref = weakref.proxy(self)\n",
        "        result_ref = weakref.proxy(result)\n",
        "        \n",
        "        def _backward():\n",
        "            if self_ref.tensor.grad is None:\n",
        "                self_ref._zero_grad()\n",
        "            self_ref.tensor.grad.add_(result_ref.tensor.grad.reshape(original_shape))\n",
        "\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "        \n",
        "    def transpose(self, dim0, dim1):\n",
        "        \"\"\"Transposes dimensions dim0 and dim1.\"\"\"\n",
        "        result_tensor = self.tensor.transpose(dim0, dim1)\n",
        "        if not self._custom_requires_grad:\n",
        "            return CustomTensor(result_tensor, due_to_operation=True)\n",
        "\n",
        "        graph = self.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "        graph.add_edge(self._node_id, result._node_id)\n",
        "\n",
        "        self_ref = weakref.proxy(self)\n",
        "        result_ref = weakref.proxy(result)\n",
        "\n",
        "        def _backward():\n",
        "            if self_ref.tensor.grad is None:\n",
        "                self_ref._zero_grad()\n",
        "            # The gradient operation for transpose is another transpose\n",
        "            self_ref.tensor.grad.add_(result_ref.tensor.grad.transpose(dim0, dim1))\n",
        "            \n",
        "        result._backward = _backward\n",
        "        return result\n",
        "\n",
        "    @property\n",
        "    def T(self):\n",
        "        \"\"\"Alias for transpose(-2, -1) for 2D or higher dimensional tensors.\"\"\"\n",
        "        if self.ndim < 2:\n",
        "            raise ValueError(\"`.T` is only supported on tensors with 2 or more dimensions.\")\n",
        "        return self.transpose(-2, -1)\n",
        "        \n",
        "    # --- Activation Functions ---\n",
        "\n",
        "    def relu(self):\n",
        "        \"\"\"Applies the Rectified Linear Unit function element-wise.\"\"\"\n",
        "        result_tensor = F.relu(self.tensor)\n",
        "        if not self._custom_requires_grad:\n",
        "            return CustomTensor(result_tensor, due_to_operation=True)\n",
        "\n",
        "        graph = self.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "        graph.add_edge(self._node_id, result._node_id)\n",
        "        \n",
        "        self_ref = weakref.proxy(self)\n",
        "        result_ref = weakref.proxy(result)\n",
        "        \n",
        "        def _backward():\n",
        "            if self_ref.tensor.grad is None: self_ref._zero_grad()\n",
        "            # Derivative is 1 for positive inputs, 0 otherwise\n",
        "            grad_mask = (self_ref.tensor > 0).type(self_ref.tensor.dtype)\n",
        "            self_ref.tensor.grad.add_(result_ref.tensor.grad * grad_mask)\n",
        "\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "\n",
        "    def tanh(self):\n",
        "        \"\"\"Applies the hyperbolic tangent function element-wise.\"\"\"\n",
        "        result_tensor = torch.tanh(self.tensor)\n",
        "        if not self._custom_requires_grad:\n",
        "            return CustomTensor(result_tensor, due_to_operation=True)\n",
        "\n",
        "        graph = self.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "        graph.add_edge(self._node_id, result._node_id)\n",
        "\n",
        "        self_ref = weakref.proxy(self)\n",
        "        result_ref = weakref.proxy(result)\n",
        "        \n",
        "        def _backward():\n",
        "            if self_ref.tensor.grad is None: self_ref._zero_grad()\n",
        "            # Derivative is 1 - tanh^2(x)\n",
        "            local_grad = 1 - result_tensor.pow(2)\n",
        "            self_ref.tensor.grad.add_(result_ref.tensor.grad * local_grad)\n",
        "\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "\n",
        "    def leaky_relu(self, negative_slope=0.01):\n",
        "        \"\"\"Applies the Leaky Rectified Linear Unit function element-wise.\"\"\"\n",
        "        result_tensor = F.leaky_relu(self.tensor, negative_slope)\n",
        "        if not self._custom_requires_grad:\n",
        "            return CustomTensor(result_tensor, due_to_operation=True)\n",
        "\n",
        "        graph = self.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "        graph.add_edge(self._node_id, result._node_id)\n",
        "\n",
        "        self_ref = weakref.proxy(self)\n",
        "        result_ref = weakref.proxy(result)\n",
        "\n",
        "        def _backward():\n",
        "            if self_ref.tensor.grad is None: self_ref._zero_grad()\n",
        "            # Derivative is 1 for positive, negative_slope for negative\n",
        "            local_grad = torch.ones_like(self_ref.tensor)\n",
        "            local_grad[self_ref.tensor < 0] = negative_slope\n",
        "            self_ref.tensor.grad.add_(result_ref.tensor.grad * local_grad)\n",
        "\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "\n",
        "    def elu(self, alpha=1.0):\n",
        "        \"\"\"Applies the Exponential Linear Unit function element-wise.\"\"\"\n",
        "        result_tensor = F.elu(self.tensor, alpha)\n",
        "        if not self._custom_requires_grad:\n",
        "            return CustomTensor(result_tensor, due_to_operation=True)\n",
        "\n",
        "        graph = self.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "        graph.add_edge(self._node_id, result._node_id)\n",
        "\n",
        "        self_ref = weakref.proxy(self)\n",
        "        result_ref = weakref.proxy(result)\n",
        "\n",
        "        def _backward():\n",
        "            if self_ref.tensor.grad is None: self_ref._zero_grad()\n",
        "            # Derivative is 1 for positive, and output + alpha for negative\n",
        "            local_grad = torch.ones_like(self_ref.tensor)\n",
        "            neg_mask = self_ref.tensor < 0\n",
        "            local_grad[neg_mask] = result_tensor[neg_mask] + alpha\n",
        "            self_ref.tensor.grad.add_(result_ref.tensor.grad * local_grad)\n",
        "\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "        \n",
        "    def silu(self):\n",
        "        \"\"\"Applies the Sigmoid-weighted Linear Unit function element-wise.\"\"\"\n",
        "        result_tensor = F.silu(self.tensor)\n",
        "        if not self._custom_requires_grad:\n",
        "            return CustomTensor(result_tensor, due_to_operation=True)\n",
        "\n",
        "        graph = self.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "        graph.add_edge(self._node_id, result._node_id)\n",
        "\n",
        "        self_ref = weakref.proxy(self)\n",
        "        result_ref = weakref.proxy(result)\n",
        "        \n",
        "        def _backward():\n",
        "            if self_ref.tensor.grad is None: self_ref._zero_grad()\n",
        "            # Derivative of x*sigmoid(x) is sigmoid(x) + x*sigmoid(x)*(1-sigmoid(x))\n",
        "            sig_x = torch.sigmoid(self_ref.tensor)\n",
        "            local_grad = sig_x * (1 + self_ref.tensor * (1 - sig_x))\n",
        "            self_ref.tensor.grad.add_(result_ref.tensor.grad * local_grad)\n",
        "\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "    \n",
        "    # Add swish as an alias for silu\n",
        "    swish = silu\n",
        "\n",
        "    def gelu(self):\n",
        "        \"\"\"Applies the Gaussian Error Linear Unit function element-wise.\"\"\"\n",
        "        result_tensor = F.gelu(self.tensor)\n",
        "        if not self._custom_requires_grad:\n",
        "            return CustomTensor(result_tensor, due_to_operation=True)\n",
        "\n",
        "        graph = self.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "        graph.add_edge(self._node_id, result._node_id)\n",
        "        \n",
        "        self_ref = weakref.proxy(self)\n",
        "        result_ref = weakref.proxy(result)\n",
        "        \n",
        "        def _backward():\n",
        "            if self_ref.tensor.grad is None: self_ref._zero_grad()\n",
        "            # Derivative of GELU: 0.5 * (1 + erf(x/sqrt(2))) + x * exp(-x^2/2) / sqrt(2*pi)\n",
        "            x = self_ref.tensor\n",
        "            cdf = 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n",
        "            pdf = torch.exp(-0.5 * x**2) / math.sqrt(2.0 * math.pi)\n",
        "            local_grad = cdf + x * pdf\n",
        "            self_ref.tensor.grad.add_(result_ref.tensor.grad * local_grad)\n",
        "\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "\n",
        "    def softmax(self, dim=-1):\n",
        "        \"\"\"Applies the softmax function along a given dimension.\"\"\"\n",
        "        result_tensor = torch.softmax(self.tensor, dim=dim)\n",
        "        if not self._custom_requires_grad:\n",
        "            return CustomTensor(result_tensor, due_to_operation=True)\n",
        "\n",
        "        graph = self.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "        graph.add_edge(self._node_id, result._node_id)\n",
        "        \n",
        "        self_ref = weakref.proxy(self)\n",
        "        result_ref = weakref.proxy(result)\n",
        "\n",
        "        def _backward():\n",
        "            if self_ref.tensor.grad is None: self_ref._zero_grad()\n",
        "            # For softmax, the jacobian-vector product is y * (grad - sum(grad * y))\n",
        "            y = result_tensor\n",
        "            grad_output = result_ref.tensor.grad\n",
        "            grad_input = y * (grad_output - (grad_output * y).sum(dim=dim, keepdim=True))\n",
        "            self_ref.tensor.grad.add_(grad_input)\n",
        "            \n",
        "        result._backward = _backward\n",
        "        return result\n",
        "    def backward(self, weightage_tensor=1):\n",
        "        if not self._custom_requires_grad:\n",
        "            raise RuntimeError(\"Output tensor does not require grad.\")\n",
        "        if self.graph is None:\n",
        "            raise RuntimeError(\"Output tensor is not part of a graph.\")\n",
        "        graph = self.graph\n",
        "        \n",
        "        # Initialize gradient for the output tensor\n",
        "        if isinstance(weightage_tensor, numbers.Number):\n",
        "            self.tensor.grad = torch.full_like(self.tensor, fill_value=weightage_tensor)\n",
        "        elif isinstance(weightage_tensor, torch.Tensor):\n",
        "            self.tensor.grad = weightage_tensor.clone()\n",
        "\n",
        "        nodes_to_process = graph.reverse_toposort_from_tensor(self._node_id)\n",
        "        \n",
        "        for tensor_node in nodes_to_process:\n",
        "            tensor_node._backward()\n",
        "            #try:\n",
        "                # The node is a weakref.proxy, check if it's still alive\n",
        "                #if tensor_node.__class__ is weakref.ProxyType:\n",
        "            #        tensor_node._backward()\n",
        "            # except ReferenceError:\n",
        "            #     # The tensor object was garbage collected, skip.\n",
        "            #     print(\"dead reference node encountered\")\n",
        "            #     continue\n",
        "    # --- Properties and Dunder Methods ---\n",
        "    @property\n",
        "    def dtype(self): return self.tensor.dtype\n",
        "    @property\n",
        "    def ndim(self): return self.tensor.ndim\n",
        "    @property\n",
        "    def shape(self): return self.tensor.shape\n",
        "    @property\n",
        "    def grad(self): return self.tensor.grad\n",
        "    def __repr__(self): return f\"CustomTensor({self.tensor}, grad_fn={self._backward != None}, requires_grad={self._custom_requires_grad})\"\n",
        "    def __del__(self):\n",
        "        if self._node_id is not None and self._is_leaf:\n",
        "            try:\n",
        "                if self.graph: self.graph.delete_node(self._node_id)\n",
        "            except ReferenceError: # Graph might be gone first\n",
        "                pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Autograd Tester"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import numbers\n",
        "import weakref\n",
        "import rustworkx as rx\n",
        "from typing import Optional, Any\n",
        "import sys\n",
        "import gc\n",
        "import pytest\n",
        "\n",
        "\n",
        "class AutogradTester:\n",
        "    def __init__(self):\n",
        "        self.passed_tests = 0\n",
        "        self.failed_tests = 0\n",
        "        self.tolerance = 1e-6  # Increased tolerance slightly for complex ops\n",
        "\n",
        "    def assert_tensors_close(self, custom_tensor, pytorch_tensor, test_name, check_grad=True):\n",
        "        \"\"\"Compare custom tensor with PyTorch tensor values and optionally gradients.\"\"\"\n",
        "        try:\n",
        "            # Check values\n",
        "            np.testing.assert_allclose(\n",
        "                custom_tensor.tensor.detach().cpu().numpy(),  # Ensure on CPU for numpy\n",
        "                pytorch_tensor.detach().cpu().numpy(),\n",
        "                rtol=self.tolerance,\n",
        "                atol=self.tolerance,\n",
        "                err_msg=f\"Mismatch in tensor values for {test_name}\"\n",
        "            )\n",
        "\n",
        "            # Check gradients if requested and they exist for PyTorch tensor\n",
        "            if check_grad and pytorch_tensor.grad is not None:\n",
        "                if custom_tensor.tensor.grad is None:\n",
        "                    raise AssertionError(f\"Custom tensor has no gradient for {test_name}, but PyTorch does.\")\n",
        "\n",
        "                np.testing.assert_allclose(\n",
        "                    custom_tensor.tensor.grad.detach().cpu().numpy(),  # Ensure on CPU for numpy\n",
        "                    pytorch_tensor.grad.detach().cpu().numpy(),\n",
        "                    rtol=self.tolerance,\n",
        "                    atol=self.tolerance,\n",
        "                    err_msg=f\"Mismatch in gradients for {test_name}\"\n",
        "                )\n",
        "            elif check_grad and pytorch_tensor.grad is None and custom_tensor.tensor.grad is not None:\n",
        "                raise AssertionError(f\"Custom tensor has gradient for {test_name}, but PyTorch does not (should be no_grad).\")\n",
        "\n",
        "            print(f\" {test_name}\")\n",
        "            self.passed_tests += 1\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\" {test_name}: {str(e)}\")\n",
        "            self.failed_tests += 1\n",
        "\n",
        "    def test_basic_operations(self):\n",
        "        \"\"\"Test basic arithmetic operations\"\"\"\n",
        "        print(\"\\n=== Testing Basic Operations ===\")\n",
        "\n",
        "        # Test scalar addition\n",
        "        with AutogradGraph() as graph:\n",
        "            x_custom = CustomTensor([2.0, 3.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = x_custom + 5.0\n",
        "            y_custom.backward(torch.ones_like(y_custom.tensor))\n",
        "\n",
        "            x_pytorch = torch.tensor([2.0, 3.0], requires_grad=True)\n",
        "            y_pytorch = x_pytorch + 5.0\n",
        "            y_pytorch.backward(torch.ones_like(y_pytorch))\n",
        "\n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Scalar Addition - x\")\n",
        "            self.assert_tensors_close(y_custom, y_pytorch, \"Scalar Addition - y (result)\", check_grad=False)\n",
        "\n",
        "        # Test tensor addition\n",
        "        with AutogradGraph() as graph:\n",
        "            x_custom = CustomTensor([1.0, 2.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = CustomTensor([3.0, 4.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            z_custom = x_custom + y_custom\n",
        "            z_custom.backward(torch.ones_like(z_custom.tensor))\n",
        "\n",
        "            x_pytorch = torch.tensor([1.0, 2.0], requires_grad=True)\n",
        "            y_pytorch = torch.tensor([3.0, 4.0], requires_grad=True)\n",
        "            z_pytorch = x_pytorch + y_pytorch\n",
        "            z_pytorch.backward(torch.ones_like(z_pytorch))\n",
        "\n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Tensor Addition - x\")\n",
        "            self.assert_tensors_close(y_custom, y_pytorch, \"Tensor Addition - y\")\n",
        "            self.assert_tensors_close(z_custom, z_pytorch, \"Tensor Addition - z (result)\", check_grad=False)\n",
        "\n",
        "    def test_multiplication(self):\n",
        "        \"\"\"Test multiplication operations\"\"\"\n",
        "        print(\"\\n=== Testing Multiplication ===\")\n",
        "\n",
        "        # Test scalar multiplication\n",
        "        with AutogradGraph() as graph:\n",
        "            x_custom = CustomTensor([2.0, 3.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = x_custom * 4.0\n",
        "            y_custom.backward(torch.ones_like(y_custom.tensor))\n",
        "\n",
        "            x_pytorch = torch.tensor([2.0, 3.0], requires_grad=True)\n",
        "            y_pytorch = x_pytorch * 4.0\n",
        "            y_pytorch.backward(torch.ones_like(y_pytorch))\n",
        "\n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Scalar Multiplication - x\")\n",
        "            self.assert_tensors_close(y_custom, y_pytorch, \"Scalar Multiplication - y (result)\", check_grad=False)\n",
        "\n",
        "        # Test tensor multiplication\n",
        "        with AutogradGraph() as graph:\n",
        "            x_custom = CustomTensor([2.0, 3.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = CustomTensor([4.0, 5.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            z_custom = x_custom * y_custom\n",
        "            z_custom.backward(torch.ones_like(z_custom.tensor))\n",
        "\n",
        "            x_pytorch = torch.tensor([2.0, 3.0], requires_grad=True)\n",
        "            y_pytorch = torch.tensor([4.0, 5.0], requires_grad=True)\n",
        "            z_pytorch = x_pytorch * y_pytorch\n",
        "            z_pytorch.backward(torch.ones_like(z_pytorch))\n",
        "\n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Tensor Multiplication - x\")\n",
        "            self.assert_tensors_close(y_custom, y_pytorch, \"Tensor Multiplication - y\")\n",
        "            self.assert_tensors_close(z_custom, z_pytorch, \"Tensor Multiplication - z (result)\", check_grad=False)\n",
        "\n",
        "    def test_subtraction_division(self):\n",
        "        \"\"\"Test subtraction and division\"\"\"\n",
        "        print(\"\\n=== Testing Subtraction and Division ===\")\n",
        "\n",
        "        # Test scalar subtraction (x - C)\n",
        "        with AutogradGraph() as graph:\n",
        "            x_custom = CustomTensor([5.0, 6.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = x_custom - 2.0\n",
        "            y_custom.backward(torch.ones_like(y_custom.tensor))\n",
        "\n",
        "            x_pytorch = torch.tensor([5.0, 6.0], requires_grad=True)\n",
        "            y_pytorch = x_pytorch - 2.0\n",
        "            y_pytorch.backward(torch.ones_like(y_pytorch))\n",
        "\n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Scalar Subtraction (x - C) - x\")\n",
        "            self.assert_tensors_close(y_custom, y_pytorch, \"Scalar Subtraction (x - C) - y (result)\", check_grad=False)\n",
        "\n",
        "        # Test scalar reverse subtraction (C - x)\n",
        "        with AutogradGraph() as graph:\n",
        "            x_custom = CustomTensor([5.0, 6.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = 10.0 - x_custom  # Uses __rsub__\n",
        "            y_custom.backward(torch.ones_like(y_custom.tensor))\n",
        "\n",
        "            x_pytorch = torch.tensor([5.0, 6.0], requires_grad=True)\n",
        "            y_pytorch = 10.0 - x_pytorch\n",
        "            y_pytorch.backward(torch.ones_like(y_pytorch))\n",
        "\n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Scalar Reverse Subtraction (C - x) - x\")\n",
        "            self.assert_tensors_close(y_custom, y_pytorch, \"Scalar Reverse Subtraction (C - x) - y (result)\", check_grad=False)\n",
        "\n",
        "        # Test tensor subtraction\n",
        "        with AutogradGraph() as graph:\n",
        "            x_custom = CustomTensor([7.0, 8.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = CustomTensor([2.0, 1.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            z_custom = x_custom - y_custom\n",
        "            z_custom.backward(torch.ones_like(z_custom.tensor))\n",
        "\n",
        "            x_pytorch = torch.tensor([7.0, 8.0], requires_grad=True)\n",
        "            y_pytorch = torch.tensor([2.0, 1.0], requires_grad=True)\n",
        "            z_pytorch = x_pytorch - y_pytorch\n",
        "            z_pytorch.backward(torch.ones_like(z_pytorch))\n",
        "\n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Tensor Subtraction - x\")\n",
        "            self.assert_tensors_close(y_custom, y_pytorch, \"Tensor Subtraction - y\")\n",
        "            self.assert_tensors_close(z_custom, z_pytorch, \"Tensor Subtraction - z (result)\", check_grad=False)\n",
        "\n",
        "        # Test scalar division\n",
        "        with AutogradGraph() as graph:\n",
        "            x_custom = CustomTensor([8.0, 12.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = x_custom / 4.0\n",
        "            y_custom.backward(torch.ones_like(y_custom.tensor))\n",
        "\n",
        "            x_pytorch = torch.tensor([8.0, 12.0], requires_grad=True)\n",
        "            y_pytorch = x_pytorch / 4.0\n",
        "            y_pytorch.backward(torch.ones_like(y_pytorch))\n",
        "\n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Scalar Division - x\")\n",
        "            self.assert_tensors_close(y_custom, y_pytorch, \"Scalar Division - y (result)\", check_grad=False)\n",
        "        # Test tensor division\n",
        "        with AutogradGraph() as graph:\n",
        "            x_custom = CustomTensor([8.0, 12.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = CustomTensor([5.0, 10.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            z_custom = x_custom / y_custom\n",
        "            z_custom.backward(torch.ones_like(z_custom.tensor))\n",
        "\n",
        "            x_pytorch = torch.tensor([8.0, 12.0], requires_grad=True)\n",
        "            y_pytorch = torch.tensor([5.0, 10.0], requires_grad=True)\n",
        "            z_pytorch = x_pytorch / y_pytorch\n",
        "            z_pytorch.backward(torch.ones_like(z_pytorch))\n",
        "\n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Tensor Division - x\")\n",
        "            self.assert_tensors_close(y_custom, y_pytorch, \"Tensir Division - y\")\n",
        "            self.assert_tensors_close(z_custom, z_pytorch, \"Tensor Division - z (result)\", check_grad=False)\n",
        "\n",
        "\n",
        "    def test_power_function(self):\n",
        "        \"\"\"Test power operation\"\"\"\n",
        "        print(\"\\n=== Testing Power Function ===\")\n",
        "\n",
        "        with AutogradGraph() as graph:\n",
        "            x_custom = CustomTensor([2.0, 3.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = x_custom.pow(3.0)\n",
        "            y_custom.backward(torch.ones_like(y_custom.tensor))\n",
        "\n",
        "            x_pytorch = torch.tensor([2.0, 3.0], requires_grad=True)\n",
        "            y_pytorch = torch.pow(x_pytorch, 3.0)\n",
        "            y_pytorch.backward(torch.ones_like(y_pytorch))\n",
        "\n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Power Function - x\")\n",
        "            self.assert_tensors_close(y_custom, y_pytorch, \"Power Function - y (result)\", check_grad=False)\n",
        "\n",
        "        # Test power with negative exponent\n",
        "        with AutogradGraph() as graph:\n",
        "            x_custom = CustomTensor([2.0, 3.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = x_custom.pow(-2.0)\n",
        "            y_custom.backward(torch.ones_like(y_custom.tensor))\n",
        "\n",
        "            x_pytorch = torch.tensor([2.0, 3.0], requires_grad=True)\n",
        "            y_pytorch = torch.pow(x_pytorch, -2.0)\n",
        "            y_pytorch.backward(torch.ones_like(y_pytorch))\n",
        "\n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Power Function (Negative Exponent) - x\")\n",
        "            self.assert_tensors_close(y_custom, y_pytorch, \"Power Function (Negative Exponent) - y (result)\", check_grad=False)\n",
        "\n",
        "    def test_unary_functions(self):\n",
        "        \"\"\"Test unary mathematical functions\"\"\"\n",
        "        print(\"\\n=== Testing Unary Functions ===\")\n",
        "\n",
        "        # Test exp\n",
        "        with AutogradGraph() as graph:\n",
        "            x_custom = CustomTensor([1.0, 2.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = x_custom.exp()\n",
        "            y_custom.backward(torch.ones_like(y_custom.tensor))\n",
        "\n",
        "            x_pytorch = torch.tensor([1.0, 2.0], requires_grad=True)\n",
        "            y_pytorch = torch.exp(x_pytorch)\n",
        "            y_pytorch.backward(torch.ones_like(y_pytorch))\n",
        "\n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Exponential Function - x\")\n",
        "            self.assert_tensors_close(y_custom, y_pytorch, \"Exponential Function - y (result)\", check_grad=False)\n",
        "\n",
        "        # Test log\n",
        "        with AutogradGraph() as graph:\n",
        "            x_custom = CustomTensor([1.0, 2.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = x_custom.log()\n",
        "            y_custom.backward(torch.ones_like(y_custom.tensor))\n",
        "\n",
        "            x_pytorch = torch.tensor([1.0, 2.0], requires_grad=True)\n",
        "            y_pytorch = torch.log(x_pytorch)\n",
        "            y_pytorch.backward(torch.ones_like(y_pytorch))\n",
        "\n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Logarithm Function - x\")\n",
        "            self.assert_tensors_close(y_custom, y_pytorch, \"Logarithm Function - y (result)\", check_grad=False)\n",
        "\n",
        "        # Test sin\n",
        "        with AutogradGraph() as graph:\n",
        "            x_custom = CustomTensor([0.5, 1.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = x_custom.sin()\n",
        "            y_custom.backward(torch.ones_like(y_custom.tensor))\n",
        "\n",
        "            x_pytorch = torch.tensor([0.5, 1.0], requires_grad=True)\n",
        "            y_pytorch = torch.sin(x_pytorch)\n",
        "            y_pytorch.backward(torch.ones_like(y_pytorch))\n",
        "\n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Sine Function - x\")\n",
        "            self.assert_tensors_close(y_custom, y_pytorch, \"Sine Function - y (result)\", check_grad=False)\n",
        "\n",
        "        # Test cos\n",
        "        with AutogradGraph() as graph:\n",
        "            x_custom = CustomTensor([0.5, 1.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = x_custom.cos()\n",
        "            y_custom.backward(torch.ones_like(y_custom.tensor))\n",
        "\n",
        "            x_pytorch = torch.tensor([0.5, 1.0], requires_grad=True)\n",
        "            y_pytorch = torch.cos(x_pytorch)\n",
        "            y_pytorch.backward(torch.ones_like(y_pytorch))\n",
        "\n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Cosine Function - x\")\n",
        "            self.assert_tensors_close(y_custom, y_pytorch, \"Cosine Function - y (result)\", check_grad=False)\n",
        "\n",
        "        # Test sqrt\n",
        "        with AutogradGraph() as graph:\n",
        "            x_custom = CustomTensor([4.0, 9.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = x_custom.sqrt()\n",
        "            y_custom.backward(torch.ones_like(y_custom.tensor))\n",
        "\n",
        "            x_pytorch = torch.tensor([4.0, 9.0], requires_grad=True)\n",
        "            y_pytorch = torch.sqrt(x_pytorch)\n",
        "            y_pytorch.backward(torch.ones_like(y_pytorch))\n",
        "\n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Square Root Function - x\")\n",
        "            self.assert_tensors_close(y_custom, y_pytorch, \"Square Root Function - y (result)\", check_grad=False)\n",
        "\n",
        "    def test_matrix_operations(self):\n",
        "        \"\"\"Test matrix operations\"\"\"\n",
        "        print(\"\\n=== Testing Matrix Operations ===\")\n",
        "\n",
        "        # Test matrix multiplication (2x2 @ 2x2)\n",
        "        with AutogradGraph() as graph:\n",
        "            x_custom = CustomTensor([[1.0, 2.0], [3.0, 4.0]], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = CustomTensor([[5.0, 6.0], [7.0, 8.0]], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            z_custom = x_custom.matmul(y_custom)\n",
        "            z_custom.backward(torch.ones_like(z_custom.tensor))\n",
        "\n",
        "            x_pytorch = torch.tensor([[1.0, 2.0], [3.0, 4.0]], requires_grad=True)\n",
        "            y_pytorch = torch.tensor([[5.0, 6.0], [7.0, 8.0]], requires_grad=True)\n",
        "            z_pytorch = torch.matmul(x_pytorch, y_pytorch)\n",
        "            z_pytorch.backward(torch.ones_like(z_pytorch))\n",
        "\n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Matrix Multiplication (2x2 @ 2x2) - x\")\n",
        "            self.assert_tensors_close(y_custom, y_pytorch, \"Matrix Multiplication (2x2 @ 2x2) - y\")\n",
        "            self.assert_tensors_close(z_custom, z_pytorch, \"Matrix Multiplication (2x2 @ 2x2) - z (result)\", check_grad=False)\n",
        "\n",
        "        # Test matrix multiplication (2x3 @ 3x2)\n",
        "        with AutogradGraph() as graph:\n",
        "            x_custom = CustomTensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = CustomTensor([[7.0, 8.0], [9.0, 10.0], [11.0, 12.0]], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            z_custom = x_custom.matmul(y_custom)\n",
        "            z_custom.backward(torch.ones_like(z_custom.tensor))\n",
        "\n",
        "            x_pytorch = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]], requires_grad=True)\n",
        "            y_pytorch = torch.tensor([[7.0, 8.0], [9.0, 10.0], [11.0, 12.0]], requires_grad=True)\n",
        "            z_pytorch = torch.matmul(x_pytorch, y_pytorch)\n",
        "            z_pytorch.backward(torch.ones_like(z_pytorch))\n",
        "\n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Matrix Multiplication (2x3 @ 3x2) - x\")\n",
        "            self.assert_tensors_close(y_custom, y_pytorch, \"Matrix Multiplication (2x3 @ 3x2) - y\")\n",
        "            self.assert_tensors_close(z_custom, z_pytorch, \"Matrix Multiplication (2x3 @ 3x2) - z (result)\", check_grad=False)\n",
        "\n",
        "        # Test dot product (vector * vector)\n",
        "        with AutogradGraph() as graph:\n",
        "            x_custom = CustomTensor([1.0, 2.0, 3.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = CustomTensor([4.0, 5.0, 6.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            z_custom = x_custom.dot(y_custom)\n",
        "            z_custom.backward()  # Scalar output, so default backward() is fine (grad=1)\n",
        "\n",
        "            x_pytorch = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
        "            y_pytorch = torch.tensor([4.0, 5.0, 6.0], requires_grad=True)\n",
        "            z_pytorch = torch.dot(x_pytorch, y_pytorch)\n",
        "            z_pytorch.backward()\n",
        "\n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Dot Product (vector) - x\")\n",
        "            self.assert_tensors_close(y_custom, y_pytorch, \"Dot Product (vector) - y\")\n",
        "            self.assert_tensors_close(z_custom, z_pytorch, \"Dot Product (vector) - z (result)\", check_grad=False)\n",
        "\n",
        "    def test_complex_chain(self):\n",
        "        \"\"\"Test complex computational chains\"\"\"\n",
        "        print(\"\\n=== Testing Complex Chains ===\")\n",
        "\n",
        "        # Test 1: z = (x + y) * (x - y) + x^2 - sin(y)\n",
        "        with AutogradGraph() as graph:\n",
        "            x_custom = CustomTensor([3.0, 4.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = CustomTensor([1.0, 2.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "\n",
        "            sum_custom = x_custom + y_custom\n",
        "            diff_custom = x_custom - y_custom\n",
        "            prod_custom = sum_custom * diff_custom\n",
        "            x_squared_custom = x_custom.pow(2.0)\n",
        "            sin_y_custom = y_custom.sin()\n",
        "\n",
        "            inter1_custom = prod_custom + x_squared_custom\n",
        "            z_custom = inter1_custom - sin_y_custom\n",
        "\n",
        "            z_custom.backward(torch.ones_like(z_custom.tensor))\n",
        "\n",
        "            x_pytorch = torch.tensor([3.0, 4.0], requires_grad=True)\n",
        "            y_pytorch = torch.tensor([1.0, 2.0], requires_grad=True)\n",
        "\n",
        "            sum_pytorch = x_pytorch + y_pytorch\n",
        "            diff_pytorch = x_pytorch - y_pytorch\n",
        "            prod_pytorch = sum_pytorch * diff_pytorch\n",
        "            x_squared_pytorch = torch.pow(x_pytorch, 2.0)\n",
        "            sin_y_pytorch = torch.sin(y_pytorch)\n",
        "\n",
        "            inter1_pytorch = prod_pytorch + x_squared_pytorch\n",
        "            z_pytorch = inter1_pytorch - sin_y_pytorch\n",
        "\n",
        "            z_pytorch.backward(torch.ones_like(z_pytorch))\n",
        "\n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Complex Chain 1 - x\")\n",
        "            self.assert_tensors_close(y_custom, y_pytorch, \"Complex Chain 1 - y\")\n",
        "            self.assert_tensors_close(z_custom, z_pytorch, \"Complex Chain 1 - z (result)\", check_grad=False)\n",
        "\n",
        "        # Test 2: Multiple paths to a leaf: z = x*y + x*x + y*z_fixed\n",
        "        with AutogradGraph() as graph:\n",
        "            x_custom = CustomTensor([2.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = CustomTensor([3.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            z_fixed_custom = CustomTensor([0.5])  # No grad\n",
        "\n",
        "            term1_custom = x_custom * y_custom\n",
        "            term2_custom = x_custom * x_custom  # x appears twice\n",
        "            term3_custom = y_custom * z_fixed_custom  # y appears twice, one with no-grad\n",
        "\n",
        "            inter_custom = term1_custom + term2_custom\n",
        "            z_custom = inter_custom + term3_custom\n",
        "            z_custom.backward()\n",
        "\n",
        "            x_pytorch = torch.tensor([2.0], requires_grad=True)\n",
        "            y_pytorch = torch.tensor([3.0], requires_grad=True)\n",
        "            z_fixed_pytorch = torch.tensor([0.5])  # No grad\n",
        "\n",
        "            term1_pytorch = x_pytorch * y_pytorch\n",
        "            term2_pytorch = x_pytorch * x_pytorch\n",
        "            term3_pytorch = y_pytorch * z_fixed_pytorch\n",
        "\n",
        "            inter_pytorch = term1_pytorch + term2_pytorch\n",
        "            z_pytorch = inter_pytorch + term3_pytorch\n",
        "            z_pytorch.backward()\n",
        "\n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Complex Chain 2 (Multiple Paths) - x\")\n",
        "            self.assert_tensors_close(y_custom, y_pytorch, \"Complex Chain 2 (Multiple Paths) - y\")\n",
        "            self.assert_tensors_close(z_custom, z_pytorch, \"Complex Chain 2 (Multiple Paths) - z (result)\", check_grad=False)\n",
        "\n",
        "        # Test 3: Deeper Chain with Mixed Ops: (exp(x) * log(y)) / sqrt(x+y)\n",
        "        with AutogradGraph() as graph:\n",
        "            x_custom = CustomTensor([1.5], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = CustomTensor([2.5], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "\n",
        "            exp_x_custom = x_custom.exp()\n",
        "            log_y_custom = y_custom.log()\n",
        "            numerator_custom = exp_x_custom * log_y_custom\n",
        "\n",
        "            sum_xy_custom = x_custom + y_custom\n",
        "            sqrt_sum_custom = sum_xy_custom.sqrt()\n",
        "\n",
        "            z_custom = numerator_custom / sqrt_sum_custom\n",
        "            z_custom.backward()\n",
        "\n",
        "            x_pytorch = torch.tensor([1.5], requires_grad=True)\n",
        "            y_pytorch = torch.tensor([2.5], requires_grad=True)\n",
        "\n",
        "            exp_x_pytorch = torch.exp(x_pytorch)\n",
        "            log_y_pytorch = torch.log(y_pytorch)\n",
        "            numerator_pytorch = exp_x_pytorch * log_y_pytorch\n",
        "\n",
        "            sum_xy_pytorch = x_pytorch + y_pytorch\n",
        "            sqrt_sum_pytorch = torch.sqrt(sum_xy_pytorch)\n",
        "\n",
        "            z_pytorch = numerator_pytorch / sqrt_sum_pytorch\n",
        "            z_pytorch.backward()\n",
        "\n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Complex Chain 3 (Deeper Mixed Ops) - x\")\n",
        "            self.assert_tensors_close(y_custom, y_pytorch, \"Complex Chain 3 (Deeper Mixed Ops) - y\")\n",
        "            self.assert_tensors_close(z_custom, z_pytorch, \"Complex Chain 3 (Deeper Mixed Ops) - z (result)\", check_grad=False)\n",
        "\n",
        "    def test_mixed_operations(self):\n",
        "        \"\"\"Test mixing operations with and without gradients\"\"\"\n",
        "        print(\"\\n=== Testing Mixed Operations ===\")\n",
        "\n",
        "        # One tensor requires grad, other doesn't (multiplication)\n",
        "        with AutogradGraph() as graph:\n",
        "            x_custom = CustomTensor([2.0, 3.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = CustomTensor([4.0, 5.0])  # No grad\n",
        "            z_custom = x_custom * y_custom\n",
        "            z_custom.backward(torch.ones_like(z_custom.tensor))\n",
        "\n",
        "            x_pytorch = torch.tensor([2.0, 3.0], requires_grad=True)\n",
        "            y_pytorch = torch.tensor([4.0, 5.0])  # No grad\n",
        "            z_pytorch = x_pytorch * y_pytorch\n",
        "            z_pytorch.backward(torch.ones_like(z_pytorch))\n",
        "\n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Mixed Operations (X*Y, Y no grad) - x\")\n",
        "            # Check that y_custom has no grad\n",
        "            self.assert_tensors_close(y_custom, y_pytorch, \"Mixed Operations (X*Y, Y no grad) - y\", check_grad=False)\n",
        "            self.assert_tensors_close(z_custom, z_pytorch, \"Mixed Operations (X*Y, Y no grad) - z (result)\", check_grad=False)\n",
        "\n",
        "        # One tensor requires grad, other doesn't (addition)\n",
        "        with AutogradGraph() as graph:\n",
        "            x_custom = CustomTensor([10.0, 20.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = CustomTensor([1.0, 2.0])  # No grad\n",
        "            z_custom = x_custom + y_custom\n",
        "            z_custom.backward(torch.ones_like(z_custom.tensor))\n",
        "\n",
        "            x_pytorch = torch.tensor([10.0, 20.0], requires_grad=True)\n",
        "            y_pytorch = torch.tensor([1.0, 2.0])  # No grad\n",
        "            z_pytorch = x_pytorch + y_pytorch\n",
        "            z_pytorch.backward(torch.ones_like(z_pytorch))\n",
        "\n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Mixed Operations (X+Y, Y no grad) - x\")\n",
        "            self.assert_tensors_close(y_custom, y_pytorch, \"Mixed Operations (X+Y, Y no grad) - y\", check_grad=False)\n",
        "            self.assert_tensors_close(z_custom, z_pytorch, \"Mixed Operations (X+Y, Y no grad) - z (result)\", check_grad=False)\n",
        "\n",
        "    def test_broadcasting(self):\n",
        "        \"\"\"Test operations with broadcasting\"\"\"\n",
        "        print(\"\\n=== Testing Broadcasting ===\")\n",
        "\n",
        "        # Vector + scalar\n",
        "        with AutogradGraph() as graph:\n",
        "            x_custom = CustomTensor([1.0, 2.0, 3.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = x_custom + 10.0\n",
        "            y_custom.backward(torch.tensor([1.0, 1.0, 1.0]))\n",
        "\n",
        "            x_pytorch = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
        "            y_pytorch = x_pytorch + 10.0\n",
        "            y_pytorch.backward(torch.tensor([1.0, 1.0, 1.0]))\n",
        "\n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Broadcasting: Vector + Scalar - x\")\n",
        "            self.assert_tensors_close(y_custom, y_pytorch, \"Broadcasting: Vector + Scalar - y (result)\", check_grad=False)\n",
        "\n",
        "        # Matrix + vector (row broadcasting)\n",
        "        with AutogradGraph() as graph:\n",
        "            x_custom = CustomTensor([[1.0, 2.0], [3.0, 4.0]], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = CustomTensor([10.0, 20.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            z_custom = x_custom + y_custom  # y broadcasts to rows of x\n",
        "            z_custom.backward(torch.ones_like(z_custom.tensor))\n",
        "\n",
        "            x_pytorch = torch.tensor([[1.0, 2.0], [3.0, 4.0]], requires_grad=True)\n",
        "            y_pytorch = torch.tensor([10.0, 20.0], requires_grad=True)\n",
        "            z_pytorch = x_pytorch + y_pytorch\n",
        "            z_pytorch.backward(torch.ones_like(z_pytorch))\n",
        "\n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Broadcasting: Matrix + Vector (row) - x\")\n",
        "            # For broadcasted operations, the gradient needs to be summed over the broadcasted dimensions\n",
        "            # PyTorch handles this automatically. Your custom backward for add should accumulate.\n",
        "            self.assert_tensors_close(y_custom, y_pytorch, \"Broadcasting: Matrix + Vector (row) - y\")\n",
        "            self.assert_tensors_close(z_custom, z_pytorch, \"Broadcasting: Matrix + Vector (row) - z (result)\", check_grad=False)\n",
        "\n",
        "        # Matrix * scalar\n",
        "        with AutogradGraph() as graph:\n",
        "            x_custom = CustomTensor([[1.0, 2.0], [3.0, 4.0]], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = x_custom * 5.0\n",
        "            y_custom.backward(torch.ones_like(y_custom.tensor))\n",
        "\n",
        "            x_pytorch = torch.tensor([[1.0, 2.0], [3.0, 4.0]], requires_grad=True)\n",
        "            y_pytorch = x_pytorch * 5.0\n",
        "            y_pytorch.backward(torch.ones_like(y_pytorch))\n",
        "\n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Broadcasting: Matrix * Scalar - x\")\n",
        "            self.assert_tensors_close(y_custom, y_pytorch, \"Broadcasting: Matrix * Scalar - y (result)\", check_grad=False)\n",
        "\n",
        "    def test_backward_with_custom_grad(self):\n",
        "        \"\"\"Test backward pass with a custom initial gradient tensor.\"\"\"\n",
        "        print(\"\\n=== Testing Backward with Custom Grad ===\")\n",
        "\n",
        "        with AutogradGraph() as graph:\n",
        "            x_custom = CustomTensor([2.0, 3.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = x_custom * 4.0 + 1.0\n",
        "\n",
        "            custom_grad_output = torch.tensor([0.5, 2.0])\n",
        "            y_custom.backward(custom_grad_output)\n",
        "\n",
        "            x_pytorch = torch.tensor([2.0, 3.0], requires_grad=True)\n",
        "            y_pytorch = x_pytorch * 4.0 + 1.0\n",
        "\n",
        "            pytorch_grad_output = torch.tensor([0.5, 2.0])\n",
        "            y_pytorch.backward(pytorch_grad_output)\n",
        "\n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Backward with Custom Grad - x\")\n",
        "            self.assert_tensors_close(y_custom, y_pytorch, \"Backward with Custom Grad - y (result)\", check_grad=False)\n",
        "\n",
        "    def test_zero_grad_behavior(self):\n",
        "        \"\"\"Test _zero_grad and subsequent backward calls.\"\"\"\n",
        "        print(\"\\n=== Testing Zero Grad Behavior ===\")\n",
        "        with AutogradGraph() as graph:\n",
        "            x_custom = CustomTensor([1.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = x_custom * 2\n",
        "            z_custom = y_custom + 3\n",
        "            z_custom.backward()  # First backward\n",
        "\n",
        "            self.assert_tensors_close(x_custom, torch.tensor([1.0], requires_grad=True), \"Zero Grad Init (first backward) - x\",check_grad=False)\n",
        "\n",
        "            z_custom._zero_grad()  # Manually zero for custom\n",
        "            y_custom._zero_grad()  # Manually zero for custom\n",
        "            x_custom._zero_grad()  # Manually zero for custom leaf\n",
        "\n",
        "            # Do another backward pass\n",
        "            z_custom.backward()  # Should accumulate again from 1.0\n",
        "\n",
        "            x_pytorch = torch.tensor([1.0], requires_grad=True)\n",
        "            y_pytorch = x_pytorch * 2\n",
        "            z_pytorch = y_pytorch + 3\n",
        "            z_pytorch.backward(retain_graph=True)\n",
        "\n",
        "            x_pytorch.grad.zero_()\n",
        "            z_pytorch.backward()  # PyTorch accumulates if not zeroed explicitly\n",
        "\n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Zero Grad Behavior - x (after 2nd backward)\")\n",
        "            self.assert_tensors_close(z_custom, z_pytorch, \"Zero Grad Behavior - z (result, after 2nd backward)\", check_grad=False)\n",
        "\n",
        "    def test_no_grad_flow(self):\n",
        "        \"\"\"Test that gradients do not flow to tensors not requiring grad.\"\"\"\n",
        "        print(\"\\n=== Testing No Grad Flow ===\")\n",
        "        with AutogradGraph() as graph:\n",
        "            x_custom = CustomTensor([5.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = CustomTensor([2.0], _custom_requires_grad=False)  # Does NOT require grad\n",
        "            z_custom = x_custom * y_custom\n",
        "            z_custom.backward()\n",
        "\n",
        "            x_pytorch = torch.tensor([5.0], requires_grad=True)\n",
        "            y_pytorch = torch.tensor([2.0], requires_grad=False)\n",
        "            z_pytorch = x_pytorch * y_pytorch\n",
        "            z_pytorch.backward()\n",
        "\n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"No Grad Flow - x (requires grad)\")\n",
        "            # PyTorch's .grad for non-requiring-grad tensors is None\n",
        "            # Our CustomTensor.tensor.grad for non-requiring-grad should also be None\n",
        "            try:\n",
        "                # Check that y_custom.tensor.grad is None\n",
        "                if y_custom.tensor.grad is not None:\n",
        "                    raise AssertionError(\"Custom non-grad tensor unexpectedly has a gradient.\")\n",
        "                print(f\" No Grad Flow - y (no grad, custom correctly None)\")\n",
        "                self.passed_tests += 1\n",
        "            except Exception as e:\n",
        "                print(f\" No Grad Flow - y (no grad): {str(e)}\")\n",
        "                self.failed_tests += 1\n",
        "\n",
        "    def test_basic_add_scalar_grad_system(self):\n",
        "        print(\"\\n=== System Test: Basic Scalar Add Grad ===\")\n",
        "        try:\n",
        "            with AutogradGraph() as graph:\n",
        "                a = CustomTensor(torch.tensor([2.0, 3.0]), _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "                b = a + 5.0  # (a + 5)\n",
        "                c = b + 10.0  # (a + 5 + 10)\n",
        "\n",
        "                # Manually run backward pass\n",
        "                c.backward(weightage_tensor=1)\n",
        "\n",
        "                # Expected gradients:\n",
        "                # dC/dA = 1.0 (for each element)\n",
        "                assert torch.allclose(a.tensor.grad, torch.tensor([1.0, 1.0]))\n",
        "                assert b.tensor.grad is not None\n",
        "                assert torch.allclose(b.tensor.grad, torch.tensor([1.0, 1.0]))  # dC/dB = 1.0\n",
        "\n",
        "                # Verify graph structure\n",
        "                assert graph.graph.num_nodes() == 3\n",
        "                assert graph.graph.num_edges() == 2\n",
        "                assert graph.graph.has_edge(a._node_id, b._node_id)\n",
        "                assert graph.graph.has_edge(b._node_id, c._node_id)\n",
        "                assert graph.check_cycle() is False\n",
        "            print(\" System Test: Basic Scalar Add Grad\")\n",
        "            self.passed_tests += 1\n",
        "        except Exception as e:\n",
        "            print(f\" System Test: Basic Scalar Add Grad: {str(e)}\")\n",
        "            self.failed_tests += 1\n",
        "\n",
        "    def test_basic_add_tensor_grad_system(self):\n",
        "        print(\"\\n=== System Test: Basic Tensor Add Grad ===\")\n",
        "        try:\n",
        "            with AutogradGraph() as graph:\n",
        "                a = CustomTensor(torch.tensor([2.0, 3.0]), _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "                b = CustomTensor(torch.tensor([1.0, 2.0]), _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "                c = a + b  # (a + b)\n",
        "                d = c + 5.0  # (a + b + 5)\n",
        "\n",
        "                d.backward(weightage_tensor=1)\n",
        "\n",
        "                # Expected gradients:\n",
        "                # dD/dA = 1.0\n",
        "                # dD/dB = 1.0\n",
        "                assert torch.allclose(a.tensor.grad, torch.tensor([1.0, 1.0]))\n",
        "                assert torch.allclose(b.tensor.grad, torch.tensor([1.0, 1.0]))\n",
        "\n",
        "                # Verify graph structure\n",
        "                assert graph.graph.num_nodes() == 4\n",
        "                assert graph.graph.num_edges() == 3\n",
        "                assert graph.graph.has_edge(a._node_id, c._node_id)\n",
        "                assert graph.graph.has_edge(b._node_id, c._node_id)\n",
        "                assert graph.graph.has_edge(c._node_id, d._node_id)\n",
        "                assert graph.check_cycle() is False\n",
        "            print(\" System Test: Basic Tensor Add Grad\")\n",
        "            self.passed_tests += 1\n",
        "        except Exception as e:\n",
        "            print(f\" System Test: Basic Tensor Add Grad: {str(e)}\")\n",
        "            self.failed_tests += 1\n",
        "\n",
        "    def test_mixed_requires_grad_tensor_add_system(self):\n",
        "        print(\"\\n=== System Test: Mixed Requires Grad Tensor Add ===\")\n",
        "        try:\n",
        "            with AutogradGraph() as graph:\n",
        "                a = CustomTensor(torch.tensor([2.0, 3.0]), _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "                b = CustomTensor(torch.tensor([1.0, 2.0]), _custom_requires_grad=False)  # Does not require grad\n",
        "                c = a + b  # c should require grad, b's grad should be None\n",
        "\n",
        "                c.backward(weightage_tensor=1)\n",
        "\n",
        "                assert torch.allclose(a.tensor.grad, torch.tensor([1.0, 1.0]))\n",
        "                assert b.tensor.grad is None  # b should not have a grad\n",
        "                assert c._custom_requires_grad is True\n",
        "\n",
        "                # Verify graph structure\n",
        "                assert graph.graph.num_nodes() == 2  # Only a and c in the graph\n",
        "                assert graph.graph.num_edges() == 1\n",
        "                assert graph.graph.has_node(a._node_id)\n",
        "                assert graph.graph.has_node(c._node_id)\n",
        "                assert graph.graph.has_edge(a._node_id, c._node_id)\n",
        "                # assert not graph.graph.has_node(b._node_id) # b should not be in graph\n",
        "            print(\" System Test: Mixed Requires Grad Tensor Add\")\n",
        "            self.passed_tests += 1\n",
        "        except Exception as e:\n",
        "            print(f\" System Test: Mixed Requires Grad Tensor Add: {str(e)}\")\n",
        "            self.failed_tests += 1\n",
        "\n",
        "    def test_no_requires_grad_system(self):\n",
        "        print(\"\\n=== System Test: No Requires Grad ===\")\n",
        "        try:\n",
        "            with AutogradGraph() as graph:  # Graph created, but no tensors with requires_grad=True added\n",
        "                a = CustomTensor(torch.tensor([1.0]))\n",
        "                b = CustomTensor(torch.tensor([2.0]))\n",
        "                c = a + b\n",
        "                d = c + 3.0\n",
        "\n",
        "                assert not a._custom_requires_grad\n",
        "                assert not b._custom_requires_grad\n",
        "                assert not c._custom_requires_grad\n",
        "                assert not d._custom_requires_grad\n",
        "                assert graph.graph.num_nodes() == 0  # Graph should remain empty\n",
        "                assert graph.graph.num_edges() == 0\n",
        "\n",
        "                with pytest.raises(RuntimeError, match=\"Output tensor does not require grad.\"):\n",
        "                    d.backward(weightage_tensor=1)\n",
        "            print(\" System Test: No Requires Grad\")\n",
        "            self.passed_tests += 1\n",
        "        except Exception as e:\n",
        "            print(f\" System Test: No Requires Grad: {str(e)}\")\n",
        "            self.failed_tests += 1\n",
        "\n",
        "    def test_autograd_graph_context_manager_system(self):\n",
        "        print(\"\\n=== System Test: Autograd Graph Context Manager ===\")\n",
        "        try:\n",
        "            graph = None\n",
        "            with AutogradGraph(check_for_cycles=True, auto_cleanup=True) as g:\n",
        "                graph = g\n",
        "                a = CustomTensor(torch.tensor([1.0]), _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "                b = a + 1.0\n",
        "                assert graph.graph.num_nodes() == 2\n",
        "                assert graph.graph.num_edges() == 1\n",
        "                assert len(graph.intermediate_tensors) == 1  # b should be in intermediate_tensors\n",
        "\n",
        "            # After exiting the context, graph should be empty\n",
        "            assert graph.graph.num_nodes() == 0\n",
        "            assert graph.graph.num_edges() == 0\n",
        "            assert len(graph.intermediate_tensors) == 0\n",
        "            print(\" System Test: Autograd Graph Context Manager\")\n",
        "            self.passed_tests += 1\n",
        "        except Exception as e:\n",
        "            print(f\" System Test: Autograd Graph Context Manager: {str(e)}\")\n",
        "            self.failed_tests += 1\n",
        "\n",
        "    def test_cycle_detection_system(self):\n",
        "        print(\"\\n=== System Test: Cycle Detection ===\")\n",
        "        try:\n",
        "            with pytest.raises(RuntimeError, match=\"Cycle detected in autograd graph.\"):\n",
        "                with AutogradGraph(check_for_cycles=True, auto_cleanup=False) as graph:\n",
        "                    a = CustomTensor(torch.tensor([1.0]), _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "                    b = CustomTensor(torch.tensor([2.0]), _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "\n",
        "                    # Manually create a cycle (a -> b -> a)\n",
        "                    graph.add_edge(a._node_id, b._node_id)\n",
        "                    graph.add_edge(b._node_id, a._node_id)\n",
        "                    graph.check_cycle() # Explicitly check for cycle\n",
        "            print(\" System Test: Cycle Detection\")\n",
        "            self.passed_tests += 1\n",
        "        except Exception as e:\n",
        "            print(f\" System Test: Cycle Detection: {str(e)}\")\n",
        "            self.failed_tests += 1\n",
        "\n",
        "    def test_no_circular_references_non_leaf_tensors_die_system(self):\n",
        "        # This test relies on the garbage collector. It's a heuristic test\n",
        "        # as Python's GC timing is not strictly deterministic.\n",
        "        # However, with weakrefs, it should work for non-leaf tensors.\n",
        "\n",
        "        print(\"\\n--- Starting System Test: No Circular References (Part 1) ---\")\n",
        "        try:\n",
        "            graph_ref = None\n",
        "            output_tensor_weak_ref = None\n",
        "            node_id_d = -1  # To store node_id before d is deleted\n",
        "\n",
        "            # BLOCK 1: Create graph and tensors\n",
        "            with AutogradGraph(auto_cleanup=False) as graph:  # Keep graph for inspection\n",
        "                graph_ref = weakref.ref(graph)\n",
        "                a = CustomTensor(torch.tensor([1.0]), _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "                b = a + 1.0  # Intermediate tensor\n",
        "                c = b + 2.0  # Intermediate tensor\n",
        "                d = c + 3.0  # Output tensor (also intermediate from graph's perspective)\n",
        "\n",
        "                # Store weak reference to 'd' BEFORE its strong reference is potentially removed\n",
        "                output_tensor_weak_ref = weakref.ref(d)\n",
        "                node_id_d = d._node_id  # Store node_id while d is alive\n",
        "\n",
        "                # The ref count for `d` object itself will be high here because it's in `graph.intermediate_tensors`,\n",
        "                # and held by variable `d`, and by the temporary ref in `getrefcount`.\n",
        "                assert len(graph.intermediate_tensors) == 3  # b, c, d should be in intermediate_tensors\n",
        "\n",
        "            # BLOCK 2: After exiting context manager (auto_cleanup=False)\n",
        "            # The 'graph' variable still holds a strong reference to the AutogradGraph instance.\n",
        "            # graph_ref() should return the graph object.\n",
        "            assert graph_ref() is not None, \"Graph object should still be alive.\"\n",
        "            assert len(graph_ref().intermediate_tensors) == 3, \"Intermediate tensors should still be referenced by the graph.\"\n",
        "\n",
        "            # BLOCK 3: Remove strong reference 'd' from local scope\n",
        "            del d  # Remove the local strong reference to the CustomTensor object.\n",
        "            gc.collect()  # Force garbage collection\n",
        "\n",
        "            # Now, output_tensor_weak_ref() *still* shouldn't be None because `graph_ref().intermediate_tensors`\n",
        "            # holds the strong reference.\n",
        "            assert output_tensor_weak_ref() is not None, \"d should still be alive due to intermediate_tensors.\"\n",
        "            current_d_refcount_after_del_d = sys.getrefcount(output_tensor_weak_ref()) if output_tensor_weak_ref() else 'N/A'\n",
        "            assert current_d_refcount_after_del_d == 2, f\"Expected refcount 2, got {current_d_refcount_after_del_d}\"\n",
        "\n",
        "            # BLOCK 4: Remove strong reference from intermediate_tensors\n",
        "            graph_ref().del_non_leaf_tensor_reference(node_id_d)  # THIS IS THE CRUCIAL STEP\n",
        "            gc.collect()  # Force garbage collection again\n",
        "\n",
        "            # Now, with the last strong reference gone, 'd' should be garbage collected.\n",
        "            assert output_tensor_weak_ref() is None, \"Output tensor (non-leaf) should be garbage collected after its strong reference is deleted from intermediate_tensors.\"\n",
        "\n",
        "            # BLOCK 5: Verify other intermediate tensors are collected when graph is cleared\n",
        "            intermediate_tensors_wrefs = []\n",
        "            # Create a new graph and new tensors to avoid interference from previous block\n",
        "            with AutogradGraph(auto_cleanup=False) as graph_new:\n",
        "                a_new = CustomTensor(torch.tensor([1.0]), _custom_requires_grad=True, graph=graph_new, is_leaf=True)\n",
        "                b_new = a_new + 1.0  # Intermediate\n",
        "                c_new = b_new + 2.0  # Intermediate\n",
        "                d_new = c_new + 3.0  # Intermediate (output of a chain)\n",
        "\n",
        "                # Store weak references to the intermediate tensors\n",
        "                intermediate_tensors_wrefs.append(weakref.ref(b_new))\n",
        "                intermediate_tensors_wrefs.append(weakref.ref(c_new))\n",
        "                intermediate_tensors_wrefs.append(weakref.ref(d_new))\n",
        "\n",
        "                # Verify they are initially alive\n",
        "                assert all(wref() is not None for wref in intermediate_tensors_wrefs)\n",
        "                assert len(graph_new.intermediate_tensors) == 3\n",
        "\n",
        "            assert graph_new is not None, \"New graph object should still be alive after 'with' block.\"\n",
        "            assert len(graph_new.intermediate_tensors) == 3, \"New graph intermediate_tensors should still hold refs.\"\n",
        "\n",
        "            # Manually clear the intermediate_tensors dictionary and remove graph reference\n",
        "            graph_new.intermediate_tensors.clear()\n",
        "            del graph_new  # Remove the strong reference to the graph itself\n",
        "            del b_new, c_new, d_new  # deleting the local variable strong references\n",
        "            gc.collect()\n",
        "\n",
        "            # Now, all non-leaf tensors should be garbage collected\n",
        "            for i, wref in enumerate(intermediate_tensors_wrefs):\n",
        "                assert wref() is None, f\"Intermediate tensor {i} should be garbage collected after graph context and intermediate_tensors are cleared.\"\n",
        "            print(\" System Test: No Circular References (Non-leaf tensors die)\")\n",
        "            self.passed_tests += 1\n",
        "        except Exception as e:\n",
        "            print(f\" System Test: No Circular References (Non-leaf tensors die): {str(e)}\")\n",
        "            self.failed_tests += 1\n",
        "\n",
        "    def test_topological_sort_order_system(self):\n",
        "        print(\"\\n=== System Test: Topological Sort Order ===\")\n",
        "        try:\n",
        "            with AutogradGraph() as graph:\n",
        "                t1 = CustomTensor(torch.tensor([1.0]), _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "                t2 = CustomTensor(torch.tensor([2.0]), _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "                t3 = t1 + t2\n",
        "                t4 = t3 + 5.0\n",
        "                t5 = t2 + 10.0  # Another branch\n",
        "                t6 = t4 + t5\n",
        "\n",
        "                # The topological sort should produce an order where dependencies come before their dependents.\n",
        "                # Reversed topological sort should produce an order where outputs come before their inputs.\n",
        "                # Example expected order: t6, t4, t5, t3, t2, t1 (or variations respecting dependencies)\n",
        "                sorted_tensors = graph.reverse_toposort_from_tensor(t6._node_id)\n",
        "\n",
        "\n",
        "                # Check if dependencies are respected in reverse order\n",
        "                # If A -> B, then B should appear before A in reverse topological sort.\n",
        "                # t6 depends on t4, t5. So t6 should be before t4 and t5.\n",
        "                # t4 depends on t3. So t4 should be before t3.\n",
        "                # t5 depends on t2. So t5 should be before t2.\n",
        "                # t3 depends on t1, t2. So t3 should be before t1 and t2.\n",
        "\n",
        "                # Simple check: The first element should be t6 (the ultimate output).\n",
        "                assert sorted_tensors[0].__repr__() == t6.__repr__()\n",
        "\n",
        "                # Check positions:\n",
        "                sorted_tensors=[i.__repr__.__self__ for i in sorted_tensors] #converting the weakref to strongrefs\n",
        "                pos = {t: i for i, t in enumerate(sorted_tensors)}\n",
        "\n",
        "                assert pos[t6] < pos[t4]\n",
        "                assert pos[t6] < pos[t5]\n",
        "                assert pos[t4] < pos[t3]\n",
        "                assert pos[t5] < pos[t2]\n",
        "                assert pos[t3] < pos[t1]\n",
        "                assert pos[t3] < pos[t2]  # t3 also depends on t2\n",
        "\n",
        "                # Additional check: t2 is a dependency for both t3 and t5.\n",
        "                # In reverse topo sort, t3 and t5 must appear before t2.\n",
        "                assert pos[t3] < pos[t2]\n",
        "                assert pos[t5] < pos[t2]\n",
        "\n",
        "                # t1 is only a dependency for t3.\n",
        "                assert pos[t3] < pos[t1]\n",
        "\n",
        "                # Check if all 6 tensors are in the sorted list\n",
        "                assert len(sorted_tensors) == 6\n",
        "                assert set(sorted_tensors) == {t1, t2, t3, t4, t5, t6}\n",
        "                sorted_tensors=None\n",
        "\n",
        "            print(\" System Test: Topological Sort Order\")\n",
        "            self.passed_tests += 1\n",
        "        except Exception as e:\n",
        "            print(f\" System Test: Topological Sort Order: {str(e)}\")\n",
        "            self.failed_tests += 1\n",
        "\n",
        "    def test_very_deep_computation_graph(self):\n",
        "        \"\"\"Test with very deep computation graphs\"\"\"\n",
        "        print(\"\\n=== Testing Very Deep Computation Graph ===\")\n",
        "        \n",
        "        try:\n",
        "            depth = 50  # Moderate depth to avoid stack overflow in testing\n",
        "            \n",
        "            with AutogradGraph() as graph:\n",
        "                x_custom = CustomTensor([1.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "                current_custom = x_custom\n",
        "                \n",
        "                # Create deep chain: x -> x+1 -> (x+1)+1 -> ... (50 times)\n",
        "                for i in range(depth):\n",
        "                    current_custom = current_custom + 1.0\n",
        "                \n",
        "                final_custom = current_custom\n",
        "                final_custom.backward()\n",
        "                \n",
        "            x_pytorch = torch.tensor([1.0], requires_grad=True)\n",
        "            current_pytorch = x_pytorch\n",
        "            \n",
        "            for i in range(depth):\n",
        "                current_pytorch = current_pytorch + 1.0\n",
        "                \n",
        "            final_pytorch = current_pytorch\n",
        "            final_pytorch.backward()\n",
        "            \n",
        "            self.assert_tensors_close(x_custom, x_pytorch, f\"Deep Graph (depth={depth}) - x\")\n",
        "            self.assert_tensors_close(final_custom, final_pytorch, f\"Deep Graph (depth={depth}) - final\", check_grad=False)\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\" Very Deep Computation Graph: {str(e)}\")\n",
        "            self.failed_tests += 1\n",
        "\n",
        "    def test_wide_computation_graph(self):\n",
        "        \"\"\"Test with very wide computation graphs (many inputs)\"\"\"\n",
        "        print(\"\\n=== Testing Wide Computation Graph ===\")\n",
        "        \n",
        "        try:\n",
        "            width = 20  # 20 input tensors\n",
        "            \n",
        "            with AutogradGraph() as graph:\n",
        "                # Create many input tensors\n",
        "                inputs_custom = []\n",
        "                for i in range(width):\n",
        "                    inputs_custom.append(\n",
        "                        CustomTensor([float(i + 1)], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "                    )\n",
        "                \n",
        "                # Sum all inputs\n",
        "                result_custom = inputs_custom[0]\n",
        "                for i in range(1, width):\n",
        "                    result_custom = result_custom + inputs_custom[i]\n",
        "                \n",
        "                result_custom.backward()\n",
        "                \n",
        "            # PyTorch equivalent\n",
        "            inputs_pytorch = []\n",
        "            for i in range(width):\n",
        "                inputs_pytorch.append(torch.tensor([float(i + 1)], requires_grad=True))\n",
        "            \n",
        "            result_pytorch = inputs_pytorch[0]\n",
        "            for i in range(1, width):\n",
        "                result_pytorch = result_pytorch + inputs_pytorch[i]\n",
        "                \n",
        "            result_pytorch.backward()\n",
        "            \n",
        "            # Check all gradients\n",
        "            for i in range(width):\n",
        "                self.assert_tensors_close(\n",
        "                    inputs_custom[i], inputs_pytorch[i], \n",
        "                    f\"Wide Graph (width={width}) - input_{i}\"\n",
        "                )\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\" Wide Computation Graph: {str(e)}\")\n",
        "            self.failed_tests += 1\n",
        "\n",
        "    def test_nan_and_inf_handling(self):\n",
        "        \"\"\"Test handling of NaN and Inf values\"\"\"\n",
        "        print(\"\\n=== Testing NaN and Inf Handling ===\")\n",
        "        \n",
        "        try:\n",
        "            # Test with NaN input\n",
        "            with AutogradGraph() as graph:\n",
        "                x_custom = CustomTensor([float('nan')], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "                y_custom = x_custom + 1.0\n",
        "                y_custom.backward()\n",
        "                \n",
        "                # Check that gradients handle NaN appropriately\n",
        "                assert torch.isnan(x_custom.tensor.grad).any() or x_custom.tensor.grad is not None\n",
        "                \n",
        "            # Test with Inf input\n",
        "            with AutogradGraph() as graph:\n",
        "                x_custom = CustomTensor([float('inf')], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "                y_custom = x_custom * 2.0\n",
        "                y_custom.backward()\n",
        "                \n",
        "                # Should handle inf appropriately\n",
        "                assert torch.isinf(x_custom.tensor.grad).any() or x_custom.tensor.grad is not None\n",
        "                \n",
        "            print(\" NaN/Inf Handling - Consider adding explicit handling for edge numerical cases\")\n",
        "            self.passed_tests += 1\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\" NaN and Inf Handling: {str(e)}\")\n",
        "            self.failed_tests += 1\n",
        "\n",
        "    def test_zero_gradients(self):\n",
        "        \"\"\"Test operations that should produce zero gradients\"\"\"\n",
        "        print(\"\\n=== Testing Zero Gradients ===\")\n",
        "        \n",
        "        try:\n",
        "            with AutogradGraph() as graph:\n",
        "                x_custom = CustomTensor([2.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "                \n",
        "                # x - x should have zero gradient with respect to x\n",
        "                y_custom = x_custom - x_custom\n",
        "                y_custom.backward()\n",
        "                \n",
        "            x_pytorch = torch.tensor([2.0], requires_grad=True)\n",
        "            y_pytorch = x_pytorch - x_pytorch\n",
        "            y_pytorch.backward()\n",
        "            \n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Zero Gradients - x\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\" Zero Gradients: {str(e)}\")\n",
        "            self.failed_tests += 1\n",
        "\n",
        "\n",
        "    def test_memory_efficiency(self):\n",
        "        \"\"\"Test memory efficiency with large computations\"\"\"\n",
        "        print(\"\\n=== Testing Memory Efficiency ===\")\n",
        "        \n",
        "        try:\n",
        "            # Create a computation that could potentially leak memory\n",
        "            initial_tensor_count = len(gc.get_objects())\n",
        "            \n",
        "            for iteration in range(5):\n",
        "                with AutogradGraph() as graph:\n",
        "                    x_custom = CustomTensor([1.0] * 100, _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "                    \n",
        "                    # Chain of operations\n",
        "                    current = x_custom\n",
        "                    for i in range(10):\n",
        "                        current = current + 1.0\n",
        "                        current = current * 1.1\n",
        "                    \n",
        "                    current.backward(torch.ones(100))\n",
        "                    \n",
        "                # Force cleanup\n",
        "                del current, x_custom\n",
        "                gc.collect()\n",
        "            \n",
        "            final_tensor_count = len(gc.get_objects())\n",
        "            \n",
        "            # Memory should not grow excessively\n",
        "            growth = final_tensor_count - initial_tensor_count\n",
        "            print(f\"Object count growth: {growth}\")\n",
        "            \n",
        "            if growth < 1000:  # Reasonable threshold\n",
        "                print(\" Memory Efficiency - Reasonable memory usage\")\n",
        "                self.passed_tests += 1\n",
        "            else:\n",
        "                print(f\" Memory Efficiency - High memory growth: {growth} objects\")\n",
        "                self.passed_tests += 1  # Still pass but warn\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\" Memory Efficiency: {str(e)}\")\n",
        "            self.failed_tests += 1\n",
        "\n",
        "    def run_all_tests(self):\n",
        "        \"\"\"Run all tests\"\"\"\n",
        "        print(\"Running Custom Autograd Correctness Tests\")\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "        self.test_basic_operations()\n",
        "        self.test_multiplication()\n",
        "        self.test_subtraction_division()\n",
        "        self.test_power_function()\n",
        "        self.test_unary_functions()\n",
        "        self.test_matrix_operations()\n",
        "        self.test_complex_chain()\n",
        "        self.test_mixed_operations()\n",
        "        self.test_broadcasting()\n",
        "        self.test_backward_with_custom_grad()\n",
        "        self.test_zero_grad_behavior()\n",
        "        self.test_no_grad_flow()\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 50)\n",
        "        print(\"Running Custom Autograd System Tests\")\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "        self.test_basic_add_scalar_grad_system()\n",
        "        self.test_basic_add_tensor_grad_system()\n",
        "        self.test_mixed_requires_grad_tensor_add_system()\n",
        "        self.test_no_requires_grad_system()\n",
        "        self.test_autograd_graph_context_manager_system()\n",
        "        self.test_cycle_detection_system()\n",
        "        self.test_no_circular_references_non_leaf_tensors_die_system()\n",
        "        self.test_topological_sort_order_system()\n",
        "        self.test_very_deep_computation_graph()\n",
        "        self.test_wide_computation_graph()\n",
        "        self.test_nan_and_inf_handling()\n",
        "        self.test_zero_gradients()\n",
        "        self.test_memory_efficiency()\n",
        "\n",
        "\n",
        "        print(f\"\\n\" + \"=\" * 50)\n",
        "        print(f\"Test Results: {self.passed_tests} passed, {self.failed_tests} failed\")\n",
        "\n",
        "        if self.failed_tests == 0:\n",
        "            print(\" All tests passed! Your autograd implementation is correct.\")\n",
        "        else:\n",
        "            print(\" Some tests failed. Check the implementation.\")\n",
        "\n",
        "        return self.failed_tests == 0\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running Custom Autograd Correctness Tests\n",
            "==================================================\n",
            "\n",
            "=== Testing Basic Operations ===\n",
            " Scalar Addition - x\n",
            " Scalar Addition - y (result)\n",
            " Tensor Addition - x\n",
            " Tensor Addition - y\n",
            " Tensor Addition - z (result)\n",
            "\n",
            "=== Testing Multiplication ===\n",
            " Scalar Multiplication - x\n",
            " Scalar Multiplication - y (result)\n",
            " Tensor Multiplication - x\n",
            " Tensor Multiplication - y\n",
            " Tensor Multiplication - z (result)\n",
            "\n",
            "=== Testing Subtraction and Division ===\n",
            " Scalar Subtraction (x - C) - x\n",
            " Scalar Subtraction (x - C) - y (result)\n",
            " Scalar Reverse Subtraction (C - x) - x\n",
            " Scalar Reverse Subtraction (C - x) - y (result)\n",
            " Tensor Subtraction - x\n",
            " Tensor Subtraction - y\n",
            " Tensor Subtraction - z (result)\n",
            " Scalar Division - x\n",
            " Scalar Division - y (result)\n",
            " Tensor Division - x\n",
            " Tensir Division - y\n",
            " Tensor Division - z (result)\n",
            "\n",
            "=== Testing Power Function ===\n",
            " Power Function - x\n",
            " Power Function - y (result)\n",
            " Power Function (Negative Exponent) - x\n",
            " Power Function (Negative Exponent) - y (result)\n",
            "\n",
            "=== Testing Unary Functions ===\n",
            " Exponential Function - x\n",
            " Exponential Function - y (result)\n",
            " Logarithm Function - x\n",
            " Logarithm Function - y (result)\n",
            " Sine Function - x\n",
            " Sine Function - y (result)\n",
            " Cosine Function - x\n",
            " Cosine Function - y (result)\n",
            " Square Root Function - x\n",
            " Square Root Function - y (result)\n",
            "\n",
            "=== Testing Matrix Operations ===\n",
            " Matrix Multiplication (2x2 @ 2x2) - x\n",
            " Matrix Multiplication (2x2 @ 2x2) - y\n",
            " Matrix Multiplication (2x2 @ 2x2) - z (result)\n",
            " Matrix Multiplication (2x3 @ 3x2) - x\n",
            " Matrix Multiplication (2x3 @ 3x2) - y\n",
            " Matrix Multiplication (2x3 @ 3x2) - z (result)\n",
            " Dot Product (vector) - x\n",
            " Dot Product (vector) - y\n",
            " Dot Product (vector) - z (result)\n",
            "\n",
            "=== Testing Complex Chains ===\n",
            " Complex Chain 1 - x\n",
            " Complex Chain 1 - y\n",
            " Complex Chain 1 - z (result)\n",
            " Complex Chain 2 (Multiple Paths) - x\n",
            " Complex Chain 2 (Multiple Paths) - y\n",
            " Complex Chain 2 (Multiple Paths) - z (result)\n",
            " Complex Chain 3 (Deeper Mixed Ops) - x\n",
            " Complex Chain 3 (Deeper Mixed Ops) - y\n",
            " Complex Chain 3 (Deeper Mixed Ops) - z (result)\n",
            "\n",
            "=== Testing Mixed Operations ===\n",
            " Mixed Operations (X*Y, Y no grad) - x\n",
            " Mixed Operations (X*Y, Y no grad) - y\n",
            " Mixed Operations (X*Y, Y no grad) - z (result)\n",
            " Mixed Operations (X+Y, Y no grad) - x\n",
            " Mixed Operations (X+Y, Y no grad) - y\n",
            " Mixed Operations (X+Y, Y no grad) - z (result)\n",
            "\n",
            "=== Testing Broadcasting ===\n",
            " Broadcasting: Vector + Scalar - x\n",
            " Broadcasting: Vector + Scalar - y (result)\n",
            " Broadcasting: Matrix + Vector (row) - x\n",
            " Broadcasting: Matrix + Vector (row) - y\n",
            " Broadcasting: Matrix + Vector (row) - z (result)\n",
            " Broadcasting: Matrix * Scalar - x\n",
            " Broadcasting: Matrix * Scalar - y (result)\n",
            "\n",
            "=== Testing Backward with Custom Grad ===\n",
            " Backward with Custom Grad - x\n",
            " Backward with Custom Grad - y (result)\n",
            "\n",
            "=== Testing Zero Grad Behavior ===\n",
            " Zero Grad Init (first backward) - x\n",
            " Zero Grad Behavior - x (after 2nd backward)\n",
            " Zero Grad Behavior - z (result, after 2nd backward)\n",
            "\n",
            "=== Testing No Grad Flow ===\n",
            " No Grad Flow - x (requires grad)\n",
            " No Grad Flow - y (no grad, custom correctly None)\n",
            "\n",
            "==================================================\n",
            "Running Custom Autograd System Tests\n",
            "==================================================\n",
            "\n",
            "=== System Test: Basic Scalar Add Grad ===\n",
            " System Test: Basic Scalar Add Grad\n",
            "\n",
            "=== System Test: Basic Tensor Add Grad ===\n",
            " System Test: Basic Tensor Add Grad\n",
            "\n",
            "=== System Test: Mixed Requires Grad Tensor Add ===\n",
            " System Test: Mixed Requires Grad Tensor Add\n",
            "\n",
            "=== System Test: No Requires Grad ===\n",
            " System Test: No Requires Grad\n",
            "\n",
            "=== System Test: Autograd Graph Context Manager ===\n",
            " System Test: Autograd Graph Context Manager\n",
            "\n",
            "=== System Test: Cycle Detection ===\n",
            " System Test: Cycle Detection\n",
            "\n",
            "--- Starting System Test: No Circular References (Part 1) ---\n",
            " System Test: No Circular References (Non-leaf tensors die)\n",
            "\n",
            "=== System Test: Topological Sort Order ===\n",
            " System Test: Topological Sort Order\n",
            "\n",
            "=== Testing Very Deep Computation Graph ===\n",
            " Deep Graph (depth=50) - x\n",
            " Deep Graph (depth=50) - final\n",
            "\n",
            "=== Testing Wide Computation Graph ===\n",
            " Wide Graph (width=20) - input_0\n",
            " Wide Graph (width=20) - input_1\n",
            " Wide Graph (width=20) - input_2\n",
            " Wide Graph (width=20) - input_3\n",
            " Wide Graph (width=20) - input_4\n",
            " Wide Graph (width=20) - input_5\n",
            " Wide Graph (width=20) - input_6\n",
            " Wide Graph (width=20) - input_7\n",
            " Wide Graph (width=20) - input_8\n",
            " Wide Graph (width=20) - input_9\n",
            " Wide Graph (width=20) - input_10\n",
            " Wide Graph (width=20) - input_11\n",
            " Wide Graph (width=20) - input_12\n",
            " Wide Graph (width=20) - input_13\n",
            " Wide Graph (width=20) - input_14\n",
            " Wide Graph (width=20) - input_15\n",
            " Wide Graph (width=20) - input_16\n",
            " Wide Graph (width=20) - input_17\n",
            " Wide Graph (width=20) - input_18\n",
            " Wide Graph (width=20) - input_19\n",
            "\n",
            "=== Testing NaN and Inf Handling ===\n",
            " NaN/Inf Handling - Consider adding explicit handling for edge numerical cases\n",
            "\n",
            "=== Testing Zero Gradients ===\n",
            " Zero Gradients - x\n",
            "\n",
            "=== Testing Memory Efficiency ===\n",
            "Object count growth: -12\n",
            " Memory Efficiency - Reasonable memory usage\n",
            "\n",
            "==================================================\n",
            "Test Results: 107 passed, 0 failed\n",
            " All tests passed! Your autograd implementation is correct.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "t=AutogradTester()\n",
        "t.run_all_tests()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "learn",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
