{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 19 july\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tensor defination and AutoGradGraph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import weakref\n",
        "import numbers\n",
        "import rustworkx as rx\n",
        "import math\n",
        "\n",
        "# Your existing AutogradGraph class (with minor improvements)\n",
        "class AutogradGraph:\n",
        "    \"\"\"\n",
        "    Manages the computation graph for automatic differentiation.\n",
        "    It uses a directed acyclic graph to track dependencies between tensors.\n",
        "    \"\"\"\n",
        "    __slots__ = ('graph', 'intermediate_tensors', '_check_cycles', '_auto_cleanup', '__weakref__')\n",
        "\n",
        "    def __init__(self, check_for_cycles=True, auto_cleanup=True):\n",
        "        self.graph = rx.PyDiGraph()\n",
        "        self.intermediate_tensors = {}\n",
        "        self._check_cycles = check_for_cycles\n",
        "        self._auto_cleanup = auto_cleanup\n",
        "\n",
        "    def __enter__(self):\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, exc_type, exc_value, traceback):\n",
        "        if self._check_cycles and self.check_cycle():\n",
        "            raise RuntimeError(\"Cycle detected in autograd graph on context exit.\")\n",
        "        if self._auto_cleanup:\n",
        "            self.intermediate_tensors.clear()\n",
        "            self.graph.clear()\n",
        "\n",
        "    def add_tensor_graph(self, tensor):\n",
        "        if not tensor._custom_requires_grad:\n",
        "            raise ValueError(\"Tensor with requires_grad=False cannot be added to the graph.\")\n",
        "        ref = weakref.proxy(tensor)\n",
        "        tensor_index = self.graph.add_node(ref)\n",
        "        tensor._node_id = tensor_index\n",
        "\n",
        "    def add_non_leaf_tensor_reference(self, tensor):\n",
        "        if not tensor._custom_requires_grad:\n",
        "            raise ValueError(\"Tensor must require grad.\")\n",
        "        if tensor._node_id in self.intermediate_tensors:\n",
        "            raise ValueError(\"Tensor reference already exists in intermediate tensors.\")\n",
        "        self.intermediate_tensors[tensor._node_id] = tensor\n",
        "\n",
        "    def add_edge(self, node_from, node_to, weight=None):\n",
        "        if not all(isinstance(n, int) for n in (node_from, node_to)):\n",
        "            raise TypeError(\"Node indices must be integers.\")\n",
        "        if not self.graph.has_node(node_from) or not self.graph.has_node(node_to):\n",
        "            raise ValueError(\"Nodes must exist before adding edge.\")\n",
        "        self.graph.add_edge(node_from, node_to, weight)\n",
        "\n",
        "    def check_cycle(self):\n",
        "        return not rx.is_directed_acyclic_graph(self.graph)\n",
        "\n",
        "    def reverse_toposort_from_tensor(self, tensor_index):\n",
        "        graph=self.graph\n",
        "        predecessors = list(rx.ancestors(graph, tensor_index))\n",
        "        predecessors.append(tensor_index)\n",
        "        sub_graph = graph.subgraph(predecessors)\n",
        "        return [sub_graph[i] for i in reversed(rx.topological_sort(sub_graph))]\n",
        "    # def alternative_reverse_toposort_from_tensor(self, tensor_index):\n",
        "    #     graph = self.graph\n",
        "    #     relevant_nodes = rx.ancestors(graph, tensor_index)\n",
        "    #     relevant_nodes.add(tensor_index)\n",
        "    #     full_topo = rx.topological_sort(graph)\n",
        "    #     relevant_topo = [graph[_node_id] for _node_id in reversed(full_topo) if _node_id in relevant_nodes]\n",
        "    #     return relevant_topo\n",
        "\n",
        "    def delete_node(self, node_index):\n",
        "        if not isinstance(node_index, int):\n",
        "            raise TypeError(\"Node index must be an integer.\")\n",
        "        if self.graph.has_node(node_index):\n",
        "             self.graph.remove_node(node_index)\n",
        "    def delete_edge(self, node_from, node_to):\n",
        "        if not self.graph.has_edge(node_from, node_to):\n",
        "            raise ValueError(\"Edge does not exist.\")\n",
        "        self.graph.remove_edge(node_from, node_to)\n",
        "\n",
        "    def del_non_leaf_tensor_reference(self, tensor_node_id):\n",
        "        self.intermediate_tensors.pop(tensor_node_id, None)\n",
        "\n",
        "    def delete_all_non_leaf_nodes(self):\n",
        "        # removes non leaf nodes from graph and clears the intermediate_tensors dict\n",
        "        self.graph.remove_nodes_from(list(self.intermediate_tensors.keys()))\n",
        "        self.intermediate_tensors.clear()\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"CustomAutogradGraph(nodes={self.graph.num_nodes()}, edges={self.graph.num_edges()})\"\n",
        "\n",
        "# Your existing CustomTensor class, now enhanced with new methods\n",
        "class CustomTensor:\n",
        "    \"\"\"\n",
        "    A custom tensor class that wraps a PyTorch tensor to enable a custom\n",
        "    autograd engine. It tracks operations to build a computation graph.\n",
        "    \"\"\"\n",
        "    __slots__ = ('tensor', '_node_id', '_custom_requires_grad', '_backward', 'graph', '__weakref__','_is_leaf')\n",
        "\n",
        "    def __new__(cls, data, *, _custom_requires_grad=False, device=\"cpu\", dtype=torch.float32, graph=None, due_to_operation=False, is_leaf=False):\n",
        "        if isinstance(data, CustomTensor):\n",
        "            return data  # Don't rewrap\n",
        "        return super().__new__(cls)\n",
        "\n",
        "    def __init__(self, data, *, _custom_requires_grad=False, device=\"cpu\", dtype=torch.float32, graph=None, due_to_operation=False, is_leaf=False):\n",
        "        if isinstance(data, CustomTensor):\n",
        "            return\n",
        "\n",
        "        self.tensor = data if due_to_operation else torch.as_tensor(data, dtype=dtype, device=device)\n",
        "        self.tensor.requires_grad_(False)\n",
        "        self._custom_requires_grad = _custom_requires_grad\n",
        "        self._node_id = None\n",
        "        self._backward = lambda: None\n",
        "        self.graph = None\n",
        "        self._is_leaf = is_leaf\n",
        "\n",
        "        if _custom_requires_grad:\n",
        "            self._init_graph(graph)\n",
        "\n",
        "    def _init_graph(self, graph):\n",
        "        if graph is None:\n",
        "            raise ValueError(\"Graph must be provided if requires_grad is True.\")\n",
        "        is_leaf=self._is_leaf\n",
        "        if is_leaf:\n",
        "            self.graph = weakref.proxy(graph)\n",
        "        else:\n",
        "            self.graph = graph # this line is only reached for tensors which are created by operations and graph passed is already a weakreference hence no need for wrapping\n",
        "        graph.add_tensor_graph(self)\n",
        "        if not is_leaf:\n",
        "            graph.add_non_leaf_tensor_reference(self)\n",
        "\n",
        "    def _zero_grad(self):\n",
        "        \"\"\"Sets the gradient of the underlying tensor to zero.\"\"\"\n",
        "        self.tensor.grad = torch.zeros_like(self.tensor)\n",
        "\n",
        "    def zero_(self):\n",
        "        \"\"\"Sets the gradient of the underlying tensor to zero.\"\"\"\n",
        "        if self.tensor.grad is not None:\n",
        "            self.tensor.grad.zero_()\n",
        "\n",
        "\n",
        "    # --- Broadcasting Helper ---\n",
        "    def _reduce_grad_for_broadcast(self, grad, target_shape):\n",
        "        \"\"\"Reduces a gradient to match the shape of a tensor that was broadcasted.\"\"\"\n",
        "        if grad.shape == target_shape:\n",
        "            return grad\n",
        "        \n",
        "        # Add singleton dimensions to the front of target_shape to match grad's ndim\n",
        "        padded_target_shape = (1,) * (grad.ndim - len(target_shape)) + target_shape\n",
        "        \n",
        "        # Identify dimensions that were broadcasted\n",
        "        sum_dims = [i for i, (grad_dim, target_dim) in enumerate(zip(grad.shape, padded_target_shape)) if target_dim == 1 and grad_dim > 1]\n",
        "\n",
        "        if sum_dims:\n",
        "            grad = grad.sum(dim=sum_dims, keepdim=True)\n",
        "        \n",
        "        # Remove singleton dimensions to match the final target shape\n",
        "        return grad.reshape(target_shape)\n",
        "\n",
        "\n",
        "\n",
        "    # --- Basic Operators (from your original code, now compatible with new features) ---\n",
        "    def __add__(self, other):\n",
        "        # ... [Your original implementation]\n",
        "        if isinstance(other, numbers.Number):\n",
        "            return self._add_scalar(other)\n",
        "        elif isinstance(other, CustomTensor):\n",
        "            return self._add_tensor(other)\n",
        "        return NotImplemented\n",
        "    def __radd__(self,other):\n",
        "        return self + other\n",
        "    def __iadd__(self,other):\n",
        "        if isinstance(other, numbers.Number):\n",
        "            self.tensor.add_(other)\n",
        "        elif isinstance(other,CustomTensor):\n",
        "            self.tensor.add_(other.tensor)\n",
        "    def _add_scalar(self, scalar):\n",
        "        result_tensor = torch.add(self.tensor, scalar)\n",
        "        if not self._custom_requires_grad:\n",
        "            return CustomTensor(result_tensor,due_to_operation=True)\n",
        "        graph = self.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "        graph.add_edge(self._node_id, result._node_id)\n",
        "        self_ref = weakref.proxy(self)\n",
        "        result_ref = weakref.proxy(result)\n",
        "        def _backward():\n",
        "            if self_ref.tensor.grad is None: self_ref._zero_grad()\n",
        "            self_ref.tensor.grad.add_(result_ref.tensor.grad)\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "    def _add_tensor(self, other):\n",
        "        result_tensor = torch.add(self.tensor, other.tensor)\n",
        "        requires_grad = self._custom_requires_grad or other._custom_requires_grad\n",
        "        if not requires_grad:\n",
        "            return CustomTensor(result_tensor,due_to_operation=True)\n",
        "        graph = self.graph if self._custom_requires_grad else other.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "        self_ref = weakref.proxy(self)\n",
        "        other_ref = weakref.proxy(other)\n",
        "        if self._custom_requires_grad:\n",
        "            graph.add_edge(self._node_id, result._node_id)\n",
        "        if other._custom_requires_grad:\n",
        "            graph.add_edge(other._node_id, result._node_id)\n",
        "        result_ref = weakref.proxy(result)\n",
        "        def _backward():\n",
        "            if self_ref._custom_requires_grad:\n",
        "                if self_ref.tensor.grad is None: self_ref._zero_grad()\n",
        "                grad_for_self = self_ref._reduce_grad_for_broadcast(result_ref.tensor.grad, self_ref.tensor.shape)\n",
        "                self_ref.tensor.grad.add_(grad_for_self)\n",
        "            if other_ref._custom_requires_grad:\n",
        "                if other_ref.tensor.grad is None: other_ref._zero_grad()\n",
        "                grad_for_other = other_ref._reduce_grad_for_broadcast(result_ref.tensor.grad, other_ref.tensor.shape)\n",
        "                other_ref.tensor.grad.add_(grad_for_other)\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "\n",
        "    def __mul__(self, other):\n",
        "        # ... [Your original implementation]\n",
        "        if isinstance(other, numbers.Number):\n",
        "            return self._mul_scalar(other)\n",
        "        elif isinstance(other, CustomTensor):\n",
        "            return self._mul_tensor(other)\n",
        "        return NotImplemented\n",
        "    def __rmul__(self,other):\n",
        "        return self*other\n",
        "    def __imul__(self,other):\n",
        "        if isinstance(other, numbers.Number):\n",
        "            self.tensor.mul_(other)\n",
        "        elif isinstance(other,CustomTensor):\n",
        "            self.tensor.mul_(other.tensor)\n",
        "    def _mul_scalar(self, scalar):\n",
        "        result_tensor = torch.mul(self.tensor, scalar)\n",
        "        if not self._custom_requires_grad:\n",
        "            return CustomTensor(result_tensor,due_to_operation=True)\n",
        "        graph = self.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "        graph.add_edge(self._node_id, result._node_id)\n",
        "        self_ref = weakref.proxy(self)\n",
        "        result_ref = weakref.proxy(result)\n",
        "        def _backward():\n",
        "            if self_ref.tensor.grad is None:\n",
        "                self_ref._zero_grad()\n",
        "            self_ref.tensor.grad.add_(result_ref.tensor.grad * scalar)\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "    def _mul_tensor(self, other):\n",
        "        result_tensor = torch.mul(self.tensor, other.tensor)\n",
        "        requires_grad = self._custom_requires_grad or other._custom_requires_grad\n",
        "        if not requires_grad:\n",
        "            return CustomTensor(result_tensor,due_to_operation=True)\n",
        "        graph = self.graph if self._custom_requires_grad else other.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "        self_ref = weakref.proxy(self)\n",
        "        other_ref = weakref.proxy(other)\n",
        "        result_ref = weakref.proxy(result)\n",
        "        if self._custom_requires_grad:\n",
        "            graph.add_edge(self._node_id, result._node_id)\n",
        "        if other._custom_requires_grad:\n",
        "            graph.add_edge(other._node_id, result._node_id)\n",
        "        def _backward():\n",
        "            if self_ref._custom_requires_grad:\n",
        "                if self_ref.tensor.grad is None: self_ref._zero_grad()\n",
        "                grad_for_self = self_ref._reduce_grad_for_broadcast(result_ref.tensor.grad * other_ref.tensor, self_ref.tensor.shape)\n",
        "                self_ref.tensor.grad.add_(grad_for_self)\n",
        "            if other_ref._custom_requires_grad:\n",
        "                if other_ref.tensor.grad is None: other_ref._zero_grad()\n",
        "                grad_for_other = other_ref._reduce_grad_for_broadcast(result_ref.tensor.grad * self_ref.tensor, other_ref.tensor.shape)\n",
        "                other_ref.tensor.grad.add_(grad_for_other)\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "\n",
        "    def __sub__(self, other):\n",
        "        if isinstance(other, numbers.Number):\n",
        "            return self._sub_scalar(other)\n",
        "        elif isinstance(other, CustomTensor):\n",
        "            return self._sub_tensor(other)\n",
        "        return NotImplemented\n",
        "    \n",
        "    def __rsub__(self, other):\n",
        "        if isinstance(other, numbers.Number):\n",
        "            return self._rsub_scalar(other)\n",
        "        \n",
        "    def __isub__(self,other):\n",
        "        if isinstance(other, numbers.Number):\n",
        "            self.tensor.sub_(other)\n",
        "        elif isinstance(other,CustomTensor):\n",
        "            self.tensor.sub_(other.tensor)\n",
        "        \n",
        "    def _rsub_scalar(self, scalar):\n",
        "        result_tensor = torch.sub(scalar, self.tensor)\n",
        "        if not self._custom_requires_grad:\n",
        "            return CustomTensor(result_tensor,due_to_operation=True)\n",
        "\n",
        "        graph = self.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "        graph.add_edge(self._node_id, result._node_id)\n",
        "\n",
        "        self_ref = weakref.proxy(self)\n",
        "        result_ref = weakref.proxy(result)\n",
        "        def _backward():\n",
        "            if self_ref.tensor.grad is None:\n",
        "                self_ref._zero_grad()\n",
        "            # Derivative of scalar - x is -1\n",
        "            self_ref.tensor.grad.sub_(result_ref.tensor.grad) # No broadcasting specific logic for scalar op\n",
        "\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "\n",
        "    \n",
        "    def _sub_scalar(self, scalar):\n",
        "        result_tensor = torch.sub(self.tensor, scalar)\n",
        "        if not self._custom_requires_grad:\n",
        "            return CustomTensor(result_tensor,due_to_operation=True)\n",
        "\n",
        "        graph = self.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "        graph.add_edge(self._node_id, result._node_id)\n",
        "\n",
        "        self_ref = weakref.proxy(self)\n",
        "        result_ref = weakref.proxy(result)\n",
        "        def _backward():\n",
        "            if self_ref.tensor.grad is None:\n",
        "                self_ref._zero_grad()\n",
        "            self_ref.tensor.grad.add_(result_ref.tensor.grad) # No broadcasting specific logic for scalar op\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "\n",
        "    def _sub_tensor(self, other):\n",
        "        result_tensor = torch.sub(self.tensor, other.tensor)\n",
        "        requires_grad = self._custom_requires_grad or other._custom_requires_grad\n",
        "        if not requires_grad:\n",
        "            return CustomTensor(result_tensor,due_to_operation=True)\n",
        "\n",
        "        graph = self.graph if self._custom_requires_grad else other.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "\n",
        "        self_ref = weakref.proxy(self)\n",
        "        other_ref = weakref.proxy(other)\n",
        "        result_ref = weakref.proxy(result)\n",
        "\n",
        "        if self._custom_requires_grad:\n",
        "            graph.add_edge(self._node_id, result._node_id)\n",
        "        if other._custom_requires_grad:\n",
        "            graph.add_edge(other._node_id, result._node_id)\n",
        "\n",
        "        def _backward():\n",
        "            if self_ref._custom_requires_grad:\n",
        "                if self_ref.tensor.grad is None:\n",
        "                    self_ref._zero_grad()\n",
        "                grad_for_self = self_ref._reduce_grad_for_broadcast(result_ref.tensor.grad, self_ref.tensor.shape)\n",
        "                self_ref.tensor.grad.add_(grad_for_self)\n",
        "            if other_ref._custom_requires_grad:\n",
        "                if other_ref.tensor.grad is None:\n",
        "                    other_ref._zero_grad()\n",
        "                grad_for_other = other_ref._reduce_grad_for_broadcast(-result_ref.tensor.grad, other_ref.tensor.shape)\n",
        "                other_ref.tensor.grad.add_(grad_for_other)\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "\n",
        "    def __truediv__(self, other):\n",
        "        if isinstance(other, numbers.Number):\n",
        "            return self._div_scalar(other)\n",
        "        elif isinstance(other, CustomTensor):\n",
        "            return self._div_tensor(other)\n",
        "        return NotImplemented\n",
        "    def __itruediv__(self,other):\n",
        "        if isinstance(other, numbers.Number):\n",
        "            self.tensor.div_(other)\n",
        "        elif isinstance(other,CustomTensor):\n",
        "            self.tensor.div_(other.tensor)\n",
        "    def _div_scalar(self, scalar):\n",
        "        result_tensor = torch.div(self.tensor, scalar)\n",
        "        if not self._custom_requires_grad:\n",
        "            return CustomTensor(result_tensor,due_to_operation=True)\n",
        "\n",
        "        graph = self.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "        graph.add_edge(self._node_id, result._node_id)\n",
        "\n",
        "        self_ref = weakref.proxy(self)\n",
        "        result_ref = weakref.proxy(result)\n",
        "        def _backward():\n",
        "            if self_ref.tensor.grad is None:\n",
        "                self_ref._zero_grad()\n",
        "            self_ref.tensor.grad.add_(result_ref.tensor.grad / scalar)\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "\n",
        "    def _div_tensor(self,other):\n",
        "        result_tensor = torch.div(self.tensor, other.tensor)\n",
        "        requires_grad = self._custom_requires_grad or other._custom_requires_grad\n",
        "        if not requires_grad:\n",
        "            return CustomTensor(result_tensor,due_to_operation=True)\n",
        "\n",
        "        graph = self.graph if self._custom_requires_grad else other.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "\n",
        "        self_ref = weakref.proxy(self)\n",
        "        other_ref = weakref.proxy(other)\n",
        "        result_ref = weakref.proxy(result)\n",
        "\n",
        "        if self._custom_requires_grad:\n",
        "            graph.add_edge(self._node_id, result._node_id)\n",
        "        if other._custom_requires_grad:\n",
        "            graph.add_edge(other._node_id, result._node_id)\n",
        "\n",
        "        def _backward():\n",
        "            if self_ref._custom_requires_grad:\n",
        "                if self_ref.tensor.grad is None:\n",
        "                    self_ref._zero_grad()\n",
        "                grad_for_self = self_ref._reduce_grad_for_broadcast(result_ref.tensor.grad / other_ref.tensor, self_ref.tensor.shape)\n",
        "                self_ref.tensor.grad.add_(grad_for_self)\n",
        "            if other_ref._custom_requires_grad:\n",
        "                if other_ref.tensor.grad is None:\n",
        "                    other_ref._zero_grad()\n",
        "                grad_for_other = other_ref._reduce_grad_for_broadcast(-result_ref.tensor.grad * self_ref.tensor / other_ref.tensor.pow(2), other_ref.tensor.shape)\n",
        "                other_ref.tensor.grad.add_(grad_for_other)\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "\n",
        "    def pow(self, scalar):\n",
        "        result_tensor = torch.pow(self.tensor, scalar)\n",
        "        if not self._custom_requires_grad:\n",
        "            return CustomTensor(result_tensor,due_to_operation=True)\n",
        "\n",
        "        graph = self.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "        graph.add_edge(self._node_id, result._node_id)\n",
        "\n",
        "        self_ref = weakref.proxy(self)\n",
        "        result_ref = weakref.proxy(result)\n",
        "        def _backward():\n",
        "            if self_ref.tensor.grad is None:\n",
        "                self_ref._zero_grad()\n",
        "            grad_contrib = scalar * self_ref.tensor.pow(scalar - 1)\n",
        "            self_ref.tensor.grad.add_(result_ref.tensor.grad * grad_contrib)\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "    def __ipow__(self,other):\n",
        "        self.tensor.pow_(other)\n",
        "\n",
        "    def exp(self):\n",
        "        out = torch.exp(self.tensor)\n",
        "        if not self._custom_requires_grad:\n",
        "            return CustomTensor(out,due_to_operation=True)\n",
        "        \n",
        "        graph = self.graph\n",
        "        result = CustomTensor(out, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "        graph.add_edge(self._node_id, result._node_id)\n",
        "        self_ref = weakref.proxy(self)\n",
        "        result_ref = weakref.proxy(result)\n",
        "        def _backward():\n",
        "            if self_ref.tensor.grad is None:\n",
        "                self_ref._zero_grad()\n",
        "            self_ref.tensor.grad.add_(result_ref.tensor.grad * out)\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "\n",
        "    def log(self):\n",
        "        out = torch.log(self.tensor)\n",
        "        if not self._custom_requires_grad:\n",
        "            return CustomTensor(out,due_to_operation=True)\n",
        "        \n",
        "        graph = self.graph\n",
        "        result = CustomTensor(out, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "        graph.add_edge(self._node_id, result._node_id)\n",
        "        self_ref = weakref.proxy(self)\n",
        "        result_ref = weakref.proxy(result)\n",
        "        def _backward():\n",
        "            if self_ref.tensor.grad is None:\n",
        "                self_ref._zero_grad()\n",
        "            self_ref.tensor.grad.add_(result_ref.tensor.grad / self_ref.tensor)\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "\n",
        "    def sin(self):\n",
        "        out = torch.sin(self.tensor)\n",
        "        if not self._custom_requires_grad:\n",
        "            return CustomTensor(out,due_to_operation=True)\n",
        "        \n",
        "        graph = self.graph\n",
        "        result = CustomTensor(out, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "        graph.add_edge(self._node_id, result._node_id)\n",
        "        self_ref = weakref.proxy(self)\n",
        "        result_ref = weakref.proxy(result)\n",
        "        def _backward():\n",
        "            if self_ref.tensor.grad is None:\n",
        "                self_ref._zero_grad()\n",
        "            self_ref.tensor.grad.add_(result_ref.tensor.grad * torch.cos(self_ref.tensor))\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "\n",
        "    def cos(self):\n",
        "        out = torch.cos(self.tensor)\n",
        "        if not self._custom_requires_grad:\n",
        "            return CustomTensor(out,due_to_operation=True)\n",
        "        \n",
        "        graph = self.graph\n",
        "        result = CustomTensor(out, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "        graph.add_edge(self._node_id, result._node_id)\n",
        "        self_ref = weakref.proxy(self)\n",
        "        result_ref = weakref.proxy(result)\n",
        "        def _backward():\n",
        "            if self_ref.tensor.grad is None:\n",
        "                self_ref._zero_grad()\n",
        "            self_ref.tensor.grad.add_(-result_ref.tensor.grad*torch.sin(self_ref.tensor))\n",
        "        result._backward = _backward\n",
        "        return result \n",
        "\n",
        "    def sqrt(self):\n",
        "        out = torch.sqrt(self.tensor)\n",
        "        if not self._custom_requires_grad:\n",
        "            return CustomTensor(out,due_to_operation=True)\n",
        "        \n",
        "        graph = self.graph\n",
        "        result = CustomTensor(out, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "        graph.add_edge(self._node_id, result._node_id)\n",
        "        self_ref = weakref.proxy(self)\n",
        "        result_ref = weakref.proxy(result)\n",
        "        def _backward():\n",
        "            if self_ref.tensor.grad is None:\n",
        "                self_ref._zero_grad()\n",
        "            self_ref.tensor.grad.add_(result_ref.tensor.grad*0.5*self_ref.tensor.pow(-0.5))\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "\n",
        "    def matmul(self, other):\n",
        "        result_tensor = torch.matmul(self.tensor, other.tensor)\n",
        "        requires_grad = self._custom_requires_grad or other._custom_requires_grad\n",
        "        if not requires_grad:\n",
        "            return CustomTensor(result_tensor,due_to_operation=True)\n",
        "\n",
        "        graph = self.graph if self._custom_requires_grad else other.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "\n",
        "        self_ref = weakref.proxy(self)\n",
        "        other_ref = weakref.proxy(other)\n",
        "        result_ref = weakref.proxy(result)\n",
        "\n",
        "        if self._custom_requires_grad:\n",
        "            graph.add_edge(self._node_id, result._node_id)\n",
        "        if other._custom_requires_grad:\n",
        "            graph.add_edge(other._node_id, result._node_id)\n",
        "\n",
        "        def _backward():\n",
        "            if self_ref._custom_requires_grad:\n",
        "                if self_ref.tensor.grad is None: self_ref._zero_grad()\n",
        "                # Use robust broadcasting for matmul gradient\n",
        "                grad_for_self = torch.matmul(result_ref.tensor.grad, other_ref.tensor.transpose(-2, -1))\n",
        "                self_ref.tensor.grad.add_(self_ref._reduce_grad_for_broadcast(grad_for_self, self_ref.tensor.shape))\n",
        "            if other_ref._custom_requires_grad:\n",
        "                if other_ref.tensor.grad is None: other_ref._zero_grad()\n",
        "                grad_for_other = torch.matmul(self_ref.tensor.transpose(-2, -1), result_ref.tensor.grad)\n",
        "                other_ref.tensor.grad.add_(other_ref._reduce_grad_for_broadcast(grad_for_other, other_ref.tensor.shape))\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "    def dot(self, other):\n",
        "        # torch.dot only works for 1D tensors, or for higher-D tensors,\n",
        "        # it flattens them to 1D and then computes the dot product.\n",
        "        # This means the gradients will also be 1D, so no complex broadcasting\n",
        "        # reduction is needed on the output gradient itself.\n",
        "        # However, the input tensors themselves could have been results of broadcasting ops.\n",
        "        # For a truly general dot product, you'd use torch.matmul.\n",
        "        result_tensor = torch.dot(self.tensor.reshape(-1), other.tensor.reshape(-1))\n",
        "        requires_grad = self._custom_requires_grad or other._custom_requires_grad\n",
        "        if not requires_grad:\n",
        "            return CustomTensor(result_tensor,due_to_operation=True)\n",
        "\n",
        "        graph = self.graph if self._custom_requires_grad else other.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "\n",
        "        self_ref = weakref.proxy(self)\n",
        "        other_ref = weakref.proxy(other)\n",
        "        result_ref = weakref.proxy(result)\n",
        "\n",
        "        if self._custom_requires_grad:\n",
        "            graph.add_edge(self._node_id, result._node_id)\n",
        "        if other._custom_requires_grad:\n",
        "            graph.add_edge(other._node_id, result._node_id)\n",
        "\n",
        "        def _backward():\n",
        "            if self_ref._custom_requires_grad:\n",
        "                if self_ref.tensor.grad is None:\n",
        "                    self_ref._zero_grad()\n",
        "                # The grad from result_ref.tensor.grad will be a scalar.\n",
        "                # It needs to be multiplied by the other_ref.tensor (original shape)\n",
        "                # and then potentially re-shaped if original was >1D\n",
        "                grad_contrib = result_ref.tensor.grad * other_ref.tensor\n",
        "                self_ref.tensor.grad.add_(grad_contrib)\n",
        "            if other_ref._custom_requires_grad:\n",
        "                if other_ref.tensor.grad is None:\n",
        "                    other_ref._zero_grad()\n",
        "                grad_contrib = result_ref.tensor.grad * self_ref.tensor\n",
        "                other_ref.tensor.grad.add_(grad_contrib)\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "\n",
        "\n",
        "    \n",
        "    # --- New Unary Operations ---\n",
        "    \n",
        "    def sum(self, dim=None, keepdim=False):\n",
        "        \"\"\"Computes the sum of elements along given dimensions.\"\"\"\n",
        "        result_tensor = self.tensor.sum(dim=dim, keepdim=keepdim)\n",
        "        if not self._custom_requires_grad:\n",
        "            return CustomTensor(result_tensor, due_to_operation=True)\n",
        "            \n",
        "        graph = self.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "        graph.add_edge(self._node_id, result._node_id)\n",
        "\n",
        "        self_ref = weakref.proxy(self)\n",
        "        result_ref = weakref.proxy(result)\n",
        "\n",
        "        def _backward():\n",
        "            if self_ref.tensor.grad is None:\n",
        "                self_ref._zero_grad()\n",
        "                \n",
        "            grad = result_ref.tensor.grad\n",
        "            # If keepdim was false, the summed dim was squeezed. We need to unsqueeze it back for broadcasting.\n",
        "            if not keepdim and dim is not None:\n",
        "                grad = grad.unsqueeze(dim)\n",
        "            \n",
        "            self_ref.tensor.grad.add_(grad)\n",
        "\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "\n",
        "    def mean(self, dim=None, keepdim=False):\n",
        "        \"\"\"Computes the mean of elements along given dimensions.\"\"\"\n",
        "        result_tensor = self.tensor.mean(dim=dim, keepdim=keepdim)\n",
        "        if not self._custom_requires_grad:\n",
        "            return CustomTensor(result_tensor, due_to_operation=True)\n",
        "\n",
        "        graph = self.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "        graph.add_edge(self._node_id, result._node_id)\n",
        "\n",
        "        self_ref = weakref.proxy(self)\n",
        "        result_ref = weakref.proxy(result)\n",
        "        \n",
        "        # Determine the number of elements that were averaged\n",
        "        if dim is None:\n",
        "            n = self.tensor.numel()\n",
        "        else:\n",
        "            n = self.tensor.shape[dim]\n",
        "\n",
        "        def _backward():\n",
        "            if self_ref.tensor.grad is None:\n",
        "                self_ref._zero_grad()\n",
        "            \n",
        "            grad = result_ref.tensor.grad\n",
        "            if not keepdim and dim is not None:\n",
        "                grad = grad.unsqueeze(dim)\n",
        "            \n",
        "            # Distribute gradient evenly\n",
        "            self_ref.tensor.grad.add_(grad / n)\n",
        "\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "\n",
        "    def reshape(self, *shape):\n",
        "        \"\"\"Reshapes the tensor to the given shape.\"\"\"\n",
        "        original_shape = self.shape\n",
        "        result_tensor = self.tensor.reshape(*shape)\n",
        "        if not self._custom_requires_grad:\n",
        "            return CustomTensor(result_tensor, due_to_operation=True)\n",
        "\n",
        "        graph = self.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "        graph.add_edge(self._node_id, result._node_id)\n",
        "        \n",
        "        self_ref = weakref.proxy(self)\n",
        "        result_ref = weakref.proxy(result)\n",
        "        \n",
        "        def _backward():\n",
        "            if self_ref.tensor.grad is None:\n",
        "                self_ref._zero_grad()\n",
        "            self_ref.tensor.grad.add_(result_ref.tensor.grad.reshape(original_shape))\n",
        "\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "        \n",
        "    def transpose(self, dim0, dim1):\n",
        "        \"\"\"Transposes dimensions dim0 and dim1.\"\"\"\n",
        "        result_tensor = self.tensor.transpose(dim0, dim1)\n",
        "        if not self._custom_requires_grad:\n",
        "            return CustomTensor(result_tensor, due_to_operation=True)\n",
        "\n",
        "        graph = self.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "        graph.add_edge(self._node_id, result._node_id)\n",
        "\n",
        "        self_ref = weakref.proxy(self)\n",
        "        result_ref = weakref.proxy(result)\n",
        "\n",
        "        def _backward():\n",
        "            if self_ref.tensor.grad is None:\n",
        "                self_ref._zero_grad()\n",
        "            # The gradient operation for transpose is another transpose\n",
        "            self_ref.tensor.grad.add_(result_ref.tensor.grad.transpose(dim0, dim1))\n",
        "            \n",
        "        result._backward = _backward\n",
        "        return result\n",
        "\n",
        "    @property\n",
        "    def T(self):\n",
        "        \"\"\"Alias for transpose(-2, -1) for 2D or higher dimensional tensors.\"\"\"\n",
        "        if self.ndim < 2:\n",
        "            raise ValueError(\"`.T` is only supported on tensors with 2 or more dimensions.\")\n",
        "        return self.transpose(-2, -1)\n",
        "        \n",
        "    # --- Activation Functions ---\n",
        "\n",
        "    def relu(self):\n",
        "        \"\"\"Applies the Rectified Linear Unit function element-wise.\"\"\"\n",
        "        result_tensor = F.relu(self.tensor)\n",
        "        if not self._custom_requires_grad:\n",
        "            return CustomTensor(result_tensor, due_to_operation=True)\n",
        "\n",
        "        graph = self.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "        graph.add_edge(self._node_id, result._node_id)\n",
        "        \n",
        "        self_ref = weakref.proxy(self)\n",
        "        result_ref = weakref.proxy(result)\n",
        "        \n",
        "        def _backward():\n",
        "            if self_ref.tensor.grad is None: self_ref._zero_grad()\n",
        "            # Derivative is 1 for positive inputs, 0 otherwise\n",
        "            grad_mask = (self_ref.tensor > 0).type(self_ref.tensor.dtype)\n",
        "            self_ref.tensor.grad.add_(result_ref.tensor.grad * grad_mask)\n",
        "\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "\n",
        "    def tanh(self):\n",
        "        \"\"\"Applies the hyperbolic tangent function element-wise.\"\"\"\n",
        "        result_tensor = torch.tanh(self.tensor)\n",
        "        if not self._custom_requires_grad:\n",
        "            return CustomTensor(result_tensor, due_to_operation=True)\n",
        "\n",
        "        graph = self.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "        graph.add_edge(self._node_id, result._node_id)\n",
        "\n",
        "        self_ref = weakref.proxy(self)\n",
        "        result_ref = weakref.proxy(result)\n",
        "        \n",
        "        def _backward():\n",
        "            if self_ref.tensor.grad is None: self_ref._zero_grad()\n",
        "            # Derivative is 1 - tanh^2(x)\n",
        "            local_grad = 1 - result_tensor.pow(2)\n",
        "            self_ref.tensor.grad.add_(result_ref.tensor.grad * local_grad)\n",
        "\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "\n",
        "    def leaky_relu(self, negative_slope=0.01):\n",
        "        \"\"\"Applies the Leaky Rectified Linear Unit function element-wise.\"\"\"\n",
        "        result_tensor = F.leaky_relu(self.tensor, negative_slope)\n",
        "        if not self._custom_requires_grad:\n",
        "            return CustomTensor(result_tensor, due_to_operation=True)\n",
        "\n",
        "        graph = self.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "        graph.add_edge(self._node_id, result._node_id)\n",
        "\n",
        "        self_ref = weakref.proxy(self)\n",
        "        result_ref = weakref.proxy(result)\n",
        "\n",
        "        def _backward():\n",
        "            if self_ref.tensor.grad is None: self_ref._zero_grad()\n",
        "            # Derivative is 1 for positive, negative_slope for negative\n",
        "            local_grad = torch.ones_like(self_ref.tensor)\n",
        "            local_grad[self_ref.tensor < 0] = negative_slope\n",
        "            self_ref.tensor.grad.add_(result_ref.tensor.grad * local_grad)\n",
        "\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "\n",
        "    def elu(self, alpha=1.0):\n",
        "        \"\"\"Applies the Exponential Linear Unit function element-wise.\"\"\"\n",
        "        result_tensor = F.elu(self.tensor, alpha)\n",
        "        if not self._custom_requires_grad:\n",
        "            return CustomTensor(result_tensor, due_to_operation=True)\n",
        "\n",
        "        graph = self.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "        graph.add_edge(self._node_id, result._node_id)\n",
        "\n",
        "        self_ref = weakref.proxy(self)\n",
        "        result_ref = weakref.proxy(result)\n",
        "\n",
        "        def _backward():\n",
        "            if self_ref.tensor.grad is None: self_ref._zero_grad()\n",
        "            # Derivative is 1 for positive, and output + alpha for negative\n",
        "            local_grad = torch.ones_like(self_ref.tensor)\n",
        "            neg_mask = self_ref.tensor < 0\n",
        "            local_grad[neg_mask] = result_tensor[neg_mask] + alpha\n",
        "            self_ref.tensor.grad.add_(result_ref.tensor.grad * local_grad)\n",
        "\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "        \n",
        "    def silu(self):\n",
        "        \"\"\"Applies the Sigmoid-weighted Linear Unit function element-wise.\"\"\"\n",
        "        result_tensor = F.silu(self.tensor)\n",
        "        if not self._custom_requires_grad:\n",
        "            return CustomTensor(result_tensor, due_to_operation=True)\n",
        "\n",
        "        graph = self.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "        graph.add_edge(self._node_id, result._node_id)\n",
        "\n",
        "        self_ref = weakref.proxy(self)\n",
        "        result_ref = weakref.proxy(result)\n",
        "        \n",
        "        def _backward():\n",
        "            if self_ref.tensor.grad is None: self_ref._zero_grad()\n",
        "            # Derivative of x*sigmoid(x) is sigmoid(x) + x*sigmoid(x)*(1-sigmoid(x))\n",
        "            sig_x = torch.sigmoid(self_ref.tensor)\n",
        "            local_grad = sig_x * (1 + self_ref.tensor * (1 - sig_x))\n",
        "            self_ref.tensor.grad.add_(result_ref.tensor.grad * local_grad)\n",
        "\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "    \n",
        "    # Add swish as an alias for silu\n",
        "    swish = silu\n",
        "\n",
        "    def gelu(self):\n",
        "        \"\"\"Applies the Gaussian Error Linear Unit function element-wise.\"\"\"\n",
        "        result_tensor = F.gelu(self.tensor)\n",
        "        if not self._custom_requires_grad:\n",
        "            return CustomTensor(result_tensor, due_to_operation=True)\n",
        "\n",
        "        graph = self.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "        graph.add_edge(self._node_id, result._node_id)\n",
        "        \n",
        "        self_ref = weakref.proxy(self)\n",
        "        result_ref = weakref.proxy(result)\n",
        "        \n",
        "        def _backward():\n",
        "            if self_ref.tensor.grad is None: self_ref._zero_grad()\n",
        "            # Derivative of GELU: 0.5 * (1 + erf(x/sqrt(2))) + x * exp(-x^2/2) / sqrt(2*pi)\n",
        "            x = self_ref.tensor\n",
        "            cdf = 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n",
        "            pdf = torch.exp(-0.5 * x**2) / math.sqrt(2.0 * math.pi)\n",
        "            local_grad = cdf + x * pdf\n",
        "            self_ref.tensor.grad.add_(result_ref.tensor.grad * local_grad)\n",
        "\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "\n",
        "    def softmax(self, dim=-1):\n",
        "        \"\"\"Applies the softmax function along a given dimension.\"\"\"\n",
        "        result_tensor = torch.softmax(self.tensor, dim=dim)\n",
        "        if not self._custom_requires_grad:\n",
        "            return CustomTensor(result_tensor, due_to_operation=True)\n",
        "\n",
        "        graph = self.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "        graph.add_edge(self._node_id, result._node_id)\n",
        "        \n",
        "        self_ref = weakref.proxy(self)\n",
        "        result_ref = weakref.proxy(result)\n",
        "\n",
        "        def _backward():\n",
        "            if self_ref.tensor.grad is None: self_ref._zero_grad()\n",
        "            # For softmax, the jacobian-vector product is y * (grad - sum(grad * y))\n",
        "            y = result_tensor\n",
        "            grad_output = result_ref.tensor.grad\n",
        "            grad_input = y * (grad_output - (grad_output * y).sum(dim=dim, keepdim=True))\n",
        "            self_ref.tensor.grad.add_(grad_input)\n",
        "            \n",
        "        result._backward = _backward\n",
        "        return result\n",
        "    def backward(self, weightage_tensor=1):\n",
        "        if not self._custom_requires_grad:\n",
        "            raise RuntimeError(\"Output tensor does not require grad.\")\n",
        "        if self.graph is None:\n",
        "            raise RuntimeError(\"Output tensor is not part of a graph.\")\n",
        "        graph = self.graph\n",
        "        \n",
        "        # Initialize gradient for the output tensor\n",
        "        if isinstance(weightage_tensor, numbers.Number):\n",
        "            self.tensor.grad = torch.full_like(self.tensor, fill_value=weightage_tensor)\n",
        "        elif isinstance(weightage_tensor, torch.Tensor):\n",
        "            self.tensor.grad = weightage_tensor.clone()\n",
        "\n",
        "        nodes_to_process = graph.reverse_toposort_from_tensor(self._node_id)\n",
        "        \n",
        "        for tensor_node in nodes_to_process:\n",
        "            tensor_node._backward()\n",
        "            #try:\n",
        "                # The node is a weakref.proxy, check if it's still alive\n",
        "                #if tensor_node.__class__ is weakref.ProxyType:\n",
        "            #        tensor_node._backward()\n",
        "            # except ReferenceError:\n",
        "            #     # The tensor object was garbage collected, skip.\n",
        "            #     print(\"dead reference node encountered\")\n",
        "            #     continue\n",
        "    # --- Properties and Dunder Methods ---\n",
        "    @property\n",
        "    def dtype(self): return self.tensor.dtype\n",
        "    @property\n",
        "    def ndim(self): return self.tensor.ndim\n",
        "    @property\n",
        "    def shape(self): return self.tensor.shape\n",
        "    @property\n",
        "    def grad(self): return self.tensor.grad\n",
        "    def __repr__(self): return f\"CustomTensor({self.tensor}, grad_fn={self._backward != None}, requires_grad={self._custom_requires_grad})\"\n",
        "    def __del__(self):\n",
        "        if self._node_id is not None and self._is_leaf:\n",
        "            try:\n",
        "                if self.graph: self.graph.delete_node(self._node_id)\n",
        "            except ReferenceError: # Graph might be gone first\n",
        "                pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Autograd Tester"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import numbers\n",
        "import weakref\n",
        "import rustworkx as rx\n",
        "from typing import Optional, Any\n",
        "import sys\n",
        "import gc\n",
        "import pytest\n",
        "\n",
        "\n",
        "class AutogradTester:\n",
        "    def __init__(self):\n",
        "        self.passed_tests = 0\n",
        "        self.failed_tests = 0\n",
        "        self.tolerance = 1e-6  # Increased tolerance slightly for complex ops\n",
        "\n",
        "    def assert_tensors_close(self, custom_tensor, pytorch_tensor, test_name, check_grad=True):\n",
        "        \"\"\"Compare custom tensor with PyTorch tensor values and optionally gradients.\"\"\"\n",
        "        try:\n",
        "            # Check values\n",
        "            np.testing.assert_allclose(\n",
        "                custom_tensor.tensor.detach().cpu().numpy(),  # Ensure on CPU for numpy\n",
        "                pytorch_tensor.detach().cpu().numpy(),\n",
        "                rtol=self.tolerance,\n",
        "                atol=self.tolerance,\n",
        "                err_msg=f\"Mismatch in tensor values for {test_name}\"\n",
        "            )\n",
        "\n",
        "            # Check gradients if requested and they exist for PyTorch tensor\n",
        "            if check_grad and pytorch_tensor.grad is not None:\n",
        "                if custom_tensor.tensor.grad is None:\n",
        "                    raise AssertionError(f\"Custom tensor has no gradient for {test_name}, but PyTorch does.\")\n",
        "\n",
        "                np.testing.assert_allclose(\n",
        "                    custom_tensor.tensor.grad.detach().cpu().numpy(),  # Ensure on CPU for numpy\n",
        "                    pytorch_tensor.grad.detach().cpu().numpy(),\n",
        "                    rtol=self.tolerance,\n",
        "                    atol=self.tolerance,\n",
        "                    err_msg=f\"Mismatch in gradients for {test_name}\"\n",
        "                )\n",
        "            elif check_grad and pytorch_tensor.grad is None and custom_tensor.tensor.grad is not None:\n",
        "                raise AssertionError(f\"Custom tensor has gradient for {test_name}, but PyTorch does not (should be no_grad).\")\n",
        "\n",
        "            print(f\"✓ {test_name}\")\n",
        "            self.passed_tests += 1\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"✗ {test_name}: {str(e)}\")\n",
        "            self.failed_tests += 1\n",
        "\n",
        "    def test_basic_operations(self):\n",
        "        \"\"\"Test basic arithmetic operations\"\"\"\n",
        "        print(\"\\n=== Testing Basic Operations ===\")\n",
        "\n",
        "        # Test scalar addition\n",
        "        with AutogradGraph() as graph:\n",
        "            x_custom = CustomTensor([2.0, 3.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = x_custom + 5.0\n",
        "            y_custom.backward(torch.ones_like(y_custom.tensor))\n",
        "\n",
        "            x_pytorch = torch.tensor([2.0, 3.0], requires_grad=True)\n",
        "            y_pytorch = x_pytorch + 5.0\n",
        "            y_pytorch.backward(torch.ones_like(y_pytorch))\n",
        "\n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Scalar Addition - x\")\n",
        "            self.assert_tensors_close(y_custom, y_pytorch, \"Scalar Addition - y (result)\", check_grad=False)\n",
        "\n",
        "        # Test tensor addition\n",
        "        with AutogradGraph() as graph:\n",
        "            x_custom = CustomTensor([1.0, 2.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = CustomTensor([3.0, 4.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            z_custom = x_custom + y_custom\n",
        "            z_custom.backward(torch.ones_like(z_custom.tensor))\n",
        "\n",
        "            x_pytorch = torch.tensor([1.0, 2.0], requires_grad=True)\n",
        "            y_pytorch = torch.tensor([3.0, 4.0], requires_grad=True)\n",
        "            z_pytorch = x_pytorch + y_pytorch\n",
        "            z_pytorch.backward(torch.ones_like(z_pytorch))\n",
        "\n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Tensor Addition - x\")\n",
        "            self.assert_tensors_close(y_custom, y_pytorch, \"Tensor Addition - y\")\n",
        "            self.assert_tensors_close(z_custom, z_pytorch, \"Tensor Addition - z (result)\", check_grad=False)\n",
        "\n",
        "    def test_multiplication(self):\n",
        "        \"\"\"Test multiplication operations\"\"\"\n",
        "        print(\"\\n=== Testing Multiplication ===\")\n",
        "\n",
        "        # Test scalar multiplication\n",
        "        with AutogradGraph() as graph:\n",
        "            x_custom = CustomTensor([2.0, 3.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = x_custom * 4.0\n",
        "            y_custom.backward(torch.ones_like(y_custom.tensor))\n",
        "\n",
        "            x_pytorch = torch.tensor([2.0, 3.0], requires_grad=True)\n",
        "            y_pytorch = x_pytorch * 4.0\n",
        "            y_pytorch.backward(torch.ones_like(y_pytorch))\n",
        "\n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Scalar Multiplication - x\")\n",
        "            self.assert_tensors_close(y_custom, y_pytorch, \"Scalar Multiplication - y (result)\", check_grad=False)\n",
        "\n",
        "        # Test tensor multiplication\n",
        "        with AutogradGraph() as graph:\n",
        "            x_custom = CustomTensor([2.0, 3.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = CustomTensor([4.0, 5.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            z_custom = x_custom * y_custom\n",
        "            z_custom.backward(torch.ones_like(z_custom.tensor))\n",
        "\n",
        "            x_pytorch = torch.tensor([2.0, 3.0], requires_grad=True)\n",
        "            y_pytorch = torch.tensor([4.0, 5.0], requires_grad=True)\n",
        "            z_pytorch = x_pytorch * y_pytorch\n",
        "            z_pytorch.backward(torch.ones_like(z_pytorch))\n",
        "\n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Tensor Multiplication - x\")\n",
        "            self.assert_tensors_close(y_custom, y_pytorch, \"Tensor Multiplication - y\")\n",
        "            self.assert_tensors_close(z_custom, z_pytorch, \"Tensor Multiplication - z (result)\", check_grad=False)\n",
        "\n",
        "    def test_subtraction_division(self):\n",
        "        \"\"\"Test subtraction and division\"\"\"\n",
        "        print(\"\\n=== Testing Subtraction and Division ===\")\n",
        "\n",
        "        # Test scalar subtraction (x - C)\n",
        "        with AutogradGraph() as graph:\n",
        "            x_custom = CustomTensor([5.0, 6.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = x_custom - 2.0\n",
        "            y_custom.backward(torch.ones_like(y_custom.tensor))\n",
        "\n",
        "            x_pytorch = torch.tensor([5.0, 6.0], requires_grad=True)\n",
        "            y_pytorch = x_pytorch - 2.0\n",
        "            y_pytorch.backward(torch.ones_like(y_pytorch))\n",
        "\n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Scalar Subtraction (x - C) - x\")\n",
        "            self.assert_tensors_close(y_custom, y_pytorch, \"Scalar Subtraction (x - C) - y (result)\", check_grad=False)\n",
        "\n",
        "        # Test scalar reverse subtraction (C - x)\n",
        "        with AutogradGraph() as graph:\n",
        "            x_custom = CustomTensor([5.0, 6.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = 10.0 - x_custom  # Uses __rsub__\n",
        "            y_custom.backward(torch.ones_like(y_custom.tensor))\n",
        "\n",
        "            x_pytorch = torch.tensor([5.0, 6.0], requires_grad=True)\n",
        "            y_pytorch = 10.0 - x_pytorch\n",
        "            y_pytorch.backward(torch.ones_like(y_pytorch))\n",
        "\n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Scalar Reverse Subtraction (C - x) - x\")\n",
        "            self.assert_tensors_close(y_custom, y_pytorch, \"Scalar Reverse Subtraction (C - x) - y (result)\", check_grad=False)\n",
        "\n",
        "        # Test tensor subtraction\n",
        "        with AutogradGraph() as graph:\n",
        "            x_custom = CustomTensor([7.0, 8.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = CustomTensor([2.0, 1.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            z_custom = x_custom - y_custom\n",
        "            z_custom.backward(torch.ones_like(z_custom.tensor))\n",
        "\n",
        "            x_pytorch = torch.tensor([7.0, 8.0], requires_grad=True)\n",
        "            y_pytorch = torch.tensor([2.0, 1.0], requires_grad=True)\n",
        "            z_pytorch = x_pytorch - y_pytorch\n",
        "            z_pytorch.backward(torch.ones_like(z_pytorch))\n",
        "\n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Tensor Subtraction - x\")\n",
        "            self.assert_tensors_close(y_custom, y_pytorch, \"Tensor Subtraction - y\")\n",
        "            self.assert_tensors_close(z_custom, z_pytorch, \"Tensor Subtraction - z (result)\", check_grad=False)\n",
        "\n",
        "        # Test scalar division\n",
        "        with AutogradGraph() as graph:\n",
        "            x_custom = CustomTensor([8.0, 12.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = x_custom / 4.0\n",
        "            y_custom.backward(torch.ones_like(y_custom.tensor))\n",
        "\n",
        "            x_pytorch = torch.tensor([8.0, 12.0], requires_grad=True)\n",
        "            y_pytorch = x_pytorch / 4.0\n",
        "            y_pytorch.backward(torch.ones_like(y_pytorch))\n",
        "\n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Scalar Division - x\")\n",
        "            self.assert_tensors_close(y_custom, y_pytorch, \"Scalar Division - y (result)\", check_grad=False)\n",
        "        # Test tensor division\n",
        "        with AutogradGraph() as graph:\n",
        "            x_custom = CustomTensor([8.0, 12.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = CustomTensor([5.0, 10.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            z_custom = x_custom / y_custom\n",
        "            z_custom.backward(torch.ones_like(z_custom.tensor))\n",
        "\n",
        "            x_pytorch = torch.tensor([8.0, 12.0], requires_grad=True)\n",
        "            y_pytorch = torch.tensor([5.0, 10.0], requires_grad=True)\n",
        "            z_pytorch = x_pytorch / y_pytorch\n",
        "            z_pytorch.backward(torch.ones_like(z_pytorch))\n",
        "\n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Tensor Division - x\")\n",
        "            self.assert_tensors_close(y_custom, y_pytorch, \"Tensir Division - y\")\n",
        "            self.assert_tensors_close(z_custom, z_pytorch, \"Tensor Division - z (result)\", check_grad=False)\n",
        "\n",
        "\n",
        "    def test_power_function(self):\n",
        "        \"\"\"Test power operation\"\"\"\n",
        "        print(\"\\n=== Testing Power Function ===\")\n",
        "\n",
        "        with AutogradGraph() as graph:\n",
        "            x_custom = CustomTensor([2.0, 3.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = x_custom.pow(3.0)\n",
        "            y_custom.backward(torch.ones_like(y_custom.tensor))\n",
        "\n",
        "            x_pytorch = torch.tensor([2.0, 3.0], requires_grad=True)\n",
        "            y_pytorch = torch.pow(x_pytorch, 3.0)\n",
        "            y_pytorch.backward(torch.ones_like(y_pytorch))\n",
        "\n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Power Function - x\")\n",
        "            self.assert_tensors_close(y_custom, y_pytorch, \"Power Function - y (result)\", check_grad=False)\n",
        "\n",
        "        # Test power with negative exponent\n",
        "        with AutogradGraph() as graph:\n",
        "            x_custom = CustomTensor([2.0, 3.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = x_custom.pow(-2.0)\n",
        "            y_custom.backward(torch.ones_like(y_custom.tensor))\n",
        "\n",
        "            x_pytorch = torch.tensor([2.0, 3.0], requires_grad=True)\n",
        "            y_pytorch = torch.pow(x_pytorch, -2.0)\n",
        "            y_pytorch.backward(torch.ones_like(y_pytorch))\n",
        "\n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Power Function (Negative Exponent) - x\")\n",
        "            self.assert_tensors_close(y_custom, y_pytorch, \"Power Function (Negative Exponent) - y (result)\", check_grad=False)\n",
        "\n",
        "    def test_unary_functions(self):\n",
        "        \"\"\"Test unary mathematical functions\"\"\"\n",
        "        print(\"\\n=== Testing Unary Functions ===\")\n",
        "\n",
        "        # Test exp\n",
        "        with AutogradGraph() as graph:\n",
        "            x_custom = CustomTensor([1.0, 2.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = x_custom.exp()\n",
        "            y_custom.backward(torch.ones_like(y_custom.tensor))\n",
        "\n",
        "            x_pytorch = torch.tensor([1.0, 2.0], requires_grad=True)\n",
        "            y_pytorch = torch.exp(x_pytorch)\n",
        "            y_pytorch.backward(torch.ones_like(y_pytorch))\n",
        "\n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Exponential Function - x\")\n",
        "            self.assert_tensors_close(y_custom, y_pytorch, \"Exponential Function - y (result)\", check_grad=False)\n",
        "\n",
        "        # Test log\n",
        "        with AutogradGraph() as graph:\n",
        "            x_custom = CustomTensor([1.0, 2.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = x_custom.log()\n",
        "            y_custom.backward(torch.ones_like(y_custom.tensor))\n",
        "\n",
        "            x_pytorch = torch.tensor([1.0, 2.0], requires_grad=True)\n",
        "            y_pytorch = torch.log(x_pytorch)\n",
        "            y_pytorch.backward(torch.ones_like(y_pytorch))\n",
        "\n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Logarithm Function - x\")\n",
        "            self.assert_tensors_close(y_custom, y_pytorch, \"Logarithm Function - y (result)\", check_grad=False)\n",
        "\n",
        "        # Test sin\n",
        "        with AutogradGraph() as graph:\n",
        "            x_custom = CustomTensor([0.5, 1.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = x_custom.sin()\n",
        "            y_custom.backward(torch.ones_like(y_custom.tensor))\n",
        "\n",
        "            x_pytorch = torch.tensor([0.5, 1.0], requires_grad=True)\n",
        "            y_pytorch = torch.sin(x_pytorch)\n",
        "            y_pytorch.backward(torch.ones_like(y_pytorch))\n",
        "\n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Sine Function - x\")\n",
        "            self.assert_tensors_close(y_custom, y_pytorch, \"Sine Function - y (result)\", check_grad=False)\n",
        "\n",
        "        # Test cos\n",
        "        with AutogradGraph() as graph:\n",
        "            x_custom = CustomTensor([0.5, 1.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = x_custom.cos()\n",
        "            y_custom.backward(torch.ones_like(y_custom.tensor))\n",
        "\n",
        "            x_pytorch = torch.tensor([0.5, 1.0], requires_grad=True)\n",
        "            y_pytorch = torch.cos(x_pytorch)\n",
        "            y_pytorch.backward(torch.ones_like(y_pytorch))\n",
        "\n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Cosine Function - x\")\n",
        "            self.assert_tensors_close(y_custom, y_pytorch, \"Cosine Function - y (result)\", check_grad=False)\n",
        "\n",
        "        # Test sqrt\n",
        "        with AutogradGraph() as graph:\n",
        "            x_custom = CustomTensor([4.0, 9.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = x_custom.sqrt()\n",
        "            y_custom.backward(torch.ones_like(y_custom.tensor))\n",
        "\n",
        "            x_pytorch = torch.tensor([4.0, 9.0], requires_grad=True)\n",
        "            y_pytorch = torch.sqrt(x_pytorch)\n",
        "            y_pytorch.backward(torch.ones_like(y_pytorch))\n",
        "\n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Square Root Function - x\")\n",
        "            self.assert_tensors_close(y_custom, y_pytorch, \"Square Root Function - y (result)\", check_grad=False)\n",
        "\n",
        "    def test_matrix_operations(self):\n",
        "        \"\"\"Test matrix operations\"\"\"\n",
        "        print(\"\\n=== Testing Matrix Operations ===\")\n",
        "\n",
        "        # Test matrix multiplication (2x2 @ 2x2)\n",
        "        with AutogradGraph() as graph:\n",
        "            x_custom = CustomTensor([[1.0, 2.0], [3.0, 4.0]], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = CustomTensor([[5.0, 6.0], [7.0, 8.0]], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            z_custom = x_custom.matmul(y_custom)\n",
        "            z_custom.backward(torch.ones_like(z_custom.tensor))\n",
        "\n",
        "            x_pytorch = torch.tensor([[1.0, 2.0], [3.0, 4.0]], requires_grad=True)\n",
        "            y_pytorch = torch.tensor([[5.0, 6.0], [7.0, 8.0]], requires_grad=True)\n",
        "            z_pytorch = torch.matmul(x_pytorch, y_pytorch)\n",
        "            z_pytorch.backward(torch.ones_like(z_pytorch))\n",
        "\n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Matrix Multiplication (2x2 @ 2x2) - x\")\n",
        "            self.assert_tensors_close(y_custom, y_pytorch, \"Matrix Multiplication (2x2 @ 2x2) - y\")\n",
        "            self.assert_tensors_close(z_custom, z_pytorch, \"Matrix Multiplication (2x2 @ 2x2) - z (result)\", check_grad=False)\n",
        "\n",
        "        # Test matrix multiplication (2x3 @ 3x2)\n",
        "        with AutogradGraph() as graph:\n",
        "            x_custom = CustomTensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = CustomTensor([[7.0, 8.0], [9.0, 10.0], [11.0, 12.0]], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            z_custom = x_custom.matmul(y_custom)\n",
        "            z_custom.backward(torch.ones_like(z_custom.tensor))\n",
        "\n",
        "            x_pytorch = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]], requires_grad=True)\n",
        "            y_pytorch = torch.tensor([[7.0, 8.0], [9.0, 10.0], [11.0, 12.0]], requires_grad=True)\n",
        "            z_pytorch = torch.matmul(x_pytorch, y_pytorch)\n",
        "            z_pytorch.backward(torch.ones_like(z_pytorch))\n",
        "\n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Matrix Multiplication (2x3 @ 3x2) - x\")\n",
        "            self.assert_tensors_close(y_custom, y_pytorch, \"Matrix Multiplication (2x3 @ 3x2) - y\")\n",
        "            self.assert_tensors_close(z_custom, z_pytorch, \"Matrix Multiplication (2x3 @ 3x2) - z (result)\", check_grad=False)\n",
        "\n",
        "        # Test dot product (vector * vector)\n",
        "        with AutogradGraph() as graph:\n",
        "            x_custom = CustomTensor([1.0, 2.0, 3.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = CustomTensor([4.0, 5.0, 6.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            z_custom = x_custom.dot(y_custom)\n",
        "            z_custom.backward()  # Scalar output, so default backward() is fine (grad=1)\n",
        "\n",
        "            x_pytorch = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
        "            y_pytorch = torch.tensor([4.0, 5.0, 6.0], requires_grad=True)\n",
        "            z_pytorch = torch.dot(x_pytorch, y_pytorch)\n",
        "            z_pytorch.backward()\n",
        "\n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Dot Product (vector) - x\")\n",
        "            self.assert_tensors_close(y_custom, y_pytorch, \"Dot Product (vector) - y\")\n",
        "            self.assert_tensors_close(z_custom, z_pytorch, \"Dot Product (vector) - z (result)\", check_grad=False)\n",
        "\n",
        "    def test_complex_chain(self):\n",
        "        \"\"\"Test complex computational chains\"\"\"\n",
        "        print(\"\\n=== Testing Complex Chains ===\")\n",
        "\n",
        "        # Test 1: z = (x + y) * (x - y) + x^2 - sin(y)\n",
        "        with AutogradGraph() as graph:\n",
        "            x_custom = CustomTensor([3.0, 4.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = CustomTensor([1.0, 2.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "\n",
        "            sum_custom = x_custom + y_custom\n",
        "            diff_custom = x_custom - y_custom\n",
        "            prod_custom = sum_custom * diff_custom\n",
        "            x_squared_custom = x_custom.pow(2.0)\n",
        "            sin_y_custom = y_custom.sin()\n",
        "\n",
        "            inter1_custom = prod_custom + x_squared_custom\n",
        "            z_custom = inter1_custom - sin_y_custom\n",
        "\n",
        "            z_custom.backward(torch.ones_like(z_custom.tensor))\n",
        "\n",
        "            x_pytorch = torch.tensor([3.0, 4.0], requires_grad=True)\n",
        "            y_pytorch = torch.tensor([1.0, 2.0], requires_grad=True)\n",
        "\n",
        "            sum_pytorch = x_pytorch + y_pytorch\n",
        "            diff_pytorch = x_pytorch - y_pytorch\n",
        "            prod_pytorch = sum_pytorch * diff_pytorch\n",
        "            x_squared_pytorch = torch.pow(x_pytorch, 2.0)\n",
        "            sin_y_pytorch = torch.sin(y_pytorch)\n",
        "\n",
        "            inter1_pytorch = prod_pytorch + x_squared_pytorch\n",
        "            z_pytorch = inter1_pytorch - sin_y_pytorch\n",
        "\n",
        "            z_pytorch.backward(torch.ones_like(z_pytorch))\n",
        "\n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Complex Chain 1 - x\")\n",
        "            self.assert_tensors_close(y_custom, y_pytorch, \"Complex Chain 1 - y\")\n",
        "            self.assert_tensors_close(z_custom, z_pytorch, \"Complex Chain 1 - z (result)\", check_grad=False)\n",
        "\n",
        "        # Test 2: Multiple paths to a leaf: z = x*y + x*x + y*z_fixed\n",
        "        with AutogradGraph() as graph:\n",
        "            x_custom = CustomTensor([2.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = CustomTensor([3.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            z_fixed_custom = CustomTensor([0.5])  # No grad\n",
        "\n",
        "            term1_custom = x_custom * y_custom\n",
        "            term2_custom = x_custom * x_custom  # x appears twice\n",
        "            term3_custom = y_custom * z_fixed_custom  # y appears twice, one with no-grad\n",
        "\n",
        "            inter_custom = term1_custom + term2_custom\n",
        "            z_custom = inter_custom + term3_custom\n",
        "            z_custom.backward()\n",
        "\n",
        "            x_pytorch = torch.tensor([2.0], requires_grad=True)\n",
        "            y_pytorch = torch.tensor([3.0], requires_grad=True)\n",
        "            z_fixed_pytorch = torch.tensor([0.5])  # No grad\n",
        "\n",
        "            term1_pytorch = x_pytorch * y_pytorch\n",
        "            term2_pytorch = x_pytorch * x_pytorch\n",
        "            term3_pytorch = y_pytorch * z_fixed_pytorch\n",
        "\n",
        "            inter_pytorch = term1_pytorch + term2_pytorch\n",
        "            z_pytorch = inter_pytorch + term3_pytorch\n",
        "            z_pytorch.backward()\n",
        "\n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Complex Chain 2 (Multiple Paths) - x\")\n",
        "            self.assert_tensors_close(y_custom, y_pytorch, \"Complex Chain 2 (Multiple Paths) - y\")\n",
        "            self.assert_tensors_close(z_custom, z_pytorch, \"Complex Chain 2 (Multiple Paths) - z (result)\", check_grad=False)\n",
        "\n",
        "        # Test 3: Deeper Chain with Mixed Ops: (exp(x) * log(y)) / sqrt(x+y)\n",
        "        with AutogradGraph() as graph:\n",
        "            x_custom = CustomTensor([1.5], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = CustomTensor([2.5], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "\n",
        "            exp_x_custom = x_custom.exp()\n",
        "            log_y_custom = y_custom.log()\n",
        "            numerator_custom = exp_x_custom * log_y_custom\n",
        "\n",
        "            sum_xy_custom = x_custom + y_custom\n",
        "            sqrt_sum_custom = sum_xy_custom.sqrt()\n",
        "\n",
        "            z_custom = numerator_custom / sqrt_sum_custom\n",
        "            z_custom.backward()\n",
        "\n",
        "            x_pytorch = torch.tensor([1.5], requires_grad=True)\n",
        "            y_pytorch = torch.tensor([2.5], requires_grad=True)\n",
        "\n",
        "            exp_x_pytorch = torch.exp(x_pytorch)\n",
        "            log_y_pytorch = torch.log(y_pytorch)\n",
        "            numerator_pytorch = exp_x_pytorch * log_y_pytorch\n",
        "\n",
        "            sum_xy_pytorch = x_pytorch + y_pytorch\n",
        "            sqrt_sum_pytorch = torch.sqrt(sum_xy_pytorch)\n",
        "\n",
        "            z_pytorch = numerator_pytorch / sqrt_sum_pytorch\n",
        "            z_pytorch.backward()\n",
        "\n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Complex Chain 3 (Deeper Mixed Ops) - x\")\n",
        "            self.assert_tensors_close(y_custom, y_pytorch, \"Complex Chain 3 (Deeper Mixed Ops) - y\")\n",
        "            self.assert_tensors_close(z_custom, z_pytorch, \"Complex Chain 3 (Deeper Mixed Ops) - z (result)\", check_grad=False)\n",
        "\n",
        "    def test_mixed_operations(self):\n",
        "        \"\"\"Test mixing operations with and without gradients\"\"\"\n",
        "        print(\"\\n=== Testing Mixed Operations ===\")\n",
        "\n",
        "        # One tensor requires grad, other doesn't (multiplication)\n",
        "        with AutogradGraph() as graph:\n",
        "            x_custom = CustomTensor([2.0, 3.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = CustomTensor([4.0, 5.0])  # No grad\n",
        "            z_custom = x_custom * y_custom\n",
        "            z_custom.backward(torch.ones_like(z_custom.tensor))\n",
        "\n",
        "            x_pytorch = torch.tensor([2.0, 3.0], requires_grad=True)\n",
        "            y_pytorch = torch.tensor([4.0, 5.0])  # No grad\n",
        "            z_pytorch = x_pytorch * y_pytorch\n",
        "            z_pytorch.backward(torch.ones_like(z_pytorch))\n",
        "\n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Mixed Operations (X*Y, Y no grad) - x\")\n",
        "            # Check that y_custom has no grad\n",
        "            self.assert_tensors_close(y_custom, y_pytorch, \"Mixed Operations (X*Y, Y no grad) - y\", check_grad=False)\n",
        "            self.assert_tensors_close(z_custom, z_pytorch, \"Mixed Operations (X*Y, Y no grad) - z (result)\", check_grad=False)\n",
        "\n",
        "        # One tensor requires grad, other doesn't (addition)\n",
        "        with AutogradGraph() as graph:\n",
        "            x_custom = CustomTensor([10.0, 20.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = CustomTensor([1.0, 2.0])  # No grad\n",
        "            z_custom = x_custom + y_custom\n",
        "            z_custom.backward(torch.ones_like(z_custom.tensor))\n",
        "\n",
        "            x_pytorch = torch.tensor([10.0, 20.0], requires_grad=True)\n",
        "            y_pytorch = torch.tensor([1.0, 2.0])  # No grad\n",
        "            z_pytorch = x_pytorch + y_pytorch\n",
        "            z_pytorch.backward(torch.ones_like(z_pytorch))\n",
        "\n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Mixed Operations (X+Y, Y no grad) - x\")\n",
        "            self.assert_tensors_close(y_custom, y_pytorch, \"Mixed Operations (X+Y, Y no grad) - y\", check_grad=False)\n",
        "            self.assert_tensors_close(z_custom, z_pytorch, \"Mixed Operations (X+Y, Y no grad) - z (result)\", check_grad=False)\n",
        "\n",
        "    def test_broadcasting(self):\n",
        "        \"\"\"Test operations with broadcasting\"\"\"\n",
        "        print(\"\\n=== Testing Broadcasting ===\")\n",
        "\n",
        "        # Vector + scalar\n",
        "        with AutogradGraph() as graph:\n",
        "            x_custom = CustomTensor([1.0, 2.0, 3.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = x_custom + 10.0\n",
        "            y_custom.backward(torch.tensor([1.0, 1.0, 1.0]))\n",
        "\n",
        "            x_pytorch = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
        "            y_pytorch = x_pytorch + 10.0\n",
        "            y_pytorch.backward(torch.tensor([1.0, 1.0, 1.0]))\n",
        "\n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Broadcasting: Vector + Scalar - x\")\n",
        "            self.assert_tensors_close(y_custom, y_pytorch, \"Broadcasting: Vector + Scalar - y (result)\", check_grad=False)\n",
        "\n",
        "        # Matrix + vector (row broadcasting)\n",
        "        with AutogradGraph() as graph:\n",
        "            x_custom = CustomTensor([[1.0, 2.0], [3.0, 4.0]], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = CustomTensor([10.0, 20.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            z_custom = x_custom + y_custom  # y broadcasts to rows of x\n",
        "            z_custom.backward(torch.ones_like(z_custom.tensor))\n",
        "\n",
        "            x_pytorch = torch.tensor([[1.0, 2.0], [3.0, 4.0]], requires_grad=True)\n",
        "            y_pytorch = torch.tensor([10.0, 20.0], requires_grad=True)\n",
        "            z_pytorch = x_pytorch + y_pytorch\n",
        "            z_pytorch.backward(torch.ones_like(z_pytorch))\n",
        "\n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Broadcasting: Matrix + Vector (row) - x\")\n",
        "            # For broadcasted operations, the gradient needs to be summed over the broadcasted dimensions\n",
        "            # PyTorch handles this automatically. Your custom backward for add should accumulate.\n",
        "            self.assert_tensors_close(y_custom, y_pytorch, \"Broadcasting: Matrix + Vector (row) - y\")\n",
        "            self.assert_tensors_close(z_custom, z_pytorch, \"Broadcasting: Matrix + Vector (row) - z (result)\", check_grad=False)\n",
        "\n",
        "        # Matrix * scalar\n",
        "        with AutogradGraph() as graph:\n",
        "            x_custom = CustomTensor([[1.0, 2.0], [3.0, 4.0]], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = x_custom * 5.0\n",
        "            y_custom.backward(torch.ones_like(y_custom.tensor))\n",
        "\n",
        "            x_pytorch = torch.tensor([[1.0, 2.0], [3.0, 4.0]], requires_grad=True)\n",
        "            y_pytorch = x_pytorch * 5.0\n",
        "            y_pytorch.backward(torch.ones_like(y_pytorch))\n",
        "\n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Broadcasting: Matrix * Scalar - x\")\n",
        "            self.assert_tensors_close(y_custom, y_pytorch, \"Broadcasting: Matrix * Scalar - y (result)\", check_grad=False)\n",
        "\n",
        "    def test_backward_with_custom_grad(self):\n",
        "        \"\"\"Test backward pass with a custom initial gradient tensor.\"\"\"\n",
        "        print(\"\\n=== Testing Backward with Custom Grad ===\")\n",
        "\n",
        "        with AutogradGraph() as graph:\n",
        "            x_custom = CustomTensor([2.0, 3.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = x_custom * 4.0 + 1.0\n",
        "\n",
        "            custom_grad_output = torch.tensor([0.5, 2.0])\n",
        "            y_custom.backward(custom_grad_output)\n",
        "\n",
        "            x_pytorch = torch.tensor([2.0, 3.0], requires_grad=True)\n",
        "            y_pytorch = x_pytorch * 4.0 + 1.0\n",
        "\n",
        "            pytorch_grad_output = torch.tensor([0.5, 2.0])\n",
        "            y_pytorch.backward(pytorch_grad_output)\n",
        "\n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Backward with Custom Grad - x\")\n",
        "            self.assert_tensors_close(y_custom, y_pytorch, \"Backward with Custom Grad - y (result)\", check_grad=False)\n",
        "\n",
        "    def test_zero_grad_behavior(self):\n",
        "        \"\"\"Test _zero_grad and subsequent backward calls.\"\"\"\n",
        "        print(\"\\n=== Testing Zero Grad Behavior ===\")\n",
        "        with AutogradGraph() as graph:\n",
        "            x_custom = CustomTensor([1.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = x_custom * 2\n",
        "            z_custom = y_custom + 3\n",
        "            z_custom.backward()  # First backward\n",
        "\n",
        "            self.assert_tensors_close(x_custom, torch.tensor([1.0], requires_grad=True), \"Zero Grad Init (first backward) - x\",check_grad=False)\n",
        "\n",
        "            z_custom._zero_grad()  # Manually zero for custom\n",
        "            y_custom._zero_grad()  # Manually zero for custom\n",
        "            x_custom._zero_grad()  # Manually zero for custom leaf\n",
        "\n",
        "            # Do another backward pass\n",
        "            z_custom.backward()  # Should accumulate again from 1.0\n",
        "\n",
        "            x_pytorch = torch.tensor([1.0], requires_grad=True)\n",
        "            y_pytorch = x_pytorch * 2\n",
        "            z_pytorch = y_pytorch + 3\n",
        "            z_pytorch.backward(retain_graph=True)\n",
        "\n",
        "            x_pytorch.grad.zero_()\n",
        "            z_pytorch.backward()  # PyTorch accumulates if not zeroed explicitly\n",
        "\n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Zero Grad Behavior - x (after 2nd backward)\")\n",
        "            self.assert_tensors_close(z_custom, z_pytorch, \"Zero Grad Behavior - z (result, after 2nd backward)\", check_grad=False)\n",
        "\n",
        "    def test_no_grad_flow(self):\n",
        "        \"\"\"Test that gradients do not flow to tensors not requiring grad.\"\"\"\n",
        "        print(\"\\n=== Testing No Grad Flow ===\")\n",
        "        with AutogradGraph() as graph:\n",
        "            x_custom = CustomTensor([5.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = CustomTensor([2.0], _custom_requires_grad=False)  # Does NOT require grad\n",
        "            z_custom = x_custom * y_custom\n",
        "            z_custom.backward()\n",
        "\n",
        "            x_pytorch = torch.tensor([5.0], requires_grad=True)\n",
        "            y_pytorch = torch.tensor([2.0], requires_grad=False)\n",
        "            z_pytorch = x_pytorch * y_pytorch\n",
        "            z_pytorch.backward()\n",
        "\n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"No Grad Flow - x (requires grad)\")\n",
        "            # PyTorch's .grad for non-requiring-grad tensors is None\n",
        "            # Our CustomTensor.tensor.grad for non-requiring-grad should also be None\n",
        "            try:\n",
        "                # Check that y_custom.tensor.grad is None\n",
        "                if y_custom.tensor.grad is not None:\n",
        "                    raise AssertionError(\"Custom non-grad tensor unexpectedly has a gradient.\")\n",
        "                print(f\"✓ No Grad Flow - y (no grad, custom correctly None)\")\n",
        "                self.passed_tests += 1\n",
        "            except Exception as e:\n",
        "                print(f\"✗ No Grad Flow - y (no grad): {str(e)}\")\n",
        "                self.failed_tests += 1\n",
        "\n",
        "    def test_basic_add_scalar_grad_system(self):\n",
        "        print(\"\\n=== System Test: Basic Scalar Add Grad ===\")\n",
        "        try:\n",
        "            with AutogradGraph() as graph:\n",
        "                a = CustomTensor(torch.tensor([2.0, 3.0]), _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "                b = a + 5.0  # (a + 5)\n",
        "                c = b + 10.0  # (a + 5 + 10)\n",
        "\n",
        "                # Manually run backward pass\n",
        "                c.backward(weightage_tensor=1)\n",
        "\n",
        "                # Expected gradients:\n",
        "                # dC/dA = 1.0 (for each element)\n",
        "                assert torch.allclose(a.tensor.grad, torch.tensor([1.0, 1.0]))\n",
        "                assert b.tensor.grad is not None\n",
        "                assert torch.allclose(b.tensor.grad, torch.tensor([1.0, 1.0]))  # dC/dB = 1.0\n",
        "\n",
        "                # Verify graph structure\n",
        "                assert graph.graph.num_nodes() == 3\n",
        "                assert graph.graph.num_edges() == 2\n",
        "                assert graph.graph.has_edge(a._node_id, b._node_id)\n",
        "                assert graph.graph.has_edge(b._node_id, c._node_id)\n",
        "                assert graph.check_cycle() is False\n",
        "            print(\"✓ System Test: Basic Scalar Add Grad\")\n",
        "            self.passed_tests += 1\n",
        "        except Exception as e:\n",
        "            print(f\"✗ System Test: Basic Scalar Add Grad: {str(e)}\")\n",
        "            self.failed_tests += 1\n",
        "\n",
        "    def test_basic_add_tensor_grad_system(self):\n",
        "        print(\"\\n=== System Test: Basic Tensor Add Grad ===\")\n",
        "        try:\n",
        "            with AutogradGraph() as graph:\n",
        "                a = CustomTensor(torch.tensor([2.0, 3.0]), _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "                b = CustomTensor(torch.tensor([1.0, 2.0]), _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "                c = a + b  # (a + b)\n",
        "                d = c + 5.0  # (a + b + 5)\n",
        "\n",
        "                d.backward(weightage_tensor=1)\n",
        "\n",
        "                # Expected gradients:\n",
        "                # dD/dA = 1.0\n",
        "                # dD/dB = 1.0\n",
        "                assert torch.allclose(a.tensor.grad, torch.tensor([1.0, 1.0]))\n",
        "                assert torch.allclose(b.tensor.grad, torch.tensor([1.0, 1.0]))\n",
        "\n",
        "                # Verify graph structure\n",
        "                assert graph.graph.num_nodes() == 4\n",
        "                assert graph.graph.num_edges() == 3\n",
        "                assert graph.graph.has_edge(a._node_id, c._node_id)\n",
        "                assert graph.graph.has_edge(b._node_id, c._node_id)\n",
        "                assert graph.graph.has_edge(c._node_id, d._node_id)\n",
        "                assert graph.check_cycle() is False\n",
        "            print(\"✓ System Test: Basic Tensor Add Grad\")\n",
        "            self.passed_tests += 1\n",
        "        except Exception as e:\n",
        "            print(f\"✗ System Test: Basic Tensor Add Grad: {str(e)}\")\n",
        "            self.failed_tests += 1\n",
        "\n",
        "    def test_mixed_requires_grad_tensor_add_system(self):\n",
        "        print(\"\\n=== System Test: Mixed Requires Grad Tensor Add ===\")\n",
        "        try:\n",
        "            with AutogradGraph() as graph:\n",
        "                a = CustomTensor(torch.tensor([2.0, 3.0]), _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "                b = CustomTensor(torch.tensor([1.0, 2.0]), _custom_requires_grad=False)  # Does not require grad\n",
        "                c = a + b  # c should require grad, b's grad should be None\n",
        "\n",
        "                c.backward(weightage_tensor=1)\n",
        "\n",
        "                assert torch.allclose(a.tensor.grad, torch.tensor([1.0, 1.0]))\n",
        "                assert b.tensor.grad is None  # b should not have a grad\n",
        "                assert c._custom_requires_grad is True\n",
        "\n",
        "                # Verify graph structure\n",
        "                assert graph.graph.num_nodes() == 2  # Only a and c in the graph\n",
        "                assert graph.graph.num_edges() == 1\n",
        "                assert graph.graph.has_node(a._node_id)\n",
        "                assert graph.graph.has_node(c._node_id)\n",
        "                assert graph.graph.has_edge(a._node_id, c._node_id)\n",
        "                # assert not graph.graph.has_node(b._node_id) # b should not be in graph\n",
        "            print(\"✓ System Test: Mixed Requires Grad Tensor Add\")\n",
        "            self.passed_tests += 1\n",
        "        except Exception as e:\n",
        "            print(f\"✗ System Test: Mixed Requires Grad Tensor Add: {str(e)}\")\n",
        "            self.failed_tests += 1\n",
        "\n",
        "    def test_no_requires_grad_system(self):\n",
        "        print(\"\\n=== System Test: No Requires Grad ===\")\n",
        "        try:\n",
        "            with AutogradGraph() as graph:  # Graph created, but no tensors with requires_grad=True added\n",
        "                a = CustomTensor(torch.tensor([1.0]))\n",
        "                b = CustomTensor(torch.tensor([2.0]))\n",
        "                c = a + b\n",
        "                d = c + 3.0\n",
        "\n",
        "                assert not a._custom_requires_grad\n",
        "                assert not b._custom_requires_grad\n",
        "                assert not c._custom_requires_grad\n",
        "                assert not d._custom_requires_grad\n",
        "                assert graph.graph.num_nodes() == 0  # Graph should remain empty\n",
        "                assert graph.graph.num_edges() == 0\n",
        "\n",
        "                with pytest.raises(RuntimeError, match=\"Output tensor does not require grad.\"):\n",
        "                    d.backward(weightage_tensor=1)\n",
        "            print(\"✓ System Test: No Requires Grad\")\n",
        "            self.passed_tests += 1\n",
        "        except Exception as e:\n",
        "            print(f\"✗ System Test: No Requires Grad: {str(e)}\")\n",
        "            self.failed_tests += 1\n",
        "\n",
        "    def test_autograd_graph_context_manager_system(self):\n",
        "        print(\"\\n=== System Test: Autograd Graph Context Manager ===\")\n",
        "        try:\n",
        "            graph = None\n",
        "            with AutogradGraph(check_for_cycles=True, auto_cleanup=True) as g:\n",
        "                graph = g\n",
        "                a = CustomTensor(torch.tensor([1.0]), _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "                b = a + 1.0\n",
        "                assert graph.graph.num_nodes() == 2\n",
        "                assert graph.graph.num_edges() == 1\n",
        "                assert len(graph.intermediate_tensors) == 1  # b should be in intermediate_tensors\n",
        "\n",
        "            # After exiting the context, graph should be empty\n",
        "            assert graph.graph.num_nodes() == 0\n",
        "            assert graph.graph.num_edges() == 0\n",
        "            assert len(graph.intermediate_tensors) == 0\n",
        "            print(\"✓ System Test: Autograd Graph Context Manager\")\n",
        "            self.passed_tests += 1\n",
        "        except Exception as e:\n",
        "            print(f\"✗ System Test: Autograd Graph Context Manager: {str(e)}\")\n",
        "            self.failed_tests += 1\n",
        "\n",
        "    def test_cycle_detection_system(self):\n",
        "        print(\"\\n=== System Test: Cycle Detection ===\")\n",
        "        try:\n",
        "            with pytest.raises(RuntimeError, match=\"Cycle detected in autograd graph.\"):\n",
        "                with AutogradGraph(check_for_cycles=True, auto_cleanup=False) as graph:\n",
        "                    a = CustomTensor(torch.tensor([1.0]), _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "                    b = CustomTensor(torch.tensor([2.0]), _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "\n",
        "                    # Manually create a cycle (a -> b -> a)\n",
        "                    graph.add_edge(a._node_id, b._node_id)\n",
        "                    graph.add_edge(b._node_id, a._node_id)\n",
        "                    graph.check_cycle() # Explicitly check for cycle\n",
        "            print(\"✓ System Test: Cycle Detection\")\n",
        "            self.passed_tests += 1\n",
        "        except Exception as e:\n",
        "            print(f\"✗ System Test: Cycle Detection: {str(e)}\")\n",
        "            self.failed_tests += 1\n",
        "\n",
        "    def test_no_circular_references_non_leaf_tensors_die_system(self):\n",
        "        # This test relies on the garbage collector. It's a heuristic test\n",
        "        # as Python's GC timing is not strictly deterministic.\n",
        "        # However, with weakrefs, it should work for non-leaf tensors.\n",
        "\n",
        "        print(\"\\n--- Starting System Test: No Circular References (Part 1) ---\")\n",
        "        try:\n",
        "            graph_ref = None\n",
        "            output_tensor_weak_ref = None\n",
        "            node_id_d = -1  # To store node_id before d is deleted\n",
        "\n",
        "            # BLOCK 1: Create graph and tensors\n",
        "            with AutogradGraph(auto_cleanup=False) as graph:  # Keep graph for inspection\n",
        "                graph_ref = weakref.ref(graph)\n",
        "                a = CustomTensor(torch.tensor([1.0]), _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "                b = a + 1.0  # Intermediate tensor\n",
        "                c = b + 2.0  # Intermediate tensor\n",
        "                d = c + 3.0  # Output tensor (also intermediate from graph's perspective)\n",
        "\n",
        "                # Store weak reference to 'd' BEFORE its strong reference is potentially removed\n",
        "                output_tensor_weak_ref = weakref.ref(d)\n",
        "                node_id_d = d._node_id  # Store node_id while d is alive\n",
        "\n",
        "                # The ref count for `d` object itself will be high here because it's in `graph.intermediate_tensors`,\n",
        "                # and held by variable `d`, and by the temporary ref in `getrefcount`.\n",
        "                assert len(graph.intermediate_tensors) == 3  # b, c, d should be in intermediate_tensors\n",
        "\n",
        "            # BLOCK 2: After exiting context manager (auto_cleanup=False)\n",
        "            # The 'graph' variable still holds a strong reference to the AutogradGraph instance.\n",
        "            # graph_ref() should return the graph object.\n",
        "            assert graph_ref() is not None, \"Graph object should still be alive.\"\n",
        "            assert len(graph_ref().intermediate_tensors) == 3, \"Intermediate tensors should still be referenced by the graph.\"\n",
        "\n",
        "            # BLOCK 3: Remove strong reference 'd' from local scope\n",
        "            del d  # Remove the local strong reference to the CustomTensor object.\n",
        "            gc.collect()  # Force garbage collection\n",
        "\n",
        "            # Now, output_tensor_weak_ref() *still* shouldn't be None because `graph_ref().intermediate_tensors`\n",
        "            # holds the strong reference.\n",
        "            assert output_tensor_weak_ref() is not None, \"d should still be alive due to intermediate_tensors.\"\n",
        "            current_d_refcount_after_del_d = sys.getrefcount(output_tensor_weak_ref()) if output_tensor_weak_ref() else 'N/A'\n",
        "            assert current_d_refcount_after_del_d == 2, f\"Expected refcount 2, got {current_d_refcount_after_del_d}\"\n",
        "\n",
        "            # BLOCK 4: Remove strong reference from intermediate_tensors\n",
        "            graph_ref().del_non_leaf_tensor_reference(node_id_d)  # THIS IS THE CRUCIAL STEP\n",
        "            gc.collect()  # Force garbage collection again\n",
        "\n",
        "            # Now, with the last strong reference gone, 'd' should be garbage collected.\n",
        "            assert output_tensor_weak_ref() is None, \"Output tensor (non-leaf) should be garbage collected after its strong reference is deleted from intermediate_tensors.\"\n",
        "\n",
        "            # BLOCK 5: Verify other intermediate tensors are collected when graph is cleared\n",
        "            intermediate_tensors_wrefs = []\n",
        "            # Create a new graph and new tensors to avoid interference from previous block\n",
        "            with AutogradGraph(auto_cleanup=False) as graph_new:\n",
        "                a_new = CustomTensor(torch.tensor([1.0]), _custom_requires_grad=True, graph=graph_new, is_leaf=True)\n",
        "                b_new = a_new + 1.0  # Intermediate\n",
        "                c_new = b_new + 2.0  # Intermediate\n",
        "                d_new = c_new + 3.0  # Intermediate (output of a chain)\n",
        "\n",
        "                # Store weak references to the intermediate tensors\n",
        "                intermediate_tensors_wrefs.append(weakref.ref(b_new))\n",
        "                intermediate_tensors_wrefs.append(weakref.ref(c_new))\n",
        "                intermediate_tensors_wrefs.append(weakref.ref(d_new))\n",
        "\n",
        "                # Verify they are initially alive\n",
        "                assert all(wref() is not None for wref in intermediate_tensors_wrefs)\n",
        "                assert len(graph_new.intermediate_tensors) == 3\n",
        "\n",
        "            assert graph_new is not None, \"New graph object should still be alive after 'with' block.\"\n",
        "            assert len(graph_new.intermediate_tensors) == 3, \"New graph intermediate_tensors should still hold refs.\"\n",
        "\n",
        "            # Manually clear the intermediate_tensors dictionary and remove graph reference\n",
        "            graph_new.intermediate_tensors.clear()\n",
        "            del graph_new  # Remove the strong reference to the graph itself\n",
        "            del b_new, c_new, d_new  # deleting the local variable strong references\n",
        "            gc.collect()\n",
        "\n",
        "            # Now, all non-leaf tensors should be garbage collected\n",
        "            for i, wref in enumerate(intermediate_tensors_wrefs):\n",
        "                assert wref() is None, f\"Intermediate tensor {i} should be garbage collected after graph context and intermediate_tensors are cleared.\"\n",
        "            print(\"✓ System Test: No Circular References (Non-leaf tensors die)\")\n",
        "            self.passed_tests += 1\n",
        "        except Exception as e:\n",
        "            print(f\"✗ System Test: No Circular References (Non-leaf tensors die): {str(e)}\")\n",
        "            self.failed_tests += 1\n",
        "\n",
        "    def test_topological_sort_order_system(self):\n",
        "        print(\"\\n=== System Test: Topological Sort Order ===\")\n",
        "        try:\n",
        "            with AutogradGraph() as graph:\n",
        "                t1 = CustomTensor(torch.tensor([1.0]), _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "                t2 = CustomTensor(torch.tensor([2.0]), _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "                t3 = t1 + t2\n",
        "                t4 = t3 + 5.0\n",
        "                t5 = t2 + 10.0  # Another branch\n",
        "                t6 = t4 + t5\n",
        "\n",
        "                # The topological sort should produce an order where dependencies come before their dependents.\n",
        "                # Reversed topological sort should produce an order where outputs come before their inputs.\n",
        "                # Example expected order: t6, t4, t5, t3, t2, t1 (or variations respecting dependencies)\n",
        "                sorted_tensors = graph.reverse_toposort_from_tensor(t6._node_id)\n",
        "\n",
        "\n",
        "                # Check if dependencies are respected in reverse order\n",
        "                # If A -> B, then B should appear before A in reverse topological sort.\n",
        "                # t6 depends on t4, t5. So t6 should be before t4 and t5.\n",
        "                # t4 depends on t3. So t4 should be before t3.\n",
        "                # t5 depends on t2. So t5 should be before t2.\n",
        "                # t3 depends on t1, t2. So t3 should be before t1 and t2.\n",
        "\n",
        "                # Simple check: The first element should be t6 (the ultimate output).\n",
        "                assert sorted_tensors[0].__repr__() == t6.__repr__()\n",
        "\n",
        "                # Check positions:\n",
        "                sorted_tensors=[i.__repr__.__self__ for i in sorted_tensors] #converting the weakref to strongrefs\n",
        "                pos = {t: i for i, t in enumerate(sorted_tensors)}\n",
        "\n",
        "                assert pos[t6] < pos[t4]\n",
        "                assert pos[t6] < pos[t5]\n",
        "                assert pos[t4] < pos[t3]\n",
        "                assert pos[t5] < pos[t2]\n",
        "                assert pos[t3] < pos[t1]\n",
        "                assert pos[t3] < pos[t2]  # t3 also depends on t2\n",
        "\n",
        "                # Additional check: t2 is a dependency for both t3 and t5.\n",
        "                # In reverse topo sort, t3 and t5 must appear before t2.\n",
        "                assert pos[t3] < pos[t2]\n",
        "                assert pos[t5] < pos[t2]\n",
        "\n",
        "                # t1 is only a dependency for t3.\n",
        "                assert pos[t3] < pos[t1]\n",
        "\n",
        "                # Check if all 6 tensors are in the sorted list\n",
        "                assert len(sorted_tensors) == 6\n",
        "                assert set(sorted_tensors) == {t1, t2, t3, t4, t5, t6}\n",
        "                sorted_tensors=None\n",
        "\n",
        "            print(\"✓ System Test: Topological Sort Order\")\n",
        "            self.passed_tests += 1\n",
        "        except Exception as e:\n",
        "            print(f\"✗ System Test: Topological Sort Order: {str(e)}\")\n",
        "            self.failed_tests += 1\n",
        "\n",
        "    def test_very_deep_computation_graph(self):\n",
        "        \"\"\"Test with very deep computation graphs\"\"\"\n",
        "        print(\"\\n=== Testing Very Deep Computation Graph ===\")\n",
        "        \n",
        "        try:\n",
        "            depth = 50  # Moderate depth to avoid stack overflow in testing\n",
        "            \n",
        "            with AutogradGraph() as graph:\n",
        "                x_custom = CustomTensor([1.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "                current_custom = x_custom\n",
        "                \n",
        "                # Create deep chain: x -> x+1 -> (x+1)+1 -> ... (50 times)\n",
        "                for i in range(depth):\n",
        "                    current_custom = current_custom + 1.0\n",
        "                \n",
        "                final_custom = current_custom\n",
        "                final_custom.backward()\n",
        "                \n",
        "            x_pytorch = torch.tensor([1.0], requires_grad=True)\n",
        "            current_pytorch = x_pytorch\n",
        "            \n",
        "            for i in range(depth):\n",
        "                current_pytorch = current_pytorch + 1.0\n",
        "                \n",
        "            final_pytorch = current_pytorch\n",
        "            final_pytorch.backward()\n",
        "            \n",
        "            self.assert_tensors_close(x_custom, x_pytorch, f\"Deep Graph (depth={depth}) - x\")\n",
        "            self.assert_tensors_close(final_custom, final_pytorch, f\"Deep Graph (depth={depth}) - final\", check_grad=False)\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"✗ Very Deep Computation Graph: {str(e)}\")\n",
        "            self.failed_tests += 1\n",
        "\n",
        "    def test_wide_computation_graph(self):\n",
        "        \"\"\"Test with very wide computation graphs (many inputs)\"\"\"\n",
        "        print(\"\\n=== Testing Wide Computation Graph ===\")\n",
        "        \n",
        "        try:\n",
        "            width = 20  # 20 input tensors\n",
        "            \n",
        "            with AutogradGraph() as graph:\n",
        "                # Create many input tensors\n",
        "                inputs_custom = []\n",
        "                for i in range(width):\n",
        "                    inputs_custom.append(\n",
        "                        CustomTensor([float(i + 1)], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "                    )\n",
        "                \n",
        "                # Sum all inputs\n",
        "                result_custom = inputs_custom[0]\n",
        "                for i in range(1, width):\n",
        "                    result_custom = result_custom + inputs_custom[i]\n",
        "                \n",
        "                result_custom.backward()\n",
        "                \n",
        "            # PyTorch equivalent\n",
        "            inputs_pytorch = []\n",
        "            for i in range(width):\n",
        "                inputs_pytorch.append(torch.tensor([float(i + 1)], requires_grad=True))\n",
        "            \n",
        "            result_pytorch = inputs_pytorch[0]\n",
        "            for i in range(1, width):\n",
        "                result_pytorch = result_pytorch + inputs_pytorch[i]\n",
        "                \n",
        "            result_pytorch.backward()\n",
        "            \n",
        "            # Check all gradients\n",
        "            for i in range(width):\n",
        "                self.assert_tensors_close(\n",
        "                    inputs_custom[i], inputs_pytorch[i], \n",
        "                    f\"Wide Graph (width={width}) - input_{i}\"\n",
        "                )\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"✗ Wide Computation Graph: {str(e)}\")\n",
        "            self.failed_tests += 1\n",
        "\n",
        "    def test_nan_and_inf_handling(self):\n",
        "        \"\"\"Test handling of NaN and Inf values\"\"\"\n",
        "        print(\"\\n=== Testing NaN and Inf Handling ===\")\n",
        "        \n",
        "        try:\n",
        "            # Test with NaN input\n",
        "            with AutogradGraph() as graph:\n",
        "                x_custom = CustomTensor([float('nan')], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "                y_custom = x_custom + 1.0\n",
        "                y_custom.backward()\n",
        "                \n",
        "                # Check that gradients handle NaN appropriately\n",
        "                assert torch.isnan(x_custom.tensor.grad).any() or x_custom.tensor.grad is not None\n",
        "                \n",
        "            # Test with Inf input\n",
        "            with AutogradGraph() as graph:\n",
        "                x_custom = CustomTensor([float('inf')], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "                y_custom = x_custom * 2.0\n",
        "                y_custom.backward()\n",
        "                \n",
        "                # Should handle inf appropriately\n",
        "                assert torch.isinf(x_custom.tensor.grad).any() or x_custom.tensor.grad is not None\n",
        "                \n",
        "            print(\"ℹ NaN/Inf Handling - Consider adding explicit handling for edge numerical cases\")\n",
        "            self.passed_tests += 1\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"✗ NaN and Inf Handling: {str(e)}\")\n",
        "            self.failed_tests += 1\n",
        "\n",
        "    def test_zero_gradients(self):\n",
        "        \"\"\"Test operations that should produce zero gradients\"\"\"\n",
        "        print(\"\\n=== Testing Zero Gradients ===\")\n",
        "        \n",
        "        try:\n",
        "            with AutogradGraph() as graph:\n",
        "                x_custom = CustomTensor([2.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "                \n",
        "                # x - x should have zero gradient with respect to x\n",
        "                y_custom = x_custom - x_custom\n",
        "                y_custom.backward()\n",
        "                \n",
        "            x_pytorch = torch.tensor([2.0], requires_grad=True)\n",
        "            y_pytorch = x_pytorch - x_pytorch\n",
        "            y_pytorch.backward()\n",
        "            \n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Zero Gradients - x\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"✗ Zero Gradients: {str(e)}\")\n",
        "            self.failed_tests += 1\n",
        "\n",
        "\n",
        "    def test_memory_efficiency(self):\n",
        "        \"\"\"Test memory efficiency with large computations\"\"\"\n",
        "        print(\"\\n=== Testing Memory Efficiency ===\")\n",
        "        \n",
        "        try:\n",
        "            # Create a computation that could potentially leak memory\n",
        "            initial_tensor_count = len(gc.get_objects())\n",
        "            \n",
        "            for iteration in range(5):\n",
        "                with AutogradGraph() as graph:\n",
        "                    x_custom = CustomTensor([1.0] * 100, _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "                    \n",
        "                    # Chain of operations\n",
        "                    current = x_custom\n",
        "                    for i in range(10):\n",
        "                        current = current + 1.0\n",
        "                        current = current * 1.1\n",
        "                    \n",
        "                    current.backward(torch.ones(100))\n",
        "                    \n",
        "                # Force cleanup\n",
        "                del current, x_custom\n",
        "                gc.collect()\n",
        "            \n",
        "            final_tensor_count = len(gc.get_objects())\n",
        "            \n",
        "            # Memory should not grow excessively\n",
        "            growth = final_tensor_count - initial_tensor_count\n",
        "            print(f\"Object count growth: {growth}\")\n",
        "            \n",
        "            if growth < 1000:  # Reasonable threshold\n",
        "                print(\"✓ Memory Efficiency - Reasonable memory usage\")\n",
        "                self.passed_tests += 1\n",
        "            else:\n",
        "                print(f\"⚠ Memory Efficiency - High memory growth: {growth} objects\")\n",
        "                self.passed_tests += 1  # Still pass but warn\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"✗ Memory Efficiency: {str(e)}\")\n",
        "            self.failed_tests += 1\n",
        "\n",
        "    def run_all_tests(self):\n",
        "        \"\"\"Run all tests\"\"\"\n",
        "        print(\"Running Custom Autograd Correctness Tests\")\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "        self.test_basic_operations()\n",
        "        self.test_multiplication()\n",
        "        self.test_subtraction_division()\n",
        "        self.test_power_function()\n",
        "        self.test_unary_functions()\n",
        "        self.test_matrix_operations()\n",
        "        self.test_complex_chain()\n",
        "        self.test_mixed_operations()\n",
        "        self.test_broadcasting()\n",
        "        self.test_backward_with_custom_grad()\n",
        "        self.test_zero_grad_behavior()\n",
        "        self.test_no_grad_flow()\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 50)\n",
        "        print(\"Running Custom Autograd System Tests\")\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "        self.test_basic_add_scalar_grad_system()\n",
        "        self.test_basic_add_tensor_grad_system()\n",
        "        self.test_mixed_requires_grad_tensor_add_system()\n",
        "        self.test_no_requires_grad_system()\n",
        "        self.test_autograd_graph_context_manager_system()\n",
        "        self.test_cycle_detection_system()\n",
        "        self.test_no_circular_references_non_leaf_tensors_die_system()\n",
        "        self.test_topological_sort_order_system()\n",
        "        self.test_very_deep_computation_graph()\n",
        "        self.test_wide_computation_graph()\n",
        "        self.test_nan_and_inf_handling()\n",
        "        self.test_zero_gradients()\n",
        "        self.test_memory_efficiency()\n",
        "\n",
        "\n",
        "        print(f\"\\n\" + \"=\" * 50)\n",
        "        print(f\"Test Results: {self.passed_tests} passed, {self.failed_tests} failed\")\n",
        "\n",
        "        if self.failed_tests == 0:\n",
        "            print(\"🎉 All tests passed! Your autograd implementation is correct.\")\n",
        "        else:\n",
        "            print(\"❌ Some tests failed. Check the implementation.\")\n",
        "\n",
        "        return self.failed_tests == 0\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running Custom Autograd Correctness Tests\n",
            "==================================================\n",
            "\n",
            "=== Testing Basic Operations ===\n",
            "✓ Scalar Addition - x\n",
            "✓ Scalar Addition - y (result)\n",
            "✓ Tensor Addition - x\n",
            "✓ Tensor Addition - y\n",
            "✓ Tensor Addition - z (result)\n",
            "\n",
            "=== Testing Multiplication ===\n",
            "✓ Scalar Multiplication - x\n",
            "✓ Scalar Multiplication - y (result)\n",
            "✓ Tensor Multiplication - x\n",
            "✓ Tensor Multiplication - y\n",
            "✓ Tensor Multiplication - z (result)\n",
            "\n",
            "=== Testing Subtraction and Division ===\n",
            "✓ Scalar Subtraction (x - C) - x\n",
            "✓ Scalar Subtraction (x - C) - y (result)\n",
            "✓ Scalar Reverse Subtraction (C - x) - x\n",
            "✓ Scalar Reverse Subtraction (C - x) - y (result)\n",
            "✓ Tensor Subtraction - x\n",
            "✓ Tensor Subtraction - y\n",
            "✓ Tensor Subtraction - z (result)\n",
            "✓ Scalar Division - x\n",
            "✓ Scalar Division - y (result)\n",
            "✓ Tensor Division - x\n",
            "✓ Tensir Division - y\n",
            "✓ Tensor Division - z (result)\n",
            "\n",
            "=== Testing Power Function ===\n",
            "✓ Power Function - x\n",
            "✓ Power Function - y (result)\n",
            "✓ Power Function (Negative Exponent) - x\n",
            "✓ Power Function (Negative Exponent) - y (result)\n",
            "\n",
            "=== Testing Unary Functions ===\n",
            "✓ Exponential Function - x\n",
            "✓ Exponential Function - y (result)\n",
            "✓ Logarithm Function - x\n",
            "✓ Logarithm Function - y (result)\n",
            "✓ Sine Function - x\n",
            "✓ Sine Function - y (result)\n",
            "✓ Cosine Function - x\n",
            "✓ Cosine Function - y (result)\n",
            "✓ Square Root Function - x\n",
            "✓ Square Root Function - y (result)\n",
            "\n",
            "=== Testing Matrix Operations ===\n",
            "✓ Matrix Multiplication (2x2 @ 2x2) - x\n",
            "✓ Matrix Multiplication (2x2 @ 2x2) - y\n",
            "✓ Matrix Multiplication (2x2 @ 2x2) - z (result)\n",
            "✓ Matrix Multiplication (2x3 @ 3x2) - x\n",
            "✓ Matrix Multiplication (2x3 @ 3x2) - y\n",
            "✓ Matrix Multiplication (2x3 @ 3x2) - z (result)\n",
            "✓ Dot Product (vector) - x\n",
            "✓ Dot Product (vector) - y\n",
            "✓ Dot Product (vector) - z (result)\n",
            "\n",
            "=== Testing Complex Chains ===\n",
            "✓ Complex Chain 1 - x\n",
            "✓ Complex Chain 1 - y\n",
            "✓ Complex Chain 1 - z (result)\n",
            "✓ Complex Chain 2 (Multiple Paths) - x\n",
            "✓ Complex Chain 2 (Multiple Paths) - y\n",
            "✓ Complex Chain 2 (Multiple Paths) - z (result)\n",
            "✓ Complex Chain 3 (Deeper Mixed Ops) - x\n",
            "✓ Complex Chain 3 (Deeper Mixed Ops) - y\n",
            "✓ Complex Chain 3 (Deeper Mixed Ops) - z (result)\n",
            "\n",
            "=== Testing Mixed Operations ===\n",
            "✓ Mixed Operations (X*Y, Y no grad) - x\n",
            "✓ Mixed Operations (X*Y, Y no grad) - y\n",
            "✓ Mixed Operations (X*Y, Y no grad) - z (result)\n",
            "✓ Mixed Operations (X+Y, Y no grad) - x\n",
            "✓ Mixed Operations (X+Y, Y no grad) - y\n",
            "✓ Mixed Operations (X+Y, Y no grad) - z (result)\n",
            "\n",
            "=== Testing Broadcasting ===\n",
            "✓ Broadcasting: Vector + Scalar - x\n",
            "✓ Broadcasting: Vector + Scalar - y (result)\n",
            "✓ Broadcasting: Matrix + Vector (row) - x\n",
            "✓ Broadcasting: Matrix + Vector (row) - y\n",
            "✓ Broadcasting: Matrix + Vector (row) - z (result)\n",
            "✓ Broadcasting: Matrix * Scalar - x\n",
            "✓ Broadcasting: Matrix * Scalar - y (result)\n",
            "\n",
            "=== Testing Backward with Custom Grad ===\n",
            "✓ Backward with Custom Grad - x\n",
            "✓ Backward with Custom Grad - y (result)\n",
            "\n",
            "=== Testing Zero Grad Behavior ===\n",
            "✓ Zero Grad Init (first backward) - x\n",
            "✓ Zero Grad Behavior - x (after 2nd backward)\n",
            "✓ Zero Grad Behavior - z (result, after 2nd backward)\n",
            "\n",
            "=== Testing No Grad Flow ===\n",
            "✓ No Grad Flow - x (requires grad)\n",
            "✓ No Grad Flow - y (no grad, custom correctly None)\n",
            "\n",
            "==================================================\n",
            "Running Custom Autograd System Tests\n",
            "==================================================\n",
            "\n",
            "=== System Test: Basic Scalar Add Grad ===\n",
            "✓ System Test: Basic Scalar Add Grad\n",
            "\n",
            "=== System Test: Basic Tensor Add Grad ===\n",
            "✓ System Test: Basic Tensor Add Grad\n",
            "\n",
            "=== System Test: Mixed Requires Grad Tensor Add ===\n",
            "✓ System Test: Mixed Requires Grad Tensor Add\n",
            "\n",
            "=== System Test: No Requires Grad ===\n",
            "✓ System Test: No Requires Grad\n",
            "\n",
            "=== System Test: Autograd Graph Context Manager ===\n",
            "✓ System Test: Autograd Graph Context Manager\n",
            "\n",
            "=== System Test: Cycle Detection ===\n",
            "✓ System Test: Cycle Detection\n",
            "\n",
            "--- Starting System Test: No Circular References (Part 1) ---\n",
            "✓ System Test: No Circular References (Non-leaf tensors die)\n",
            "\n",
            "=== System Test: Topological Sort Order ===\n",
            "✓ System Test: Topological Sort Order\n",
            "\n",
            "=== Testing Very Deep Computation Graph ===\n",
            "✓ Deep Graph (depth=50) - x\n",
            "✓ Deep Graph (depth=50) - final\n",
            "\n",
            "=== Testing Wide Computation Graph ===\n",
            "✓ Wide Graph (width=20) - input_0\n",
            "✓ Wide Graph (width=20) - input_1\n",
            "✓ Wide Graph (width=20) - input_2\n",
            "✓ Wide Graph (width=20) - input_3\n",
            "✓ Wide Graph (width=20) - input_4\n",
            "✓ Wide Graph (width=20) - input_5\n",
            "✓ Wide Graph (width=20) - input_6\n",
            "✓ Wide Graph (width=20) - input_7\n",
            "✓ Wide Graph (width=20) - input_8\n",
            "✓ Wide Graph (width=20) - input_9\n",
            "✓ Wide Graph (width=20) - input_10\n",
            "✓ Wide Graph (width=20) - input_11\n",
            "✓ Wide Graph (width=20) - input_12\n",
            "✓ Wide Graph (width=20) - input_13\n",
            "✓ Wide Graph (width=20) - input_14\n",
            "✓ Wide Graph (width=20) - input_15\n",
            "✓ Wide Graph (width=20) - input_16\n",
            "✓ Wide Graph (width=20) - input_17\n",
            "✓ Wide Graph (width=20) - input_18\n",
            "✓ Wide Graph (width=20) - input_19\n",
            "\n",
            "=== Testing NaN and Inf Handling ===\n",
            "ℹ NaN/Inf Handling - Consider adding explicit handling for edge numerical cases\n",
            "\n",
            "=== Testing Zero Gradients ===\n",
            "✓ Zero Gradients - x\n",
            "\n",
            "=== Testing Memory Efficiency ===\n",
            "Object count growth: -12\n",
            "✓ Memory Efficiency - Reasonable memory usage\n",
            "\n",
            "==================================================\n",
            "Test Results: 107 passed, 0 failed\n",
            "🎉 All tests passed! Your autograd implementation is correct.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "t=AutogradTester()\n",
        "t.run_all_tests()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Help on module torch.nn.functional in torch.nn:\n",
            "\n",
            "NAME\n",
            "    torch.nn.functional - Functional interface.\n",
            "\n",
            "FUNCTIONS\n",
            "    adaptive_avg_pool1d(...)\n",
            "        adaptive_avg_pool1d(input, output_size) -> Tensor\n",
            "\n",
            "        Applies a 1D adaptive average pooling over an input signal composed of\n",
            "        several input planes.\n",
            "\n",
            "        See :class:`~torch.nn.AdaptiveAvgPool1d` for details and output shape.\n",
            "\n",
            "        Args:\n",
            "            output_size: the target output size (single integer)\n",
            "\n",
            "    adaptive_avg_pool2d(input: torch.Tensor, output_size: None) -> torch.Tensor\n",
            "        Apply a 2D adaptive average pooling over an input signal composed of several input planes.\n",
            "\n",
            "        See :class:`~torch.nn.AdaptiveAvgPool2d` for details and output shape.\n",
            "\n",
            "        Args:\n",
            "            output_size: the target output size (single integer or\n",
            "                double-integer tuple)\n",
            "\n",
            "    adaptive_avg_pool3d(input: torch.Tensor, output_size: None) -> torch.Tensor\n",
            "        Apply a 3D adaptive average pooling over an input signal composed of several input planes.\n",
            "\n",
            "        See :class:`~torch.nn.AdaptiveAvgPool3d` for details and output shape.\n",
            "\n",
            "        Args:\n",
            "            output_size: the target output size (single integer or\n",
            "                triple-integer tuple)\n",
            "\n",
            "    adaptive_max_pool1d(*args, **kwargs)\n",
            "        adaptive_max_pool1d(input, output_size, return_indices=False)\n",
            "\n",
            "        Applies a 1D adaptive max pooling over an input signal composed of\n",
            "        several input planes.\n",
            "\n",
            "        See :class:`~torch.nn.AdaptiveMaxPool1d` for details and output shape.\n",
            "\n",
            "        Args:\n",
            "            output_size: the target output size (single integer)\n",
            "            return_indices: whether to return pooling indices. Default: ``False``\n",
            "\n",
            "    adaptive_max_pool1d_with_indices(input: torch.Tensor, output_size: None, return_indices: bool = False) -> tuple[torch.Tensor, torch.Tensor]\n",
            "        adaptive_max_pool1d(input, output_size, return_indices=False)\n",
            "\n",
            "        Applies a 1D adaptive max pooling over an input signal composed of\n",
            "        several input planes.\n",
            "\n",
            "        See :class:`~torch.nn.AdaptiveMaxPool1d` for details and output shape.\n",
            "\n",
            "        Args:\n",
            "            output_size: the target output size (single integer)\n",
            "            return_indices: whether to return pooling indices. Default: ``False``\n",
            "\n",
            "    adaptive_max_pool2d(*args, **kwargs)\n",
            "        adaptive_max_pool2d(input, output_size, return_indices=False)\n",
            "\n",
            "        Applies a 2D adaptive max pooling over an input signal composed of\n",
            "        several input planes.\n",
            "\n",
            "        See :class:`~torch.nn.AdaptiveMaxPool2d` for details and output shape.\n",
            "\n",
            "        Args:\n",
            "            output_size: the target output size (single integer or\n",
            "                double-integer tuple)\n",
            "            return_indices: whether to return pooling indices. Default: ``False``\n",
            "\n",
            "    adaptive_max_pool2d_with_indices(input: torch.Tensor, output_size: None, return_indices: bool = False) -> tuple[torch.Tensor, torch.Tensor]\n",
            "        adaptive_max_pool2d(input, output_size, return_indices=False)\n",
            "\n",
            "        Applies a 2D adaptive max pooling over an input signal composed of\n",
            "        several input planes.\n",
            "\n",
            "        See :class:`~torch.nn.AdaptiveMaxPool2d` for details and output shape.\n",
            "\n",
            "        Args:\n",
            "            output_size: the target output size (single integer or\n",
            "                double-integer tuple)\n",
            "            return_indices: whether to return pooling indices. Default: ``False``\n",
            "\n",
            "    adaptive_max_pool3d(*args, **kwargs)\n",
            "        adaptive_max_pool3d(input, output_size, return_indices=False)\n",
            "\n",
            "        Applies a 3D adaptive max pooling over an input signal composed of\n",
            "        several input planes.\n",
            "\n",
            "        See :class:`~torch.nn.AdaptiveMaxPool3d` for details and output shape.\n",
            "\n",
            "        Args:\n",
            "            output_size: the target output size (single integer or\n",
            "                triple-integer tuple)\n",
            "            return_indices: whether to return pooling indices. Default: ``False``\n",
            "\n",
            "    adaptive_max_pool3d_with_indices(input: torch.Tensor, output_size: None, return_indices: bool = False) -> tuple[torch.Tensor, torch.Tensor]\n",
            "        adaptive_max_pool3d(input, output_size, return_indices=False)\n",
            "\n",
            "        Applies a 3D adaptive max pooling over an input signal composed of\n",
            "        several input planes.\n",
            "\n",
            "        See :class:`~torch.nn.AdaptiveMaxPool3d` for details and output shape.\n",
            "\n",
            "        Args:\n",
            "            output_size: the target output size (single integer or\n",
            "                triple-integer tuple)\n",
            "            return_indices: whether to return pooling indices. Default: ``False``\n",
            "\n",
            "    affine_grid(theta: torch.Tensor, size: list[int], align_corners: Optional[bool] = None) -> torch.Tensor\n",
            "        Generate 2D or 3D flow field (sampling grid), given a batch of affine matrices :attr:`theta`.\n",
            "\n",
            "        .. note::\n",
            "            This function is often used in conjunction with :func:`grid_sample`\n",
            "            to build `Spatial Transformer Networks`_ .\n",
            "\n",
            "        Args:\n",
            "            theta (Tensor): input batch of affine matrices with shape\n",
            "                (:math:`N \\times 2 \\times 3`) for 2D or\n",
            "                (:math:`N \\times 3 \\times 4`) for 3D\n",
            "            size (torch.Size): the target output image size.\n",
            "                (:math:`N \\times C \\times H \\times W` for 2D or\n",
            "                :math:`N \\times C \\times D \\times H \\times W` for 3D)\n",
            "                Example: torch.Size((32, 3, 24, 24))\n",
            "            align_corners (bool, optional): if ``True``, consider ``-1`` and ``1``\n",
            "                to refer to the centers of the corner pixels rather than the image corners.\n",
            "                Refer to :func:`grid_sample` for a more complete description.\n",
            "                A grid generated by :func:`affine_grid` should be passed to :func:`grid_sample`\n",
            "                with the same setting for this option.\n",
            "                Default: ``False``\n",
            "\n",
            "        Returns:\n",
            "            output (Tensor): output Tensor of size (:math:`N \\times H \\times W \\times 2`)\n",
            "\n",
            "        .. _`Spatial Transformer Networks`:\n",
            "            https://arxiv.org/abs/1506.02025\n",
            "\n",
            "        .. warning::\n",
            "            When ``align_corners = True``, the grid positions depend on the pixel\n",
            "            size relative to the input image size, and so the locations sampled by\n",
            "            :func:`grid_sample` will differ for the same input given at different\n",
            "            resolutions (that is, after being upsampled or downsampled).\n",
            "            The default behavior up to version 1.2.0 was ``align_corners = True``.\n",
            "            Since then, the default behavior has been changed to ``align_corners = False``,\n",
            "            in order to bring it in line with the default for :func:`interpolate`.\n",
            "        .. warning::\n",
            "            When ``align_corners = True``, 2D affine transforms on 1D data and\n",
            "            3D affine transforms on 2D data (that is, when one of the spatial\n",
            "            dimensions has unit size) are ill-defined, and not an intended use case.\n",
            "            This is not a problem when ``align_corners = False``.\n",
            "            Up to version 1.2.0, all grid points along a unit dimension were\n",
            "            considered arbitrarily to be at ``-1``.\n",
            "            From version 1.3.0, under ``align_corners = True`` all grid points\n",
            "            along a unit dimension are considered to be at ``0``\n",
            "            (the center of the input image).\n",
            "\n",
            "    alpha_dropout(input: torch.Tensor, p: float = 0.5, training: bool = False, inplace: bool = False) -> torch.Tensor\n",
            "        Apply alpha dropout to the input.\n",
            "\n",
            "        See :class:`~torch.nn.AlphaDropout` for details.\n",
            "\n",
            "    assert_int_or_pair(arg: list[int], arg_name: str, message: str) -> None\n",
            "\n",
            "    avg_pool1d(...)\n",
            "        avg_pool1d(input, kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True) -> Tensor\n",
            "\n",
            "        Applies a 1D average pooling over an input signal composed of several\n",
            "        input planes.\n",
            "\n",
            "        See :class:`~torch.nn.AvgPool1d` for details and output shape.\n",
            "\n",
            "        Args:\n",
            "            input: input tensor of shape :math:`(\\text{minibatch} , \\text{in\\_channels} , iW)`\n",
            "            kernel_size: the size of the window. Can be a single number or a\n",
            "              tuple `(kW,)`\n",
            "            stride: the stride of the window. Can be a single number or a tuple\n",
            "              `(sW,)`. Default: :attr:`kernel_size`\n",
            "            padding: implicit zero paddings on both sides of the input. Can be a\n",
            "              single number or a tuple `(padW,)`. Default: 0\n",
            "            ceil_mode: when True, will use `ceil` instead of `floor` to compute the\n",
            "                output shape. Default: ``False``\n",
            "            count_include_pad: when True, will include the zero-padding in the\n",
            "                averaging calculation. Default: ``True``\n",
            "\n",
            "        Examples::\n",
            "\n",
            "            >>> # pool of square window of size=3, stride=2\n",
            "            >>> input = torch.tensor([[[1, 2, 3, 4, 5, 6, 7]]], dtype=torch.float32)\n",
            "            >>> F.avg_pool1d(input, kernel_size=3, stride=2)\n",
            "            tensor([[[ 2.,  4.,  6.]]])\n",
            "\n",
            "    avg_pool2d(...)\n",
            "        avg_pool2d(input, kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True, divisor_override=None) -> Tensor\n",
            "\n",
            "        Applies 2D average-pooling operation in :math:`kH \\times kW` regions by step size\n",
            "        :math:`sH \\times sW` steps. The number of output features is equal to the number of\n",
            "        input planes.\n",
            "\n",
            "        See :class:`~torch.nn.AvgPool2d` for details and output shape.\n",
            "\n",
            "        Args:\n",
            "            input: input tensor :math:`(\\text{minibatch} , \\text{in\\_channels} , iH , iW)`\n",
            "            kernel_size: size of the pooling region. Can be a single number or a\n",
            "              tuple `(kH, kW)`\n",
            "            stride: stride of the pooling operation. Can be a single number or a\n",
            "              tuple `(sH, sW)`. Default: :attr:`kernel_size`\n",
            "            padding: implicit zero paddings on both sides of the input. Can be a\n",
            "              single number or a tuple `(padH, padW)`. Default: 0\n",
            "            ceil_mode: when True, will use `ceil` instead of `floor` in the formula\n",
            "                to compute the output shape. Default: ``False``\n",
            "            count_include_pad: when True, will include the zero-padding in the\n",
            "                averaging calculation. Default: ``True``\n",
            "            divisor_override: if specified, it will be used as divisor, otherwise\n",
            "                 size of the pooling region will be used. Default: None\n",
            "\n",
            "    avg_pool3d(...)\n",
            "        avg_pool3d(input, kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True, divisor_override=None) -> Tensor\n",
            "\n",
            "        Applies 3D average-pooling operation in :math:`kT \\times kH \\times kW` regions by step\n",
            "        size :math:`sT \\times sH \\times sW` steps. The number of output features is equal to\n",
            "        :math:`\\lfloor\\frac{\\text{input planes}}{sT}\\rfloor`.\n",
            "\n",
            "        See :class:`~torch.nn.AvgPool3d` for details and output shape.\n",
            "\n",
            "        Args:\n",
            "            input: input tensor :math:`(\\text{minibatch} , \\text{in\\_channels} , iT \\times iH , iW)`\n",
            "            kernel_size: size of the pooling region. Can be a single number or a\n",
            "              tuple `(kT, kH, kW)`\n",
            "            stride: stride of the pooling operation. Can be a single number or a\n",
            "              tuple `(sT, sH, sW)`. Default: :attr:`kernel_size`\n",
            "            padding: implicit zero paddings on both sides of the input. Can be a\n",
            "              single number or a tuple `(padT, padH, padW)`, Default: 0\n",
            "            ceil_mode: when True, will use `ceil` instead of `floor` in the formula\n",
            "                to compute the output shape\n",
            "            count_include_pad: when True, will include the zero-padding in the\n",
            "                averaging calculation\n",
            "            divisor_override: if specified, it will be used as divisor, otherwise\n",
            "                size of the pooling region will be used. Default: None\n",
            "\n",
            "    batch_norm(input: torch.Tensor, running_mean: Optional[torch.Tensor], running_var: Optional[torch.Tensor], weight: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, training: bool = False, momentum: float = 0.1, eps: float = 1e-05) -> torch.Tensor\n",
            "        Apply Batch Normalization for each channel across a batch of data.\n",
            "\n",
            "        See :class:`~torch.nn.BatchNorm1d`, :class:`~torch.nn.BatchNorm2d`,\n",
            "        :class:`~torch.nn.BatchNorm3d` for details.\n",
            "\n",
            "    bilinear(...)\n",
            "        bilinear(input1, input2, weight, bias=None) -> Tensor\n",
            "\n",
            "        Applies a bilinear transformation to the incoming data:\n",
            "        :math:`y = x_1^T A x_2 + b`\n",
            "\n",
            "        Shape:\n",
            "\n",
            "            - input1: :math:`(N, *, H_{in1})` where :math:`H_{in1}=\\text{in1\\_features}`\n",
            "              and :math:`*` means any number of additional dimensions.\n",
            "              All but the last dimension of the inputs should be the same.\n",
            "            - input2: :math:`(N, *, H_{in2})` where :math:`H_{in2}=\\text{in2\\_features}`\n",
            "            - weight: :math:`(\\text{out\\_features}, \\text{in1\\_features},\n",
            "              \\text{in2\\_features})`\n",
            "            - bias: :math:`(\\text{out\\_features})`\n",
            "            - output: :math:`(N, *, H_{out})` where :math:`H_{out}=\\text{out\\_features}`\n",
            "              and all but the last dimension are the same shape as the input.\n",
            "\n",
            "    binary_cross_entropy(input: torch.Tensor, target: torch.Tensor, weight: Optional[torch.Tensor] = None, size_average: Optional[bool] = None, reduce: Optional[bool] = None, reduction: str = 'mean') -> torch.Tensor\n",
            "        Measure Binary Cross Entropy between the target and input probabilities.\n",
            "\n",
            "        See :class:`~torch.nn.BCELoss` for details.\n",
            "\n",
            "        Args:\n",
            "            input: Tensor of arbitrary shape as probabilities.\n",
            "            target: Tensor of the same shape as input with values between 0 and 1.\n",
            "            weight (Tensor, optional): a manual rescaling weight\n",
            "                    if provided it's repeated to match input tensor shape\n",
            "            size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,\n",
            "                the losses are averaged over each loss element in the batch. Note that for\n",
            "                some losses, there multiple elements per sample. If the field :attr:`size_average`\n",
            "                is set to ``False``, the losses are instead summed for each minibatch. Ignored\n",
            "                when reduce is ``False``. Default: ``True``\n",
            "            reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the\n",
            "                losses are averaged or summed over observations for each minibatch depending\n",
            "                on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per\n",
            "                batch element instead and ignores :attr:`size_average`. Default: ``True``\n",
            "            reduction (str, optional): Specifies the reduction to apply to the output:\n",
            "                ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,\n",
            "                ``'mean'``: the sum of the output will be divided by the number of\n",
            "                elements in the output, ``'sum'``: the output will be summed. Note: :attr:`size_average`\n",
            "                and :attr:`reduce` are in the process of being deprecated, and in the meantime,\n",
            "                specifying either of those two args will override :attr:`reduction`. Default: ``'mean'``\n",
            "\n",
            "        Examples::\n",
            "\n",
            "            >>> input = torch.randn(3, 2, requires_grad=True)\n",
            "            >>> target = torch.rand(3, 2, requires_grad=False)\n",
            "            >>> loss = F.binary_cross_entropy(torch.sigmoid(input), target)\n",
            "            >>> loss.backward()\n",
            "\n",
            "    binary_cross_entropy_with_logits(input: torch.Tensor, target: torch.Tensor, weight: Optional[torch.Tensor] = None, size_average: Optional[bool] = None, reduce: Optional[bool] = None, reduction: str = 'mean', pos_weight: Optional[torch.Tensor] = None) -> torch.Tensor\n",
            "        Calculate Binary Cross Entropy between target and input logits.\n",
            "\n",
            "        See :class:`~torch.nn.BCEWithLogitsLoss` for details.\n",
            "\n",
            "        Args:\n",
            "            input: Tensor of arbitrary shape as unnormalized scores (often referred to as logits).\n",
            "            target: Tensor of the same shape as input with values between 0 and 1\n",
            "            weight (Tensor, optional): a manual rescaling weight\n",
            "                if provided it's repeated to match input tensor shape\n",
            "            size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,\n",
            "                the losses are averaged over each loss element in the batch. Note that for\n",
            "                some losses, there multiple elements per sample. If the field :attr:`size_average`\n",
            "                is set to ``False``, the losses are instead summed for each minibatch. Ignored\n",
            "                when reduce is ``False``. Default: ``True``\n",
            "            reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the\n",
            "                losses are averaged or summed over observations for each minibatch depending\n",
            "                on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per\n",
            "                batch element instead and ignores :attr:`size_average`. Default: ``True``\n",
            "            reduction (str, optional): Specifies the reduction to apply to the output:\n",
            "                ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,\n",
            "                ``'mean'``: the sum of the output will be divided by the number of\n",
            "                elements in the output, ``'sum'``: the output will be summed. Note: :attr:`size_average`\n",
            "                and :attr:`reduce` are in the process of being deprecated, and in the meantime,\n",
            "                specifying either of those two args will override :attr:`reduction`. Default: ``'mean'``\n",
            "            pos_weight (Tensor, optional): a weight of positive examples to be broadcasted with target.\n",
            "                Must be a tensor with equal size along the class dimension to the number of classes.\n",
            "                Pay close attention to PyTorch's broadcasting semantics in order to achieve the desired\n",
            "                operations. For a target of size [B, C, H, W] (where B is batch size) pos_weight of\n",
            "                size [B, C, H, W] will apply different pos_weights to each element of the batch or\n",
            "                [C, H, W] the same pos_weights across the batch. To apply the same positive weight\n",
            "                along all spatial dimensions for a 2D multi-class target [C, H, W] use: [C, 1, 1].\n",
            "                Default: ``None``\n",
            "\n",
            "        Examples::\n",
            "\n",
            "             >>> input = torch.randn(3, requires_grad=True)\n",
            "             >>> target = torch.empty(3).random_(2)\n",
            "             >>> loss = F.binary_cross_entropy_with_logits(input, target)\n",
            "             >>> loss.backward()\n",
            "\n",
            "    celu(input: torch.Tensor, alpha: float = 1.0, inplace: bool = False) -> torch.Tensor\n",
            "        celu(input, alpha=1., inplace=False) -> Tensor\n",
            "\n",
            "        Applies element-wise,\n",
            "        :math:`\\text{CELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x/\\alpha) - 1))`.\n",
            "\n",
            "        See :class:`~torch.nn.CELU` for more details.\n",
            "\n",
            "    celu_(...)\n",
            "        celu_(input, alpha=1.) -> Tensor\n",
            "\n",
            "        In-place version of :func:`~celu`.\n",
            "\n",
            "    channel_shuffle(...)\n",
            "        channel_shuffle(input, groups) -> Tensor\n",
            "\n",
            "        Divide the channels in a tensor of shape :math:`(*, C , H, W)`\n",
            "        into g groups and rearrange them as :math:`(*, C \\frac g, g, H, W)`,\n",
            "        while keeping the original tensor shape.\n",
            "\n",
            "        See :class:`~torch.nn.ChannelShuffle` for details.\n",
            "\n",
            "        Args:\n",
            "            input (Tensor): the input tensor\n",
            "            groups (int): number of groups to divide channels in and rearrange.\n",
            "\n",
            "        Examples::\n",
            "\n",
            "            >>> input = torch.randn(1, 4, 2, 2)\n",
            "            >>> print(input)\n",
            "            [[[[1, 2],\n",
            "               [3, 4]],\n",
            "              [[5, 6],\n",
            "               [7, 8]],\n",
            "              [[9, 10],\n",
            "               [11, 12]],\n",
            "              [[13, 14],\n",
            "               [15, 16]],\n",
            "             ]]\n",
            "            >>> output = torch.nn.functional.channel_shuffle(input, 2)\n",
            "            >>> print(output)\n",
            "            [[[[1, 2],\n",
            "               [3, 4]],\n",
            "              [[9, 10],\n",
            "               [11, 12]],\n",
            "              [[5, 6],\n",
            "               [7, 8]],\n",
            "              [[13, 14],\n",
            "               [15, 16]],\n",
            "             ]]\n",
            "\n",
            "    conv1d(...)\n",
            "        conv1d(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1) -> Tensor\n",
            "\n",
            "        Applies a 1D convolution over an input signal composed of several input\n",
            "        planes.\n",
            "\n",
            "        This operator supports :ref:`TensorFloat32<tf32_on_ampere>`.\n",
            "\n",
            "        See :class:`~torch.nn.Conv1d` for details and output shape.\n",
            "\n",
            "        Note:\n",
            "            In some circumstances when given tensors on a CUDA device and using CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting ``torch.backends.cudnn.deterministic = True``. See :doc:`/notes/randomness` for more information.\n",
            "\n",
            "        Note:\n",
            "            This operator supports complex data types i.e. ``complex32, complex64, complex128``.\n",
            "\n",
            "\n",
            "        Args:\n",
            "            input: input tensor of shape :math:`(\\text{minibatch} , \\text{in\\_channels} , iW)`\n",
            "            weight: filters of shape :math:`(\\text{out\\_channels} , \\frac{\\text{in\\_channels}}{\\text{groups}} , kW)`\n",
            "            bias: optional bias of shape :math:`(\\text{out\\_channels})`. Default: ``None``\n",
            "            stride: the stride of the convolving kernel. Can be a single number or\n",
            "              a one-element tuple `(sW,)`. Default: 1\n",
            "            padding: implicit paddings on both sides of the input. Can be a string {'valid', 'same'},\n",
            "              single number or a one-element tuple `(padW,)`. Default: 0\n",
            "              ``padding='valid'`` is the same as no padding. ``padding='same'`` pads\n",
            "              the input so the output has the same shape as the input. However, this mode\n",
            "              doesn't support any stride values other than 1.\n",
            "\n",
            "              .. warning::\n",
            "                  For ``padding='same'``, if the ``weight`` is even-length and\n",
            "                  ``dilation`` is odd in any dimension, a full :func:`pad` operation\n",
            "                  may be needed internally. Lowering performance.\n",
            "            dilation: the spacing between kernel elements. Can be a single number or\n",
            "              a one-element tuple `(dW,)`. Default: 1\n",
            "            groups: split input into groups, :math:`\\text{in\\_channels}` should be divisible by\n",
            "              the number of groups. Default: 1\n",
            "\n",
            "        Examples::\n",
            "\n",
            "            >>> inputs = torch.randn(33, 16, 30)\n",
            "            >>> filters = torch.randn(20, 16, 5)\n",
            "            >>> F.conv1d(inputs, filters)\n",
            "\n",
            "    conv2d(...)\n",
            "        conv2d(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1) -> Tensor\n",
            "\n",
            "        Applies a 2D convolution over an input image composed of several input\n",
            "        planes.\n",
            "\n",
            "        This operator supports :ref:`TensorFloat32<tf32_on_ampere>`.\n",
            "\n",
            "        See :class:`~torch.nn.Conv2d` for details and output shape.\n",
            "\n",
            "        Note:\n",
            "            In some circumstances when given tensors on a CUDA device and using CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting ``torch.backends.cudnn.deterministic = True``. See :doc:`/notes/randomness` for more information.\n",
            "\n",
            "        Note:\n",
            "            This operator supports complex data types i.e. ``complex32, complex64, complex128``.\n",
            "\n",
            "\n",
            "        Args:\n",
            "            input: input tensor of shape :math:`(\\text{minibatch} , \\text{in\\_channels} , iH , iW)`\n",
            "            weight: filters of shape :math:`(\\text{out\\_channels} , \\frac{\\text{in\\_channels}}{\\text{groups}} , kH , kW)`\n",
            "            bias: optional bias tensor of shape :math:`(\\text{out\\_channels})`. Default: ``None``\n",
            "            stride: the stride of the convolving kernel. Can be a single number or a\n",
            "              tuple `(sH, sW)`. Default: 1\n",
            "            padding: implicit paddings on both sides of the input. Can be a string {'valid', 'same'},\n",
            "              single number or a tuple `(padH, padW)`. Default: 0\n",
            "              ``padding='valid'`` is the same as no padding. ``padding='same'`` pads\n",
            "              the input so the output has the same shape as the input. However, this mode\n",
            "              doesn't support any stride values other than 1.\n",
            "\n",
            "              .. warning::\n",
            "                  For ``padding='same'``, if the ``weight`` is even-length and\n",
            "                  ``dilation`` is odd in any dimension, a full :func:`pad` operation\n",
            "                  may be needed internally. Lowering performance.\n",
            "\n",
            "            dilation: the spacing between kernel elements. Can be a single number or\n",
            "              a tuple `(dH, dW)`. Default: 1\n",
            "            groups: split input into groups, both :math:`\\text{in\\_channels}` and :math:`\\text{out\\_channels}`\n",
            "              should be divisible by the number of groups. Default: 1\n",
            "\n",
            "        Examples::\n",
            "\n",
            "            >>> # With square kernels and equal stride\n",
            "            >>> filters = torch.randn(8, 4, 3, 3)\n",
            "            >>> inputs = torch.randn(1, 4, 5, 5)\n",
            "            >>> F.conv2d(inputs, filters, padding=1)\n",
            "\n",
            "    conv3d(...)\n",
            "        conv3d(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1) -> Tensor\n",
            "\n",
            "        Applies a 3D convolution over an input image composed of several input\n",
            "        planes.\n",
            "\n",
            "        This operator supports :ref:`TensorFloat32<tf32_on_ampere>`.\n",
            "\n",
            "        See :class:`~torch.nn.Conv3d` for details and output shape.\n",
            "\n",
            "        Note:\n",
            "            In some circumstances when given tensors on a CUDA device and using CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting ``torch.backends.cudnn.deterministic = True``. See :doc:`/notes/randomness` for more information.\n",
            "\n",
            "        Note:\n",
            "            This operator supports complex data types i.e. ``complex32, complex64, complex128``.\n",
            "\n",
            "\n",
            "        Args:\n",
            "            input: input tensor of shape :math:`(\\text{minibatch} , \\text{in\\_channels} , iT , iH , iW)`\n",
            "            weight: filters of shape :math:`(\\text{out\\_channels} , \\frac{\\text{in\\_channels}}{\\text{groups}} , kT , kH , kW)`\n",
            "            bias: optional bias tensor of shape :math:`(\\text{out\\_channels})`. Default: None\n",
            "            stride: the stride of the convolving kernel. Can be a single number or a\n",
            "              tuple `(sT, sH, sW)`. Default: 1\n",
            "            padding: implicit paddings on both sides of the input. Can be a string {'valid', 'same'},\n",
            "              single number or a tuple `(padT, padH, padW)`. Default: 0\n",
            "              ``padding='valid'`` is the same as no padding. ``padding='same'`` pads\n",
            "              the input so the output has the same shape as the input. However, this mode\n",
            "              doesn't support any stride values other than 1.\n",
            "\n",
            "              .. warning::\n",
            "                  For ``padding='same'``, if the ``weight`` is even-length and\n",
            "                  ``dilation`` is odd in any dimension, a full :func:`pad` operation\n",
            "                  may be needed internally. Lowering performance.\n",
            "\n",
            "            dilation: the spacing between kernel elements. Can be a single number or\n",
            "              a tuple `(dT, dH, dW)`. Default: 1\n",
            "            groups: split input into groups, :math:`\\text{in\\_channels}` should be divisible by\n",
            "              the number of groups. Default: 1\n",
            "\n",
            "        Examples::\n",
            "\n",
            "            >>> filters = torch.randn(33, 16, 3, 3, 3)\n",
            "            >>> inputs = torch.randn(20, 16, 50, 10, 20)\n",
            "            >>> F.conv3d(inputs, filters)\n",
            "\n",
            "    conv_tbc(...)\n",
            "        Applies a 1-dimensional sequence convolution over an input sequence.\n",
            "        Input and output dimensions are (Time, Batch, Channels) - hence TBC.\n",
            "\n",
            "        Args:\n",
            "            input: input tensor of shape :math:`(\\text{sequence length} \\times batch \\times \\text{in\\_channels})`\n",
            "            weight: filter of shape (:math:`\\text{kernel width} \\times \\text{in\\_channels} \\times \\text{out\\_channels}`)\n",
            "            bias: bias of shape (:math:`\\text{out\\_channels}`)\n",
            "            pad: number of timesteps to pad. Default: 0\n",
            "\n",
            "    conv_transpose1d(...)\n",
            "        conv_transpose1d(input, weight, bias=None, stride=1, padding=0, output_padding=0, groups=1, dilation=1) -> Tensor\n",
            "\n",
            "        Applies a 1D transposed convolution operator over an input signal\n",
            "        composed of several input planes, sometimes also called \"deconvolution\".\n",
            "\n",
            "        This operator supports :ref:`TensorFloat32<tf32_on_ampere>`.\n",
            "\n",
            "        See :class:`~torch.nn.ConvTranspose1d` for details and output shape.\n",
            "\n",
            "        Note:\n",
            "            In some circumstances when given tensors on a CUDA device and using CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting ``torch.backends.cudnn.deterministic = True``. See :doc:`/notes/randomness` for more information.\n",
            "\n",
            "\n",
            "        Args:\n",
            "            input: input tensor of shape :math:`(\\text{minibatch} , \\text{in\\_channels} , iW)`\n",
            "            weight: filters of shape :math:`(\\text{in\\_channels} , \\frac{\\text{out\\_channels}}{\\text{groups}} , kW)`\n",
            "            bias: optional bias of shape :math:`(\\text{out\\_channels})`. Default: None\n",
            "            stride: the stride of the convolving kernel. Can be a single number or a\n",
            "              tuple ``(sW,)``. Default: 1\n",
            "            padding: ``dilation * (kernel_size - 1) - padding`` zero-padding will be added to both\n",
            "              sides of each dimension in the input. Can be a single number or a tuple\n",
            "              ``(padW,)``. Default: 0\n",
            "            output_padding: additional size added to one side of each dimension in the\n",
            "              output shape. Can be a single number or a tuple ``(out_padW)``. Default: 0\n",
            "            groups: split input into groups, :math:`\\text{in\\_channels}` should be divisible by the\n",
            "              number of groups. Default: 1\n",
            "            dilation: the spacing between kernel elements. Can be a single number or\n",
            "              a tuple ``(dW,)``. Default: 1\n",
            "\n",
            "        Examples::\n",
            "\n",
            "            >>> inputs = torch.randn(20, 16, 50)\n",
            "            >>> weights = torch.randn(16, 33, 5)\n",
            "            >>> F.conv_transpose1d(inputs, weights)\n",
            "\n",
            "    conv_transpose2d(...)\n",
            "        conv_transpose2d(input, weight, bias=None, stride=1, padding=0, output_padding=0, groups=1, dilation=1) -> Tensor\n",
            "\n",
            "        Applies a 2D transposed convolution operator over an input image\n",
            "        composed of several input planes, sometimes also called \"deconvolution\".\n",
            "\n",
            "        This operator supports :ref:`TensorFloat32<tf32_on_ampere>`.\n",
            "\n",
            "        See :class:`~torch.nn.ConvTranspose2d` for details and output shape.\n",
            "\n",
            "        Note:\n",
            "            In some circumstances when given tensors on a CUDA device and using CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting ``torch.backends.cudnn.deterministic = True``. See :doc:`/notes/randomness` for more information.\n",
            "\n",
            "\n",
            "        Args:\n",
            "            input: input tensor of shape :math:`(\\text{minibatch} , \\text{in\\_channels} , iH , iW)`\n",
            "            weight: filters of shape :math:`(\\text{in\\_channels} , \\frac{\\text{out\\_channels}}{\\text{groups}} , kH , kW)`\n",
            "            bias: optional bias of shape :math:`(\\text{out\\_channels})`. Default: None\n",
            "            stride: the stride of the convolving kernel. Can be a single number or a\n",
            "              tuple ``(sH, sW)``. Default: 1\n",
            "            padding: ``dilation * (kernel_size - 1) - padding`` zero-padding will be added to both\n",
            "              sides of each dimension in the input. Can be a single number or a tuple\n",
            "              ``(padH, padW)``. Default: 0\n",
            "            output_padding: additional size added to one side of each dimension in the\n",
            "              output shape. Can be a single number or a tuple ``(out_padH, out_padW)``.\n",
            "              Default: 0\n",
            "            groups: split input into groups, :math:`\\text{in\\_channels}` should be divisible by the\n",
            "              number of groups. Default: 1\n",
            "            dilation: the spacing between kernel elements. Can be a single number or\n",
            "              a tuple ``(dH, dW)``. Default: 1\n",
            "\n",
            "        Examples::\n",
            "\n",
            "            >>> # With square kernels and equal stride\n",
            "            >>> inputs = torch.randn(1, 4, 5, 5)\n",
            "            >>> weights = torch.randn(4, 8, 3, 3)\n",
            "            >>> F.conv_transpose2d(inputs, weights, padding=1)\n",
            "\n",
            "    conv_transpose3d(...)\n",
            "        conv_transpose3d(input, weight, bias=None, stride=1, padding=0, output_padding=0, groups=1, dilation=1) -> Tensor\n",
            "\n",
            "        Applies a 3D transposed convolution operator over an input image\n",
            "        composed of several input planes, sometimes also called \"deconvolution\"\n",
            "\n",
            "        This operator supports :ref:`TensorFloat32<tf32_on_ampere>`.\n",
            "\n",
            "        See :class:`~torch.nn.ConvTranspose3d` for details and output shape.\n",
            "\n",
            "        Note:\n",
            "            In some circumstances when given tensors on a CUDA device and using CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting ``torch.backends.cudnn.deterministic = True``. See :doc:`/notes/randomness` for more information.\n",
            "\n",
            "\n",
            "        Args:\n",
            "            input: input tensor of shape :math:`(\\text{minibatch} , \\text{in\\_channels} , iT , iH , iW)`\n",
            "            weight: filters of shape :math:`(\\text{in\\_channels} , \\frac{\\text{out\\_channels}}{\\text{groups}} , kT , kH , kW)`\n",
            "            bias: optional bias of shape :math:`(\\text{out\\_channels})`. Default: None\n",
            "            stride: the stride of the convolving kernel. Can be a single number or a\n",
            "              tuple ``(sT, sH, sW)``. Default: 1\n",
            "            padding: ``dilation * (kernel_size - 1) - padding`` zero-padding will be added to both\n",
            "              sides of each dimension in the input. Can be a single number or a tuple\n",
            "              ``(padT, padH, padW)``. Default: 0\n",
            "            output_padding: additional size added to one side of each dimension in the\n",
            "              output shape. Can be a single number or a tuple\n",
            "              ``(out_padT, out_padH, out_padW)``. Default: 0\n",
            "            groups: split input into groups, :math:`\\text{in\\_channels}` should be divisible by the\n",
            "              number of groups. Default: 1\n",
            "            dilation: the spacing between kernel elements. Can be a single number or\n",
            "              a tuple `(dT, dH, dW)`. Default: 1\n",
            "\n",
            "        Examples::\n",
            "\n",
            "            >>> inputs = torch.randn(20, 16, 50, 10, 20)\n",
            "            >>> weights = torch.randn(16, 33, 3, 3, 3)\n",
            "            >>> F.conv_transpose3d(inputs, weights)\n",
            "\n",
            "    cosine_embedding_loss(input1: torch.Tensor, input2: torch.Tensor, target: torch.Tensor, margin: float = 0, size_average: Optional[bool] = None, reduce: Optional[bool] = None, reduction: str = 'mean') -> torch.Tensor\n",
            "        cosine_embedding_loss(input1, input2, target, margin=0, size_average=None, reduce=None, reduction='mean') -> Tensor\n",
            "\n",
            "        See :class:`~torch.nn.CosineEmbeddingLoss` for details.\n",
            "\n",
            "    cosine_similarity(...)\n",
            "        cosine_similarity(x1, x2, dim=1, eps=1e-8) -> Tensor\n",
            "\n",
            "        Returns cosine similarity between ``x1`` and ``x2``, computed along dim. ``x1`` and ``x2`` must be broadcastable\n",
            "        to a common shape. ``dim`` refers to the dimension in this common shape. Dimension ``dim`` of the output is\n",
            "        squeezed (see :func:`torch.squeeze`), resulting in the\n",
            "        output tensor having 1 fewer dimension.\n",
            "\n",
            "        .. math ::\n",
            "            \\text{similarity} = \\dfrac{x_1 \\cdot x_2}{\\max(\\Vert x_1 \\Vert _2, \\epsilon) \\cdot \\max(\\Vert x_2 \\Vert _2, \\epsilon)}\n",
            "\n",
            "        Supports :ref:`type promotion <type-promotion-doc>`.\n",
            "\n",
            "        Args:\n",
            "            x1 (Tensor): First input.\n",
            "            x2 (Tensor): Second input.\n",
            "            dim (int, optional): Dimension along which cosine similarity is computed. Default: 1\n",
            "            eps (float, optional): Small value to avoid division by zero.\n",
            "                Default: 1e-8\n",
            "\n",
            "        Example::\n",
            "\n",
            "            >>> input1 = torch.randn(100, 128)\n",
            "            >>> input2 = torch.randn(100, 128)\n",
            "            >>> output = F.cosine_similarity(input1, input2)\n",
            "            >>> print(output)\n",
            "\n",
            "    cross_entropy(input: torch.Tensor, target: torch.Tensor, weight: Optional[torch.Tensor] = None, size_average: Optional[bool] = None, ignore_index: int = -100, reduce: Optional[bool] = None, reduction: str = 'mean', label_smoothing: float = 0.0) -> torch.Tensor\n",
            "        Compute the cross entropy loss between input logits and target.\n",
            "\n",
            "        See :class:`~torch.nn.CrossEntropyLoss` for details.\n",
            "\n",
            "        Args:\n",
            "            input (Tensor) : Predicted unnormalized logits;\n",
            "                see Shape section below for supported shapes.\n",
            "            target (Tensor) : Ground truth class indices or class probabilities;\n",
            "                see Shape section below for supported shapes.\n",
            "            weight (Tensor, optional): a manual rescaling weight given to each\n",
            "                class. If given, has to be a Tensor of size `C`\n",
            "            size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,\n",
            "                the losses are averaged over each loss element in the batch. Note that for\n",
            "                some losses, there multiple elements per sample. If the field :attr:`size_average`\n",
            "                is set to ``False``, the losses are instead summed for each minibatch. Ignored\n",
            "                when reduce is ``False``. Default: ``True``\n",
            "            ignore_index (int, optional): Specifies a target value that is ignored\n",
            "                and does not contribute to the input gradient. When :attr:`size_average` is\n",
            "                ``True``, the loss is averaged over non-ignored targets. Note that\n",
            "                :attr:`ignore_index` is only applicable when the target contains class indices.\n",
            "                Default: -100\n",
            "            reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the\n",
            "                losses are averaged or summed over observations for each minibatch depending\n",
            "                on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per\n",
            "                batch element instead and ignores :attr:`size_average`. Default: ``True``\n",
            "            reduction (str, optional): Specifies the reduction to apply to the output:\n",
            "                ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,\n",
            "                ``'mean'``: the sum of the output will be divided by the number of\n",
            "                elements in the output, ``'sum'``: the output will be summed. Note: :attr:`size_average`\n",
            "                and :attr:`reduce` are in the process of being deprecated, and in the meantime,\n",
            "                specifying either of those two args will override :attr:`reduction`. Default: ``'mean'``\n",
            "            label_smoothing (float, optional): A float in [0.0, 1.0]. Specifies the amount\n",
            "                of smoothing when computing the loss, where 0.0 means no smoothing. The targets\n",
            "                become a mixture of the original ground truth and a uniform distribution as described in\n",
            "                `Rethinking the Inception Architecture for Computer Vision <https://arxiv.org/abs/1512.00567>`__. Default: :math:`0.0`.\n",
            "\n",
            "        Shape:\n",
            "            - Input: Shape :math:`(C)`, :math:`(N, C)` or :math:`(N, C, d_1, d_2, ..., d_K)` with :math:`K \\geq 1`\n",
            "              in the case of `K`-dimensional loss.\n",
            "            - Target: If containing class indices, shape :math:`()`, :math:`(N)` or :math:`(N, d_1, d_2, ..., d_K)` with\n",
            "              :math:`K \\geq 1` in the case of K-dimensional loss where each value should be between :math:`[0, C)`.\n",
            "              If containing class probabilities, same shape as the input and each value should be between :math:`[0, 1]`.\n",
            "\n",
            "            where:\n",
            "\n",
            "            .. math::\n",
            "                \\begin{aligned}\n",
            "                    C ={} & \\text{number of classes} \\\\\n",
            "                    N ={} & \\text{batch size} \\\\\n",
            "                \\end{aligned}\n",
            "\n",
            "        Examples::\n",
            "\n",
            "            >>> # Example of target with class indices\n",
            "            >>> input = torch.randn(3, 5, requires_grad=True)\n",
            "            >>> target = torch.randint(5, (3,), dtype=torch.int64)\n",
            "            >>> loss = F.cross_entropy(input, target)\n",
            "            >>> loss.backward()\n",
            "            >>>\n",
            "            >>> # Example of target with class probabilities\n",
            "            >>> input = torch.randn(3, 5, requires_grad=True)\n",
            "            >>> target = torch.randn(3, 5).softmax(dim=1)\n",
            "            >>> loss = F.cross_entropy(input, target)\n",
            "            >>> loss.backward()\n",
            "\n",
            "    ctc_loss(log_probs: torch.Tensor, targets: torch.Tensor, input_lengths: torch.Tensor, target_lengths: torch.Tensor, blank: int = 0, reduction: str = 'mean', zero_infinity: bool = False) -> torch.Tensor\n",
            "        Apply the Connectionist Temporal Classification loss.\n",
            "\n",
            "        See :class:`~torch.nn.CTCLoss` for details.\n",
            "\n",
            "        Note:\n",
            "            In some circumstances when given tensors on a CUDA device and using CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting ``torch.backends.cudnn.deterministic = True``. See :doc:`/notes/randomness` for more information.\n",
            "\n",
            "        Note:\n",
            "            This operation may produce nondeterministic gradients when given tensors on a CUDA device. See :doc:`/notes/randomness` for more information.\n",
            "\n",
            "        Args:\n",
            "            log_probs: :math:`(T, N, C)` or :math:`(T, C)` where `C = number of characters in alphabet including blank`,\n",
            "                `T = input length`, and `N = batch size`.\n",
            "                The logarithmized probabilities of the outputs\n",
            "                (e.g. obtained with :func:`torch.nn.functional.log_softmax`).\n",
            "            targets: :math:`(N, S)` or `(sum(target_lengths))`.\n",
            "                Targets cannot be blank. In the second form, the targets are assumed to be concatenated.\n",
            "            input_lengths: :math:`(N)` or :math:`()`.\n",
            "                Lengths of the inputs (must each be :math:`\\leq T`)\n",
            "            target_lengths: :math:`(N)` or :math:`()`.\n",
            "                Lengths of the targets\n",
            "            blank (int, optional):\n",
            "                Blank label. Default :math:`0`.\n",
            "            reduction (str, optional): Specifies the reduction to apply to the output:\n",
            "                ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,\n",
            "                ``'mean'``: the output losses will be divided by the target lengths and\n",
            "                then the mean over the batch is taken, ``'sum'``: the output will be\n",
            "                summed. Default: ``'mean'``\n",
            "            zero_infinity (bool, optional):\n",
            "                Whether to zero infinite losses and the associated gradients.\n",
            "                Default: ``False``\n",
            "                Infinite losses mainly occur when the inputs are too short\n",
            "                to be aligned to the targets.\n",
            "\n",
            "        Example::\n",
            "\n",
            "            >>> log_probs = torch.randn(50, 16, 20).log_softmax(2).detach().requires_grad_()\n",
            "            >>> targets = torch.randint(1, 20, (16, 30), dtype=torch.long)\n",
            "            >>> input_lengths = torch.full((16,), 50, dtype=torch.long)\n",
            "            >>> target_lengths = torch.randint(10, 30, (16,), dtype=torch.long)\n",
            "            >>> loss = F.ctc_loss(log_probs, targets, input_lengths, target_lengths)\n",
            "            >>> loss.backward()\n",
            "\n",
            "    dropout(input: torch.Tensor, p: float = 0.5, training: bool = True, inplace: bool = False) -> torch.Tensor\n",
            "        During training, randomly zeroes some elements of the input tensor with probability :attr:`p`.\n",
            "\n",
            "        Uses samples from a Bernoulli distribution.\n",
            "\n",
            "        See :class:`~torch.nn.Dropout` for details.\n",
            "\n",
            "        Args:\n",
            "            p: probability of an element to be zeroed. Default: 0.5\n",
            "            training: apply dropout if is ``True``. Default: ``True``\n",
            "            inplace: If set to ``True``, will do this operation in-place. Default: ``False``\n",
            "\n",
            "    dropout1d(input: torch.Tensor, p: float = 0.5, training: bool = True, inplace: bool = False) -> torch.Tensor\n",
            "        Randomly zero out entire channels (a channel is a 1D feature map).\n",
            "\n",
            "        For example, the :math:`j`-th channel of the :math:`i`-th sample in the\n",
            "        batched input is a 1D tensor :math:`\\text{input}[i, j]` of the input tensor.\n",
            "        Each channel will be zeroed out independently on every forward call with\n",
            "        probability :attr:`p` using samples from a Bernoulli distribution.\n",
            "\n",
            "        See :class:`~torch.nn.Dropout1d` for details.\n",
            "\n",
            "        Args:\n",
            "            p: probability of a channel to be zeroed. Default: 0.5\n",
            "            training: apply dropout if is ``True``. Default: ``True``\n",
            "            inplace: If set to ``True``, will do this operation in-place. Default: ``False``\n",
            "\n",
            "    dropout2d(input: torch.Tensor, p: float = 0.5, training: bool = True, inplace: bool = False) -> torch.Tensor\n",
            "        Randomly zero out entire channels (a channel is a 2D feature map).\n",
            "\n",
            "        For example, the :math:`j`-th channel of the :math:`i`-th sample in the\n",
            "        batched input is a 2D tensor :math:`\\text{input}[i, j]` of the input tensor.\n",
            "        Each channel will be zeroed out independently on every forward call with\n",
            "        probability :attr:`p` using samples from a Bernoulli distribution.\n",
            "\n",
            "        See :class:`~torch.nn.Dropout2d` for details.\n",
            "\n",
            "        Args:\n",
            "            p: probability of a channel to be zeroed. Default: 0.5\n",
            "            training: apply dropout if is ``True``. Default: ``True``\n",
            "            inplace: If set to ``True``, will do this operation in-place. Default: ``False``\n",
            "\n",
            "    dropout3d(input: torch.Tensor, p: float = 0.5, training: bool = True, inplace: bool = False) -> torch.Tensor\n",
            "        Randomly zero out entire channels (a channel is a 3D feature map).\n",
            "\n",
            "        For example, the :math:`j`-th channel of the :math:`i`-th sample in the\n",
            "        batched input is a 3D tensor :math:`\\text{input}[i, j]` of the input tensor.\n",
            "        Each channel will be zeroed out independently on every forward call with\n",
            "        probability :attr:`p` using samples from a Bernoulli distribution.\n",
            "\n",
            "        See :class:`~torch.nn.Dropout3d` for details.\n",
            "\n",
            "        Args:\n",
            "            p: probability of a channel to be zeroed. Default: 0.5\n",
            "            training: apply dropout if is ``True``. Default: ``True``\n",
            "            inplace: If set to ``True``, will do this operation in-place. Default: ``False``\n",
            "\n",
            "    elu(input: torch.Tensor, alpha: float = 1.0, inplace: bool = False) -> torch.Tensor\n",
            "        Apply the Exponential Linear Unit (ELU) function element-wise.\n",
            "\n",
            "        See :class:`~torch.nn.ELU` for more details.\n",
            "\n",
            "    elu_(...)\n",
            "        elu_(input, alpha=1.) -> Tensor\n",
            "\n",
            "        In-place version of :func:`~elu`.\n",
            "\n",
            "    embedding(input: torch.Tensor, weight: torch.Tensor, padding_idx: Optional[int] = None, max_norm: Optional[float] = None, norm_type: float = 2.0, scale_grad_by_freq: bool = False, sparse: bool = False) -> torch.Tensor\n",
            "        Generate a simple lookup table that looks up embeddings in a fixed dictionary and size.\n",
            "\n",
            "        This module is often used to retrieve word embeddings using indices.\n",
            "        The input to the module is a list of indices, and the embedding matrix,\n",
            "        and the output is the corresponding word embeddings.\n",
            "\n",
            "        See :class:`torch.nn.Embedding` for more details.\n",
            "\n",
            "        .. note::\n",
            "            Note that the analytical gradients of this function with respect to\n",
            "            entries in :attr:`weight` at the row specified by :attr:`padding_idx`\n",
            "            are expected to differ from the numerical ones.\n",
            "\n",
            "        .. note::\n",
            "            Note that `:class:`torch.nn.Embedding` differs from this function in\n",
            "            that it initializes the row of :attr:`weight` specified by\n",
            "            :attr:`padding_idx` to all zeros on construction.\n",
            "\n",
            "        Args:\n",
            "            input (LongTensor): Tensor containing indices into the embedding matrix\n",
            "            weight (Tensor): The embedding matrix with number of rows equal to the maximum possible index + 1,\n",
            "                and number of columns equal to the embedding size\n",
            "            padding_idx (int, optional): If specified, the entries at :attr:`padding_idx` do not contribute to the gradient;\n",
            "                                         therefore, the embedding vector at :attr:`padding_idx` is not updated during training,\n",
            "                                         i.e. it remains as a fixed \"pad\".\n",
            "            max_norm (float, optional): If given, each embedding vector with norm larger than :attr:`max_norm`\n",
            "                                        is renormalized to have norm :attr:`max_norm`.\n",
            "                                        Note: this will modify :attr:`weight` in-place.\n",
            "            norm_type (float, optional): The p of the p-norm to compute for the :attr:`max_norm` option. Default ``2``.\n",
            "            scale_grad_by_freq (bool, optional): If given, this will scale gradients by the inverse of frequency of\n",
            "                                                    the words in the mini-batch. Default ``False``.\n",
            "            sparse (bool, optional): If ``True``, gradient w.r.t. :attr:`weight` will be a sparse tensor. See Notes under\n",
            "                                     :class:`torch.nn.Embedding` for more details regarding sparse gradients.\n",
            "\n",
            "        Shape:\n",
            "            - Input: LongTensor of arbitrary shape containing the indices to extract\n",
            "            - Weight: Embedding matrix of floating point type with shape `(V, embedding_dim)`,\n",
            "              where V = maximum index + 1 and embedding_dim = the embedding size\n",
            "            - Output: `(*, embedding_dim)`, where `*` is the input shape\n",
            "\n",
            "        Examples::\n",
            "\n",
            "            >>> # a batch of 2 samples of 4 indices each\n",
            "            >>> input = torch.tensor([[1, 2, 4, 5], [4, 3, 2, 9]])\n",
            "            >>> # an embedding matrix containing 10 tensors of size 3\n",
            "            >>> embedding_matrix = torch.rand(10, 3)\n",
            "            >>> # xdoctest: +IGNORE_WANT(\"non-deterministic\")\n",
            "            >>> F.embedding(input, embedding_matrix)\n",
            "            tensor([[[ 0.8490,  0.9625,  0.6753],\n",
            "                     [ 0.9666,  0.7761,  0.6108],\n",
            "                     [ 0.6246,  0.9751,  0.3618],\n",
            "                     [ 0.4161,  0.2419,  0.7383]],\n",
            "\n",
            "                    [[ 0.6246,  0.9751,  0.3618],\n",
            "                     [ 0.0237,  0.7794,  0.0528],\n",
            "                     [ 0.9666,  0.7761,  0.6108],\n",
            "                     [ 0.3385,  0.8612,  0.1867]]])\n",
            "\n",
            "            >>> # example with padding_idx\n",
            "            >>> weights = torch.rand(10, 3)\n",
            "            >>> weights[0, :].zero_()\n",
            "            >>> embedding_matrix = weights\n",
            "            >>> input = torch.tensor([[0, 2, 0, 5]])\n",
            "            >>> F.embedding(input, embedding_matrix, padding_idx=0)\n",
            "            tensor([[[ 0.0000,  0.0000,  0.0000],\n",
            "                     [ 0.5609,  0.5384,  0.8720],\n",
            "                     [ 0.0000,  0.0000,  0.0000],\n",
            "                     [ 0.6262,  0.2438,  0.7471]]])\n",
            "\n",
            "    embedding_bag(input: torch.Tensor, weight: torch.Tensor, offsets: Optional[torch.Tensor] = None, max_norm: Optional[float] = None, norm_type: float = 2, scale_grad_by_freq: bool = False, mode: str = 'mean', sparse: bool = False, per_sample_weights: Optional[torch.Tensor] = None, include_last_offset: bool = False, padding_idx: Optional[int] = None) -> torch.Tensor\n",
            "        Compute sums, means or maxes of `bags` of embeddings.\n",
            "\n",
            "        Calculation is done without instantiating the intermediate embeddings.\n",
            "        See :class:`torch.nn.EmbeddingBag` for more details.\n",
            "\n",
            "        Note:\n",
            "            This operation may produce nondeterministic gradients when given tensors on a CUDA device. See :doc:`/notes/randomness` for more information.\n",
            "\n",
            "        Args:\n",
            "            input (LongTensor): Tensor containing bags of indices into the embedding matrix\n",
            "            weight (Tensor): The embedding matrix with number of rows equal to the maximum possible index + 1,\n",
            "                and number of columns equal to the embedding size\n",
            "            offsets (LongTensor, optional): Only used when :attr:`input` is 1D. :attr:`offsets` determines\n",
            "                                 the starting index position of each bag (sequence) in :attr:`input`.\n",
            "            max_norm (float, optional): If given, each embedding vector with norm larger than :attr:`max_norm`\n",
            "                                        is renormalized to have norm :attr:`max_norm`.\n",
            "                                        Note: this will modify :attr:`weight` in-place.\n",
            "            norm_type (float, optional): The ``p`` in the ``p``-norm to compute for the :attr:`max_norm` option.\n",
            "                                         Default ``2``.\n",
            "            scale_grad_by_freq (bool, optional): if given, this will scale gradients by the inverse of frequency of\n",
            "                                                    the words in the mini-batch. Default ``False``.\n",
            "                                                    Note: this option is not supported when ``mode=\"max\"``.\n",
            "            mode (str, optional): ``\"sum\"``, ``\"mean\"`` or ``\"max\"``. Specifies the way to reduce the bag.\n",
            "                                     Default: ``\"mean\"``\n",
            "            sparse (bool, optional): if ``True``, gradient w.r.t. :attr:`weight` will be a sparse tensor. See Notes under\n",
            "                                     :class:`torch.nn.Embedding` for more details regarding sparse gradients.\n",
            "                                     Note: this option is not supported when ``mode=\"max\"``.\n",
            "            per_sample_weights (Tensor, optional): a tensor of float / double weights, or None\n",
            "                to indicate all weights should be taken to be 1. If specified, :attr:`per_sample_weights`\n",
            "                must have exactly the same shape as input and is treated as having the same\n",
            "                :attr:`offsets`, if those are not None.\n",
            "\n",
            "            include_last_offset (bool, optional): if ``True``, the size of offsets is equal to the number of bags + 1.\n",
            "                The last element is the size of the input, or the ending index position of the last bag (sequence).\n",
            "\n",
            "            padding_idx (int, optional): If specified, the entries at :attr:`padding_idx` do not contribute to the\n",
            "                                         gradient; therefore, the embedding vector at :attr:`padding_idx` is not updated\n",
            "                                         during training, i.e. it remains as a fixed \"pad\". Note that the embedding\n",
            "                                         vector at :attr:`padding_idx` is excluded from the reduction.\n",
            "\n",
            "        Shape:\n",
            "            - :attr:`input` (LongTensor) and :attr:`offsets` (LongTensor, optional)\n",
            "\n",
            "              - If :attr:`input` is 2D of shape `(B, N)`, it will be treated as ``B`` bags (sequences)\n",
            "                each of fixed length ``N``, and this will return ``B`` values aggregated in a way\n",
            "                depending on the :attr:`mode`. :attr:`offsets` is ignored and required to be ``None`` in this case.\n",
            "\n",
            "              - If :attr:`input` is 1D of shape `(N)`, it will be treated as a concatenation of\n",
            "                multiple bags (sequences). :attr:`offsets` is required to be a 1D tensor containing\n",
            "                the starting index positions of each bag in :attr:`input`. Therefore, for :attr:`offsets`\n",
            "                of shape `(B)`, :attr:`input` will be viewed as having ``B`` bags.\n",
            "                Empty bags (i.e., having 0-length) will have returned vectors filled by zeros.\n",
            "\n",
            "            - :attr:`weight` (Tensor): the learnable weights of the module of shape `(num_embeddings, embedding_dim)`\n",
            "\n",
            "            - :attr:`per_sample_weights` (Tensor, optional). Has the same shape as :attr:`input`.\n",
            "\n",
            "            - :attr:`output`: aggregated embedding values of shape `(B, embedding_dim)`\n",
            "\n",
            "        Examples::\n",
            "\n",
            "            >>> # an Embedding module containing 10 tensors of size 3\n",
            "            >>> embedding_matrix = torch.rand(10, 3)\n",
            "            >>> # a batch of 2 samples of 4 indices each\n",
            "            >>> input = torch.tensor([1, 2, 4, 5, 4, 3, 2, 9])\n",
            "            >>> offsets = torch.tensor([0, 4])\n",
            "            >>> # xdoctest: +IGNORE_WANT(\"non-deterministic\")\n",
            "            >>> F.embedding_bag(input, embedding_matrix, offsets)\n",
            "            tensor([[ 0.3397,  0.3552,  0.5545],\n",
            "                    [ 0.5893,  0.4386,  0.5882]])\n",
            "\n",
            "            >>> # example with padding_idx\n",
            "            >>> embedding_matrix = torch.rand(10, 3)\n",
            "            >>> input = torch.tensor([2, 2, 2, 2, 4, 3, 2, 9])\n",
            "            >>> offsets = torch.tensor([0, 4])\n",
            "            >>> F.embedding_bag(input, embedding_matrix, offsets, padding_idx=2, mode='sum')\n",
            "            tensor([[ 0.0000,  0.0000,  0.0000],\n",
            "                    [-0.7082,  3.2145, -2.6251]])\n",
            "\n",
            "    feature_alpha_dropout(input: torch.Tensor, p: float = 0.5, training: bool = False, inplace: bool = False) -> torch.Tensor\n",
            "        Randomly masks out entire channels (a channel is a feature map).\n",
            "\n",
            "        For example, the :math:`j`-th channel of the :math:`i`-th sample in the batch input\n",
            "        is a tensor :math:`\\text{input}[i, j]` of the input tensor. Instead of\n",
            "        setting activations to zero, as in regular Dropout, the activations are set\n",
            "        to the negative saturation value of the SELU activation function.\n",
            "\n",
            "        Each element will be masked independently on every forward call with\n",
            "        probability :attr:`p` using samples from a Bernoulli distribution.\n",
            "        The elements to be masked are randomized on every forward call, and scaled\n",
            "        and shifted to maintain zero mean and unit variance.\n",
            "\n",
            "        See :class:`~torch.nn.FeatureAlphaDropout` for details.\n",
            "\n",
            "        Args:\n",
            "            p: dropout probability of a channel to be zeroed. Default: 0.5\n",
            "            training: apply dropout if is ``True``. Default: ``True``\n",
            "            inplace: If set to ``True``, will do this operation in-place. Default: ``False``\n",
            "\n",
            "    fold(input: torch.Tensor, output_size: None, kernel_size: None, dilation: None = 1, padding: None = 0, stride: None = 1) -> torch.Tensor\n",
            "        Combine an array of sliding local blocks into a large containing tensor.\n",
            "\n",
            "        .. warning::\n",
            "            Currently, only unbatched (3D) or batched (4D) image-like output tensors are supported.\n",
            "\n",
            "        See :class:`torch.nn.Fold` for details\n",
            "\n",
            "    fractional_max_pool2d(*args, **kwargs)\n",
            "        fractional_max_pool2d(input, kernel_size, output_size=None, output_ratio=None, return_indices=False, _random_samples=None)\n",
            "\n",
            "        Applies 2D fractional max pooling over an input signal composed of several input planes.\n",
            "\n",
            "        Fractional MaxPooling is described in detail in the paper `Fractional MaxPooling`_ by Ben Graham\n",
            "\n",
            "        The max-pooling operation is applied in :math:`kH \\times kW` regions by a stochastic\n",
            "        step size determined by the target output size.\n",
            "        The number of output features is equal to the number of input planes.\n",
            "\n",
            "        Args:\n",
            "            kernel_size: the size of the window to take a max over.\n",
            "                         Can be a single number :math:`k` (for a square kernel of :math:`k \\times k`)\n",
            "                         or a tuple `(kH, kW)`\n",
            "            output_size: the target output size of the image of the form :math:`oH \\times oW`.\n",
            "                         Can be a tuple `(oH, oW)` or a single number :math:`oH` for a square image :math:`oH \\times oH`\n",
            "            output_ratio: If one wants to have an output size as a ratio of the input size, this option can be given.\n",
            "                          This has to be a number or tuple in the range (0, 1)\n",
            "            return_indices: if ``True``, will return the indices along with the outputs.\n",
            "                            Useful to pass to :func:`~torch.nn.functional.max_unpool2d`.\n",
            "\n",
            "        Examples::\n",
            "            >>> input = torch.randn(20, 16, 50, 32)\n",
            "            >>> # pool of square window of size=3, and target output size 13x12\n",
            "            >>> F.fractional_max_pool2d(input, 3, output_size=(13, 12))\n",
            "            >>> # pool of square window and target output size being half of input image size\n",
            "            >>> F.fractional_max_pool2d(input, 3, output_ratio=(0.5, 0.5))\n",
            "\n",
            "        .. _Fractional MaxPooling:\n",
            "            http://arxiv.org/abs/1412.6071\n",
            "\n",
            "    fractional_max_pool2d_with_indices(input: torch.Tensor, kernel_size: None, output_size: NoneType = None, output_ratio: NoneType = None, return_indices: bool = False, _random_samples: Optional[torch.Tensor] = None) -> tuple[torch.Tensor, torch.Tensor]\n",
            "        fractional_max_pool2d(input, kernel_size, output_size=None, output_ratio=None, return_indices=False, _random_samples=None)\n",
            "\n",
            "        Applies 2D fractional max pooling over an input signal composed of several input planes.\n",
            "\n",
            "        Fractional MaxPooling is described in detail in the paper `Fractional MaxPooling`_ by Ben Graham\n",
            "\n",
            "        The max-pooling operation is applied in :math:`kH \\times kW` regions by a stochastic\n",
            "        step size determined by the target output size.\n",
            "        The number of output features is equal to the number of input planes.\n",
            "\n",
            "        Args:\n",
            "            kernel_size: the size of the window to take a max over.\n",
            "                         Can be a single number :math:`k` (for a square kernel of :math:`k \\times k`)\n",
            "                         or a tuple `(kH, kW)`\n",
            "            output_size: the target output size of the image of the form :math:`oH \\times oW`.\n",
            "                         Can be a tuple `(oH, oW)` or a single number :math:`oH` for a square image :math:`oH \\times oH`\n",
            "            output_ratio: If one wants to have an output size as a ratio of the input size, this option can be given.\n",
            "                          This has to be a number or tuple in the range (0, 1)\n",
            "            return_indices: if ``True``, will return the indices along with the outputs.\n",
            "                            Useful to pass to :func:`~torch.nn.functional.max_unpool2d`.\n",
            "\n",
            "        Examples::\n",
            "            >>> input = torch.randn(20, 16, 50, 32)\n",
            "            >>> # pool of square window of size=3, and target output size 13x12\n",
            "            >>> F.fractional_max_pool2d(input, 3, output_size=(13, 12))\n",
            "            >>> # pool of square window and target output size being half of input image size\n",
            "            >>> F.fractional_max_pool2d(input, 3, output_ratio=(0.5, 0.5))\n",
            "\n",
            "        .. _Fractional MaxPooling:\n",
            "            http://arxiv.org/abs/1412.6071\n",
            "\n",
            "    fractional_max_pool3d(*args, **kwargs)\n",
            "        fractional_max_pool3d(input, kernel_size, output_size=None, output_ratio=None, return_indices=False, _random_samples=None)\n",
            "\n",
            "        Applies 3D fractional max pooling over an input signal composed of several input planes.\n",
            "\n",
            "        Fractional MaxPooling is described in detail in the paper `Fractional MaxPooling`_ by Ben Graham\n",
            "\n",
            "        The max-pooling operation is applied in :math:`kT \\times kH \\times kW` regions by a stochastic\n",
            "        step size determined by the target output size.\n",
            "        The number of output features is equal to the number of input planes.\n",
            "\n",
            "        Args:\n",
            "            kernel_size: the size of the window to take a max over.\n",
            "                         Can be a single number :math:`k` (for a square kernel of :math:`k \\times k \\times k`)\n",
            "                         or a tuple `(kT, kH, kW)`\n",
            "            output_size: the target output size of the form :math:`oT \\times oH \\times oW`.\n",
            "                         Can be a tuple `(oT, oH, oW)` or a single number :math:`oH` for a cubic output\n",
            "                         :math:`oH \\times oH \\times oH`\n",
            "            output_ratio: If one wants to have an output size as a ratio of the input size, this option can be given.\n",
            "                          This has to be a number or tuple in the range (0, 1)\n",
            "            return_indices: if ``True``, will return the indices along with the outputs.\n",
            "                            Useful to pass to :func:`~torch.nn.functional.max_unpool3d`.\n",
            "\n",
            "        Shape:\n",
            "            - Input: :math:`(N, C, T_{in}, H_{in}, W_{in})` or :math:`(C, T_{in}, H_{in}, W_{in})`.\n",
            "            - Output: :math:`(N, C, T_{out}, H_{out}, W_{out})` or :math:`(C, T_{out}, H_{out}, W_{out})`, where\n",
            "              :math:`(T_{out}, H_{out}, W_{out})=\\text{output\\_size}` or\n",
            "              :math:`(T_{out}, H_{out}, W_{out})=\\text{output\\_ratio} \\times (T_{in}, H_{in}, W_{in})`\n",
            "\n",
            "        Examples::\n",
            "            >>> input = torch.randn(20, 16, 50, 32, 16)\n",
            "            >>> # pool of cubic window of size=3, and target output size 13x12x11\n",
            "            >>> F.fractional_max_pool3d(input, 3, output_size=(13, 12, 11))\n",
            "            >>> # pool of cubic window and target output size being half of input size\n",
            "            >>> F.fractional_max_pool3d(input, 3, output_ratio=(0.5, 0.5, 0.5))\n",
            "\n",
            "        .. _Fractional MaxPooling:\n",
            "            http://arxiv.org/abs/1412.6071\n",
            "\n",
            "    fractional_max_pool3d_with_indices(input: torch.Tensor, kernel_size: None, output_size: NoneType = None, output_ratio: NoneType = None, return_indices: bool = False, _random_samples: Optional[torch.Tensor] = None) -> tuple[torch.Tensor, torch.Tensor]\n",
            "        fractional_max_pool3d(input, kernel_size, output_size=None, output_ratio=None, return_indices=False, _random_samples=None)\n",
            "\n",
            "        Applies 3D fractional max pooling over an input signal composed of several input planes.\n",
            "\n",
            "        Fractional MaxPooling is described in detail in the paper `Fractional MaxPooling`_ by Ben Graham\n",
            "\n",
            "        The max-pooling operation is applied in :math:`kT \\times kH \\times kW` regions by a stochastic\n",
            "        step size determined by the target output size.\n",
            "        The number of output features is equal to the number of input planes.\n",
            "\n",
            "        Args:\n",
            "            kernel_size: the size of the window to take a max over.\n",
            "                         Can be a single number :math:`k` (for a square kernel of :math:`k \\times k \\times k`)\n",
            "                         or a tuple `(kT, kH, kW)`\n",
            "            output_size: the target output size of the form :math:`oT \\times oH \\times oW`.\n",
            "                         Can be a tuple `(oT, oH, oW)` or a single number :math:`oH` for a cubic output\n",
            "                         :math:`oH \\times oH \\times oH`\n",
            "            output_ratio: If one wants to have an output size as a ratio of the input size, this option can be given.\n",
            "                          This has to be a number or tuple in the range (0, 1)\n",
            "            return_indices: if ``True``, will return the indices along with the outputs.\n",
            "                            Useful to pass to :func:`~torch.nn.functional.max_unpool3d`.\n",
            "\n",
            "        Shape:\n",
            "            - Input: :math:`(N, C, T_{in}, H_{in}, W_{in})` or :math:`(C, T_{in}, H_{in}, W_{in})`.\n",
            "            - Output: :math:`(N, C, T_{out}, H_{out}, W_{out})` or :math:`(C, T_{out}, H_{out}, W_{out})`, where\n",
            "              :math:`(T_{out}, H_{out}, W_{out})=\\text{output\\_size}` or\n",
            "              :math:`(T_{out}, H_{out}, W_{out})=\\text{output\\_ratio} \\times (T_{in}, H_{in}, W_{in})`\n",
            "\n",
            "        Examples::\n",
            "            >>> input = torch.randn(20, 16, 50, 32, 16)\n",
            "            >>> # pool of cubic window of size=3, and target output size 13x12x11\n",
            "            >>> F.fractional_max_pool3d(input, 3, output_size=(13, 12, 11))\n",
            "            >>> # pool of cubic window and target output size being half of input size\n",
            "            >>> F.fractional_max_pool3d(input, 3, output_ratio=(0.5, 0.5, 0.5))\n",
            "\n",
            "        .. _Fractional MaxPooling:\n",
            "            http://arxiv.org/abs/1412.6071\n",
            "\n",
            "    gaussian_nll_loss(input: torch.Tensor, target: torch.Tensor, var: Union[torch.Tensor, float], full: bool = False, eps: float = 1e-06, reduction: str = 'mean') -> torch.Tensor\n",
            "        Gaussian negative log likelihood loss.\n",
            "\n",
            "        See :class:`~torch.nn.GaussianNLLLoss` for details.\n",
            "\n",
            "        Args:\n",
            "            input: expectation of the Gaussian distribution.\n",
            "            target: sample from the Gaussian distribution.\n",
            "            var: tensor of positive variance(s), one for each of the expectations\n",
            "                in the input (heteroscedastic), or a single one (homoscedastic),\n",
            "                or a positive scalar value to be used for all expectations.\n",
            "            full (bool, optional): include the constant term in the loss calculation. Default: ``False``.\n",
            "            eps (float, optional): value added to var, for stability. Default: 1e-6.\n",
            "            reduction (str, optional): specifies the reduction to apply to the output:\n",
            "                ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,\n",
            "                ``'mean'``: the output is the average of all batch member losses,\n",
            "                ``'sum'``: the output is the sum of all batch member losses.\n",
            "                Default: ``'mean'``.\n",
            "\n",
            "    gelu(...)\n",
            "        gelu(input, approximate = 'none') -> Tensor\n",
            "\n",
            "        When the approximate argument is 'none', it applies element-wise the function\n",
            "        :math:`\\text{GELU}(x) = x * \\Phi(x)`\n",
            "\n",
            "        where :math:`\\Phi(x)` is the Cumulative Distribution Function for Gaussian Distribution.\n",
            "\n",
            "        When the approximate argument is 'tanh', Gelu is estimated with\n",
            "\n",
            "        .. math::\n",
            "            \\text{GELU}(x) = 0.5 * x * (1 + \\text{Tanh}(\\sqrt{2 / \\pi} * (x + 0.044715 * x^3)))\n",
            "\n",
            "        See `Gaussian Error Linear Units (GELUs) <https://arxiv.org/abs/1606.08415>`_.\n",
            "\n",
            "    glu(input: torch.Tensor, dim: int = -1) -> torch.Tensor\n",
            "        glu(input, dim=-1) -> Tensor\n",
            "\n",
            "        The gated linear unit. Computes:\n",
            "\n",
            "        .. math ::\n",
            "            \\text{GLU}(a, b) = a \\otimes \\sigma(b)\n",
            "\n",
            "        where `input` is split in half along `dim` to form `a` and `b`, :math:`\\sigma`\n",
            "        is the sigmoid function and :math:`\\otimes` is the element-wise product between matrices.\n",
            "\n",
            "        See `Language Modeling with Gated Convolutional Networks <https://arxiv.org/abs/1612.08083>`_.\n",
            "\n",
            "        Args:\n",
            "            input (Tensor): input tensor\n",
            "            dim (int): dimension on which to split the input. Default: -1\n",
            "\n",
            "    grid_sample(input: torch.Tensor, grid: torch.Tensor, mode: str = 'bilinear', padding_mode: str = 'zeros', align_corners: Optional[bool] = None) -> torch.Tensor\n",
            "        Compute grid sample.\n",
            "\n",
            "        Given an :attr:`input` and a flow-field :attr:`grid`, computes the\n",
            "        ``output`` using :attr:`input` values and pixel locations from :attr:`grid`.\n",
            "\n",
            "        Currently, only spatial (4-D) and volumetric (5-D) :attr:`input` are\n",
            "        supported.\n",
            "\n",
            "        In the spatial (4-D) case, for :attr:`input` with shape\n",
            "        :math:`(N, C, H_\\text{in}, W_\\text{in})` and :attr:`grid` with shape\n",
            "        :math:`(N, H_\\text{out}, W_\\text{out}, 2)`, the output will have shape\n",
            "        :math:`(N, C, H_\\text{out}, W_\\text{out})`.\n",
            "\n",
            "        For each output location ``output[n, :, h, w]``, the size-2 vector\n",
            "        ``grid[n, h, w]`` specifies :attr:`input` pixel locations ``x`` and ``y``,\n",
            "        which are used to interpolate the output value ``output[n, :, h, w]``.\n",
            "        In the case of 5D inputs, ``grid[n, d, h, w]`` specifies the\n",
            "        ``x``, ``y``, ``z`` pixel locations for interpolating\n",
            "        ``output[n, :, d, h, w]``. :attr:`mode` argument specifies ``nearest`` or\n",
            "        ``bilinear`` interpolation method to sample the input pixels.\n",
            "\n",
            "        :attr:`grid` specifies the sampling pixel locations normalized by the\n",
            "        :attr:`input` spatial dimensions. Therefore, it should have most values in\n",
            "        the range of ``[-1, 1]``. For example, values ``x = -1, y = -1`` is the\n",
            "        left-top pixel of :attr:`input`, and values  ``x = 1, y = 1`` is the\n",
            "        right-bottom pixel of :attr:`input`.\n",
            "\n",
            "        If :attr:`grid` has values outside the range of ``[-1, 1]``, the corresponding\n",
            "        outputs are handled as defined by :attr:`padding_mode`. Options are\n",
            "\n",
            "            * ``padding_mode=\"zeros\"``: use ``0`` for out-of-bound grid locations,\n",
            "            * ``padding_mode=\"border\"``: use border values for out-of-bound grid locations,\n",
            "            * ``padding_mode=\"reflection\"``: use values at locations reflected by\n",
            "              the border for out-of-bound grid locations. For location far away\n",
            "              from the border, it will keep being reflected until becoming in bound,\n",
            "              e.g., (normalized) pixel location ``x = -3.5`` reflects by border ``-1``\n",
            "              and becomes ``x' = 1.5``, then reflects by border ``1`` and becomes\n",
            "              ``x'' = -0.5``.\n",
            "\n",
            "        Note:\n",
            "            This function is often used in conjunction with :func:`affine_grid`\n",
            "            to build `Spatial Transformer Networks`_ .\n",
            "\n",
            "        Note:\n",
            "            When using the CUDA backend, this operation may induce nondeterministic\n",
            "            behaviour in its backward pass that is not easily switched off.\n",
            "            Please see the notes on :doc:`/notes/randomness` for background.\n",
            "\n",
            "        Note:\n",
            "            NaN values in :attr:`grid` would be interpreted as ``-1``.\n",
            "\n",
            "        Args:\n",
            "            input (Tensor): input of shape :math:`(N, C, H_\\text{in}, W_\\text{in})` (4-D case)\n",
            "                            or :math:`(N, C, D_\\text{in}, H_\\text{in}, W_\\text{in})` (5-D case)\n",
            "            grid (Tensor): flow-field of shape :math:`(N, H_\\text{out}, W_\\text{out}, 2)` (4-D case)\n",
            "                           or :math:`(N, D_\\text{out}, H_\\text{out}, W_\\text{out}, 3)` (5-D case)\n",
            "            mode (str): interpolation mode to calculate output values\n",
            "                ``'bilinear'`` | ``'nearest'`` | ``'bicubic'``. Default: ``'bilinear'``\n",
            "                Note: ``mode='bicubic'`` supports only 4-D input.\n",
            "                When ``mode='bilinear'`` and the input is 5-D, the interpolation mode\n",
            "                used internally will actually be trilinear. However, when the input is 4-D,\n",
            "                the interpolation mode will legitimately be bilinear.\n",
            "            padding_mode (str): padding mode for outside grid values\n",
            "                ``'zeros'`` | ``'border'`` | ``'reflection'``. Default: ``'zeros'``\n",
            "            align_corners (bool, optional): Geometrically, we consider the pixels of the\n",
            "                input  as squares rather than points.\n",
            "                If set to ``True``, the extrema (``-1`` and ``1``) are considered as referring\n",
            "                to the center points of the input's corner pixels. If set to ``False``, they\n",
            "                are instead considered as referring to the corner points of the input's corner\n",
            "                pixels, making the sampling more resolution agnostic.\n",
            "                This option parallels the ``align_corners`` option in\n",
            "                :func:`interpolate`, and so whichever option is used here\n",
            "                should also be used there to resize the input image before grid sampling.\n",
            "                Default: ``False``\n",
            "\n",
            "        Returns:\n",
            "            output (Tensor): output Tensor\n",
            "\n",
            "        .. _`Spatial Transformer Networks`:\n",
            "            https://arxiv.org/abs/1506.02025\n",
            "\n",
            "        .. warning::\n",
            "            When ``align_corners = True``, the grid positions depend on the pixel\n",
            "            size relative to the input image size, and so the locations sampled by\n",
            "            :func:`grid_sample` will differ for the same input given at different\n",
            "            resolutions (that is, after being upsampled or downsampled).\n",
            "            The default behavior up to version 1.2.0 was ``align_corners = True``.\n",
            "            Since then, the default behavior has been changed to ``align_corners = False``,\n",
            "            in order to bring it in line with the default for :func:`interpolate`.\n",
            "\n",
            "        .. note::\n",
            "            ``mode='bicubic'`` is implemented using the `cubic convolution algorithm`_ with :math:`\\alpha=-0.75`.\n",
            "            The constant :math:`\\alpha` might be different from packages to packages.\n",
            "            For example, `PIL`_ and `OpenCV`_ use -0.5 and -0.75 respectively.\n",
            "            This algorithm may \"overshoot\" the range of values it's interpolating.\n",
            "            For example, it may produce negative values or values greater than 255 when interpolating input in [0, 255].\n",
            "            Clamp the results with :func:`torch.clamp` to ensure they are within the valid range.\n",
            "        .. _`cubic convolution algorithm`: https://en.wikipedia.org/wiki/Bicubic_interpolation\n",
            "        .. _`PIL`: https://github.com/python-pillow/Pillow/blob/4634eafe3c695a014267eefdce830b4a825beed7/src/libImaging/Resample.c#L51\n",
            "        .. _`OpenCV`: https://github.com/opencv/opencv/blob/f345ed564a06178670750bad59526cfa4033be55/modules/imgproc/src/resize.cpp#L908\n",
            "\n",
            "    group_norm(input: torch.Tensor, num_groups: int, weight: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, eps: float = 1e-05) -> torch.Tensor\n",
            "        Apply Group Normalization for last certain number of dimensions.\n",
            "\n",
            "        See :class:`~torch.nn.GroupNorm` for details.\n",
            "\n",
            "    gumbel_softmax(logits: torch.Tensor, tau: float = 1, hard: bool = False, eps: float = 1e-10, dim: int = -1) -> torch.Tensor\n",
            "        Sample from the Gumbel-Softmax distribution (`Link 1`_  `Link 2`_) and optionally discretize.\n",
            "\n",
            "        Args:\n",
            "          logits: `[..., num_features]` unnormalized log probabilities\n",
            "          tau: non-negative scalar temperature\n",
            "          hard: if ``True``, the returned samples will be discretized as one-hot vectors,\n",
            "                but will be differentiated as if it is the soft sample in autograd\n",
            "          dim (int): A dimension along which softmax will be computed. Default: -1.\n",
            "\n",
            "        Returns:\n",
            "          Sampled tensor of same shape as `logits` from the Gumbel-Softmax distribution.\n",
            "          If ``hard=True``, the returned samples will be one-hot, otherwise they will\n",
            "          be probability distributions that sum to 1 across `dim`.\n",
            "\n",
            "        .. note::\n",
            "          This function is here for legacy reasons, may be removed from nn.Functional in the future.\n",
            "\n",
            "        .. note::\n",
            "          The main trick for `hard` is to do  `y_hard - y_soft.detach() + y_soft`\n",
            "\n",
            "          It achieves two things:\n",
            "          - makes the output value exactly one-hot\n",
            "          (since we add then subtract y_soft value)\n",
            "          - makes the gradient equal to y_soft gradient\n",
            "          (since we strip all other gradients)\n",
            "\n",
            "        Examples::\n",
            "            >>> logits = torch.randn(20, 32)\n",
            "            >>> # Sample soft categorical using reparametrization trick:\n",
            "            >>> F.gumbel_softmax(logits, tau=1, hard=False)\n",
            "            >>> # Sample hard categorical using \"Straight-through\" trick:\n",
            "            >>> F.gumbel_softmax(logits, tau=1, hard=True)\n",
            "\n",
            "        .. _Link 1:\n",
            "            https://arxiv.org/abs/1611.00712\n",
            "        .. _Link 2:\n",
            "            https://arxiv.org/abs/1611.01144\n",
            "\n",
            "    hardshrink(...)\n",
            "        hardshrink(input, lambd=0.5) -> Tensor\n",
            "\n",
            "        Applies the hard shrinkage function element-wise\n",
            "\n",
            "        See :class:`~torch.nn.Hardshrink` for more details.\n",
            "\n",
            "    hardsigmoid(input: torch.Tensor, inplace: bool = False) -> torch.Tensor\n",
            "        Apply the Hardsigmoid function element-wise.\n",
            "\n",
            "        .. math::\n",
            "            \\text{Hardsigmoid}(x) = \\begin{cases}\n",
            "                0 & \\text{if~} x \\le -3, \\\\\n",
            "                1 & \\text{if~} x \\ge +3, \\\\\n",
            "                x / 6 + 1 / 2 & \\text{otherwise}\n",
            "            \\end{cases}\n",
            "\n",
            "        Args:\n",
            "            inplace: If set to ``True``, will do this operation in-place. Default: ``False``\n",
            "\n",
            "        See :class:`~torch.nn.Hardsigmoid` for more details.\n",
            "\n",
            "    hardswish(input: torch.Tensor, inplace: bool = False) -> torch.Tensor\n",
            "        Apply hardswish function, element-wise.\n",
            "\n",
            "        Follows implementation as described in the paper:\n",
            "        `Searching for MobileNetV3`_.\n",
            "\n",
            "        .. math::\n",
            "            \\text{Hardswish}(x) = \\begin{cases}\n",
            "                0 & \\text{if~} x \\le -3, \\\\\n",
            "                x & \\text{if~} x \\ge +3, \\\\\n",
            "                x \\cdot (x + 3) /6 & \\text{otherwise}\n",
            "            \\end{cases}\n",
            "\n",
            "        See :class:`~torch.nn.Hardswish` for more details.\n",
            "\n",
            "        .. _`Searching for MobileNetV3`:\n",
            "            https://arxiv.org/abs/1905.02244\n",
            "\n",
            "    hardtanh(input: torch.Tensor, min_val: float = -1.0, max_val: float = 1.0, inplace: bool = False) -> torch.Tensor\n",
            "        hardtanh(input, min_val=-1., max_val=1., inplace=False) -> Tensor\n",
            "\n",
            "        Applies the HardTanh function element-wise. See :class:`~torch.nn.Hardtanh` for more\n",
            "        details.\n",
            "\n",
            "    hardtanh_(...)\n",
            "        hardtanh_(input, min_val=-1., max_val=1.) -> Tensor\n",
            "\n",
            "        In-place version of :func:`~hardtanh`.\n",
            "\n",
            "    has_torch_function = _has_torch_function(...)\n",
            "        Check for __torch_function__ implementations in the elements of an iterable\n",
            "        or if a __torch_function__ mode is enabled.  Considers exact ``Tensor`` s\n",
            "        and ``Parameter`` s non-dispatchable.  Use this to guard a call to\n",
            "        :func:`handle_torch_function`; don't use it to test if something\n",
            "        is Tensor-like, use :func:`is_tensor_like` instead.\n",
            "        Arguments\n",
            "        ---------\n",
            "        relevant_args : iterable\n",
            "            Iterable or arguments to check for __torch_function__ methods.\n",
            "        Returns\n",
            "        -------\n",
            "        bool\n",
            "            True if any of the elements of relevant_args have __torch_function__\n",
            "            implementations, False otherwise.\n",
            "        See Also\n",
            "        ________\n",
            "        torch.is_tensor_like\n",
            "            Checks if something is a Tensor-like, including an exact ``Tensor``.\n",
            "\n",
            "    has_torch_function_unary = _has_torch_function_unary(...)\n",
            "        Special case of `has_torch_function` for single inputs.\n",
            "        Instead of:\n",
            "          `has_torch_function((t,))`\n",
            "        call:\n",
            "          `has_torch_function_unary(t)`\n",
            "        which skips unnecessary packing and unpacking work.\n",
            "\n",
            "    has_torch_function_variadic = _has_torch_function_variadic(...)\n",
            "        Special case of `has_torch_function` that skips tuple creation.\n",
            "\n",
            "        This uses the METH_FASTCALL protocol introduced in Python 3.7\n",
            "\n",
            "        Instead of:\n",
            "          `has_torch_function((a, b))`\n",
            "        call:\n",
            "          `has_torch_function_variadic(a, b)`\n",
            "        which skips unnecessary packing and unpacking work.\n",
            "\n",
            "    hinge_embedding_loss(input: torch.Tensor, target: torch.Tensor, margin: float = 1.0, size_average: Optional[bool] = None, reduce: Optional[bool] = None, reduction: str = 'mean') -> torch.Tensor\n",
            "        hinge_embedding_loss(input, target, margin=1.0, size_average=None, reduce=None, reduction='mean') -> Tensor\n",
            "\n",
            "        See :class:`~torch.nn.HingeEmbeddingLoss` for details.\n",
            "\n",
            "    huber_loss(input: torch.Tensor, target: torch.Tensor, reduction: str = 'mean', delta: float = 1.0, weight: Optional[torch.Tensor] = None) -> torch.Tensor\n",
            "        huber_loss(input, target, reduction='mean', delta=1.0, weight=None) -> Tensor\n",
            "\n",
            "        Computes the Huber loss, with optional weighting.\n",
            "\n",
            "        Function uses a squared term if the absolute\n",
            "        element-wise error falls below delta and a delta-scaled L1 term otherwise.\n",
            "\n",
            "        When delta equals 1, this loss is equivalent to SmoothL1Loss.\n",
            "        In general, Huber loss differs from SmoothL1Loss by a factor of delta (AKA beta in Smooth L1).\n",
            "\n",
            "        Args:\n",
            "            input (Tensor): Predicted values.\n",
            "            target (Tensor): Ground truth values.\n",
            "            reduction (str, optional): Specifies the reduction to apply to the output:\n",
            "                                       'none' | 'mean' | 'sum'. 'mean': the mean of the output is taken.\n",
            "                                       'sum': the output will be summed. 'none': no reduction will be applied.\n",
            "                                       Default: 'mean'.\n",
            "            delta (float, optional): The threshold at which to change between delta-scaled L1 and L2 loss. Default: 1.0.\n",
            "            weight (Tensor, optional): Weights for each sample. Default: None.\n",
            "\n",
            "        Returns:\n",
            "            Tensor: Huber loss (optionally weighted).\n",
            "\n",
            "    instance_norm(input: torch.Tensor, running_mean: Optional[torch.Tensor] = None, running_var: Optional[torch.Tensor] = None, weight: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, use_input_stats: bool = True, momentum: float = 0.1, eps: float = 1e-05) -> torch.Tensor\n",
            "        Apply Instance Normalization independently for each channel in every data sample within a batch.\n",
            "\n",
            "        See :class:`~torch.nn.InstanceNorm1d`, :class:`~torch.nn.InstanceNorm2d`,\n",
            "        :class:`~torch.nn.InstanceNorm3d` for details.\n",
            "\n",
            "    interpolate(input: torch.Tensor, size: Optional[int] = None, scale_factor: Optional[list[float]] = None, mode: str = 'nearest', align_corners: Optional[bool] = None, recompute_scale_factor: Optional[bool] = None, antialias: bool = False) -> torch.Tensor\n",
            "        Down/up samples the input.\n",
            "\n",
            "        Tensor interpolated to either the given :attr:`size` or the given\n",
            "        :attr:`scale_factor`\n",
            "\n",
            "        The algorithm used for interpolation is determined by :attr:`mode`.\n",
            "\n",
            "        Currently temporal, spatial and volumetric sampling are supported, i.e.\n",
            "        expected inputs are 3-D, 4-D or 5-D in shape.\n",
            "\n",
            "        The input dimensions are interpreted in the form:\n",
            "        `mini-batch x channels x [optional depth] x [optional height] x width`.\n",
            "\n",
            "        The modes available for resizing are: `nearest`, `linear` (3D-only),\n",
            "        `bilinear`, `bicubic` (4D-only), `trilinear` (5D-only), `area`, `nearest-exact`\n",
            "\n",
            "        Args:\n",
            "            input (Tensor): the input tensor\n",
            "            size (int or Tuple[int] or Tuple[int, int] or Tuple[int, int, int]):\n",
            "                output spatial size.\n",
            "            scale_factor (float or Tuple[float]): multiplier for spatial size. If `scale_factor` is a tuple,\n",
            "                its length has to match the number of spatial dimensions; `input.dim() - 2`.\n",
            "            mode (str): algorithm used for upsampling:\n",
            "                ``'nearest'`` | ``'linear'`` | ``'bilinear'`` | ``'bicubic'`` |\n",
            "                ``'trilinear'`` | ``'area'`` | ``'nearest-exact'``. Default: ``'nearest'``\n",
            "            align_corners (bool, optional): Geometrically, we consider the pixels of the\n",
            "                input and output as squares rather than points.\n",
            "                If set to ``True``, the input and output tensors are aligned by the\n",
            "                center points of their corner pixels, preserving the values at the corner pixels.\n",
            "                If set to ``False``, the input and output tensors are aligned by the corner\n",
            "                points of their corner pixels, and the interpolation uses edge value padding\n",
            "                for out-of-boundary values, making this operation *independent* of input size\n",
            "                when :attr:`scale_factor` is kept the same. This only has an effect when :attr:`mode`\n",
            "                is ``'linear'``, ``'bilinear'``, ``'bicubic'`` or ``'trilinear'``.\n",
            "                Default: ``False``\n",
            "            recompute_scale_factor (bool, optional): recompute the scale_factor for use in the\n",
            "                interpolation calculation. If `recompute_scale_factor` is ``True``, then\n",
            "                `scale_factor` must be passed in and `scale_factor` is used to compute the\n",
            "                output `size`. The computed output `size` will be used to infer new scales for\n",
            "                the interpolation. Note that when `scale_factor` is floating-point, it may differ\n",
            "                from the recomputed `scale_factor` due to rounding and precision issues.\n",
            "                If `recompute_scale_factor` is ``False``, then `size` or `scale_factor` will\n",
            "                be used directly for interpolation. Default: ``None``.\n",
            "            antialias (bool, optional): flag to apply anti-aliasing. Default: ``False``. Using anti-alias\n",
            "                option together with ``align_corners=False``, interpolation result would match Pillow\n",
            "                result for downsampling operation. Supported modes: ``'bilinear'``, ``'bicubic'``.\n",
            "\n",
            "        .. note::\n",
            "            With ``mode='bicubic'``, it's possible to cause overshoot, in other words it can produce\n",
            "            negative values or values greater than 255 for images.\n",
            "            Explicitly call ``result.clamp(min=0, max=255)`` if you want to reduce the overshoot\n",
            "            when displaying the image.\n",
            "\n",
            "        .. note::\n",
            "            Mode ``mode='nearest-exact'`` matches Scikit-Image and PIL nearest neighbours interpolation\n",
            "            algorithms and fixes known issues with ``mode='nearest'``. This mode is introduced to keep\n",
            "            backward compatibility.\n",
            "            Mode ``mode='nearest'`` matches buggy OpenCV's ``INTER_NEAREST`` interpolation algorithm.\n",
            "\n",
            "        .. note::\n",
            "            The gradients for the dtype ``float16`` on CUDA may be inaccurate in the upsample operation\n",
            "            when using modes ``['linear', 'bilinear', 'bicubic', 'trilinear', 'area']``.\n",
            "            For more details, please refer to the discussion in\n",
            "            `issue#104157 <https://github.com/pytorch/pytorch/issues/104157>`_.\n",
            "\n",
            "        Note:\n",
            "            This operation may produce nondeterministic gradients when given tensors on a CUDA device. See :doc:`/notes/randomness` for more information.\n",
            "\n",
            "    kl_div(input: torch.Tensor, target: torch.Tensor, size_average: Optional[bool] = None, reduce: Optional[bool] = None, reduction: str = 'mean', log_target: bool = False) -> torch.Tensor\n",
            "        Compute the KL Divergence loss.\n",
            "\n",
            "        Refer - The `Kullback-Leibler divergence Loss\n",
            "        <https://en.wikipedia.org/wiki/Kullback-Leibler_divergence>`__\n",
            "\n",
            "        See :class:`~torch.nn.KLDivLoss` for details.\n",
            "\n",
            "        Args:\n",
            "            input: Tensor of arbitrary shape in log-probabilities.\n",
            "            target: Tensor of the same shape as input. See :attr:`log_target` for\n",
            "                the target's interpretation.\n",
            "            size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,\n",
            "                the losses are averaged over each loss element in the batch. Note that for\n",
            "                some losses, there multiple elements per sample. If the field :attr:`size_average`\n",
            "                is set to ``False``, the losses are instead summed for each minibatch. Ignored\n",
            "                when reduce is ``False``. Default: ``True``\n",
            "            reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the\n",
            "                losses are averaged or summed over observations for each minibatch depending\n",
            "                on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per\n",
            "                batch element instead and ignores :attr:`size_average`. Default: ``True``\n",
            "            reduction (str, optional): Specifies the reduction to apply to the output:\n",
            "                ``'none'`` | ``'batchmean'`` | ``'sum'`` | ``'mean'``.\n",
            "                ``'none'``: no reduction will be applied\n",
            "                ``'batchmean'``: the sum of the output will be divided by the batchsize\n",
            "                ``'sum'``: the output will be summed\n",
            "                ``'mean'``: the output will be divided by the number of elements in the output\n",
            "                Default: ``'mean'``\n",
            "            log_target (bool): A flag indicating whether ``target`` is passed in the log space.\n",
            "                It is recommended to pass certain distributions (like ``softmax``)\n",
            "                in the log space to avoid numerical issues caused by explicit ``log``.\n",
            "                Default: ``False``\n",
            "\n",
            "        .. note::\n",
            "            :attr:`size_average` and :attr:`reduce` are in the process of being deprecated,\n",
            "            and in the meantime, specifying either of those two args will override :attr:`reduction`.\n",
            "\n",
            "        .. warning::\n",
            "            :attr:`reduction` = ``'mean'`` doesn't return the true kl divergence value, please use\n",
            "            :attr:`reduction` = ``'batchmean'`` which aligns with KL math definition.\n",
            "\n",
            "    l1_loss(input: torch.Tensor, target: torch.Tensor, size_average: Optional[bool] = None, reduce: Optional[bool] = None, reduction: str = 'mean', weight: Optional[torch.Tensor] = None) -> torch.Tensor\n",
            "        l1_loss(input, target, size_average=None, reduce=None, reduction='mean') -> Tensor\n",
            "\n",
            "        Function that takes the mean element-wise absolute value difference.\n",
            "\n",
            "        See :class:`~torch.nn.L1Loss` for details.\n",
            "\n",
            "    layer_norm(input: torch.Tensor, normalized_shape: list[int], weight: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, eps: float = 1e-05) -> torch.Tensor\n",
            "        Apply Layer Normalization for last certain number of dimensions.\n",
            "\n",
            "        See :class:`~torch.nn.LayerNorm` for details.\n",
            "\n",
            "    leaky_relu(input: torch.Tensor, negative_slope: float = 0.01, inplace: bool = False) -> torch.Tensor\n",
            "        leaky_relu(input, negative_slope=0.01, inplace=False) -> Tensor\n",
            "\n",
            "        Applies element-wise,\n",
            "        :math:`\\text{LeakyReLU}(x) = \\max(0, x) + \\text{negative\\_slope} * \\min(0, x)`\n",
            "\n",
            "        See :class:`~torch.nn.LeakyReLU` for more details.\n",
            "\n",
            "    leaky_relu_(...)\n",
            "        leaky_relu_(input, negative_slope=0.01) -> Tensor\n",
            "\n",
            "        In-place version of :func:`~leaky_relu`.\n",
            "\n",
            "    linear(...)\n",
            "        linear(input, weight, bias=None) -> Tensor\n",
            "\n",
            "        Applies a linear transformation to the incoming data: :math:`y = xA^T + b`.\n",
            "\n",
            "        This operation supports 2-D :attr:`weight` with :ref:`sparse layout<sparse-docs>`\n",
            "\n",
            "\n",
            "        .. warning::\n",
            "            Sparse support is a beta feature and some layout(s)/dtype/device combinations may not be supported,\n",
            "            or may not have autograd support. If you notice missing functionality please\n",
            "            open a feature request.\n",
            "\n",
            "        This operator supports :ref:`TensorFloat32<tf32_on_ampere>`.\n",
            "\n",
            "        Shape:\n",
            "\n",
            "            - Input: :math:`(*, in\\_features)` where `*` means any number of\n",
            "              additional dimensions, including none\n",
            "            - Weight: :math:`(out\\_features, in\\_features)` or :math:`(in\\_features)`\n",
            "            - Bias: :math:`(out\\_features)` or :math:`()`\n",
            "            - Output: :math:`(*, out\\_features)` or :math:`(*)`, based on the shape of the weight\n",
            "\n",
            "    local_response_norm(input: torch.Tensor, size: int, alpha: float = 0.0001, beta: float = 0.75, k: float = 1.0) -> torch.Tensor\n",
            "        Apply local response normalization over an input signal.\n",
            "\n",
            "        The input signal is composed of several input planes, where channels occupy the second dimension.\n",
            "        Normalization is applied across channels.\n",
            "\n",
            "        See :class:`~torch.nn.LocalResponseNorm` for details.\n",
            "\n",
            "    log_softmax(input: torch.Tensor, dim: Optional[int] = None, _stacklevel: int = 3, dtype: Optional[int] = None) -> torch.Tensor\n",
            "        Apply a softmax followed by a logarithm.\n",
            "\n",
            "        While mathematically equivalent to log(softmax(x)), doing these two\n",
            "        operations separately is slower and numerically unstable. This function\n",
            "        uses an alternative formulation to compute the output and gradient correctly.\n",
            "\n",
            "        See :class:`~torch.nn.LogSoftmax` for more details.\n",
            "\n",
            "        Args:\n",
            "            input (Tensor): input\n",
            "            dim (int): A dimension along which log_softmax will be computed.\n",
            "            dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.\n",
            "              If specified, the input tensor is cast to :attr:`dtype` before the operation\n",
            "              is performed. This is useful for preventing data type overflows. Default: None.\n",
            "\n",
            "    logsigmoid = log_sigmoid(...)\n",
            "        logsigmoid(input) -> Tensor\n",
            "\n",
            "        Applies element-wise :math:`\\text{LogSigmoid}(x_i) = \\log \\left(\\frac{1}{1 + \\exp(-x_i)}\\right)`\n",
            "\n",
            "        See :class:`~torch.nn.LogSigmoid` for more details.\n",
            "\n",
            "    lp_pool1d(input: torch.Tensor, norm_type: Union[int, float], kernel_size: int, stride: NoneType = None, ceil_mode: bool = False) -> torch.Tensor\n",
            "        Apply a 1D power-average pooling over an input signal composed of several input planes.\n",
            "\n",
            "        If the sum of all inputs to the power of `p` is\n",
            "        zero, the gradient is set to zero as well.\n",
            "\n",
            "        See :class:`~torch.nn.LPPool1d` for details.\n",
            "\n",
            "    lp_pool2d(input: torch.Tensor, norm_type: Union[int, float], kernel_size: None, stride: NoneType = None, ceil_mode: bool = False) -> torch.Tensor\n",
            "        Apply a 2D power-average pooling over an input signal composed of several input planes.\n",
            "\n",
            "        If the sum of all inputs to the power of `p` is\n",
            "        zero, the gradient is set to zero as well.\n",
            "\n",
            "        See :class:`~torch.nn.LPPool2d` for details.\n",
            "\n",
            "    lp_pool3d(input: torch.Tensor, norm_type: Union[int, float], kernel_size: None, stride: NoneType = None, ceil_mode: bool = False) -> torch.Tensor\n",
            "        Apply a 3D power-average pooling over an input signal composed of several input planes.\n",
            "\n",
            "        If the sum of all inputs to the power of `p` is\n",
            "        zero, the gradient is set to zero as well.\n",
            "\n",
            "        See :class:`~torch.nn.LPPool3d` for details.\n",
            "\n",
            "    margin_ranking_loss(input1: torch.Tensor, input2: torch.Tensor, target: torch.Tensor, margin: float = 0, size_average: Optional[bool] = None, reduce: Optional[bool] = None, reduction: str = 'mean') -> torch.Tensor\n",
            "        margin_ranking_loss(input1, input2, target, margin=0, size_average=None, reduce=None, reduction='mean') -> Tensor\n",
            "\n",
            "        See :class:`~torch.nn.MarginRankingLoss` for details.\n",
            "\n",
            "    max_pool1d(*args, **kwargs)\n",
            "        max_pool1d(input, kernel_size, stride=None, padding=0, dilation=1, ceil_mode=False, return_indices=False)\n",
            "\n",
            "        Applies a 1D max pooling over an input signal composed of several input\n",
            "        planes.\n",
            "\n",
            "        .. note::\n",
            "            The order of :attr:`ceil_mode` and :attr:`return_indices` is different from\n",
            "            what seen in :class:`~torch.nn.MaxPool1d`, and will change in a future release.\n",
            "\n",
            "        See :class:`~torch.nn.MaxPool1d` for details.\n",
            "\n",
            "        Args:\n",
            "            input: input tensor of shape :math:`(\\text{minibatch} , \\text{in\\_channels} , iW)`, minibatch dim optional.\n",
            "            kernel_size: the size of the window. Can be a single number or a\n",
            "                tuple `(kW,)`\n",
            "            stride: the stride of the window. Can be a single number or a tuple\n",
            "                `(sW,)`. Default: :attr:`kernel_size`\n",
            "            padding: Implicit negative infinity padding to be added on both sides, must be >= 0 and <= kernel_size / 2.\n",
            "            dilation: The stride between elements within a sliding window, must be > 0.\n",
            "            ceil_mode: If ``True``, will use `ceil` instead of `floor` to compute the output shape. This\n",
            "                       ensures that every element in the input tensor is covered by a sliding window.\n",
            "            return_indices: If ``True``, will return the argmax along with the max values.\n",
            "                            Useful for :class:`torch.nn.functional.max_unpool1d` later\n",
            "\n",
            "    max_pool1d_with_indices(input: torch.Tensor, kernel_size: None, stride: NoneType = None, padding: None = 0, dilation: None = 1, ceil_mode: bool = False, return_indices: bool = False) -> tuple[torch.Tensor, torch.Tensor]\n",
            "        max_pool1d(input, kernel_size, stride=None, padding=0, dilation=1, ceil_mode=False, return_indices=False)\n",
            "\n",
            "        Applies a 1D max pooling over an input signal composed of several input\n",
            "        planes.\n",
            "\n",
            "        .. note::\n",
            "            The order of :attr:`ceil_mode` and :attr:`return_indices` is different from\n",
            "            what seen in :class:`~torch.nn.MaxPool1d`, and will change in a future release.\n",
            "\n",
            "        See :class:`~torch.nn.MaxPool1d` for details.\n",
            "\n",
            "        Args:\n",
            "            input: input tensor of shape :math:`(\\text{minibatch} , \\text{in\\_channels} , iW)`, minibatch dim optional.\n",
            "            kernel_size: the size of the window. Can be a single number or a\n",
            "                tuple `(kW,)`\n",
            "            stride: the stride of the window. Can be a single number or a tuple\n",
            "                `(sW,)`. Default: :attr:`kernel_size`\n",
            "            padding: Implicit negative infinity padding to be added on both sides, must be >= 0 and <= kernel_size / 2.\n",
            "            dilation: The stride between elements within a sliding window, must be > 0.\n",
            "            ceil_mode: If ``True``, will use `ceil` instead of `floor` to compute the output shape. This\n",
            "                       ensures that every element in the input tensor is covered by a sliding window.\n",
            "            return_indices: If ``True``, will return the argmax along with the max values.\n",
            "                            Useful for :class:`torch.nn.functional.max_unpool1d` later\n",
            "\n",
            "    max_pool2d(*args, **kwargs)\n",
            "        max_pool2d(input, kernel_size, stride=None, padding=0, dilation=1, ceil_mode=False, return_indices=False)\n",
            "\n",
            "        Applies a 2D max pooling over an input signal composed of several input\n",
            "        planes.\n",
            "\n",
            "        .. note::\n",
            "            The order of :attr:`ceil_mode` and :attr:`return_indices` is different from\n",
            "            what seen in :class:`~torch.nn.MaxPool2d`, and will change in a future release.\n",
            "\n",
            "        See :class:`~torch.nn.MaxPool2d` for details.\n",
            "\n",
            "        Args:\n",
            "            input: input tensor :math:`(\\text{minibatch} , \\text{in\\_channels} , iH , iW)`, minibatch dim optional.\n",
            "            kernel_size: size of the pooling region. Can be a single number or a\n",
            "                tuple `(kH, kW)`\n",
            "            stride: stride of the pooling operation. Can be a single number or a\n",
            "                tuple `(sH, sW)`. Default: :attr:`kernel_size`\n",
            "            padding: Implicit negative infinity padding to be added on both sides, must be >= 0 and <= kernel_size / 2.\n",
            "            dilation: The stride between elements within a sliding window, must be > 0.\n",
            "            ceil_mode: If ``True``, will use `ceil` instead of `floor` to compute the output shape. This\n",
            "                       ensures that every element in the input tensor is covered by a sliding window.\n",
            "            return_indices: If ``True``, will return the argmax along with the max values.\n",
            "                            Useful for :class:`torch.nn.functional.max_unpool2d` later\n",
            "\n",
            "    max_pool2d_with_indices(input: torch.Tensor, kernel_size: None, stride: NoneType = None, padding: None = 0, dilation: None = 1, ceil_mode: bool = False, return_indices: bool = False) -> tuple[torch.Tensor, torch.Tensor]\n",
            "        max_pool2d(input, kernel_size, stride=None, padding=0, dilation=1, ceil_mode=False, return_indices=False)\n",
            "\n",
            "        Applies a 2D max pooling over an input signal composed of several input\n",
            "        planes.\n",
            "\n",
            "        .. note::\n",
            "            The order of :attr:`ceil_mode` and :attr:`return_indices` is different from\n",
            "            what seen in :class:`~torch.nn.MaxPool2d`, and will change in a future release.\n",
            "\n",
            "        See :class:`~torch.nn.MaxPool2d` for details.\n",
            "\n",
            "        Args:\n",
            "            input: input tensor :math:`(\\text{minibatch} , \\text{in\\_channels} , iH , iW)`, minibatch dim optional.\n",
            "            kernel_size: size of the pooling region. Can be a single number or a\n",
            "                tuple `(kH, kW)`\n",
            "            stride: stride of the pooling operation. Can be a single number or a\n",
            "                tuple `(sH, sW)`. Default: :attr:`kernel_size`\n",
            "            padding: Implicit negative infinity padding to be added on both sides, must be >= 0 and <= kernel_size / 2.\n",
            "            dilation: The stride between elements within a sliding window, must be > 0.\n",
            "            ceil_mode: If ``True``, will use `ceil` instead of `floor` to compute the output shape. This\n",
            "                       ensures that every element in the input tensor is covered by a sliding window.\n",
            "            return_indices: If ``True``, will return the argmax along with the max values.\n",
            "                            Useful for :class:`torch.nn.functional.max_unpool2d` later\n",
            "\n",
            "    max_pool3d(*args, **kwargs)\n",
            "        max_pool3d(input, kernel_size, stride=None, padding=0, dilation=1, ceil_mode=False, return_indices=False)\n",
            "\n",
            "        Applies a 3D max pooling over an input signal composed of several input\n",
            "        planes.\n",
            "\n",
            "        .. note::\n",
            "            The order of :attr:`ceil_mode` and :attr:`return_indices` is different from\n",
            "            what seen in :class:`~torch.nn.MaxPool3d`, and will change in a future release.\n",
            "\n",
            "        See :class:`~torch.nn.MaxPool3d` for details.\n",
            "\n",
            "        Args:\n",
            "            input: input tensor :math:`(\\text{minibatch} , \\text{in\\_channels} , iD, iH , iW)`, minibatch dim optional.\n",
            "            kernel_size: size of the pooling region. Can be a single number or a\n",
            "                         tuple `(kT, kH, kW)`\n",
            "            stride: stride of the pooling operation. Can be a single number or a\n",
            "                    tuple `(sT, sH, sW)`. Default: :attr:`kernel_size`\n",
            "            padding: Implicit negative infinity padding to be added on both sides, must be >= 0 and <= kernel_size / 2.\n",
            "            dilation: The stride between elements within a sliding window, must be > 0.\n",
            "            ceil_mode: If ``True``, will use `ceil` instead of `floor` to compute the output shape. This\n",
            "                       ensures that every element in the input tensor is covered by a sliding window.\n",
            "            return_indices: If ``True``, will return the argmax along with the max values.\n",
            "                            Useful for :class:`torch.nn.functional.max_unpool3d` later\n",
            "\n",
            "    max_pool3d_with_indices(input: torch.Tensor, kernel_size: None, stride: NoneType = None, padding: None = 0, dilation: None = 1, ceil_mode: bool = False, return_indices: bool = False) -> tuple[torch.Tensor, torch.Tensor]\n",
            "        max_pool3d(input, kernel_size, stride=None, padding=0, dilation=1, ceil_mode=False, return_indices=False)\n",
            "\n",
            "        Applies a 3D max pooling over an input signal composed of several input\n",
            "        planes.\n",
            "\n",
            "        .. note::\n",
            "            The order of :attr:`ceil_mode` and :attr:`return_indices` is different from\n",
            "            what seen in :class:`~torch.nn.MaxPool3d`, and will change in a future release.\n",
            "\n",
            "        See :class:`~torch.nn.MaxPool3d` for details.\n",
            "\n",
            "        Args:\n",
            "            input: input tensor :math:`(\\text{minibatch} , \\text{in\\_channels} , iD, iH , iW)`, minibatch dim optional.\n",
            "            kernel_size: size of the pooling region. Can be a single number or a\n",
            "                         tuple `(kT, kH, kW)`\n",
            "            stride: stride of the pooling operation. Can be a single number or a\n",
            "                    tuple `(sT, sH, sW)`. Default: :attr:`kernel_size`\n",
            "            padding: Implicit negative infinity padding to be added on both sides, must be >= 0 and <= kernel_size / 2.\n",
            "            dilation: The stride between elements within a sliding window, must be > 0.\n",
            "            ceil_mode: If ``True``, will use `ceil` instead of `floor` to compute the output shape. This\n",
            "                       ensures that every element in the input tensor is covered by a sliding window.\n",
            "            return_indices: If ``True``, will return the argmax along with the max values.\n",
            "                            Useful for :class:`torch.nn.functional.max_unpool3d` later\n",
            "\n",
            "    max_unpool1d(input: torch.Tensor, indices: torch.Tensor, kernel_size: None, stride: NoneType = None, padding: None = 0, output_size: NoneType = None) -> torch.Tensor\n",
            "        Compute a partial inverse of :class:`MaxPool1d`.\n",
            "\n",
            "        See :class:`~torch.nn.MaxUnpool1d` for details.\n",
            "\n",
            "    max_unpool2d(input: torch.Tensor, indices: torch.Tensor, kernel_size: None, stride: NoneType = None, padding: None = 0, output_size: NoneType = None) -> torch.Tensor\n",
            "        Compute a partial inverse of :class:`MaxPool2d`.\n",
            "\n",
            "        See :class:`~torch.nn.MaxUnpool2d` for details.\n",
            "\n",
            "    max_unpool3d(input: torch.Tensor, indices: torch.Tensor, kernel_size: None, stride: NoneType = None, padding: None = 0, output_size: NoneType = None) -> torch.Tensor\n",
            "        Compute a partial inverse of :class:`MaxPool3d`.\n",
            "\n",
            "        See :class:`~torch.nn.MaxUnpool3d` for details.\n",
            "\n",
            "    mish(input: torch.Tensor, inplace: bool = False) -> torch.Tensor\n",
            "        Apply the Mish function, element-wise.\n",
            "\n",
            "        Mish: A Self Regularized Non-Monotonic Neural Activation Function.\n",
            "\n",
            "        .. math::\n",
            "            \\text{Mish}(x) = x * \\text{Tanh}(\\text{Softplus}(x))\n",
            "\n",
            "        .. note::\n",
            "            See `Mish: A Self Regularized Non-Monotonic Neural Activation Function <https://arxiv.org/abs/1908.08681>`_\n",
            "\n",
            "        See :class:`~torch.nn.Mish` for more details.\n",
            "\n",
            "    mse_loss(input: torch.Tensor, target: torch.Tensor, size_average: Optional[bool] = None, reduce: Optional[bool] = None, reduction: str = 'mean', weight: Optional[torch.Tensor] = None) -> torch.Tensor\n",
            "        mse_loss(input, target, size_average=None, reduce=None, reduction='mean', weight=None) -> Tensor\n",
            "\n",
            "        Measures the element-wise mean squared error, with optional weighting.\n",
            "\n",
            "        Args:\n",
            "            input (Tensor): Predicted values.\n",
            "            target (Tensor): Ground truth values.\n",
            "            size_average (bool, optional): Deprecated (use reduction).\n",
            "            reduce (bool, optional): Deprecated (use reduction).\n",
            "            reduction (str, optional): Specifies the reduction to apply to the output:\n",
            "                                       'none' | 'mean' | 'sum'. 'mean': the mean of the output is taken.\n",
            "                                       'sum': the output will be summed. 'none': no reduction will be applied.\n",
            "                                       Default: 'mean'.\n",
            "            weight (Tensor, optional): Weights for each sample. Default: None.\n",
            "\n",
            "        Returns:\n",
            "            Tensor: Mean Squared Error loss (optionally weighted).\n",
            "\n",
            "    multi_head_attention_forward(query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, embed_dim_to_check: int, num_heads: int, in_proj_weight: Optional[torch.Tensor], in_proj_bias: Optional[torch.Tensor], bias_k: Optional[torch.Tensor], bias_v: Optional[torch.Tensor], add_zero_attn: bool, dropout_p: float, out_proj_weight: torch.Tensor, out_proj_bias: Optional[torch.Tensor], training: bool = True, key_padding_mask: Optional[torch.Tensor] = None, need_weights: bool = True, attn_mask: Optional[torch.Tensor] = None, use_separate_proj_weight: bool = False, q_proj_weight: Optional[torch.Tensor] = None, k_proj_weight: Optional[torch.Tensor] = None, v_proj_weight: Optional[torch.Tensor] = None, static_k: Optional[torch.Tensor] = None, static_v: Optional[torch.Tensor] = None, average_attn_weights: bool = True, is_causal: bool = False) -> tuple[torch.Tensor, typing.Optional[torch.Tensor]]\n",
            "        Forward method for MultiHeadAttention.\n",
            "\n",
            "        .. note::\n",
            "            See `this tutorial <https://pytorch.org/tutorials/intermediate/transformer_building_blocks.html>`_\n",
            "            for an in depth discussion of the performant building blocks PyTorch offers for building your own\n",
            "            transformer layers.\n",
            "\n",
            "        See :class:`torch.nn.MultiheadAttention` for details.\n",
            "\n",
            "        Args:\n",
            "            query, key, value: map a query and a set of key-value pairs to an output.\n",
            "                See \"Attention Is All You Need\" for more details.\n",
            "            embed_dim_to_check: total dimension of the model.\n",
            "            num_heads: parallel attention heads.\n",
            "            in_proj_weight, in_proj_bias: input projection weight and bias.\n",
            "            bias_k, bias_v: bias of the key and value sequences to be added at dim=0.\n",
            "            add_zero_attn: add a new batch of zeros to the key and\n",
            "                           value sequences at dim=1.\n",
            "            dropout_p: probability of an element to be zeroed.\n",
            "            out_proj_weight, out_proj_bias: the output projection weight and bias.\n",
            "            training: apply dropout if is ``True``.\n",
            "            key_padding_mask: if provided, specified padding elements in the key will\n",
            "                be ignored by the attention. This is an binary mask. When the value is True,\n",
            "                the corresponding value on the attention layer will be filled with -inf.\n",
            "            need_weights: output attn_output_weights.\n",
            "                Default: `True`\n",
            "                Note: `needs_weight` defaults to `True`, but should be set to `False`\n",
            "                For best performance when attention weights are not needed.\n",
            "                *Setting needs_weights to `True`\n",
            "                leads to a significant performance degradation.*\n",
            "            attn_mask: 2D or 3D mask that prevents attention to certain positions. A 2D mask will be broadcasted for all\n",
            "                the batches while a 3D mask allows to specify a different mask for the entries of each batch.\n",
            "            is_causal: If specified, applies a causal mask as attention mask, and ignores\n",
            "                attn_mask for computing scaled dot product attention.\n",
            "                Default: ``False``.\n",
            "                .. warning::\n",
            "                    is_causal is provides a hint that the attn_mask is the\n",
            "                    causal mask.Providing incorrect hints can result in\n",
            "                    incorrect execution, including forward and backward\n",
            "                    compatibility.\n",
            "            use_separate_proj_weight: the function accept the proj. weights for query, key,\n",
            "                and value in different forms. If false, in_proj_weight will be used, which is\n",
            "                a combination of q_proj_weight, k_proj_weight, v_proj_weight.\n",
            "            q_proj_weight, k_proj_weight, v_proj_weight, in_proj_bias: input projection weight and bias.\n",
            "            static_k, static_v: static key and value used for attention operators.\n",
            "            average_attn_weights: If true, indicates that the returned ``attn_weights`` should be averaged across heads.\n",
            "                Otherwise, ``attn_weights`` are provided separately per head. Note that this flag only has an effect\n",
            "                when ``need_weights=True.``. Default: True\n",
            "\n",
            "\n",
            "        Shape:\n",
            "            Inputs:\n",
            "            - query: :math:`(L, E)` or :math:`(L, N, E)` where L is the target sequence length, N is the batch size, E is\n",
            "              the embedding dimension.\n",
            "            - key: :math:`(S, E)` or :math:`(S, N, E)`, where S is the source sequence length, N is the batch size, E is\n",
            "              the embedding dimension.\n",
            "            - value: :math:`(S, E)` or :math:`(S, N, E)` where S is the source sequence length, N is the batch size, E is\n",
            "              the embedding dimension.\n",
            "            - key_padding_mask: :math:`(S)` or :math:`(N, S)` where N is the batch size, S is the source sequence length.\n",
            "              If a FloatTensor is provided, it will be directly added to the value.\n",
            "              If a BoolTensor is provided, the positions with the\n",
            "              value of ``True`` will be ignored while the position with the value of ``False`` will be unchanged.\n",
            "            - attn_mask: 2D mask :math:`(L, S)` where L is the target sequence length, S is the source sequence length.\n",
            "              3D mask :math:`(N*num_heads, L, S)` where N is the batch size, L is the target sequence length,\n",
            "              S is the source sequence length. attn_mask ensures that position i is allowed to attend the unmasked\n",
            "              positions. If a BoolTensor is provided, positions with ``True``\n",
            "              are not allowed to attend while ``False`` values will be unchanged. If a FloatTensor\n",
            "              is provided, it will be added to the attention weight.\n",
            "            - static_k: :math:`(N*num_heads, S, E/num_heads)`, where S is the source sequence length,\n",
            "              N is the batch size, E is the embedding dimension. E/num_heads is the head dimension.\n",
            "            - static_v: :math:`(N*num_heads, S, E/num_heads)`, where S is the source sequence length,\n",
            "              N is the batch size, E is the embedding dimension. E/num_heads is the head dimension.\n",
            "\n",
            "            Outputs:\n",
            "            - attn_output: :math:`(L, E)` or :math:`(L, N, E)` where L is the target sequence length, N is the batch size,\n",
            "              E is the embedding dimension.\n",
            "            - attn_output_weights: Only returned when ``need_weights=True``. If ``average_attn_weights=True``, returns\n",
            "              attention weights averaged across heads of shape :math:`(L, S)` when input is unbatched or\n",
            "              :math:`(N, L, S)`, where :math:`N` is the batch size, :math:`L` is the target sequence length, and\n",
            "              :math:`S` is the source sequence length. If ``average_attn_weights=False``, returns attention weights per\n",
            "              head of shape :math:`(num_heads, L, S)` when input is unbatched or :math:`(N, num_heads, L, S)`.\n",
            "\n",
            "    multi_margin_loss(input: torch.Tensor, target: torch.Tensor, p: int = 1, margin: float = 1.0, weight: Optional[torch.Tensor] = None, size_average: Optional[bool] = None, reduce: Optional[bool] = None, reduction: str = 'mean') -> torch.Tensor\n",
            "        multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None, reduce=None, reduction='mean') -> Tensor\n",
            "\n",
            "        See :class:`~torch.nn.MultiMarginLoss` for details.\n",
            "\n",
            "    multilabel_margin_loss(input: torch.Tensor, target: torch.Tensor, size_average: Optional[bool] = None, reduce: Optional[bool] = None, reduction: str = 'mean') -> torch.Tensor\n",
            "        multilabel_margin_loss(input, target, size_average=None, reduce=None, reduction='mean') -> Tensor\n",
            "\n",
            "        See :class:`~torch.nn.MultiLabelMarginLoss` for details.\n",
            "\n",
            "    multilabel_soft_margin_loss(input: torch.Tensor, target: torch.Tensor, weight: Optional[torch.Tensor] = None, size_average: Optional[bool] = None, reduce: Optional[bool] = None, reduction: str = 'mean') -> torch.Tensor\n",
            "        multilabel_soft_margin_loss(input, target, weight=None, size_average=None, reduce=None, reduction='mean') -> Tensor\n",
            "\n",
            "        See :class:`~torch.nn.MultiLabelSoftMarginLoss` for details.\n",
            "\n",
            "    native_channel_shuffle(...)\n",
            "        native_channel_shuffle(input, groups) -> Tensor\n",
            "\n",
            "        Native kernel level implementation of the `channel_shuffle`.\n",
            "        This function might become private in future releases, use with caution.\n",
            "\n",
            "        Divide the channels in a tensor of shape :math:`(*, C , H, W)`\n",
            "        into g groups and rearrange them as :math:`(*, C \\frac g, g, H, W)`,\n",
            "        while keeping the original tensor shape.\n",
            "\n",
            "        See :class:`~torch.nn.ChannelShuffle` for details.\n",
            "\n",
            "        Args:\n",
            "            input (Tensor): the input tensor\n",
            "            groups (int): number of groups to divide channels in and rearrange.\n",
            "\n",
            "        Examples::\n",
            "\n",
            "            >>> input = torch.randn(1, 4, 2, 2)\n",
            "            >>> print(input)\n",
            "            [[[[1, 2],\n",
            "               [3, 4]],\n",
            "              [[5, 6],\n",
            "               [7, 8]],\n",
            "              [[9, 10],\n",
            "               [11, 12]],\n",
            "              [[13, 14],\n",
            "               [15, 16]],\n",
            "             ]]\n",
            "            >>> output = torch.nn.functional.native_channel_shuffle(input, 2)\n",
            "            >>> print(output)\n",
            "            [[[[1, 2],\n",
            "               [3, 4]],\n",
            "              [[9, 10],\n",
            "               [11, 12]],\n",
            "              [[5, 6],\n",
            "               [7, 8]],\n",
            "              [[13, 14],\n",
            "               [15, 16]],\n",
            "             ]]\n",
            "\n",
            "    nll_loss(input: torch.Tensor, target: torch.Tensor, weight: Optional[torch.Tensor] = None, size_average: Optional[bool] = None, ignore_index: int = -100, reduce: Optional[bool] = None, reduction: str = 'mean') -> torch.Tensor\n",
            "        Compute the negative log likelihood loss.\n",
            "\n",
            "        See :class:`~torch.nn.NLLLoss` for details.\n",
            "\n",
            "        Args:\n",
            "            input: :math:`(N, C)` where `C = number of classes` or :math:`(N, C, H, W)`\n",
            "                in case of 2D Loss, or :math:`(N, C, d_1, d_2, ..., d_K)` where :math:`K \\geq 1`\n",
            "                in the case of K-dimensional loss. `input` is expected to be log-probabilities.\n",
            "            target: :math:`(N)` where each value is :math:`0 \\leq \\text{targets}[i] \\leq C-1`,\n",
            "                or :math:`(N, d_1, d_2, ..., d_K)` where :math:`K \\geq 1` for\n",
            "                K-dimensional loss.\n",
            "            weight (Tensor, optional): a manual rescaling weight given to each\n",
            "                class. If given, has to be a Tensor of size `C`\n",
            "            size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,\n",
            "                the losses are averaged over each loss element in the batch. Note that for\n",
            "                some losses, there multiple elements per sample. If the field :attr:`size_average`\n",
            "                is set to ``False``, the losses are instead summed for each minibatch. Ignored\n",
            "                when reduce is ``False``. Default: ``True``\n",
            "            ignore_index (int, optional): Specifies a target value that is ignored\n",
            "                and does not contribute to the input gradient. When :attr:`size_average` is\n",
            "                ``True``, the loss is averaged over non-ignored targets. Default: -100\n",
            "            reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the\n",
            "                losses are averaged or summed over observations for each minibatch depending\n",
            "                on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per\n",
            "                batch element instead and ignores :attr:`size_average`. Default: ``True``\n",
            "            reduction (str, optional): Specifies the reduction to apply to the output:\n",
            "                ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,\n",
            "                ``'mean'``: the sum of the output will be divided by the number of\n",
            "                elements in the output, ``'sum'``: the output will be summed. Note: :attr:`size_average`\n",
            "                and :attr:`reduce` are in the process of being deprecated, and in the meantime,\n",
            "                specifying either of those two args will override :attr:`reduction`. Default: ``'mean'``\n",
            "\n",
            "        Example::\n",
            "\n",
            "            >>> # input is of size N x C = 3 x 5\n",
            "            >>> input = torch.randn(3, 5, requires_grad=True)\n",
            "            >>> # each element in target has to have 0 <= value < C\n",
            "            >>> target = torch.tensor([1, 0, 4])\n",
            "            >>> output = F.nll_loss(F.log_softmax(input, dim=1), target)\n",
            "            >>> output.backward()\n",
            "\n",
            "    normalize(input: torch.Tensor, p: float = 2.0, dim: int = 1, eps: float = 1e-12, out: Optional[torch.Tensor] = None) -> torch.Tensor\n",
            "        Perform :math:`L_p` normalization of inputs over specified dimension.\n",
            "\n",
            "        For a tensor :attr:`input` of sizes :math:`(n_0, ..., n_{dim}, ..., n_k)`, each\n",
            "        :math:`n_{dim}` -element vector :math:`v` along dimension :attr:`dim` is transformed as\n",
            "\n",
            "        .. math::\n",
            "            v = \\frac{v}{\\max(\\lVert v \\rVert_p, \\epsilon)}.\n",
            "\n",
            "        With the default arguments it uses the Euclidean norm over vectors along dimension :math:`1` for normalization.\n",
            "\n",
            "        Args:\n",
            "            input: input tensor of any shape\n",
            "            p (float): the exponent value in the norm formulation. Default: 2\n",
            "            dim (int or tuple of ints): the dimension to reduce. Default: 1\n",
            "            eps (float): small value to avoid division by zero. Default: 1e-12\n",
            "            out (Tensor, optional): the output tensor. If :attr:`out` is used, this\n",
            "                                    operation won't be differentiable.\n",
            "\n",
            "    one_hot(...)\n",
            "        one_hot(tensor, num_classes=-1) -> LongTensor\n",
            "\n",
            "        Takes LongTensor with index values of shape ``(*)`` and returns a tensor\n",
            "        of shape ``(*, num_classes)`` that have zeros everywhere except where the\n",
            "        index of last dimension matches the corresponding value of the input tensor,\n",
            "        in which case it will be 1.\n",
            "\n",
            "        See also `One-hot on Wikipedia`_ .\n",
            "\n",
            "        .. _One-hot on Wikipedia:\n",
            "            https://en.wikipedia.org/wiki/One-hot\n",
            "\n",
            "        Arguments:\n",
            "            tensor (LongTensor): class values of any shape.\n",
            "            num_classes (int, optional):  Total number of classes. If set to -1, the number\n",
            "                of classes will be inferred as one greater than the largest class\n",
            "                value in the input tensor. Default: -1\n",
            "\n",
            "        Returns:\n",
            "            LongTensor that has one more dimension with 1 values at the\n",
            "            index of last dimension indicated by the input, and 0 everywhere\n",
            "            else.\n",
            "\n",
            "        Examples:\n",
            "            >>> F.one_hot(torch.arange(0, 5) % 3)\n",
            "            tensor([[1, 0, 0],\n",
            "                    [0, 1, 0],\n",
            "                    [0, 0, 1],\n",
            "                    [1, 0, 0],\n",
            "                    [0, 1, 0]])\n",
            "            >>> F.one_hot(torch.arange(0, 5) % 3, num_classes=5)\n",
            "            tensor([[1, 0, 0, 0, 0],\n",
            "                    [0, 1, 0, 0, 0],\n",
            "                    [0, 0, 1, 0, 0],\n",
            "                    [1, 0, 0, 0, 0],\n",
            "                    [0, 1, 0, 0, 0]])\n",
            "            >>> F.one_hot(torch.arange(0, 6).view(3,2) % 3)\n",
            "            tensor([[[1, 0, 0],\n",
            "                     [0, 1, 0]],\n",
            "                    [[0, 0, 1],\n",
            "                     [1, 0, 0]],\n",
            "                    [[0, 1, 0],\n",
            "                     [0, 0, 1]]])\n",
            "\n",
            "    pad(input: torch.Tensor, pad: list[int], mode: str = 'constant', value: Optional[float] = None) -> torch.Tensor\n",
            "        pad(input, pad, mode=\"constant\", value=None) -> Tensor\n",
            "\n",
            "        Pads tensor.\n",
            "\n",
            "        Padding size:\n",
            "            The padding size by which to pad some dimensions of :attr:`input`\n",
            "            are described starting from the last dimension and moving forward.\n",
            "            :math:`\\left\\lfloor\\frac{\\text{len(pad)}}{2}\\right\\rfloor` dimensions\n",
            "            of ``input`` will be padded.\n",
            "            For example, to pad only the last dimension of the input tensor, then\n",
            "            :attr:`pad` has the form\n",
            "            :math:`(\\text{padding\\_left}, \\text{padding\\_right})`;\n",
            "            to pad the last 2 dimensions of the input tensor, then use\n",
            "            :math:`(\\text{padding\\_left}, \\text{padding\\_right},`\n",
            "            :math:`\\text{padding\\_top}, \\text{padding\\_bottom})`;\n",
            "            to pad the last 3 dimensions, use\n",
            "            :math:`(\\text{padding\\_left}, \\text{padding\\_right},`\n",
            "            :math:`\\text{padding\\_top}, \\text{padding\\_bottom}`\n",
            "            :math:`\\text{padding\\_front}, \\text{padding\\_back})`.\n",
            "\n",
            "        Padding mode:\n",
            "            See :class:`torch.nn.CircularPad2d`, :class:`torch.nn.ConstantPad2d`,\n",
            "            :class:`torch.nn.ReflectionPad2d`, and :class:`torch.nn.ReplicationPad2d`\n",
            "            for concrete examples on how each of the padding modes works. Constant\n",
            "            padding is implemented for arbitrary dimensions. Circular, replicate and\n",
            "            reflection padding are implemented for padding the last 3 dimensions of a\n",
            "            4D or 5D input tensor, the last 2 dimensions of a 3D or 4D input tensor,\n",
            "            or the last dimension of a 2D or 3D input tensor.\n",
            "\n",
            "        Note:\n",
            "            When using the CUDA backend, this operation may induce nondeterministic\n",
            "            behaviour in its backward pass that is not easily switched off.\n",
            "            Please see the notes on :doc:`/notes/randomness` for background.\n",
            "\n",
            "        Args:\n",
            "            input (Tensor): N-dimensional tensor\n",
            "            pad (tuple): m-elements tuple, where\n",
            "                :math:`\\frac{m}{2} \\leq` input dimensions and :math:`m` is even.\n",
            "            mode: ``'constant'``, ``'reflect'``, ``'replicate'`` or ``'circular'``.\n",
            "                Default: ``'constant'``\n",
            "            value: fill value for ``'constant'`` padding. Default: ``0``\n",
            "\n",
            "        Examples::\n",
            "\n",
            "            >>> t4d = torch.empty(3, 3, 4, 2)\n",
            "            >>> p1d = (1, 1) # pad last dim by 1 on each side\n",
            "            >>> out = F.pad(t4d, p1d, \"constant\", 0)  # effectively zero padding\n",
            "            >>> print(out.size())\n",
            "            torch.Size([3, 3, 4, 4])\n",
            "            >>> p2d = (1, 1, 2, 2) # pad last dim by (1, 1) and 2nd to last by (2, 2)\n",
            "            >>> out = F.pad(t4d, p2d, \"constant\", 0)\n",
            "            >>> print(out.size())\n",
            "            torch.Size([3, 3, 8, 4])\n",
            "            >>> t4d = torch.empty(3, 3, 4, 2)\n",
            "            >>> p3d = (0, 1, 2, 1, 3, 3) # pad by (0, 1), (2, 1), and (3, 3)\n",
            "            >>> out = F.pad(t4d, p3d, \"constant\", 0)\n",
            "            >>> print(out.size())\n",
            "            torch.Size([3, 9, 7, 3])\n",
            "\n",
            "    pairwise_distance(...)\n",
            "        pairwise_distance(x1, x2, p=2.0, eps=1e-6, keepdim=False) -> Tensor\n",
            "\n",
            "        See :class:`torch.nn.PairwiseDistance` for details\n",
            "\n",
            "    pdist(...)\n",
            "        pdist(input, p=2) -> Tensor\n",
            "\n",
            "        Computes the p-norm distance between every pair of row vectors in the input.\n",
            "        This is identical to the upper triangular portion, excluding the diagonal, of\n",
            "        `torch.norm(input[:, None] - input, dim=2, p=p)`. This function will be faster\n",
            "        if the rows are contiguous.\n",
            "\n",
            "        If input has shape :math:`N \\times M` then the output will have shape\n",
            "        :math:`\\frac{1}{2} N (N - 1)`.\n",
            "\n",
            "        This function is equivalent to ``scipy.spatial.distance.pdist(input,\n",
            "        'minkowski', p=p)`` if :math:`p \\in (0, \\infty)`. When :math:`p = 0` it is\n",
            "        equivalent to ``scipy.spatial.distance.pdist(input, 'hamming') * M``.\n",
            "        When :math:`p = \\infty`, the closest scipy function is\n",
            "        ``scipy.spatial.distance.pdist(xn, lambda x, y: np.abs(x - y).max())``.\n",
            "\n",
            "        Args:\n",
            "            input: input tensor of shape :math:`N \\times M`.\n",
            "            p: p value for the p-norm distance to calculate between each vector pair\n",
            "                :math:`\\in [0, \\infty]`.\n",
            "\n",
            "    pixel_shuffle(...)\n",
            "        pixel_shuffle(input, upscale_factor) -> Tensor\n",
            "\n",
            "        Rearranges elements in a tensor of shape :math:`(*, C \\times r^2, H, W)` to a\n",
            "        tensor of shape :math:`(*, C, H \\times r, W \\times r)`, where r is the :attr:`upscale_factor`.\n",
            "\n",
            "        See :class:`~torch.nn.PixelShuffle` for details.\n",
            "\n",
            "        Args:\n",
            "            input (Tensor): the input tensor\n",
            "            upscale_factor (int): factor to increase spatial resolution by\n",
            "\n",
            "        Examples::\n",
            "\n",
            "            >>> input = torch.randn(1, 9, 4, 4)\n",
            "            >>> output = torch.nn.functional.pixel_shuffle(input, 3)\n",
            "            >>> print(output.size())\n",
            "            torch.Size([1, 1, 12, 12])\n",
            "\n",
            "    pixel_unshuffle(...)\n",
            "        pixel_unshuffle(input, downscale_factor) -> Tensor\n",
            "\n",
            "        Reverses the :class:`~torch.nn.PixelShuffle` operation by rearranging elements in a\n",
            "        tensor of shape :math:`(*, C, H \\times r, W \\times r)` to a tensor of shape\n",
            "        :math:`(*, C \\times r^2, H, W)`, where r is the :attr:`downscale_factor`.\n",
            "\n",
            "        See :class:`~torch.nn.PixelUnshuffle` for details.\n",
            "\n",
            "        Args:\n",
            "            input (Tensor): the input tensor\n",
            "            downscale_factor (int): factor to increase spatial resolution by\n",
            "\n",
            "        Examples::\n",
            "\n",
            "            >>> input = torch.randn(1, 1, 12, 12)\n",
            "            >>> output = torch.nn.functional.pixel_unshuffle(input, 3)\n",
            "            >>> print(output.size())\n",
            "            torch.Size([1, 9, 4, 4])\n",
            "\n",
            "    poisson_nll_loss(input: torch.Tensor, target: torch.Tensor, log_input: bool = True, full: bool = False, size_average: Optional[bool] = None, eps: float = 1e-08, reduce: Optional[bool] = None, reduction: str = 'mean') -> torch.Tensor\n",
            "        Poisson negative log likelihood loss.\n",
            "\n",
            "        See :class:`~torch.nn.PoissonNLLLoss` for details.\n",
            "\n",
            "        Args:\n",
            "            input: expectation of underlying Poisson distribution.\n",
            "            target: random sample :math:`target \\sim \\text{Poisson}(input)`.\n",
            "            log_input: if ``True`` the loss is computed as\n",
            "                :math:`\\exp(\\text{input}) - \\text{target} * \\text{input}`, if ``False`` then loss is\n",
            "                :math:`\\text{input} - \\text{target} * \\log(\\text{input}+\\text{eps})`. Default: ``True``\n",
            "            full: whether to compute full loss, i. e. to add the Stirling\n",
            "                approximation term. Default: ``False``\n",
            "                :math:`\\text{target} * \\log(\\text{target}) - \\text{target} + 0.5 * \\log(2 * \\pi * \\text{target})`.\n",
            "            size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,\n",
            "                the losses are averaged over each loss element in the batch. Note that for\n",
            "                some losses, there multiple elements per sample. If the field :attr:`size_average`\n",
            "                is set to ``False``, the losses are instead summed for each minibatch. Ignored\n",
            "                when reduce is ``False``. Default: ``True``\n",
            "            eps (float, optional): Small value to avoid evaluation of :math:`\\log(0)` when\n",
            "                :attr:`log_input`\\ =\\ ``False``. Default: 1e-8\n",
            "            reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the\n",
            "                losses are averaged or summed over observations for each minibatch depending\n",
            "                on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per\n",
            "                batch element instead and ignores :attr:`size_average`. Default: ``True``\n",
            "            reduction (str, optional): Specifies the reduction to apply to the output:\n",
            "                ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,\n",
            "                ``'mean'``: the sum of the output will be divided by the number of\n",
            "                elements in the output, ``'sum'``: the output will be summed. Note: :attr:`size_average`\n",
            "                and :attr:`reduce` are in the process of being deprecated, and in the meantime,\n",
            "                specifying either of those two args will override :attr:`reduction`. Default: ``'mean'``\n",
            "\n",
            "    prelu(...)\n",
            "        prelu(input, weight) -> Tensor\n",
            "\n",
            "        Applies element-wise the function\n",
            "        :math:`\\text{PReLU}(x) = \\max(0,x) + \\text{weight} * \\min(0,x)` where weight is a\n",
            "        learnable parameter.\n",
            "\n",
            "        .. note::\n",
            "            `weight` is expected to be a scalar or 1-D tensor. If `weight` is 1-D,\n",
            "            its size must match the number of input channels, determined by\n",
            "            `input.size(1)` when `input.dim() >= 2`, otherwise 1.\n",
            "            In the 1-D case, note that when `input` has dim > 2, `weight` can be expanded\n",
            "            to the shape of `input` in a way that is not possible using normal\n",
            "            :ref:`broadcasting semantics<broadcasting-semantics>`.\n",
            "\n",
            "        See :class:`~torch.nn.PReLU` for more details.\n",
            "\n",
            "    relu(input: torch.Tensor, inplace: bool = False) -> torch.Tensor\n",
            "        relu(input, inplace=False) -> Tensor\n",
            "\n",
            "        Applies the rectified linear unit function element-wise. See\n",
            "        :class:`~torch.nn.ReLU` for more details.\n",
            "\n",
            "    relu6(input: torch.Tensor, inplace: bool = False) -> torch.Tensor\n",
            "        relu6(input, inplace=False) -> Tensor\n",
            "\n",
            "        Applies the element-wise function :math:`\\text{ReLU6}(x) = \\min(\\max(0,x), 6)`.\n",
            "\n",
            "        See :class:`~torch.nn.ReLU6` for more details.\n",
            "\n",
            "    relu_(...)\n",
            "        relu_(input) -> Tensor\n",
            "\n",
            "        In-place version of :func:`~relu`.\n",
            "\n",
            "    rms_norm(input: torch.Tensor, normalized_shape: list[int], weight: Optional[torch.Tensor] = None, eps: Optional[float] = None) -> torch.Tensor\n",
            "        Apply Root Mean Square Layer Normalization.\n",
            "\n",
            "        See :class:`~torch.nn.RMSNorm` for details.\n",
            "\n",
            "    rrelu(input: torch.Tensor, lower: float = 0.125, upper: float = 0.3333333333333333, training: bool = False, inplace: bool = False) -> torch.Tensor\n",
            "        rrelu(input, lower=1./8, upper=1./3, training=False, inplace=False) -> Tensor\n",
            "\n",
            "        Randomized leaky ReLU.\n",
            "\n",
            "        See :class:`~torch.nn.RReLU` for more details.\n",
            "\n",
            "    rrelu_(...)\n",
            "        rrelu_(input, lower=1./8, upper=1./3, training=False) -> Tensor\n",
            "\n",
            "        In-place version of :func:`~rrelu`.\n",
            "\n",
            "    scaled_dot_product_attention(...)\n",
            "        scaled_dot_product_attention(query, key, value, attn_mask=None, dropout_p=0.0,\n",
            "            is_causal=False, scale=None, enable_gqa=False) -> Tensor:\n",
            "\n",
            "        Computes scaled dot product attention on query, key and value tensors, using an optional attention mask if passed,\n",
            "        and applying dropout if a probability greater than 0.0 is specified. The optional scale argument can only be\n",
            "        specified as a keyword argument.\n",
            "\n",
            "        .. code-block:: python\n",
            "\n",
            "            # Efficient implementation equivalent to the following:\n",
            "            def scaled_dot_product_attention(query, key, value, attn_mask=None, dropout_p=0.0,\n",
            "                    is_causal=False, scale=None, enable_gqa=False) -> torch.Tensor:\n",
            "                L, S = query.size(-2), key.size(-2)\n",
            "                scale_factor = 1 / math.sqrt(query.size(-1)) if scale is None else scale\n",
            "                attn_bias = torch.zeros(L, S, dtype=query.dtype, device=query.device)\n",
            "                if is_causal:\n",
            "                    assert attn_mask is None\n",
            "                    temp_mask = torch.ones(L, S, dtype=torch.bool).tril(diagonal=0)\n",
            "                    attn_bias.masked_fill_(temp_mask.logical_not(), float(\"-inf\"))\n",
            "                    attn_bias.to(query.dtype)\n",
            "\n",
            "                if attn_mask is not None:\n",
            "                    if attn_mask.dtype == torch.bool:\n",
            "                        attn_bias.masked_fill_(attn_mask.logical_not(), float(\"-inf\"))\n",
            "                    else:\n",
            "                        attn_bias = attn_mask + attn_bias\n",
            "\n",
            "                if enable_gqa:\n",
            "                    key = key.repeat_interleave(query.size(-3)//key.size(-3), -3)\n",
            "                    value = value.repeat_interleave(query.size(-3)//value.size(-3), -3)\n",
            "\n",
            "                attn_weight = query @ key.transpose(-2, -1) * scale_factor\n",
            "                attn_weight += attn_bias\n",
            "                attn_weight = torch.softmax(attn_weight, dim=-1)\n",
            "                attn_weight = torch.dropout(attn_weight, dropout_p, train=True)\n",
            "                return attn_weight @ value\n",
            "\n",
            "        .. warning::\n",
            "            This function is beta and subject to change.\n",
            "\n",
            "        .. warning::\n",
            "            This function always applies dropout according to the specified ``dropout_p`` argument.\n",
            "            To disable dropout during evaluation, be sure to pass a value of ``0.0`` when the module\n",
            "            that makes the function call is not in training mode.\n",
            "\n",
            "            For example:\n",
            "\n",
            "            .. code-block:: python\n",
            "\n",
            "                class MyModel(nn.Module):\n",
            "                    def __init__(self, p=0.5):\n",
            "                        super().__init__()\n",
            "                        self.p = p\n",
            "\n",
            "                    def forward(self, ...):\n",
            "                        return F.scaled_dot_product_attention(...,\n",
            "                            dropout_p=(self.p if self.training else 0.0))\n",
            "\n",
            "        Note:\n",
            "\n",
            "            There are currently three supported implementations of scaled dot product attention:\n",
            "\n",
            "                - `FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning`_\n",
            "                - `Memory-Efficient Attention`_\n",
            "                - A PyTorch implementation defined in C++ matching the above formulation\n",
            "\n",
            "            The function may call optimized kernels for improved performance when using the CUDA backend.\n",
            "            For all other backends, the PyTorch implementation will be used.\n",
            "\n",
            "            All implementations are enabled by default. Scaled dot product attention attempts to automatically select the\n",
            "            most optimal implementation based on the inputs. In order to provide more fine-grained control over what implementation\n",
            "            is used, the following functions are provided for enabling and disabling implementations.\n",
            "            The context manager is the preferred mechanism:\n",
            "\n",
            "                - :func:`torch.nn.attention.sdpa_kernel`: A context manager used to enable or disable any of the implementations.\n",
            "                - :func:`torch.backends.cuda.enable_flash_sdp`: Globally enables or disables FlashAttention.\n",
            "                - :func:`torch.backends.cuda.enable_mem_efficient_sdp`: Globally enables or disables  Memory-Efficient Attention.\n",
            "                - :func:`torch.backends.cuda.enable_math_sdp`: Globally enables or disables  the PyTorch C++ implementation.\n",
            "\n",
            "            Each of the fused kernels has specific input limitations. If the user requires the use of a specific fused implementation,\n",
            "            disable the PyTorch C++ implementation using :func:`torch.nn.attention.sdpa_kernel`.\n",
            "            In the event that a fused implementation is not available, a warning will be raised with the\n",
            "            reasons why the fused implementation cannot run.\n",
            "\n",
            "            Due to the nature of fusing floating point operations, the output of this function may be different\n",
            "            depending on what backend kernel is chosen.\n",
            "            The c++ implementation supports torch.float64 and can be used when higher precision is required.\n",
            "            For math backend, all intermediates are kept in torch.float if inputs are in torch.half or torch.bfloat16.\n",
            "        For more information please see :doc:`/notes/numerical_accuracy`\n",
            "\n",
            "            Grouped Query Attention (GQA) is an experimental feature. It currently works only for Flash_attention\n",
            "            and math kernel on CUDA tensor, and does not support Nested tensor.\n",
            "            Constraints for GQA:\n",
            "\n",
            "                - number_of_heads_query % number_of_heads_key_value == 0 and,\n",
            "                - number_of_heads_key == number_of_heads_value\n",
            "\n",
            "        Note:\n",
            "\n",
            "            In some circumstances when given tensors on a CUDA device and using CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting ``torch.backends.cudnn.deterministic = True``. See :doc:`/notes/randomness` for more information.\n",
            "\n",
            "        Args:\n",
            "            query (Tensor): Query tensor; shape :math:`(N, ..., Hq, L, E)`.\n",
            "            key (Tensor): Key tensor; shape :math:`(N, ..., H, S, E)`.\n",
            "            value (Tensor): Value tensor; shape :math:`(N, ..., H, S, Ev)`.\n",
            "            attn_mask (optional Tensor): Attention mask; shape must be broadcastable to the shape of attention weights,\n",
            "                which is :math:`(N,..., L, S)`. Two types of masks are supported.\n",
            "                A boolean mask where a value of True indicates that the element *should* take part in attention.\n",
            "                A float mask of the same type as query, key, value that is added to the attention score.\n",
            "            dropout_p (float): Dropout probability; if greater than 0.0, dropout is applied\n",
            "            is_causal (bool): If set to true, the attention masking is a lower triangular matrix when the mask is a\n",
            "                square matrix. The attention masking has the form of the upper left causal bias due to the alignment\n",
            "                (see :class:`torch.nn.attention.bias.CausalBias`) when the mask is a non-square matrix.\n",
            "                An error is thrown if both attn_mask and is_causal are set.\n",
            "            scale (optional float, keyword-only): Scaling factor applied prior to softmax. If None, the default value is set\n",
            "                to :math:`\\frac{1}{\\sqrt{E}}`.\n",
            "            enable_gqa (bool): If set to True, Grouped Query Attention (GQA) is enabled, by default it is set to False.\n",
            "\n",
            "        Returns:\n",
            "            output (Tensor): Attention output; shape :math:`(N, ..., Hq, L, Ev)`.\n",
            "\n",
            "        Shape legend:\n",
            "            - :math:`N: \\text{Batch size} ... : \\text{Any number of other batch dimensions (optional)}`\n",
            "            - :math:`S: \\text{Source sequence length}`\n",
            "            - :math:`L: \\text{Target sequence length}`\n",
            "            - :math:`E: \\text{Embedding dimension of the query and key}`\n",
            "            - :math:`Ev: \\text{Embedding dimension of the value}`\n",
            "            - :math:`Hq: \\text{Number of heads of query}`\n",
            "            - :math:`H: \\text{Number of heads of key and value}`\n",
            "\n",
            "        Examples:\n",
            "\n",
            "            >>> # Optionally use the context manager to ensure one of the fused kernels is run\n",
            "            >>> query = torch.rand(32, 8, 128, 64, dtype=torch.float16, device=\"cuda\")\n",
            "            >>> key = torch.rand(32, 8, 128, 64, dtype=torch.float16, device=\"cuda\")\n",
            "            >>> value = torch.rand(32, 8, 128, 64, dtype=torch.float16, device=\"cuda\")\n",
            "            >>> with sdpa_kernel(backends=[SDPBackend.FLASH_ATTENTION]):\n",
            "            >>>     F.scaled_dot_product_attention(query,key,value)\n",
            "\n",
            "\n",
            "            >>> # Sample for GQA for llama3\n",
            "            >>> query = torch.rand(32, 32, 128, 64, dtype=torch.float16, device=\"cuda\")\n",
            "            >>> key = torch.rand(32, 8, 128, 64, dtype=torch.float16, device=\"cuda\")\n",
            "            >>> value = torch.rand(32, 8, 128, 64, dtype=torch.float16, device=\"cuda\")\n",
            "            >>> with sdpa_kernel(backends=[SDPBackend.MATH]):\n",
            "            >>>     F.scaled_dot_product_attention(query,key,value,enable_gqa=True)\n",
            "\n",
            "\n",
            "        .. _FlashAttention-2\\: Faster Attention with Better Parallelism and Work Partitioning:\n",
            "            https://arxiv.org/abs/2307.08691\n",
            "        .. _Memory-Efficient Attention:\n",
            "            https://github.com/facebookresearch/xformers\n",
            "        .. _Grouped-Query Attention:\n",
            "            https://arxiv.org/pdf/2305.13245\n",
            "\n",
            "    selu(input: torch.Tensor, inplace: bool = False) -> torch.Tensor\n",
            "        selu(input, inplace=False) -> Tensor\n",
            "\n",
            "        Applies element-wise,\n",
            "        :math:`\\text{SELU}(x) = scale * (\\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1)))`,\n",
            "        with :math:`\\alpha=1.6732632423543772848170429916717` and\n",
            "        :math:`scale=1.0507009873554804934193349852946`.\n",
            "\n",
            "        See :class:`~torch.nn.SELU` for more details.\n",
            "\n",
            "    selu_(...)\n",
            "        selu_(input) -> Tensor\n",
            "\n",
            "        In-place version of :func:`~selu`.\n",
            "\n",
            "    sigmoid(input)\n",
            "        sigmoid(input) -> Tensor\n",
            "\n",
            "        Applies the element-wise function :math:`\\text{Sigmoid}(x) = \\frac{1}{1 + \\exp(-x)}`\n",
            "\n",
            "        See :class:`~torch.nn.Sigmoid` for more details.\n",
            "\n",
            "    silu(input: torch.Tensor, inplace: bool = False) -> torch.Tensor\n",
            "        Apply the Sigmoid Linear Unit (SiLU) function, element-wise.\n",
            "\n",
            "        The SiLU function is also known as the swish function.\n",
            "\n",
            "        .. math::\n",
            "            \\text{silu}(x) = x * \\sigma(x), \\text{where } \\sigma(x) \\text{ is the logistic sigmoid.}\n",
            "\n",
            "        .. note::\n",
            "            See `Gaussian Error Linear Units (GELUs) <https://arxiv.org/abs/1606.08415>`_\n",
            "            where the SiLU (Sigmoid Linear Unit) was originally coined, and see\n",
            "            `Sigmoid-Weighted Linear Units for Neural Network Function Approximation\n",
            "            in Reinforcement Learning <https://arxiv.org/abs/1702.03118>`_ and `Swish:\n",
            "            a Self-Gated Activation Function <https://arxiv.org/abs/1710.05941v1>`_\n",
            "            where the SiLU was experimented with later.\n",
            "\n",
            "        See :class:`~torch.nn.SiLU` for more details.\n",
            "\n",
            "    smooth_l1_loss(input: torch.Tensor, target: torch.Tensor, size_average: Optional[bool] = None, reduce: Optional[bool] = None, reduction: str = 'mean', beta: float = 1.0) -> torch.Tensor\n",
            "        Compute the Smooth L1 loss.\n",
            "\n",
            "        Function uses a squared term if the absolute\n",
            "        element-wise error falls below beta and an L1 term otherwise.\n",
            "\n",
            "        See :class:`~torch.nn.SmoothL1Loss` for details.\n",
            "\n",
            "    soft_margin_loss(input: torch.Tensor, target: torch.Tensor, size_average: Optional[bool] = None, reduce: Optional[bool] = None, reduction: str = 'mean') -> torch.Tensor\n",
            "        soft_margin_loss(input, target, size_average=None, reduce=None, reduction='mean') -> Tensor\n",
            "\n",
            "        See :class:`~torch.nn.SoftMarginLoss` for details.\n",
            "\n",
            "    softmax(input: torch.Tensor, dim: Optional[int] = None, _stacklevel: int = 3, dtype: Optional[int] = None) -> torch.Tensor\n",
            "        Apply a softmax function.\n",
            "\n",
            "        Softmax is defined as:\n",
            "\n",
            "        :math:`\\text{Softmax}(x_{i}) = \\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)}`\n",
            "\n",
            "        It is applied to all slices along dim, and will re-scale them so that the elements\n",
            "        lie in the range `[0, 1]` and sum to 1.\n",
            "\n",
            "        See :class:`~torch.nn.Softmax` for more details.\n",
            "\n",
            "        Args:\n",
            "            input (Tensor): input\n",
            "            dim (int): A dimension along which softmax will be computed.\n",
            "            dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.\n",
            "              If specified, the input tensor is casted to :attr:`dtype` before the operation\n",
            "              is performed. This is useful for preventing data type overflows. Default: None.\n",
            "\n",
            "        .. note::\n",
            "            This function doesn't work directly with NLLLoss,\n",
            "            which expects the Log to be computed between the Softmax and itself.\n",
            "            Use log_softmax instead (it's faster and has better numerical properties).\n",
            "\n",
            "    softmin(input: torch.Tensor, dim: Optional[int] = None, _stacklevel: int = 3, dtype: Optional[int] = None) -> torch.Tensor\n",
            "        Apply a softmin function.\n",
            "\n",
            "        Note that :math:`\\text{Softmin}(x) = \\text{Softmax}(-x)`. See softmax definition for mathematical formula.\n",
            "\n",
            "        See :class:`~torch.nn.Softmin` for more details.\n",
            "\n",
            "        Args:\n",
            "            input (Tensor): input\n",
            "            dim (int): A dimension along which softmin will be computed (so every slice\n",
            "                along dim will sum to 1).\n",
            "            dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.\n",
            "              If specified, the input tensor is casted to :attr:`dtype` before the operation\n",
            "              is performed. This is useful for preventing data type overflows. Default: None.\n",
            "\n",
            "    softplus(...)\n",
            "        softplus(input, beta=1, threshold=20) -> Tensor\n",
            "\n",
            "        Applies element-wise, the function :math:`\\text{Softplus}(x) = \\frac{1}{\\beta} * \\log(1 + \\exp(\\beta * x))`.\n",
            "\n",
            "        For numerical stability the implementation reverts to the linear function\n",
            "        when :math:`input \\times \\beta > threshold`.\n",
            "\n",
            "        See :class:`~torch.nn.Softplus` for more details.\n",
            "\n",
            "    softshrink(...)\n",
            "        softshrink(input, lambd=0.5) -> Tensor\n",
            "\n",
            "        Applies the soft shrinkage function elementwise\n",
            "\n",
            "        See :class:`~torch.nn.Softshrink` for more details.\n",
            "\n",
            "    softsign(input)\n",
            "        softsign(input) -> Tensor\n",
            "\n",
            "        Applies element-wise, the function :math:`\\text{SoftSign}(x) = \\frac{x}{1 + |x|}`\n",
            "\n",
            "        See :class:`~torch.nn.Softsign` for more details.\n",
            "\n",
            "    tanh(input)\n",
            "        tanh(input) -> Tensor\n",
            "\n",
            "        Applies element-wise,\n",
            "        :math:`\\text{Tanh}(x) = \\tanh(x) = \\frac{\\exp(x) - \\exp(-x)}{\\exp(x) + \\exp(-x)}`\n",
            "\n",
            "        See :class:`~torch.nn.Tanh` for more details.\n",
            "\n",
            "    tanhshrink(input)\n",
            "        tanhshrink(input) -> Tensor\n",
            "\n",
            "        Applies element-wise, :math:`\\text{Tanhshrink}(x) = x - \\text{Tanh}(x)`\n",
            "\n",
            "        See :class:`~torch.nn.Tanhshrink` for more details.\n",
            "\n",
            "    threshold = _threshold(input: torch.Tensor, threshold: float, value: float, inplace: bool = False) -> torch.Tensor\n",
            "        Apply a threshold to each element of the input Tensor.\n",
            "\n",
            "        See :class:`~torch.nn.Threshold` for more details.\n",
            "\n",
            "    threshold_(...)\n",
            "        threshold_(input, threshold, value) -> Tensor\n",
            "\n",
            "        In-place version of :func:`~threshold`.\n",
            "\n",
            "    triplet_margin_loss(anchor: torch.Tensor, positive: torch.Tensor, negative: torch.Tensor, margin: float = 1.0, p: float = 2, eps: float = 1e-06, swap: bool = False, size_average: Optional[bool] = None, reduce: Optional[bool] = None, reduction: str = 'mean') -> torch.Tensor\n",
            "        Compute the triplet loss between given input tensors and a margin greater than 0.\n",
            "\n",
            "        See :class:`~torch.nn.TripletMarginLoss` for details.\n",
            "\n",
            "    triplet_margin_with_distance_loss(anchor: torch.Tensor, positive: torch.Tensor, negative: torch.Tensor, *, distance_function: Optional[Callable[[torch.Tensor, torch.Tensor], torch.Tensor]] = None, margin: float = 1.0, swap: bool = False, reduction: str = 'mean') -> torch.Tensor\n",
            "        Compute the triplet margin loss for input tensors using a custom distance function.\n",
            "\n",
            "        See :class:`~torch.nn.TripletMarginWithDistanceLoss` for details.\n",
            "\n",
            "    unfold(input: torch.Tensor, kernel_size: None, dilation: None = 1, padding: None = 0, stride: None = 1) -> torch.Tensor\n",
            "        Extract sliding local blocks from a batched input tensor.\n",
            "\n",
            "        .. warning::\n",
            "            Currently, only 4-D input tensors (batched image-like tensors) are\n",
            "            supported.\n",
            "\n",
            "        .. warning::\n",
            "\n",
            "            More than one element of the unfolded tensor may refer to a single\n",
            "            memory location. As a result, in-place operations (especially ones that\n",
            "            are vectorized) may result in incorrect behavior. If you need to write\n",
            "            to the tensor, please clone it first.\n",
            "\n",
            "\n",
            "        See :class:`torch.nn.Unfold` for details\n",
            "\n",
            "    upsample(input, size=None, scale_factor=None, mode='nearest', align_corners=None)\n",
            "        Upsample input.\n",
            "\n",
            "        Provided tensor is upsampled to either the given :attr:`size` or the given\n",
            "        :attr:`scale_factor`\n",
            "\n",
            "        .. warning::\n",
            "            This function is deprecated in favor of :func:`torch.nn.functional.interpolate`.\n",
            "            This is equivalent with ``nn.functional.interpolate(...)``.\n",
            "\n",
            "        Note:\n",
            "            This operation may produce nondeterministic gradients when given tensors on a CUDA device. See :doc:`/notes/randomness` for more information.\n",
            "\n",
            "        The algorithm used for upsampling is determined by :attr:`mode`.\n",
            "\n",
            "        Currently temporal, spatial and volumetric upsampling are supported, i.e.\n",
            "        expected inputs are 3-D, 4-D or 5-D in shape.\n",
            "\n",
            "        The input dimensions are interpreted in the form:\n",
            "        `mini-batch x channels x [optional depth] x [optional height] x width`.\n",
            "\n",
            "        The modes available for upsampling are: `nearest`, `linear` (3D-only),\n",
            "        `bilinear`, `bicubic` (4D-only), `trilinear` (5D-only)\n",
            "\n",
            "        Args:\n",
            "            input (Tensor): the input tensor\n",
            "            size (int or Tuple[int] or Tuple[int, int] or Tuple[int, int, int]):\n",
            "                output spatial size.\n",
            "            scale_factor (float or Tuple[float]): multiplier for spatial size. Has to match input size if it is a tuple.\n",
            "            mode (str): algorithm used for upsampling:\n",
            "                ``'nearest'`` | ``'linear'`` | ``'bilinear'`` | ``'bicubic'`` |\n",
            "                ``'trilinear'``. Default: ``'nearest'``\n",
            "            align_corners (bool, optional): Geometrically, we consider the pixels of the\n",
            "                input and output as squares rather than points.\n",
            "                If set to ``True``, the input and output tensors are aligned by the\n",
            "                center points of their corner pixels, preserving the values at the corner pixels.\n",
            "                If set to ``False``, the input and output tensors are aligned by the corner\n",
            "                points of their corner pixels, and the interpolation uses edge value padding\n",
            "                for out-of-boundary values, making this operation *independent* of input size\n",
            "                when :attr:`scale_factor` is kept the same. This only has an effect when :attr:`mode`\n",
            "                is ``'linear'``, ``'bilinear'``, ``'bicubic'`` or ``'trilinear'``.\n",
            "                Default: ``False``\n",
            "\n",
            "        .. note::\n",
            "            With ``mode='bicubic'``, it's possible to cause overshoot, in other words it can produce\n",
            "            negative values or values greater than 255 for images.\n",
            "            Explicitly call ``result.clamp(min=0, max=255)`` if you want to reduce the overshoot\n",
            "            when displaying the image.\n",
            "\n",
            "        .. warning::\n",
            "            With ``align_corners = True``, the linearly interpolating modes\n",
            "            (`linear`, `bilinear`, and `trilinear`) don't proportionally align the\n",
            "            output and input pixels, and thus the output values can depend on the\n",
            "            input size. This was the default behavior for these modes up to version\n",
            "            0.3.1. Since then, the default behavior is ``align_corners = False``.\n",
            "            See :class:`~torch.nn.Upsample` for concrete examples on how this\n",
            "            affects the outputs.\n",
            "\n",
            "    upsample_bilinear(input, size=None, scale_factor=None)\n",
            "        Upsamples the input, using bilinear upsampling.\n",
            "\n",
            "        .. warning::\n",
            "            This function is deprecated in favor of :func:`torch.nn.functional.interpolate`.\n",
            "            This is equivalent with\n",
            "            ``nn.functional.interpolate(..., mode='bilinear', align_corners=True)``.\n",
            "\n",
            "        Expected inputs are spatial (4 dimensional). Use `upsample_trilinear` fo\n",
            "        volumetric (5 dimensional) inputs.\n",
            "\n",
            "        Args:\n",
            "            input (Tensor): input\n",
            "            size (int or Tuple[int, int]): output spatial size.\n",
            "            scale_factor (int or Tuple[int, int]): multiplier for spatial size\n",
            "\n",
            "        Note:\n",
            "            This operation may produce nondeterministic gradients when given tensors on a CUDA device. See :doc:`/notes/randomness` for more information.\n",
            "\n",
            "    upsample_nearest(input, size=None, scale_factor=None)\n",
            "        Upsamples the input, using nearest neighbours' pixel values.\n",
            "\n",
            "        .. warning::\n",
            "            This function is deprecated in favor of :func:`torch.nn.functional.interpolate`.\n",
            "            This is equivalent with ``nn.functional.interpolate(..., mode='nearest')``.\n",
            "\n",
            "        Currently spatial and volumetric upsampling are supported (i.e. expected\n",
            "        inputs are 4 or 5 dimensional).\n",
            "\n",
            "        Args:\n",
            "            input (Tensor): input\n",
            "            size (int or Tuple[int, int] or Tuple[int, int, int]): output spatia\n",
            "                size.\n",
            "            scale_factor (int): multiplier for spatial size. Has to be an integer.\n",
            "\n",
            "        Note:\n",
            "            This operation may produce nondeterministic gradients when given tensors on a CUDA device. See :doc:`/notes/randomness` for more information.\n",
            "\n",
            "DATA\n",
            "    BroadcastingList1 = <torch._jit_internal.BroadcastingListCls object>\n",
            "    BroadcastingList2 = <torch._jit_internal.BroadcastingListCls object>\n",
            "    BroadcastingList3 = <torch._jit_internal.BroadcastingListCls object>\n",
            "    Callable = typing.Callable\n",
            "        Deprecated alias to collections.abc.Callable.\n",
            "\n",
            "        Callable[[int], str] signifies a function that takes a single\n",
            "        parameter of type int and returns a str.\n",
            "\n",
            "        The subscription syntax must always be used with exactly two\n",
            "        values: the argument list and the return type.\n",
            "        The argument list must be a list of types, a ParamSpec,\n",
            "        Concatenate or ellipsis. The return type must be a single type.\n",
            "\n",
            "        There is no syntax to indicate optional or keyword arguments;\n",
            "        such function types are rarely used as callback types.\n",
            "\n",
            "    GRID_SAMPLE_INTERPOLATION_MODES = {'bicubic': 2, 'bilinear': 0, 'neare...\n",
            "    GRID_SAMPLE_PADDING_MODES = {'border': 1, 'reflection': 2, 'zeros': 0}\n",
            "    Optional = typing.Optional\n",
            "        Optional[X] is equivalent to Union[X, None].\n",
            "\n",
            "    TYPE_CHECKING = False\n",
            "    Union = typing.Union\n",
            "        Union type; Union[X, Y] means either X or Y.\n",
            "\n",
            "        On Python 3.10 and higher, the | operator\n",
            "        can also be used to denote unions;\n",
            "        X | Y means the same thing to the type checker as Union[X, Y].\n",
            "\n",
            "        To define a union, use e.g. Union[int, str]. Details:\n",
            "        - The arguments must be types and there must be at least one.\n",
            "        - None as an argument is a special case and is replaced by\n",
            "          type(None).\n",
            "        - Unions of unions are flattened, e.g.::\n",
            "\n",
            "            assert Union[Union[int, str], float] == Union[int, str, float]\n",
            "\n",
            "        - Unions of a single argument vanish, e.g.::\n",
            "\n",
            "            assert Union[int] == int  # The constructor actually returns int\n",
            "\n",
            "        - Redundant arguments are skipped, e.g.::\n",
            "\n",
            "            assert Union[int, str, int] == Union[int, str]\n",
            "\n",
            "        - When comparing unions, the argument order is ignored, e.g.::\n",
            "\n",
            "            assert Union[int, str] == Union[str, int]\n",
            "\n",
            "        - You cannot subclass or instantiate a union.\n",
            "        - You can use Optional[X] as a shorthand for Union[X, None].\n",
            "\n",
            "    reproducibility_notes = {'backward_reproducibility_note': 'This operat...\n",
            "    sparse_support_notes = {'sparse_beta_warning': '\\n.. warning::\\n    Sp...\n",
            "    tf32_notes = {'tf32_note': 'This operator supports :ref:`TensorFloat32...\n",
            "\n",
            "FILE\n",
            "    c:\\users\\darth\\anaconda3\\envs\\learn\\lib\\site-packages\\torch\\nn\\functional.py\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "help(torch.nn.functional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Verifying Conv2d Backward Pass Logic ---\n",
            "Setup: Inputtorch.Size([4, 8, 32, 32]), Weighttorch.Size([16, 2, 3, 3]), Biastorch.Size([16]), Groups=4\n",
            "\n",
            "--- 2. Calculating Ground Truth Gradients (PyTorch Autograd) ---\n",
            "Autograd calculations complete.\n",
            "\n",
            "--- 3. Calculating Gradients Manually (torch.nn.functional) ---\n",
            "Manual calculations complete.\n",
            "\n",
            "--- 4. Verification ---\n",
            "Bias Gradient Correct:   ✅ Yes\n",
            "Input Gradient Correct:  ✅ Yes\n",
            "Weight Gradient Correct: ✅ Yes\n",
            "\n",
            "Verification Finished.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def verify_conv2d_backward_logic():\n",
        "    \"\"\"\n",
        "    Verifies that the manual backward pass logic for a 2D convolution,\n",
        "    implemented with torch.nn.functional, matches PyTorch's autograd.\n",
        "\n",
        "    This corrected version properly handles grouped convolutions for the\n",
        "    weight gradient calculation and correctly calculates output_padding for\n",
        "    the input gradient calculation.\n",
        "    \"\"\"\n",
        "    print(\"--- Verifying Conv2d Backward Pass Logic ---\")\n",
        "\n",
        "    # ---- 1. Setup Parameters and Random Tensors ----\n",
        "    # Use double precision for higher accuracy in gradient comparisons\n",
        "    dtype = torch.float64\n",
        "    torch.manual_seed(42) # for reproducibility\n",
        "\n",
        "    # Layer parameters\n",
        "    batch_size = 4\n",
        "    in_channels, out_channels = 8, 16\n",
        "    kernel_size = (3, 3)\n",
        "    stride = (2, 2)\n",
        "    padding = (1, 1)\n",
        "    dilation = (2, 2) # Use non-trivial dilation\n",
        "    groups = 4 # Use grouped convolution\n",
        "\n",
        "    # Input and output spatial dimensions\n",
        "    input_size = (32, 32)\n",
        "\n",
        "    # Create random tensors that require gradients\n",
        "    X = torch.randn(batch_size, in_channels, *input_size, dtype=dtype, requires_grad=True)\n",
        "    W = torch.randn(out_channels, in_channels // groups, *kernel_size, dtype=dtype, requires_grad=True)\n",
        "    B = torch.randn(out_channels, dtype=dtype, requires_grad=True)\n",
        "\n",
        "    print(f\"Setup: Input{X.shape}, Weight{W.shape}, Bias{B.shape}, Groups={groups}\\n\")\n",
        "\n",
        "    # ---- 2. Ground Truth: Calculate Gradients using PyTorch Autograd ----\n",
        "    print(\"--- 2. Calculating Ground Truth Gradients (PyTorch Autograd) ---\")\n",
        "\n",
        "    # Perform the forward pass\n",
        "    O_autograd = F.conv2d(X, W, B, stride, padding, dilation, groups)\n",
        "\n",
        "    # Generate a random upstream gradient (from a hypothetical subsequent layer)\n",
        "    grad_output = torch.randn_like(O_autograd)\n",
        "\n",
        "    # Perform the backward pass\n",
        "    O_autograd.backward(grad_output)\n",
        "\n",
        "    # Store the results\n",
        "    grad_X_autograd = X.grad.clone()\n",
        "    grad_W_autograd = W.grad.clone()\n",
        "    grad_B_autograd = B.grad.clone()\n",
        "\n",
        "    print(\"Autograd calculations complete.\\n\")\n",
        "\n",
        "    # ---- 3. Manual Calculation: Compute Gradients using Functional Ops ----\n",
        "    print(\"--- 3. Calculating Gradients Manually (torch.nn.functional) ---\")\n",
        "\n",
        "    # (A) Gradient with respect to Bias\n",
        "    # This is the sum of the output gradients over batch, height, and width dimensions.\n",
        "    grad_B_manual = grad_output.sum(dim=[0, 2, 3])\n",
        "\n",
        "    # (B) Gradient with respect to Input\n",
        "    # This is a transposed convolution of the output gradient with the original weight kernel.\n",
        "    # The output shape of a transposed convolution can be ambiguous. We must calculate\n",
        "    # the correct `output_padding` to ensure the output shape of this operation\n",
        "    # exactly matches the original input shape `X.shape`.\n",
        "    h_in, w_in = X.shape[2], X.shape[3]\n",
        "    h_out, w_out = O_autograd.shape[2], O_autograd.shape[3]\n",
        "\n",
        "    # The formula relating input size to output size in a transposed convolution is:\n",
        "    # InputSize = (OutputSize - 1) * stride - 2 * padding + dilation * (kernel - 1) + output_padding + 1\n",
        "    # We rearrange this to solve for the required output_padding.\n",
        "    output_padding_h = h_in - ((h_out - 1) * stride[0] - 2 * padding[0] + dilation[0] * (kernel_size[0] - 1) + 1)\n",
        "    output_padding_w = w_in - ((w_out - 1) * stride[1] - 2 * padding[1] + dilation[1] * (kernel_size[1] - 1) + 1)\n",
        "    output_padding = (output_padding_h, output_padding_w)\n",
        "\n",
        "    grad_X_manual = F.conv_transpose2d(\n",
        "        grad_output,\n",
        "        W,\n",
        "        stride=stride,\n",
        "        padding=padding,\n",
        "        output_padding=output_padding,\n",
        "        dilation=dilation,\n",
        "        groups=groups\n",
        "    )\n",
        "\n",
        "    # (C) Gradient with respect to Weight (Kernel)\n",
        "    # This is the most complex part. The gradient w.r.t. the weights is a convolution\n",
        "    # of the input (X) and the output gradient (grad_output).\n",
        "    # For grouped convolutions, we must perform this calculation for each group separately.\n",
        "    # in_channels_per_group = in_channels // groups\n",
        "    # out_channels_per_group = out_channels // groups\n",
        "    # grad_W_groups = []\n",
        "\n",
        "    # for g in range(groups):\n",
        "    #     # Slice the input tensor to get the channels for the current group\n",
        "    #     start_in_ch = g * in_channels_per_group\n",
        "    #     end_in_ch = start_in_ch + in_channels_per_group\n",
        "    #     X_g = X[:, start_in_ch:end_in_ch, :, :]\n",
        "\n",
        "    #     # Slice the output gradient tensor to get the channels for the current group\n",
        "    #     start_out_ch = g * out_channels_per_group\n",
        "    #     end_out_ch = start_out_ch + out_channels_per_group\n",
        "    #     grad_output_g = grad_output[:, start_out_ch:end_out_ch, :, :]\n",
        "\n",
        "    #     # To calculate the weight gradient via a convolution, we must cleverly\n",
        "    #     # permute the input (X_g) and output gradient (grad_output_g) tensors.\n",
        "    #     # We treat X_g as the input and grad_output_g as the kernel.\n",
        "    #     # X_g: (N, Cin/g, H, W) -> permute -> (Cin/g, N, H, W)\n",
        "    #     # grad_output_g: (N, Cout/g, oH, oW) -> permute -> (Cout/g, N, oH, oW)\n",
        "    #     # The F.conv2d call then treats 'Cin/g' as the batch size and 'N' as the input channels.\n",
        "    #     # The stride and dilation parameters from the original convolution are swapped.\n",
        "    #     X_g_permuted = X_g.transpose(0, 1)\n",
        "    #     grad_output_g_permuted = grad_output_g.transpose(0, 1)\n",
        "\n",
        "    #     grad_W_g_permuted = F.conv2d(\n",
        "    #         X_g_permuted,\n",
        "    #         grad_output_g_permuted,\n",
        "    #         stride=dilation,\n",
        "    #         padding=padding,\n",
        "    #         dilation=stride,\n",
        "    #         groups=1 # The group calculation is handled by our loop, so this is a standard conv.\n",
        "    #     )\n",
        "\n",
        "    #     # The result has shape (Cin/g, Cout/g, kH, kW). We must permute it back to\n",
        "    #     # the standard weight layout of (Cout/g, Cin/g, kH, kW).\n",
        "    #     grad_W_g = grad_W_g_permuted.transpose(0, 1)\n",
        "    #     grad_W_groups.append(grad_W_g)\n",
        "\n",
        "    # # Concatenate the gradients from all groups along the output channel dimension.\n",
        "    # # The weight tensor for grouped convolutions is laid out by stacking the weights\n",
        "    # # for each group, so we do the same for the gradient.\n",
        "    # grad_W_manual = torch.cat(grad_W_groups, dim=0)\n",
        "    grad_W_manual = torch.nn.grad.conv2d_weight(\n",
        "        input=X,\n",
        "        weight_size=W.shape,\n",
        "        grad_output=grad_output,\n",
        "        stride=stride,\n",
        "        padding=padding,\n",
        "        dilation=dilation,\n",
        "        groups=groups\n",
        "    )\n",
        "\n",
        "\n",
        "    print(\"Manual calculations complete.\\n\")\n",
        "\n",
        "    # ---- 4. Verification ----\n",
        "    print(\"--- 4. Verification ---\")\n",
        "\n",
        "    bias_correct = torch.allclose(grad_B_manual, grad_B_autograd)\n",
        "    input_correct = torch.allclose(grad_X_manual, grad_X_autograd)\n",
        "    weight_correct = torch.allclose(grad_W_manual, grad_W_autograd)\n",
        "\n",
        "    print(f\"Bias Gradient Correct:   {'✅ Yes' if bias_correct else '❌ No'}\")\n",
        "    print(f\"Input Gradient Correct:  {'✅ Yes' if input_correct else '❌ No'}\")\n",
        "    print(f\"Weight Gradient Correct: {'✅ Yes' if weight_correct else '❌ No'}\")\n",
        "\n",
        "    print(\"\\nVerification Finished.\")\n",
        "\n",
        "    assert bias_correct and input_correct and weight_correct, \"One or more manual gradients did not match autograd.\"\n",
        "\n",
        "# Run the verification\n",
        "if __name__ == \"__main__\":\n",
        "    verify_conv2d_backward_logic()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "learn",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
