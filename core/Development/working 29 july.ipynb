{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Q4cPp78KYVm",
        "outputId": "6ab7e763-abd7-45b4-bfbc-65b9e162da36"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting rustworkx\n",
            "  Downloading rustworkx-0.16.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy<3,>=1.16.0 in /usr/local/lib/python3.11/dist-packages (from rustworkx) (2.0.2)\n",
            "Downloading rustworkx-0.16.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rustworkx\n",
            "Successfully installed rustworkx-0.16.0\n"
          ]
        }
      ],
      "source": [
        "!pip install rustworkx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4MLII47KF9E"
      },
      "source": [
        "# defination"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "yT6kY7fzKIel"
      },
      "outputs": [],
      "source": [
        "import rustworkx as rx\n",
        "import weakref\n",
        "class AutogradGraph:\n",
        "    \"\"\"\n",
        "    Manages the computation graph for automatic differentiation.\n",
        "    It uses a directed acyclic graph to track dependencies between tensors.\n",
        "    \"\"\"\n",
        "    __slots__ = ('graph', 'intermediate_tensors', '_check_cycles', '_auto_cleanup', '__weakref__')\n",
        "\n",
        "    def __init__(self, check_for_cycles=True, auto_cleanup=True):\n",
        "        self.graph = rx.PyDiGraph()\n",
        "        self.intermediate_tensors = {}\n",
        "        self._check_cycles = check_for_cycles\n",
        "        self._auto_cleanup = auto_cleanup\n",
        "\n",
        "    def __enter__(self):\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, exc_type, exc_value, traceback):\n",
        "        if self._check_cycles and self.check_cycle():\n",
        "            raise RuntimeError(\"Cycle detected in autograd graph on context exit.\")\n",
        "        if self._auto_cleanup:\n",
        "            self.intermediate_tensors.clear()\n",
        "            self.graph.clear()\n",
        "\n",
        "    def add_tensor_graph(self, tensor):\n",
        "        if not tensor._custom_requires_grad:\n",
        "            raise ValueError(\"Tensor with requires_grad=False cannot be added to the graph.\")\n",
        "        ref = weakref.proxy(tensor)\n",
        "        tensor_index = self.graph.add_node(ref)\n",
        "        tensor._node_id = tensor_index\n",
        "\n",
        "    def add_non_leaf_tensor_reference(self, tensor):\n",
        "        if not tensor._custom_requires_grad:\n",
        "            raise ValueError(\"Tensor must require grad.\")\n",
        "        if tensor._node_id in self.intermediate_tensors:\n",
        "            raise ValueError(\"Tensor reference already exists in intermediate tensors.\")\n",
        "        self.intermediate_tensors[tensor._node_id] = tensor\n",
        "\n",
        "    def add_edge(self, node_from, node_to, weight=None):\n",
        "        if not all(isinstance(n, int) for n in (node_from, node_to)):\n",
        "            raise TypeError(\"Node indices must be integers.\")\n",
        "        if not self.graph.has_node(node_from) or not self.graph.has_node(node_to):\n",
        "            raise ValueError(\"Nodes must exist before adding edge.\")\n",
        "        self.graph.add_edge(node_from, node_to, weight)\n",
        "\n",
        "    def check_cycle(self):\n",
        "        return not rx.is_directed_acyclic_graph(self.graph)\n",
        "\n",
        "    def reverse_toposort_from_tensor(self, tensor_index):\n",
        "        graph=self.graph\n",
        "        predecessors = list(rx.ancestors(graph, tensor_index))\n",
        "        predecessors.append(tensor_index)\n",
        "        sub_graph = graph.subgraph(predecessors)\n",
        "        return [sub_graph[i] for i in reversed(rx.topological_sort(sub_graph))]\n",
        "    # def alternative_reverse_toposort_from_tensor(self, tensor_index):\n",
        "    #     graph = self.graph\n",
        "    #     relevant_nodes = rx.ancestors(graph, tensor_index)\n",
        "    #     relevant_nodes.add(tensor_index)\n",
        "    #     full_topo = rx.topological_sort(graph)\n",
        "    #     relevant_topo = [graph[_node_id] for _node_id in reversed(full_topo) if _node_id in relevant_nodes]\n",
        "    #     return relevant_topo\n",
        "\n",
        "    def delete_node(self, node_index):\n",
        "        if not isinstance(node_index, int):\n",
        "            raise TypeError(\"Node index must be an integer.\")\n",
        "        if self.graph.has_node(node_index):\n",
        "             self.graph.remove_node(node_index)\n",
        "    def delete_edge(self, node_from, node_to):\n",
        "        if not self.graph.has_edge(node_from, node_to):\n",
        "            raise ValueError(\"Edge does not exist.\")\n",
        "        self.graph.remove_edge(node_from, node_to)\n",
        "\n",
        "    def del_non_leaf_tensor_reference(self, tensor_node_id):\n",
        "        self.intermediate_tensors.pop(tensor_node_id, None)\n",
        "\n",
        "    def delete_all_non_leaf_nodes(self):\n",
        "        # removes non leaf nodes from graph and clears the intermediate_tensors dict\n",
        "        self.graph.remove_nodes_from(list(self.intermediate_tensors.keys()))\n",
        "        for custom_tensor in self.intermediate_tensors.values():custom_tensor.clear()\n",
        "        self.intermediate_tensors.clear()\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"CustomAutogradGraph(nodes={self.graph.num_nodes()}, edges={self.graph.num_edges()})\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEYibHauKeDl"
      },
      "source": [
        "# Tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "0uZO2fZEKeh7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import weakref\n",
        "import numbers\n",
        "import math\n",
        "device = \"cpu\"\n",
        "dtype = torch.float32\n",
        "\n",
        "class CustomTensor:\n",
        "    \"\"\"\n",
        "    A custom tensor class that wraps a PyTorch tensor to enable a custom\n",
        "    autograd engine. It tracks operations to build a computation graph.\n",
        "    \"\"\"\n",
        "    __slots__ = ('tensor', '_node_id', '_custom_requires_grad', '_backward', 'graph', '__weakref__','_is_leaf')\n",
        "\n",
        "    def __new__(cls, data, *, _custom_requires_grad=False, device=device, dtype=dtype, graph=None, due_to_operation=False, is_leaf=False):\n",
        "        assert device is not None\n",
        "        assert dtype is not None\n",
        "        if isinstance(data, CustomTensor):\n",
        "            return data  # Don't rewrap\n",
        "        return super().__new__(cls)\n",
        "\n",
        "    def __init__(self, data, *, _custom_requires_grad=False, device=device, dtype=dtype, graph=None, due_to_operation=False, is_leaf=False):\n",
        "        if isinstance(data, CustomTensor):\n",
        "            return\n",
        "\n",
        "        self.tensor = data if due_to_operation else torch.as_tensor(data, dtype=dtype, device=device)\n",
        "        self.tensor.requires_grad_(False)\n",
        "        self._custom_requires_grad = _custom_requires_grad\n",
        "        self._node_id = None\n",
        "        self._backward = lambda: None\n",
        "        self.graph = None\n",
        "        self._is_leaf = is_leaf\n",
        "\n",
        "        if _custom_requires_grad:\n",
        "            self._init_graph(graph)\n",
        "\n",
        "    def _init_graph(self, graph):\n",
        "        if graph is None:\n",
        "            raise ValueError(\"Graph must be provided if requires_grad is True.\")\n",
        "        is_leaf=self._is_leaf\n",
        "        if is_leaf:\n",
        "            self.graph = weakref.proxy(graph)\n",
        "        else:\n",
        "            self.graph = graph # this line is only reached for tensors which are created by operations and graph passed is already a weakreference hence no need for wrapping\n",
        "        graph.add_tensor_graph(self)\n",
        "        if not is_leaf:\n",
        "            graph.add_non_leaf_tensor_reference(self)\n",
        "\n",
        "    def clear(self):\n",
        "        # NEVER CALL FOR LEAF TENSORS\n",
        "        if self._is_leaf: return # ideally this line should never execute, this is just a gaurd rail\n",
        "        self.tensor.grad = None\n",
        "        self._custom_requires_grad = False\n",
        "        self._node_id = None\n",
        "        self._backward = lambda: None\n",
        "        self.graph = None\n",
        "\n",
        "    def clear_full(self):\n",
        "        self.tensor = None\n",
        "        self._custom_requires_grad = False\n",
        "        self._node_id = None\n",
        "        self._backward = lambda: None\n",
        "        self.graph = None\n",
        "\n",
        "    def _zero_grad(self):\n",
        "        \"\"\"Sets the gradient of the underlying tensor to zero.\"\"\"\n",
        "        if self.tensor.grad is None:\n",
        "            self.tensor.grad = torch.zeros_like(self.tensor)\n",
        "        else:\n",
        "            self.tensor.grad.zero_()\n",
        "\n",
        "    def zero_(self):\n",
        "        \"\"\"Sets the gradient of the underlying tensor to zero.\"\"\"\n",
        "        if self.tensor.grad is not None:\n",
        "            self.tensor.grad.zero_()\n",
        "\n",
        "\n",
        "    # --- Broadcasting Helper ---\n",
        "    @torch.compile\n",
        "    def _reduce_grad_for_broadcast(self, grad, target_shape):\n",
        "        \"\"\"Reduces a gradient to match the shape of a tensor that was broadcasted.\"\"\"\n",
        "        if grad.shape == target_shape:\n",
        "            return grad\n",
        "\n",
        "        # Add singleton dimensions to the front of target_shape to match grad's ndim\n",
        "        padded_target_shape = (1,) * (grad.ndim - len(target_shape)) + target_shape\n",
        "\n",
        "        # Identify dimensions that were broadcasted\n",
        "        sum_dims = [i for i, (grad_dim, target_dim) in enumerate(zip(grad.shape, padded_target_shape)) if target_dim == 1 and grad_dim > 1]\n",
        "\n",
        "        if sum_dims:\n",
        "            grad = grad.sum(dim=sum_dims, keepdim=True)\n",
        "\n",
        "        # Remove singleton dimensions to match the final target shape\n",
        "        return grad.reshape(target_shape)\n",
        "\n",
        "\n",
        "\n",
        "    def __add__(self, other):\n",
        "\n",
        "        if isinstance(other, numbers.Number):\n",
        "            return self._add_scalar(other)\n",
        "        elif isinstance(other, CustomTensor):\n",
        "            return self._add_tensor(other)\n",
        "        return NotImplemented\n",
        "    def __radd__(self,other):\n",
        "        return self + other\n",
        "    def __iadd__(self,other):\n",
        "        if isinstance(other, numbers.Number):\n",
        "            self.tensor.add_(other)\n",
        "        elif isinstance(other,CustomTensor):\n",
        "            self.tensor.add_(other.tensor)\n",
        "    def _add_scalar(self, scalar):\n",
        "        result_tensor = torch.add(self.tensor, scalar)\n",
        "        if not self._custom_requires_grad:\n",
        "            return CustomTensor(result_tensor,due_to_operation=True)\n",
        "        graph = self.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "        graph.add_edge(self._node_id, result._node_id)\n",
        "        self_ref = weakref.proxy(self)\n",
        "        result_ref = weakref.proxy(result)\n",
        "        def _backward():\n",
        "            if self_ref.tensor.grad is None: self_ref._zero_grad()\n",
        "            self_ref.tensor.grad.add_(result_ref.tensor.grad)\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "    def _add_tensor(self, other):\n",
        "        result_tensor = torch.add(self.tensor, other.tensor)\n",
        "        requires_grad = self._custom_requires_grad or other._custom_requires_grad\n",
        "        if not requires_grad:\n",
        "            return CustomTensor(result_tensor,due_to_operation=True)\n",
        "        graph = self.graph if self._custom_requires_grad else other.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "        self_ref = weakref.proxy(self)\n",
        "        other_ref = weakref.proxy(other)\n",
        "        if self._custom_requires_grad:\n",
        "            graph.add_edge(self._node_id, result._node_id)\n",
        "        if other._custom_requires_grad:\n",
        "            graph.add_edge(other._node_id, result._node_id)\n",
        "        result_ref = weakref.proxy(result)\n",
        "        def _backward():\n",
        "            if self_ref._custom_requires_grad:\n",
        "                if self_ref.tensor.grad is None: self_ref._zero_grad()\n",
        "                grad_for_self = self_ref._reduce_grad_for_broadcast(result_ref.tensor.grad, self_ref.tensor.shape)\n",
        "                self_ref.tensor.grad.add_(grad_for_self)\n",
        "            if other_ref._custom_requires_grad:\n",
        "                if other_ref.tensor.grad is None: other_ref._zero_grad()\n",
        "                grad_for_other = other_ref._reduce_grad_for_broadcast(result_ref.tensor.grad, other_ref.tensor.shape)\n",
        "                other_ref.tensor.grad.add_(grad_for_other)\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "\n",
        "    def __mul__(self, other):\n",
        "        if isinstance(other, numbers.Number):\n",
        "            return self._mul_scalar(other)\n",
        "        elif isinstance(other, CustomTensor):\n",
        "            return self._mul_tensor(other)\n",
        "        return NotImplemented\n",
        "    def __rmul__(self,other):\n",
        "        return self*other\n",
        "    def __imul__(self,other):\n",
        "        if isinstance(other, numbers.Number):\n",
        "            self.tensor.mul_(other)\n",
        "        elif isinstance(other,CustomTensor):\n",
        "            self.tensor.mul_(other.tensor)\n",
        "    def _mul_scalar(self, scalar):\n",
        "        result_tensor = torch.mul(self.tensor, scalar)\n",
        "        if not self._custom_requires_grad:\n",
        "            return CustomTensor(result_tensor,due_to_operation=True)\n",
        "        graph = self.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "        graph.add_edge(self._node_id, result._node_id)\n",
        "        self_ref = weakref.proxy(self)\n",
        "        result_ref = weakref.proxy(result)\n",
        "        def _backward():\n",
        "            if self_ref.tensor.grad is None:\n",
        "                self_ref._zero_grad()\n",
        "            self_ref.tensor.grad.add_(result_ref.tensor.grad * scalar)\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "    def _mul_tensor(self, other):\n",
        "        result_tensor = torch.mul(self.tensor, other.tensor)\n",
        "        requires_grad = self._custom_requires_grad or other._custom_requires_grad\n",
        "        if not requires_grad:\n",
        "            return CustomTensor(result_tensor,due_to_operation=True)\n",
        "        graph = self.graph if self._custom_requires_grad else other.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "        self_ref = weakref.proxy(self)\n",
        "        other_ref = weakref.proxy(other)\n",
        "        result_ref = weakref.proxy(result)\n",
        "        if self._custom_requires_grad:\n",
        "            graph.add_edge(self._node_id, result._node_id)\n",
        "        if other._custom_requires_grad:\n",
        "            graph.add_edge(other._node_id, result._node_id)\n",
        "        def _backward():\n",
        "            if self_ref._custom_requires_grad:\n",
        "                if self_ref.tensor.grad is None: self_ref._zero_grad()\n",
        "                grad_for_self = self_ref._reduce_grad_for_broadcast(result_ref.tensor.grad * other_ref.tensor, self_ref.tensor.shape)\n",
        "                self_ref.tensor.grad.add_(grad_for_self)\n",
        "            if other_ref._custom_requires_grad:\n",
        "                if other_ref.tensor.grad is None: other_ref._zero_grad()\n",
        "                grad_for_other = other_ref._reduce_grad_for_broadcast(result_ref.tensor.grad * self_ref.tensor, other_ref.tensor.shape)\n",
        "                other_ref.tensor.grad.add_(grad_for_other)\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "\n",
        "    def __sub__(self, other):\n",
        "        if isinstance(other, numbers.Number):\n",
        "            return self._sub_scalar(other)\n",
        "        elif isinstance(other, CustomTensor):\n",
        "            return self._sub_tensor(other)\n",
        "        return NotImplemented\n",
        "\n",
        "    def __rsub__(self, other):\n",
        "        if isinstance(other, numbers.Number):\n",
        "            return self._rsub_scalar(other)\n",
        "\n",
        "    def __isub__(self,other):\n",
        "        if isinstance(other, numbers.Number):\n",
        "            self.tensor.sub_(other)\n",
        "        elif isinstance(other,CustomTensor):\n",
        "            self.tensor.sub_(other.tensor)\n",
        "\n",
        "    def _rsub_scalar(self, scalar):\n",
        "        result_tensor = torch.sub(scalar, self.tensor)\n",
        "        if not self._custom_requires_grad:\n",
        "            return CustomTensor(result_tensor,due_to_operation=True)\n",
        "\n",
        "        graph = self.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "        graph.add_edge(self._node_id, result._node_id)\n",
        "\n",
        "        self_ref = weakref.proxy(self)\n",
        "        result_ref = weakref.proxy(result)\n",
        "        def _backward():\n",
        "            if self_ref.tensor.grad is None:\n",
        "                self_ref._zero_grad()\n",
        "            # Derivative of scalar - x is -1\n",
        "            self_ref.tensor.grad.sub_(result_ref.tensor.grad) # No broadcasting specific logic for scalar op\n",
        "\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "\n",
        "\n",
        "    def _sub_scalar(self, scalar):\n",
        "        result_tensor = torch.sub(self.tensor, scalar)\n",
        "        if not self._custom_requires_grad:\n",
        "            return CustomTensor(result_tensor,due_to_operation=True)\n",
        "\n",
        "        graph = self.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "        graph.add_edge(self._node_id, result._node_id)\n",
        "\n",
        "        self_ref = weakref.proxy(self)\n",
        "        result_ref = weakref.proxy(result)\n",
        "        def _backward():\n",
        "            if self_ref.tensor.grad is None:\n",
        "                self_ref._zero_grad()\n",
        "            self_ref.tensor.grad.add_(result_ref.tensor.grad) # No broadcasting specific logic for scalar op\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "\n",
        "    def _sub_tensor(self, other):\n",
        "        result_tensor = torch.sub(self.tensor, other.tensor)\n",
        "        requires_grad = self._custom_requires_grad or other._custom_requires_grad\n",
        "        if not requires_grad:\n",
        "            return CustomTensor(result_tensor,due_to_operation=True)\n",
        "\n",
        "        graph = self.graph if self._custom_requires_grad else other.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "\n",
        "        self_ref = weakref.proxy(self)\n",
        "        other_ref = weakref.proxy(other)\n",
        "        result_ref = weakref.proxy(result)\n",
        "\n",
        "        if self._custom_requires_grad:\n",
        "            graph.add_edge(self._node_id, result._node_id)\n",
        "        if other._custom_requires_grad:\n",
        "            graph.add_edge(other._node_id, result._node_id)\n",
        "\n",
        "        def _backward():\n",
        "            if self_ref._custom_requires_grad:\n",
        "                if self_ref.tensor.grad is None:\n",
        "                    self_ref._zero_grad()\n",
        "                grad_for_self = self_ref._reduce_grad_for_broadcast(result_ref.tensor.grad, self_ref.tensor.shape)\n",
        "                self_ref.tensor.grad.add_(grad_for_self)\n",
        "            if other_ref._custom_requires_grad:\n",
        "                if other_ref.tensor.grad is None:\n",
        "                    other_ref._zero_grad()\n",
        "                grad_for_other = other_ref._reduce_grad_for_broadcast(-result_ref.tensor.grad, other_ref.tensor.shape)\n",
        "                other_ref.tensor.grad.add_(grad_for_other)\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "\n",
        "    def __truediv__(self, other):\n",
        "        if isinstance(other, numbers.Number):\n",
        "            return self._div_scalar(other)\n",
        "        elif isinstance(other, CustomTensor):\n",
        "            return self._div_tensor(other)\n",
        "        return NotImplemented\n",
        "    def __itruediv__(self,other):\n",
        "        if isinstance(other, numbers.Number):\n",
        "            self.tensor.div_(other)\n",
        "        elif isinstance(other,CustomTensor):\n",
        "            self.tensor.div_(other.tensor)\n",
        "    def _div_scalar(self, scalar):\n",
        "        result_tensor = torch.div(self.tensor, scalar)\n",
        "        if not self._custom_requires_grad:\n",
        "            return CustomTensor(result_tensor,due_to_operation=True)\n",
        "\n",
        "        graph = self.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "        graph.add_edge(self._node_id, result._node_id)\n",
        "\n",
        "        self_ref = weakref.proxy(self)\n",
        "        result_ref = weakref.proxy(result)\n",
        "        def _backward():\n",
        "            if self_ref.tensor.grad is None:\n",
        "                self_ref._zero_grad()\n",
        "            self_ref.tensor.grad.add_(result_ref.tensor.grad / scalar)\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "\n",
        "    def _div_tensor(self,other):\n",
        "        result_tensor = torch.div(self.tensor, other.tensor)\n",
        "        requires_grad = self._custom_requires_grad or other._custom_requires_grad\n",
        "        if not requires_grad:\n",
        "            return CustomTensor(result_tensor,due_to_operation=True)\n",
        "\n",
        "        graph = self.graph if self._custom_requires_grad else other.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "\n",
        "        self_ref = weakref.proxy(self)\n",
        "        other_ref = weakref.proxy(other)\n",
        "        result_ref = weakref.proxy(result)\n",
        "\n",
        "        if self._custom_requires_grad:\n",
        "            graph.add_edge(self._node_id, result._node_id)\n",
        "        if other._custom_requires_grad:\n",
        "            graph.add_edge(other._node_id, result._node_id)\n",
        "\n",
        "        def _backward():\n",
        "            if self_ref._custom_requires_grad:\n",
        "                if self_ref.tensor.grad is None:\n",
        "                    self_ref._zero_grad()\n",
        "                grad_for_self = self_ref._reduce_grad_for_broadcast(result_ref.tensor.grad / other_ref.tensor, self_ref.tensor.shape)\n",
        "                self_ref.tensor.grad.add_(grad_for_self)\n",
        "            if other_ref._custom_requires_grad:\n",
        "                if other_ref.tensor.grad is None:\n",
        "                    other_ref._zero_grad()\n",
        "                grad_for_other = other_ref._reduce_grad_for_broadcast(-result_ref.tensor.grad * self_ref.tensor / other_ref.tensor.pow(2), other_ref.tensor.shape)\n",
        "                other_ref.tensor.grad.add_(grad_for_other)\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "\n",
        "    def pow(self, scalar):\n",
        "        result_tensor = torch.pow(self.tensor, scalar)\n",
        "        if not self._custom_requires_grad:\n",
        "            return CustomTensor(result_tensor,due_to_operation=True)\n",
        "\n",
        "        graph = self.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "        graph.add_edge(self._node_id, result._node_id)\n",
        "\n",
        "        self_ref = weakref.proxy(self)\n",
        "        result_ref = weakref.proxy(result)\n",
        "        def _backward():\n",
        "            if self_ref.tensor.grad is None:\n",
        "                self_ref._zero_grad()\n",
        "            grad_contrib = scalar * self_ref.tensor.pow(scalar - 1)\n",
        "            self_ref.tensor.grad.add_(result_ref.tensor.grad * grad_contrib)\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "    def __ipow__(self,other):\n",
        "        self.tensor.pow_(other)\n",
        "\n",
        "    def exp(self):\n",
        "        out = torch.exp(self.tensor)\n",
        "        if not self._custom_requires_grad:\n",
        "            return CustomTensor(out,due_to_operation=True)\n",
        "\n",
        "        graph = self.graph\n",
        "        result = CustomTensor(out, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "        graph.add_edge(self._node_id, result._node_id)\n",
        "        self_ref = weakref.proxy(self)\n",
        "        result_ref = weakref.proxy(result)\n",
        "        def _backward():\n",
        "            if self_ref.tensor.grad is None:\n",
        "                self_ref._zero_grad()\n",
        "            self_ref.tensor.grad.add_(result_ref.tensor.grad * out)\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "\n",
        "    def log(self):\n",
        "        out = torch.log(self.tensor)\n",
        "        if not self._custom_requires_grad:\n",
        "            return CustomTensor(out,due_to_operation=True)\n",
        "\n",
        "        graph = self.graph\n",
        "        result = CustomTensor(out, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "        graph.add_edge(self._node_id, result._node_id)\n",
        "        self_ref = weakref.proxy(self)\n",
        "        result_ref = weakref.proxy(result)\n",
        "        def _backward():\n",
        "            if self_ref.tensor.grad is None:\n",
        "                self_ref._zero_grad()\n",
        "            self_ref.tensor.grad.add_(result_ref.tensor.grad / self_ref.tensor)\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "\n",
        "    def sin(self):\n",
        "        out = torch.sin(self.tensor)\n",
        "        if not self._custom_requires_grad:\n",
        "            return CustomTensor(out,due_to_operation=True)\n",
        "\n",
        "        graph = self.graph\n",
        "        result = CustomTensor(out, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "        graph.add_edge(self._node_id, result._node_id)\n",
        "        self_ref = weakref.proxy(self)\n",
        "        result_ref = weakref.proxy(result)\n",
        "        def _backward():\n",
        "            if self_ref.tensor.grad is None:\n",
        "                self_ref._zero_grad()\n",
        "            self_ref.tensor.grad.add_(result_ref.tensor.grad * torch.cos(self_ref.tensor))\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "\n",
        "    def cos(self):\n",
        "        out = torch.cos(self.tensor)\n",
        "        if not self._custom_requires_grad:\n",
        "            return CustomTensor(out,due_to_operation=True)\n",
        "\n",
        "        graph = self.graph\n",
        "        result = CustomTensor(out, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "        graph.add_edge(self._node_id, result._node_id)\n",
        "        self_ref = weakref.proxy(self)\n",
        "        result_ref = weakref.proxy(result)\n",
        "        def _backward():\n",
        "            if self_ref.tensor.grad is None:\n",
        "                self_ref._zero_grad()\n",
        "            self_ref.tensor.grad.add_(-result_ref.tensor.grad*torch.sin(self_ref.tensor))\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "\n",
        "    def sqrt(self):\n",
        "        out = torch.sqrt(self.tensor)\n",
        "        if not self._custom_requires_grad:\n",
        "            return CustomTensor(out,due_to_operation=True)\n",
        "\n",
        "        graph = self.graph\n",
        "        result = CustomTensor(out, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "        graph.add_edge(self._node_id, result._node_id)\n",
        "        self_ref = weakref.proxy(self)\n",
        "        result_ref = weakref.proxy(result)\n",
        "        def _backward():\n",
        "            if self_ref.tensor.grad is None:\n",
        "                self_ref._zero_grad()\n",
        "            self_ref.tensor.grad.add_(result_ref.tensor.grad*0.5*self_ref.tensor.pow(-0.5))\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "\n",
        "    def matmul(self, other):\n",
        "        result_tensor = torch.matmul(self.tensor, other.tensor)\n",
        "        requires_grad = self._custom_requires_grad or other._custom_requires_grad\n",
        "        if not requires_grad:\n",
        "            return CustomTensor(result_tensor,due_to_operation=True)\n",
        "\n",
        "        graph = self.graph if self._custom_requires_grad else other.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "\n",
        "        self_ref = weakref.proxy(self)\n",
        "        other_ref = weakref.proxy(other)\n",
        "        result_ref = weakref.proxy(result)\n",
        "\n",
        "        if self._custom_requires_grad:\n",
        "            graph.add_edge(self._node_id, result._node_id)\n",
        "        if other._custom_requires_grad:\n",
        "            graph.add_edge(other._node_id, result._node_id)\n",
        "\n",
        "        def _backward():\n",
        "            if self_ref._custom_requires_grad:\n",
        "                if self_ref.tensor.grad is None: self_ref._zero_grad()\n",
        "                # Use robust broadcasting for matmul gradient\n",
        "                grad_for_self = torch.matmul(result_ref.tensor.grad, other_ref.tensor.transpose(-2, -1))\n",
        "                self_ref.tensor.grad.add_(self_ref._reduce_grad_for_broadcast(grad_for_self, self_ref.tensor.shape))\n",
        "            if other_ref._custom_requires_grad:\n",
        "                if other_ref.tensor.grad is None: other_ref._zero_grad()\n",
        "                grad_for_other = torch.matmul(self_ref.tensor.transpose(-2, -1), result_ref.tensor.grad)\n",
        "                other_ref.tensor.grad.add_(other_ref._reduce_grad_for_broadcast(grad_for_other, other_ref.tensor.shape))\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "    def dot(self, other):\n",
        "        # torch.dot only works for 1D tensors, or for higher-D tensors,\n",
        "        # it flattens them to 1D and then computes the dot product.\n",
        "        # This means the gradients will also be 1D, so no complex broadcasting\n",
        "        # reduction is needed on the output gradient itself.\n",
        "        # However, the input tensors themselves could have been results of broadcasting ops.\n",
        "        # For a truly general dot product, you'd use torch.matmul.\n",
        "        result_tensor = torch.dot(self.tensor.reshape(-1), other.tensor.reshape(-1))\n",
        "        requires_grad = self._custom_requires_grad or other._custom_requires_grad\n",
        "        if not requires_grad:\n",
        "            return CustomTensor(result_tensor,due_to_operation=True)\n",
        "\n",
        "        graph = self.graph if self._custom_requires_grad else other.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "\n",
        "        self_ref = weakref.proxy(self)\n",
        "        other_ref = weakref.proxy(other)\n",
        "        result_ref = weakref.proxy(result)\n",
        "\n",
        "        if self._custom_requires_grad:\n",
        "            graph.add_edge(self._node_id, result._node_id)\n",
        "        if other._custom_requires_grad:\n",
        "            graph.add_edge(other._node_id, result._node_id)\n",
        "\n",
        "        def _backward():\n",
        "            if self_ref._custom_requires_grad:\n",
        "                if self_ref.tensor.grad is None:\n",
        "                    self_ref._zero_grad()\n",
        "                # The grad from result_ref.tensor.grad will be a scalar.\n",
        "                # It needs to be multiplied by the other_ref.tensor (original shape)\n",
        "                # and then potentially re-shaped if original was >1D\n",
        "                grad_contrib = result_ref.tensor.grad * other_ref.tensor\n",
        "                self_ref.tensor.grad.add_(grad_contrib)\n",
        "            if other_ref._custom_requires_grad:\n",
        "                if other_ref.tensor.grad is None:\n",
        "                    other_ref._zero_grad()\n",
        "                grad_contrib = result_ref.tensor.grad * self_ref.tensor\n",
        "                other_ref.tensor.grad.add_(grad_contrib)\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "\n",
        "\n",
        "\n",
        "    # --- Unary Operations ---\n",
        "\n",
        "    def sum(self, dim=None, keepdim=False):\n",
        "        \"\"\"Computes the sum of elements along given dimensions.\"\"\"\n",
        "        result_tensor = self.tensor.sum(dim=dim, keepdim=keepdim)\n",
        "        if not self._custom_requires_grad:\n",
        "            return CustomTensor(result_tensor, due_to_operation=True)\n",
        "\n",
        "        graph = self.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "        graph.add_edge(self._node_id, result._node_id)\n",
        "\n",
        "        self_ref = weakref.proxy(self)\n",
        "        result_ref = weakref.proxy(result)\n",
        "\n",
        "        def _backward():\n",
        "            if self_ref.tensor.grad is None:\n",
        "                self_ref._zero_grad()\n",
        "\n",
        "            grad = result_ref.tensor.grad\n",
        "            # If keepdim was false, the summed dim was squeezed. We need to unsqueeze it back for broadcasting.\n",
        "            if not keepdim and dim is not None:\n",
        "                grad = grad.unsqueeze(dim)\n",
        "\n",
        "            self_ref.tensor.grad.add_(grad)\n",
        "\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "\n",
        "    def mean(self, dim=None, keepdim=False):\n",
        "        \"\"\"Computes the mean of elements along given dimensions.\"\"\"\n",
        "        result_tensor = self.tensor.mean(dim=dim, keepdim=keepdim)\n",
        "        if not self._custom_requires_grad:\n",
        "            return CustomTensor(result_tensor, due_to_operation=True)\n",
        "\n",
        "        graph = self.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "        graph.add_edge(self._node_id, result._node_id)\n",
        "\n",
        "        self_ref = weakref.proxy(self)\n",
        "        result_ref = weakref.proxy(result)\n",
        "\n",
        "        # Determine the number of elements that were averaged\n",
        "        if dim is None:\n",
        "            n = self.tensor.numel()\n",
        "        else:\n",
        "            n = self.tensor.shape[dim]\n",
        "\n",
        "        def _backward():\n",
        "            if self_ref.tensor.grad is None:\n",
        "                self_ref._zero_grad()\n",
        "\n",
        "            grad = result_ref.tensor.grad\n",
        "            if not keepdim and dim is not None:\n",
        "                grad = grad.unsqueeze(dim)\n",
        "\n",
        "            # Distribute gradient evenly\n",
        "            self_ref.tensor.grad.add_(grad / n)\n",
        "\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "\n",
        "    def reshape(self, *shape):\n",
        "        \"\"\"Reshapes the tensor to the given shape.\"\"\"\n",
        "        original_shape = self.shape\n",
        "        result_tensor = self.tensor.reshape(*shape)\n",
        "        if not self._custom_requires_grad:\n",
        "            return CustomTensor(result_tensor, due_to_operation=True)\n",
        "\n",
        "        graph = self.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "        graph.add_edge(self._node_id, result._node_id)\n",
        "\n",
        "        self_ref = weakref.proxy(self)\n",
        "        result_ref = weakref.proxy(result)\n",
        "\n",
        "        def _backward():\n",
        "            if self_ref.tensor.grad is None:\n",
        "                self_ref._zero_grad()\n",
        "            self_ref.tensor.grad.add_(result_ref.tensor.grad.reshape(original_shape))\n",
        "\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "\n",
        "    def transpose(self, dim0, dim1):\n",
        "        \"\"\"Transposes dimensions dim0 and dim1.\"\"\"\n",
        "        result_tensor = self.tensor.transpose(dim0, dim1)\n",
        "        if not self._custom_requires_grad:\n",
        "            return CustomTensor(result_tensor, due_to_operation=True)\n",
        "\n",
        "        graph = self.graph\n",
        "        result = CustomTensor(result_tensor, _custom_requires_grad=True, graph=graph, due_to_operation=True, is_leaf=False)\n",
        "        graph.add_edge(self._node_id, result._node_id)\n",
        "\n",
        "        self_ref = weakref.proxy(self)\n",
        "        result_ref = weakref.proxy(result)\n",
        "\n",
        "        def _backward():\n",
        "            if self_ref.tensor.grad is None:\n",
        "                self_ref._zero_grad()\n",
        "            # The gradient operation for transpose is another transpose\n",
        "            self_ref.tensor.grad.add_(result_ref.tensor.grad.transpose(dim0, dim1))\n",
        "\n",
        "        result._backward = _backward\n",
        "        return result\n",
        "\n",
        "    @property\n",
        "    def T(self):\n",
        "        \"\"\"Alias for transpose(-2, -1) for 2D or higher dimensional tensors.\"\"\"\n",
        "        if self.ndim < 2:\n",
        "            raise ValueError(\"`.T` is only supported on tensors with 2 or more dimensions.\")\n",
        "        return self.transpose(-2, -1)\n",
        "\n",
        "    def backward(self, weightage_tensor=1,retain_graph=False):\n",
        "        if not self._custom_requires_grad:\n",
        "            raise RuntimeError(\"Output tensor does not require grad.\")\n",
        "        if self.graph is None:\n",
        "            raise RuntimeError(\"Output tensor is not part of a graph.\")\n",
        "        graph = self.graph\n",
        "\n",
        "        # Initialize gradient for the output tensor\n",
        "        if isinstance(weightage_tensor, numbers.Number):\n",
        "            self.tensor.grad = torch.full_like(self.tensor, fill_value=weightage_tensor)\n",
        "        elif isinstance(weightage_tensor, torch.Tensor):\n",
        "            self.tensor.grad = weightage_tensor.clone()\n",
        "\n",
        "        nodes_to_process = graph.reverse_toposort_from_tensor(self._node_id)\n",
        "\n",
        "        for tensor_node in nodes_to_process:\n",
        "            tensor_node._backward()\n",
        "        if not retain_graph:\n",
        "            graph.delete_all_non_leaf_nodes()\n",
        "\n",
        "            #try:\n",
        "                # The node is a weakref.proxy, check if it's still alive\n",
        "                #if tensor_node.__class__ is weakref.ProxyType:\n",
        "            #        tensor_node._backward()\n",
        "            # except ReferenceError:\n",
        "            #     # The tensor object was garbage collected, skip.\n",
        "            #     print(\"dead reference node encountered\")\n",
        "            #     continue\n",
        "    # --- Properties and Dunder Methods ---\n",
        "    @property\n",
        "    def dtype(self): return self.tensor.dtype\n",
        "    @property\n",
        "    def ndim(self): return self.tensor.ndim\n",
        "    @property\n",
        "    def shape(self): return self.tensor.shape\n",
        "    @property\n",
        "    def grad(self): return self.tensor.grad\n",
        "    def __repr__(self): return f\"CustomTensor({self.tensor}, grad_fn={self._backward != None}, requires_grad={self._custom_requires_grad})\"\n",
        "    # def __del__(self):\n",
        "    #     if self._node_id is not None and self._is_leaf:\n",
        "    #         try:\n",
        "    #             if self.graph: self.graph.delete_node(self._node_id)\n",
        "    #         except ReferenceError: # Graph might be gone first\n",
        "    #             pass\n",
        "if __name__ == \"__main__\":\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oAQMtGIHawCy"
      },
      "source": [
        "# Modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Alamzuz_atWw"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import math\n",
        "import weakref\n",
        "import torch.nn.functional as F\n",
        "from collections import OrderedDict\n",
        "device = \"cpu\"\n",
        "dtype = torch.float32\n",
        "class Module:\n",
        "    \"\"\"\n",
        "    Base class for all neural network modules. Your models should also subclass this class.\n",
        "    Modules can also contain other Modules, allowing to nest them in a tree structure.\n",
        "    \"\"\"\n",
        "    device=device\n",
        "    dtype=dtype\n",
        "    def __init__(self):\n",
        "        self._parameters = OrderedDict()\n",
        "        self._modules = OrderedDict()\n",
        "        # self._buffers = OrderedDict()\n",
        "        self.training = True #\n",
        "\n",
        "    def __setattr__(self, name, value):\n",
        "        if isinstance(value, CustomTensor):\n",
        "            if value._custom_requires_grad:\n",
        "                self._parameters[name] = value\n",
        "        elif isinstance(value, Module):\n",
        "            self._modules[name] = value\n",
        "        # Handle buffers (non-parameter tensors like running_mean in BatchNorm)\n",
        "        # elif isinstance(value, torch.Tensor):\n",
        "        #     self._buffers[name] = value\n",
        "        super().__setattr__(name, value)\n",
        "\n",
        "    def parameters(self):\n",
        "        \"\"\"Returns a list of all parameters in the module and its submodules.\"\"\"\n",
        "        params = list(self._parameters.values())\n",
        "        for module in self._modules.values():\n",
        "            params.extend(module.parameters())\n",
        "        return params\n",
        "\n",
        "    def zero_grad(self):\n",
        "        \"\"\"Sets gradients of all model parameters to zero.\"\"\"\n",
        "        for p in self.parameters():\n",
        "            p._zero_grad()\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"Sets the module and all its submodules to training mode.\"\"\"\n",
        "        self.training = True\n",
        "        for module in self._modules.values():\n",
        "            module.train()\n",
        "\n",
        "    def eval(self):\n",
        "        \"\"\"Sets the module and all its submodules to evaluation mode.\"\"\"\n",
        "        self.training = False\n",
        "        for module in self._modules.values():\n",
        "            module.eval()\n",
        "\n",
        "    def __call__(self, *args, **kwargs):\n",
        "        return self.forward(*args, **kwargs)\n",
        "\n",
        "    def forward(self, *args, **kwargs):\n",
        "        raise NotImplementedError(\"Subclasses of Module must implement a forward method.\")\n",
        "\n",
        "class Linear(Module):\n",
        "    \"\"\"Applies a linear transformation to the incoming data: y = xA^T + b\n",
        "    types of activation relu,leaky_relu, gelu, sigmoid, tanh, silu,elu\"\"\"\n",
        "\n",
        "    _ACTIVATION_INIT = {\n",
        "        \"relu\": (\"kaiming_uniform_\", \"relu\"),\n",
        "        \"gelu\": (\"kaiming_uniform_\", \"relu\"),\n",
        "        \"silu\": (\"kaiming_uniform_\", \"relu\"),\n",
        "        \"elu\": (\"kaiming_uniform_\", \"relu\"),\n",
        "        \"gelu_approx\": (\"kaiming_uniform_\", \"relu\"),\n",
        "        \"leaky_relu\": (\"kaiming_uniform_\", \"leaky_relu\"),\n",
        "        \"sigmoid\": (\"xavier_uniform_\", 1.0),\n",
        "        \"tanh\": (\"xavier_uniform_\", 5/3)\n",
        "    }\n",
        "\n",
        "    def __new__(cls, in_features, out_features, bias=True, *, graph=None, activation=\"relu\"):\n",
        "        assert activation in cls._ACTIVATION_INIT\n",
        "        return super().__new__(cls)\n",
        "\n",
        "    def __init__(self, in_features, out_features, bias=True, *, graph=None, activation=\"relu\"):\n",
        "        super().__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.graph = weakref.proxy(graph) if graph is not None else None\n",
        "\n",
        "        # Initialize weight\n",
        "        self.weight = CustomTensor(torch.empty(out_features, in_features, device=Linear.device, dtype=Linear.dtype),\n",
        "                                 _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "\n",
        "        init_method, init_param = self._ACTIVATION_INIT[activation]\n",
        "        if init_method == \"kaiming_uniform_\":\n",
        "            torch.nn.init.kaiming_uniform_(self.weight.tensor, nonlinearity=init_param)\n",
        "        else:  # xavier_uniform_\n",
        "            torch.nn.init.xavier_uniform_(self.weight.tensor, gain=init_param)\n",
        "\n",
        "        # Initialize bias\n",
        "        self.bias = CustomTensor(torch.zeros(out_features,device=Linear.device, dtype=Linear.dtype),\n",
        "                               _custom_requires_grad=True, graph=graph, is_leaf=True) if bias else None\n",
        "\n",
        "    def forward(self, input_tensor):\n",
        "        inp = input_tensor.tensor\n",
        "        is_1d = inp.ndim==1\n",
        "        if is_1d:\n",
        "            inp = inp.unsqueeze(0)\n",
        "        output = inp @ self.weight.tensor.transpose(-2, -1)\n",
        "        if self.bias is not None:\n",
        "            output.add_(self.bias.tensor)\n",
        "\n",
        "        if is_1d:\n",
        "            output = output.squeeze(0)\n",
        "        if not self.training:\n",
        "            return CustomTensor(output, due_to_operation=True)\n",
        "\n",
        "        # Training mode - setup gradient computation\n",
        "        result = CustomTensor(output, _custom_requires_grad=True, graph=self.graph,\n",
        "                            due_to_operation=True, is_leaf=False)\n",
        "\n",
        "        # Add edges to computation graph\n",
        "        if input_tensor._custom_requires_grad:\n",
        "            self.graph.add_edge(input_tensor._node_id, result._node_id)\n",
        "        self.graph.add_edge(self.weight._node_id, result._node_id)\n",
        "        if self.bias is not None:\n",
        "            self.graph.add_edge(self.bias._node_id, result._node_id)\n",
        "\n",
        "        # Create weak references for backward pass\n",
        "        refs = {\n",
        "            'weight': weakref.proxy(self.weight),\n",
        "            'input': weakref.proxy(input_tensor),\n",
        "            'result': weakref.proxy(result),\n",
        "            'bias': weakref.proxy(self.bias) if self.bias is not None else None,\n",
        "            'is_1d': is_1d\n",
        "        }\n",
        "\n",
        "        result._backward = self._create_backward(refs)\n",
        "        return result\n",
        "\n",
        "    def _create_backward(self, refs):\n",
        "        def _backward():\n",
        "            weight_ref, input_ref, result_ref, bias_ref, is_1d = refs['weight'], refs['input'], refs['result'], refs['bias'], refs['is_1d']\n",
        "            grad_output = result_ref.tensor.grad\n",
        "            inp = input_ref.tensor\n",
        "            if is_1d:\n",
        "                inp = inp.unsqueeze(0)\n",
        "                grad_output = grad_output.unsqueeze(0)\n",
        "\n",
        "            # Weight gradient\n",
        "            if weight_ref._custom_requires_grad:\n",
        "                if weight_ref.tensor.grad is None:\n",
        "                    weight_ref._zero_grad()\n",
        "                grad_w = torch.matmul(grad_output.transpose(-2, -1), inp)\n",
        "                weight_ref.tensor.grad.add_(weight_ref._reduce_grad_for_broadcast(grad_w, weight_ref.tensor.shape))\n",
        "\n",
        "            # Bias gradient\n",
        "            if bias_ref is not None and bias_ref._custom_requires_grad:\n",
        "                if bias_ref.tensor.grad is None:\n",
        "                    bias_ref._zero_grad()\n",
        "                grad_b = bias_ref._reduce_grad_for_broadcast(grad_output, bias_ref.tensor.shape)\n",
        "                bias_ref.tensor.grad.add_(grad_b)\n",
        "\n",
        "            # Input gradient\n",
        "            if input_ref._custom_requires_grad:\n",
        "                if input_ref.tensor.grad is None:\n",
        "                    input_ref._zero_grad()\n",
        "                grad_in = torch.matmul(grad_output, weight_ref.tensor)\n",
        "                if is_1d:\n",
        "                    grad_in = grad_in.squeeze(0)\n",
        "                input_ref.tensor.grad.add_(input_ref._reduce_grad_for_broadcast(grad_in, input_ref.tensor.shape))\n",
        "\n",
        "        return _backward\n",
        "\n",
        "class Conv2d(Module):\n",
        "    \"\"\"Applies a 2D convolution over an input signal composed of several input planes.\n",
        "    types of activation relu,leaky_relu, gelu, sigmoid, tanh, silu,elu\"\"\"\n",
        "\n",
        "    # Lookup table for activation initialization\n",
        "    _ACTIVATION_INIT = {\n",
        "        \"relu\": (\"kaiming_uniform_\", \"relu\"),\n",
        "        \"gelu\": (\"kaiming_uniform_\", \"relu\"),\n",
        "        \"silu\": (\"kaiming_uniform_\", \"relu\"),\n",
        "        \"elu\": (\"kaiming_uniform_\", \"relu\"),\n",
        "        \"gelu_approx\": (\"kaiming_uniform_\", \"relu\"),\n",
        "        \"leaky_relu\": (\"kaiming_uniform_\", \"leaky_relu\"),\n",
        "        \"sigmoid\": (\"xavier_uniform_\", 1.0),\n",
        "        \"tanh\": (\"xavier_uniform_\", 5/3)\n",
        "    }\n",
        "\n",
        "    def __new__(cls, *,in_channels, out_channels, kernel_size, stride=1,dilation=1,groups=1,bias=True, padding=0, graph=None,activation=\"relu\"):\n",
        "        assert isinstance(kernel_size, int) or len(kernel_size) == 2\n",
        "        assert isinstance(stride, int) or len(stride) == 2\n",
        "        assert isinstance(dilation, int) or len(dilation) == 2\n",
        "        assert isinstance(padding, int) or len(padding) == 2\n",
        "        assert activation in cls._ACTIVATION_INIT\n",
        "        return super().__new__(cls)\n",
        "\n",
        "    def __init__(self, *,in_channels, out_channels, kernel_size, stride=1,dilation=1,groups=1,bias=True, padding=0, graph=None,activation=\"relu\"):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.kernel_size = (kernel_size, kernel_size) if isinstance(kernel_size, int) else kernel_size\n",
        "        self.stride = (stride, stride) if isinstance(stride, int) else stride\n",
        "        self.dilation = (dilation, dilation) if isinstance(dilation, int) else dilation\n",
        "        self.padding = (padding, padding) if isinstance(padding, int) else padding\n",
        "        self.groups = groups\n",
        "        self.graph = weakref.proxy(graph) if graph is not None else None\n",
        "\n",
        "        weight_shape = (out_channels, in_channels // groups, *self.kernel_size)\n",
        "        self.weight = CustomTensor(torch.empty(weight_shape,device=Conv2d.device,dtype=Conv2d.dtype), _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "\n",
        "        # Use lookup table for initialization\n",
        "        init_method, init_param = self._ACTIVATION_INIT[activation]\n",
        "        if init_method == \"kaiming_uniform_\":\n",
        "            torch.nn.init.kaiming_uniform_(self.weight.tensor, nonlinearity=init_param)\n",
        "        else:  # xavier_uniform_\n",
        "            torch.nn.init.xavier_uniform_(self.weight.tensor, gain=init_param)\n",
        "\n",
        "        self.bias = CustomTensor(torch.zeros(out_channels,device=Conv2d.device,dtype=Conv2d.dtype), _custom_requires_grad=True, graph=graph, is_leaf=True) if bias else None\n",
        "\n",
        "    def forward(self, input_tensor):\n",
        "        output_tensor = F.conv2d(\n",
        "            input = input_tensor.tensor,\n",
        "            weight = self.weight.tensor,\n",
        "            bias = self.bias.tensor if self.bias else None,\n",
        "            stride = self.stride,\n",
        "            padding = self.padding,\n",
        "            groups=self.groups\n",
        "        )\n",
        "        if not self.training:\n",
        "            return CustomTensor(output_tensor, due_to_operation=True)\n",
        "\n",
        "        result = CustomTensor(output_tensor, _custom_requires_grad=True, graph=self.graph, due_to_operation=True, is_leaf=False)\n",
        "\n",
        "        self.graph.add_edge(input_tensor._node_id, result._node_id)\n",
        "        self.graph.add_edge(self.weight._node_id, result._node_id)\n",
        "        if self.bias is not None:\n",
        "            self.graph.add_edge(self.bias._node_id, result._node_id)\n",
        "\n",
        "        # Create weak references for backward pass\n",
        "        refs = {\n",
        "            'input': weakref.proxy(input_tensor),\n",
        "            'weight': weakref.proxy(self.weight),\n",
        "            'bias': weakref.proxy(self.bias) if self.bias is not None else None,\n",
        "            'result': weakref.proxy(result)\n",
        "        }\n",
        "\n",
        "        result._backward = self._create_backward(refs)\n",
        "        return result\n",
        "\n",
        "    def _create_backward(self, refs):\n",
        "        def _backward():\n",
        "            input_ref, weight_ref, bias_ref, result_ref = refs['input'], refs['weight'], refs['bias'], refs['result']\n",
        "            grad_output = result_ref.tensor.grad\n",
        "\n",
        "            if bias_ref is not None:\n",
        "                if bias_ref._custom_requires_grad:\n",
        "                    if bias_ref.tensor.grad is None: bias_ref._zero_grad()\n",
        "                    bias_ref.tensor.grad.add_(grad_output.sum(dim=[0, 2, 3]))\n",
        "\n",
        "            if input_ref._custom_requires_grad:\n",
        "                if input_ref.tensor.grad is None: input_ref._zero_grad()\n",
        "                input_ref.tensor.grad.add_(\n",
        "                    self._calculate_gradient_input_tensor(input_ref.tensor,weight_ref.tensor,grad_output)\n",
        "                )\n",
        "\n",
        "            if weight_ref._custom_requires_grad:\n",
        "                if weight_ref.tensor.grad is None: weight_ref._zero_grad()\n",
        "                # tried vectorizing groups but failed hence using autograd for computing weight for efficiency (NOTE This is considered cheating)\n",
        "                weight_ref.tensor.grad.add_(\n",
        "                    torch.nn.grad.conv2d_weight(\n",
        "                    input=input_ref.tensor,\n",
        "                    weight_size=weight_ref.tensor.shape,\n",
        "                    grad_output=grad_output,\n",
        "                    stride=self.stride,\n",
        "                    padding=self.padding,\n",
        "                    dilation=self.dilation,\n",
        "                    groups=self.groups\n",
        "                    )\n",
        "                )\n",
        "        return _backward\n",
        "\n",
        "    @torch.compile\n",
        "    def _calculate_gradient_input_tensor(self, input_tensor,weight_tensor,grad_output):\n",
        "        h_in, w_in = input_tensor.shape[2], input_tensor.shape[3]\n",
        "        h_out, w_out = grad_output.shape[2], grad_output.shape[3]\n",
        "        stride = self.stride\n",
        "        padding = self.padding\n",
        "        kernel_size = self.kernel_size\n",
        "        dilation = self.dilation\n",
        "        # The formula relating input size to output size in a transposed convolution is:\n",
        "        # InputSize = (OutputSize - 1) * stride - 2 * padding + dilation * (kernel - 1) + output_padding + 1\n",
        "        # We rearrange this to solve for the required output_padding.\n",
        "        output_padding_h = h_in - ((h_out - 1) * stride[0] - 2 * padding[0] + dilation[0] * (kernel_size[0] - 1) + 1)\n",
        "        output_padding_w = w_in - ((w_out - 1) * stride[1] - 2 * padding[1] + dilation[1] * (kernel_size[1] - 1) + 1)\n",
        "        output_padding = (output_padding_h, output_padding_w)\n",
        "\n",
        "        grad_input = F.conv_transpose2d(\n",
        "            grad_output,\n",
        "            weight_tensor,\n",
        "            stride=stride,\n",
        "            padding=padding,\n",
        "            output_padding=output_padding,\n",
        "            dilation=dilation,\n",
        "            groups=self.groups\n",
        "        )\n",
        "        return grad_input\n",
        "\n",
        "    @torch.compile\n",
        "    def _calculate_gradient_weight_tensor_loop(self,input_tensor,grad_output):\n",
        "        #The gradient w.r.t. the weights is a convolution\n",
        "        # of the input (X) and the output gradient (grad_output).\n",
        "        # For grouped convolutions, we must perform this calculation for each group separately.\n",
        "        #O(b,co,oh,ow)=B(co)+ kh =0∑KH −1  kw =0∑KW −1  ci=(co/G)⋅(Cin/G)∑((co/G)+1)⋅(Cin/G)−1\n",
        "        #  Ipadded(b,ci,ih,iw)K(co ,ci ,kh ,kw ),\n",
        "        # where ih  = oh.sh+kh.dh, iw = ow.sw+kw.dw\n",
        "\n",
        "        # ∂L/∂K(ci′ ,co′ ,kh′ ,kw′ ) =b,oh,ow∑ G(b,co',oh,ow)\n",
        "        # Ipadded(b,ci', oh.sh + kh'.dh, ow.sw + kw'.dw)\n",
        "\n",
        "        # the original operation is a summation over kh and kw and the input image\n",
        "        # coordinates ih iw are sampled with dilation. (oh and ow for individual coordinates are constant)\n",
        "\n",
        "\n",
        "        # the equation for the gradient is a summation over oh and ow and the input image\n",
        "        # coordinates ih iw are sampled with stride.\n",
        "        # (kh and kw are constant for individual coordinates are constant)\n",
        "\n",
        "        # hence when calling conv2d we need to switch stride and dilation\n",
        "        # and also transpose the dimensions of batch and channel as for derivative with respect to weight the channels are fixed in the summation\n",
        "\n",
        "        in_channels = self.in_channels\n",
        "        groups = self.groups\n",
        "        out_channels = self.out_channels\n",
        "        in_channels_per_group = in_channels // groups\n",
        "        out_channels_per_group = out_channels // groups\n",
        "        grad_W_groups = []\n",
        "\n",
        "        for g in range(groups):\n",
        "            # Slice the input tensor to get the channels for the current group\n",
        "            start_in_ch = g * in_channels_per_group\n",
        "            end_in_ch = start_in_ch + in_channels_per_group\n",
        "            X_g = input_tensor[:, start_in_ch:end_in_ch, :, :]\n",
        "\n",
        "            # Slice the output gradient tensor to get the channels for the current group\n",
        "            start_out_ch = g * out_channels_per_group\n",
        "            end_out_ch = start_out_ch + out_channels_per_group\n",
        "            grad_output_g = grad_output[:, start_out_ch:end_out_ch, :, :]\n",
        "\n",
        "            # To calculate the weight gradient via a convolution, we must cleverly\n",
        "            # permute the input (X_g) and output gradient (grad_output_g) tensors.\n",
        "            # We treat X_g as the input and grad_output_g as the kernel.\n",
        "            # X_g: (N, Cin/g, H, W) -> permute -> (Cin/g, N, H, W)\n",
        "            # grad_output_g: (N, Cout/g, oH, oW) -> permute -> (Cout/g, N, oH, oW)\n",
        "            # The F.conv2d call then treats 'Cin/g' as the batch size and 'N' as the input channels.\n",
        "            # The stride and dilation parameters from the original convolution are swapped.\n",
        "            X_g_permuted = X_g.transpose(0, 1)\n",
        "            grad_output_g_permuted = grad_output_g.transpose(0, 1)\n",
        "\n",
        "            grad_W_g_permuted = F.conv2d(\n",
        "                X_g_permuted,\n",
        "                grad_output_g_permuted,\n",
        "                stride=self.dilation,\n",
        "                padding=self.padding,\n",
        "                dilation=self.stride,\n",
        "                groups=1 # The group calculation is handled by our loop, so this is a standard conv.\n",
        "            )\n",
        "\n",
        "            # The result has shape (Cin/g, Cout/g, kH, kW). We must permute it back to\n",
        "            # the standard weight layout of (Cout/g, Cin/g, kH, kW).\n",
        "            grad_W_g = grad_W_g_permuted.transpose(0, 1)\n",
        "            grad_W_groups.append(grad_W_g)\n",
        "\n",
        "        # Concatenate the gradients from all groups along the output channel dimension.\n",
        "        # The weight tensor for grouped convolutions is laid out by stacking the weights\n",
        "        # for each group, so we do the same for the gradient.\n",
        "        grad_weight = torch.cat(grad_W_groups, dim=0)\n",
        "        return grad_weight\n",
        "\n",
        "    # def _calculate_gradient_weight_tensor_cheating(self,input_tensor,grad_output):\n",
        "    #     return torch.nn.grad.conv2d_weight(\n",
        "    #     input=input_tensor,\n",
        "    #     weight_size=self.weight.tensor.shape,\n",
        "    #     grad_output=grad_output,\n",
        "    #     stride=self.stride,\n",
        "    #     padding=self.padding,\n",
        "    #     dilation=self.dilation,\n",
        "    #     groups=self.groups\n",
        "    #     )\n",
        "\n",
        "class BatchNorm_Nd(Module):\n",
        "    def __new__(cls, num_features, eps=1e-5, momentum=0.1, *, graph=None):\n",
        "        assert num_features > 0\n",
        "        return super().__new__(cls)\n",
        "\n",
        "    def __init__(self, num_features, eps=1e-5, momentum=0.1, *, graph=None):\n",
        "        super().__init__()\n",
        "        self.num_features = num_features\n",
        "        self.eps = eps\n",
        "        self.momentum = momentum\n",
        "        self.graph = weakref.proxy(graph)\n",
        "\n",
        "        self.weight = CustomTensor(torch.ones(num_features,device=BatchNorm_Nd.device,dtype=BatchNorm_Nd.dtype), _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "        self.bias = CustomTensor(torch.zeros(num_features,device=BatchNorm_Nd.device,dtype=BatchNorm_Nd.dtype), _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "\n",
        "        self.running_mean = torch.zeros(num_features,device=BatchNorm_Nd.device,dtype=BatchNorm_Nd.dtype)\n",
        "        self.running_var = torch.ones(num_features,device=BatchNorm_Nd.device,dtype=BatchNorm_Nd.dtype)\n",
        "\n",
        "        self._channel_axis = 1\n",
        "        self._shape_cache = {}\n",
        "\n",
        "    def _get_broadcast_shape(self, input_shape):\n",
        "        if input_shape not in self._shape_cache:\n",
        "            self._shape_cache[input_shape] = (1,) + (input_shape[1],) + (1,) * (len(input_shape) - 2)\n",
        "        return self._shape_cache[input_shape]\n",
        "\n",
        "    @torch.compile\n",
        "    def _compute_stats(self, x: torch.Tensor):\n",
        "        reduce_dims = tuple(i for i in range(x.dim()) if i != self._channel_axis)\n",
        "\n",
        "        mean = x.mean(dim=reduce_dims, keepdim=False)\n",
        "        var = x.var(dim=reduce_dims, keepdim=False, unbiased=False)\n",
        "\n",
        "        return mean, var\n",
        "\n",
        "    def _create_backward(self, input_tensor, result, torch_input_tensor, normalized,\n",
        "                        shape_to, weight_shaped, input_minus_mean, inv_std, total_elements):\n",
        "        \"\"\"Creates the _backward hook for result tensor\"\"\"\n",
        "        input_ref = weakref.proxy(input_tensor)\n",
        "        result_ref = weakref.proxy(result)\n",
        "        weight_ref = weakref.proxy(self.weight)\n",
        "        bias_ref = weakref.proxy(self.bias)\n",
        "\n",
        "        def _backward():\n",
        "            result_gradient = result_ref.tensor.grad\n",
        "            reduce_dims = tuple(i for i in range(input_ref.tensor.dim()) if i != self._channel_axis)\n",
        "            if bias_ref._custom_requires_grad:\n",
        "                if bias_ref.tensor.grad is None:\n",
        "                    bias_ref._zero_grad()\n",
        "                grad_bias = result_gradient.sum(dim=reduce_dims)\n",
        "                bias_ref.tensor.grad.add_(grad_bias.view(bias_ref.tensor.shape))\n",
        "\n",
        "            if weight_ref._custom_requires_grad:\n",
        "                if weight_ref.tensor.grad is None:\n",
        "                    weight_ref._zero_grad()\n",
        "                grad_weight = (result_gradient * normalized).sum(dim=reduce_dims)\n",
        "                weight_ref.tensor.grad.add_(grad_weight.view(weight_ref.tensor.shape))\n",
        "\n",
        "            if input_ref._custom_requires_grad:\n",
        "                if input_ref.tensor.grad is None:\n",
        "                    input_ref._zero_grad()\n",
        "                grad_input = self.batchnorm_gradient_for_input_tensor(\n",
        "                    result_gradient=result_gradient,\n",
        "                    input_tensor=torch_input_tensor,\n",
        "                    weight_shaped=weight_shaped,\n",
        "                    input_minus_mean=input_minus_mean,\n",
        "                    inv_std=inv_std,\n",
        "                    total_elements=total_elements\n",
        "                )\n",
        "                input_ref.tensor.grad.add_(grad_input)\n",
        "\n",
        "        return _backward\n",
        "\n",
        "    def forward(self, input_tensor):\n",
        "        torch_input_tensor = input_tensor.tensor\n",
        "        shape_to = self._get_broadcast_shape(torch_input_tensor.shape)\n",
        "\n",
        "        # Pre-compute shaped tensors once\n",
        "        weight_shaped = self.weight.tensor.view(shape_to)\n",
        "        bias_shaped = self.bias.tensor.view(shape_to)\n",
        "\n",
        "        if self.training:\n",
        "            batch_mean, batch_var = self._compute_stats(torch_input_tensor)\n",
        "            total_elements = torch_input_tensor.numel() // torch_input_tensor.shape[self._channel_axis]\n",
        "            unbiased_var = batch_var * total_elements / (total_elements - 1) if total_elements > 1 else batch_var\n",
        "\n",
        "            # Update running statistics in-place\n",
        "            self.running_mean.mul_(1-self.momentum).add_(batch_mean, alpha=self.momentum)\n",
        "            self.running_var.mul_(1-self.momentum).add_(unbiased_var, alpha=self.momentum)\n",
        "\n",
        "            mean, var = batch_mean, batch_var\n",
        "        else:\n",
        "            mean, var = self.running_mean, self.running_var\n",
        "            mean_shaped = mean.view(shape_to)\n",
        "            var_shaped = var.view(shape_to)\n",
        "            normalized = (torch_input_tensor - mean_shaped) / torch.sqrt(var_shaped + self.eps)\n",
        "            result = normalized * weight_shaped + bias_shaped\n",
        "            return CustomTensor(result, due_to_operation=True)\n",
        "\n",
        "        # Forward pass computation (training mode)\n",
        "        mean_shaped = mean.view(shape_to)\n",
        "        var_shaped = var.view(shape_to)\n",
        "\n",
        "        inv_std = torch.rsqrt(var_shaped + self.eps)\n",
        "        input_minus_mean = torch_input_tensor - mean_shaped\n",
        "        normalized = input_minus_mean * inv_std\n",
        "        output = normalized * weight_shaped + bias_shaped\n",
        "\n",
        "        result = CustomTensor(output, _custom_requires_grad=True, graph=self.graph,due_to_operation=True, is_leaf=False)\n",
        "\n",
        "        # Build computation graph\n",
        "        graph = self.graph\n",
        "        graph.add_edge(input_tensor._node_id, result._node_id)\n",
        "        graph.add_edge(self.weight._node_id, result._node_id)\n",
        "        graph.add_edge(self.bias._node_id, result._node_id)\n",
        "\n",
        "        # Create and assign backward function\n",
        "        result._backward = self._create_backward(\n",
        "            input_tensor, result, torch_input_tensor, normalized,\n",
        "            shape_to, weight_shaped, input_minus_mean, inv_std, total_elements\n",
        "        )\n",
        "\n",
        "        return result\n",
        "\n",
        "    @torch.compile\n",
        "    def batchnorm_gradient_for_input_tensor(self, *, result_gradient, input_tensor, weight_shaped,\n",
        "                                          input_minus_mean, inv_std, total_elements):\n",
        "        reduce_dims = tuple(i for i in range(input_tensor.dim()) if i != self._channel_axis)\n",
        "\n",
        "        outer_term = weight_shaped * inv_std\n",
        "        term_1 = result_gradient\n",
        "        term_2 = (-1/total_elements) * result_gradient.sum(dim=reduce_dims, keepdim=True)\n",
        "        term3_sum_component = (input_minus_mean * result_gradient).sum(dim=reduce_dims, keepdim=True)\n",
        "        term3 = inv_std**2 * (-1/total_elements) * input_minus_mean * term3_sum_component\n",
        "        return outer_term * (term_1 + term_2 + term3)\n",
        "\n",
        "class MaxPool2d(Module):\n",
        "    def __new__(cls, *, kernel_size, stride=1, padding=0, dilation=1, graph=None):\n",
        "        assert isinstance(kernel_size, int) or len(kernel_size) == 2\n",
        "        assert isinstance(stride, int) or len(stride) == 2\n",
        "        assert isinstance(dilation, int) or len(dilation) == 2\n",
        "        assert isinstance(padding, int) or len(padding) == 2\n",
        "        return super().__new__(cls)\n",
        "\n",
        "    def __init__(self, *, kernel_size, stride=1, padding=0, dilation=1, graph=None):\n",
        "        super().__init__()\n",
        "        self.kernel_size = (kernel_size, kernel_size) if isinstance(kernel_size, int) else kernel_size\n",
        "        self.stride = (stride, stride) if isinstance(stride, int) else stride\n",
        "        self.dilation = (dilation, dilation) if isinstance(dilation, int) else dilation\n",
        "        self.padding = (padding, padding) if isinstance(padding, int) else padding\n",
        "        self.graph = weakref.proxy(graph) if graph is not None else None\n",
        "\n",
        "    def _create_backward(self, input_tensor, result, cached_indices):\n",
        "        \"\"\"Creates the _backward hook for result tensor\"\"\"\n",
        "        input_ref = weakref.proxy(input_tensor)\n",
        "        result_ref = weakref.proxy(result)\n",
        "\n",
        "        def _backward():\n",
        "            if input_ref.tensor.grad is None:\n",
        "                input_ref._zero_grad()\n",
        "            grad_output = result_ref.tensor.grad\n",
        "            input = input_ref.tensor\n",
        "            grad_input = MaxPool2d._calculate_gradient_input_tensor(grad_output, cached_indices, input)\n",
        "            input_ref.tensor.grad.add_(grad_input)\n",
        "\n",
        "        return _backward\n",
        "\n",
        "    def forward(self, input_tensor):\n",
        "        kernel_size = self.kernel_size\n",
        "        stride = self.stride\n",
        "        padding = self.padding\n",
        "        dilation = self.dilation\n",
        "\n",
        "        output_tensor, max_indices = F.max_pool2d(\n",
        "            input=input_tensor.tensor,\n",
        "            kernel_size=kernel_size,\n",
        "            stride=stride,\n",
        "            padding=padding,\n",
        "            dilation=dilation,\n",
        "            return_indices=True\n",
        "        )\n",
        "\n",
        "        if not self.training:\n",
        "            return CustomTensor(output_tensor, due_to_operation=True)\n",
        "\n",
        "        graph = self.graph\n",
        "        result = CustomTensor(output_tensor, _custom_requires_grad=True, graph=graph,due_to_operation=True, is_leaf=False)\n",
        "        graph.add_edge(input_tensor._node_id, result._node_id)\n",
        "\n",
        "\n",
        "        result._backward = self._create_backward(input_tensor, result, max_indices)\n",
        "\n",
        "        return result\n",
        "    @staticmethod\n",
        "    @torch.compile\n",
        "    def _calculate_gradient_input_tensor(grad_output, indices, input):\n",
        "      # grad_output: (N, C, H_out, W_out)\n",
        "      # indices:     (N, C, H_out, W_out)\n",
        "      N, C, H_out, W_out = grad_output.shape\n",
        "      # Initialize grad_input\n",
        "      grad_input = torch.zeros_like(input)\n",
        "      # Flatten spatial dims\n",
        "      grad_output_flat = grad_output.view(N, C, -1)\n",
        "      indices_flat = indices.view(N, C, -1)\n",
        "      grad_input_flat = grad_input.view(N, C, -1)\n",
        "      # Scatter gradients into appropriate positions\n",
        "      grad_input_flat.scatter_add_(2, indices_flat, grad_output_flat)\n",
        "      # Reshape back to input shape\n",
        "      grad_input = grad_input_flat.view(input.shape)\n",
        "      return grad_input\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"MaxPool2d(kernel_size={self.kernel_size}, stride={self.stride}, padding={self.padding})\"\n",
        "\n",
        "class AvgPool2d(Module):\n",
        "    def __new__(cls, *, kernel_size, stride=1, padding=0, graph=None):\n",
        "        assert isinstance(kernel_size, int) or len(kernel_size) == 2\n",
        "        assert isinstance(stride, int) or len(stride) == 2\n",
        "        assert isinstance(padding, int) or len(padding) == 2\n",
        "        return super().__new__(cls)\n",
        "\n",
        "    def __init__(self, *, kernel_size, stride=1, padding=0, graph=None):\n",
        "        super().__init__()\n",
        "        self.kernel_size = (kernel_size, kernel_size) if isinstance(kernel_size, int) else kernel_size\n",
        "        self.stride = (stride, stride) if isinstance(stride, int) else stride\n",
        "        self.padding = (padding, padding) if isinstance(padding, int) else padding\n",
        "        self.graph = weakref.proxy(graph) if graph is not None else None\n",
        "\n",
        "    def create_backward(self, input_tensor, result):\n",
        "        \"\"\"Creates the _backward hook for result tensor\"\"\"\n",
        "        input_ref = weakref.proxy(input_tensor)\n",
        "        result_ref = weakref.proxy(result)\n",
        "\n",
        "        def _backward():\n",
        "            if input_ref.tensor.grad is None:\n",
        "                input_ref._zero_grad()\n",
        "            grad_output = result_ref.tensor.grad\n",
        "            input = input_ref.tensor\n",
        "            grad_input = self._calculate_gradient_input_tensor(grad_output,input)\n",
        "            input_ref.tensor.grad.add_(grad_input)\n",
        "\n",
        "        return _backward\n",
        "\n",
        "    def forward(self, input_tensor):\n",
        "        kernel_size = self.kernel_size\n",
        "        stride = self.stride\n",
        "        padding = self.padding\n",
        "\n",
        "        output_tensor = F.avg_pool2d(\n",
        "            input=input_tensor.tensor,\n",
        "            kernel_size=kernel_size,\n",
        "            stride=stride,\n",
        "            padding=padding,\n",
        "            count_include_pad=True\n",
        "        )\n",
        "\n",
        "        if not self.training:\n",
        "            return CustomTensor(output_tensor, due_to_operation=True)\n",
        "\n",
        "        result = CustomTensor(output_tensor, _custom_requires_grad=True, graph=self.graph, due_to_operation=True, is_leaf=False)\n",
        "        graph = self.graph\n",
        "        graph.add_edge(input_tensor._node_id, result._node_id)\n",
        "\n",
        "        result._backward = self.create_backward(input_tensor, result)\n",
        "\n",
        "        return result\n",
        "\n",
        "    @torch.compile\n",
        "    def _calculate_gradient_input_tensor(self,grad_output,input):\n",
        "\n",
        "            h_in, w_in = input.shape[2], input.shape[3]\n",
        "            h_out, w_out = grad_output.shape[2], grad_output.shape[3]\n",
        "            kernel_size=self.kernel_size\n",
        "            stride=self.stride\n",
        "            padding=self.padding\n",
        "\n",
        "            # The formula relating input size to output size in a transposed convolution is:\n",
        "            # InputSize = (OutputSize - 1) * stride - 2 * padding + dilation * (kernel - 1) + output_padding + 1\n",
        "            # We rearrange this to solve for the required output_padding.\n",
        "            output_padding_h = h_in - ((h_out - 1) * stride[0] - 2 * padding[0] +  (kernel_size[0] - 1) + 1)\n",
        "            output_padding_w = w_in - ((w_out - 1) * stride[1] - 2 * padding[1] +  (kernel_size[1] - 1) + 1)\n",
        "            output_padding = (output_padding_h, output_padding_w)\n",
        "            pool_size = kernel_size[0] * kernel_size[1]\n",
        "            grad_kernel = torch.ones(grad_output.shape[1], 1, kernel_size[0], kernel_size[1],device=grad_output.device,dtype=grad_output.dtype) / pool_size\n",
        "            grad_input = F.conv_transpose2d(\n",
        "                input= grad_output,\n",
        "                weight = grad_kernel,\n",
        "                stride = stride,\n",
        "                padding = padding,\n",
        "                output_padding=output_padding,\n",
        "                groups = input.shape[1]\n",
        "            )\n",
        "            return grad_input\n",
        "\n",
        "class ReLu(Module):\n",
        "    def __init__(self, *, graph=None):\n",
        "        super().__init__()\n",
        "        self.graph = weakref.proxy(graph) if graph is not None else None\n",
        "\n",
        "    def _create_backward(self, input_tensor, result):\n",
        "        \"\"\"Creates the _backward hook for result tensor\"\"\"\n",
        "        input_ref = weakref.proxy(input_tensor)\n",
        "        result_ref = weakref.proxy(result)\n",
        "\n",
        "        def _backward():\n",
        "            if input_ref.tensor.grad is None:\n",
        "                input_ref._zero_grad()\n",
        "            grad_output = result_ref.tensor.grad\n",
        "            grad_input = grad_output.clone()\n",
        "            grad_input[input_ref.tensor <= 0] = 0\n",
        "            input_ref.tensor.grad.add_(grad_input)\n",
        "\n",
        "        return _backward\n",
        "\n",
        "    def forward(self, input_tensor):\n",
        "        output_tensor = F.relu(input_tensor.tensor)\n",
        "        if not self.training:\n",
        "            return CustomTensor(output_tensor, due_to_operation=True)\n",
        "\n",
        "        result = CustomTensor(output_tensor, _custom_requires_grad=True, graph=self.graph,due_to_operation=True, is_leaf=False)\n",
        "        self.graph.add_edge(input_tensor._node_id, result._node_id)\n",
        "        result._backward = self._create_backward(input_tensor, result)\n",
        "        return result\n",
        "\n",
        "class Leaky_ReLu(Module):\n",
        "    def __new__(cls, *, negative_slope=0.01, graph=None):\n",
        "        assert negative_slope > 0\n",
        "        return super().__new__(cls)\n",
        "\n",
        "    def __init__(self, *, negative_slope=0.01, graph=None):\n",
        "        super().__init__()\n",
        "        self.graph = weakref.proxy(graph) if graph is not None else None\n",
        "        self.negative_slope = negative_slope\n",
        "\n",
        "    def _create_backward(self, input_tensor, result):\n",
        "        \"\"\"Creates the _backward hook for result tensor\"\"\"\n",
        "        input_ref = weakref.proxy(input_tensor)\n",
        "        result_ref = weakref.proxy(result)\n",
        "\n",
        "        def _backward():\n",
        "            if input_ref.tensor.grad is None:\n",
        "                input_ref._zero_grad()\n",
        "            grad_output = result_ref.tensor.grad\n",
        "            grad_input = grad_output.clone()\n",
        "            grad_input[input_ref.tensor <= 0] *= self.negative_slope\n",
        "            input_ref.tensor.grad.add_(grad_input)\n",
        "\n",
        "        return _backward\n",
        "\n",
        "    def forward(self, input_tensor):\n",
        "        output_tensor = F.leaky_relu(input_tensor.tensor, negative_slope=self.negative_slope)\n",
        "        if not self.training:\n",
        "            return CustomTensor(output_tensor, due_to_operation=True)\n",
        "\n",
        "        result = CustomTensor(output_tensor, _custom_requires_grad=True, graph=self.graph,due_to_operation=True, is_leaf=False)\n",
        "        self.graph.add_edge(input_tensor._node_id, result._node_id)\n",
        "        result._backward = self._create_backward(input_tensor, result)\n",
        "        return result\n",
        "\n",
        "class Elu(Module):\n",
        "    def __new__(cls, *, alpha=1.0, graph=None):\n",
        "        assert alpha > 0\n",
        "        return super().__new__(cls)\n",
        "\n",
        "    def __init__(self, *, alpha=1.0, graph=None):\n",
        "        super().__init__()\n",
        "        self.graph = weakref.proxy(graph) if graph is not None else None\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def _create_backward(self, input_tensor, result, output_tensor):\n",
        "        \"\"\"Creates the _backward hook for result tensor\"\"\"\n",
        "        input_ref = weakref.proxy(input_tensor)\n",
        "        result_ref = weakref.proxy(result)\n",
        "\n",
        "        def _backward():\n",
        "            if input_ref.tensor.grad is None:\n",
        "                input_ref._zero_grad()\n",
        "            grad_output = result_ref.tensor.grad\n",
        "            grad_input = grad_output.clone()\n",
        "            mask_neg = (input_ref.tensor.data <= 0)\n",
        "            grad_input[mask_neg] *= (self.alpha + output_tensor[mask_neg])\n",
        "            input_ref.tensor.grad.add_(grad_input)\n",
        "\n",
        "        return _backward\n",
        "\n",
        "    def forward(self, input_tensor):\n",
        "        output_tensor = F.elu(input_tensor.tensor, alpha=self.alpha)\n",
        "        if not self.training:\n",
        "            return CustomTensor(output_tensor, due_to_operation=True)\n",
        "\n",
        "        result = CustomTensor(output_tensor, _custom_requires_grad=True, graph=self.graph,due_to_operation=True, is_leaf=False)\n",
        "        self.graph.add_edge(input_tensor._node_id, result._node_id)\n",
        "        result._backward = self._create_backward(input_tensor, result, output_tensor)\n",
        "        return result\n",
        "\n",
        "class GeLu(Module):\n",
        "    def __new__(cls, *, approximate='none', graph=None):\n",
        "        assert approximate in {\"none\", \"tanh\"}\n",
        "        return super().__new__(cls)\n",
        "\n",
        "    def __init__(self, *, approximate='none', graph=None):\n",
        "        super().__init__()\n",
        "        self.graph = weakref.proxy(graph) if graph is not None else None\n",
        "        self.approximate = approximate\n",
        "\n",
        "    def _create_backward(self, input_tensor, result):\n",
        "        \"\"\"Creates the _backward hook for result tensor\"\"\"\n",
        "        input_ref = weakref.proxy(input_tensor)\n",
        "        result_ref = weakref.proxy(result)\n",
        "\n",
        "        def _backward():\n",
        "            if input_ref.tensor.grad is None:\n",
        "                input_ref._zero_grad()\n",
        "            grad_output = result_ref.tensor.grad\n",
        "            grad_input = GeLu.gelu_derivative(input_ref.tensor, grad_output, self.approximate)\n",
        "            input_ref.tensor.grad.add_(grad_input)\n",
        "\n",
        "        return _backward\n",
        "\n",
        "    def forward(self, input_tensor):\n",
        "        output_tensor = F.gelu(input_tensor.tensor, approximate=self.approximate)\n",
        "        if not self.training:\n",
        "            return CustomTensor(output_tensor, due_to_operation=True)\n",
        "\n",
        "        result = CustomTensor(output_tensor, _custom_requires_grad=True, graph=self.graph,due_to_operation=True, is_leaf=False)\n",
        "        self.graph.add_edge(input_tensor._node_id, result._node_id)\n",
        "        result._backward = self._create_backward(input_tensor, result)\n",
        "        return result\n",
        "\n",
        "    @torch.compile\n",
        "    @staticmethod\n",
        "    def gelu_derivative(x: torch.Tensor, grad_output: torch.Tensor, approximate: str) -> torch.Tensor:\n",
        "        if approximate == \"none\":\n",
        "            sqrt_2_pi = 2.5066282749176025  # torch.tensor(2 * torch.pi).sqrt()\n",
        "            phi_x_cdf = 0.5 * (1 + torch.special.erf(x / 1.4142135381698608))  # torch.sqrt(torch.tensor(2.0))))\n",
        "            phi_x_pdf = torch.exp(-0.5 * x**2) / sqrt_2_pi\n",
        "            return (phi_x_cdf + x * phi_x_pdf) * grad_output\n",
        "        else:\n",
        "            sqrt_2_over_pi = 0.7978845238685608  # torch.tensor(2.0 / torch.pi).sqrt()\n",
        "            coeff_cubic = 0.044715\n",
        "            x2 = x.square()\n",
        "            inner = x + coeff_cubic * x2 * x\n",
        "            u = sqrt_2_over_pi * inner\n",
        "            tanh_u = torch.tanh(u)\n",
        "            poly = 1 + 3 * coeff_cubic * x2\n",
        "            return (0.5 * tanh_u + 0.5 * (1 - tanh_u.square()) * (sqrt_2_over_pi * poly * x) + 0.5) * grad_output\n",
        "\n",
        "class Sigmoid(Module):\n",
        "    def __new__(cls, *, graph=None):\n",
        "        return super().__new__(cls)\n",
        "\n",
        "    def __init__(self, *, graph=None):\n",
        "        super().__init__()\n",
        "        self.graph = weakref.proxy(graph) if graph is not None else None\n",
        "\n",
        "    def _create_backward(self, input_tensor, result, output_tensor):\n",
        "        \"\"\"Creates the _backward hook for result tensor\"\"\"\n",
        "        input_ref = weakref.proxy(input_tensor)\n",
        "        result_ref = weakref.proxy(result)\n",
        "\n",
        "        def _backward():\n",
        "            if input_ref.tensor.grad is None:\n",
        "                input_ref._zero_grad()\n",
        "            grad_output = result_ref.tensor.grad\n",
        "            grad_input = grad_output * output_tensor * (1 - output_tensor)\n",
        "            input_ref.tensor.grad.add_(grad_input)\n",
        "\n",
        "        return _backward\n",
        "\n",
        "    def forward(self, input_tensor):\n",
        "        output_tensor = F.sigmoid(input_tensor.tensor)\n",
        "        if not self.training:\n",
        "            return CustomTensor(output_tensor, due_to_operation=True)\n",
        "\n",
        "        result = CustomTensor(output_tensor, _custom_requires_grad=True, graph=self.graph,due_to_operation=True, is_leaf=False)\n",
        "        self.graph.add_edge(input_tensor._node_id, result._node_id)\n",
        "        result._backward = self._create_backward(input_tensor, result, output_tensor)\n",
        "        return result\n",
        "\n",
        "class Tanh(Module):\n",
        "    def __new__(cls, *, graph=None):\n",
        "        return super().__new__(cls)\n",
        "\n",
        "    def __init__(self, *, graph=None):\n",
        "        super().__init__()\n",
        "        self.graph = weakref.proxy(graph) if graph is not None else None\n",
        "\n",
        "    def _create_backward(self, input_tensor, result, output_tensor):\n",
        "        \"\"\"Creates the _backward hook for result tensor\"\"\"\n",
        "        input_ref = weakref.proxy(input_tensor)\n",
        "        result_ref = weakref.proxy(result)\n",
        "\n",
        "        def _backward():\n",
        "            if input_ref.tensor.grad is None:\n",
        "                input_ref._zero_grad()\n",
        "            grad_output = result_ref.tensor.grad\n",
        "            grad_input = grad_output * (1 - output_tensor**2)\n",
        "            input_ref.tensor.grad.add_(grad_input)\n",
        "\n",
        "        return _backward\n",
        "\n",
        "    def forward(self, input_tensor):\n",
        "        output_tensor = F.tanh(input_tensor.tensor)\n",
        "        if not self.training:\n",
        "            return CustomTensor(output_tensor, due_to_operation=True)\n",
        "\n",
        "        result = CustomTensor(output_tensor, _custom_requires_grad=True, graph=self.graph,due_to_operation=True, is_leaf=False)\n",
        "        self.graph.add_edge(input_tensor._node_id, result._node_id)\n",
        "        result._backward = self._create_backward(input_tensor, result, output_tensor)\n",
        "        return result\n",
        "\n",
        "class Silu(Module):\n",
        "    def __new__(cls, *, graph=None):\n",
        "        return super().__new__(cls)\n",
        "\n",
        "    def __init__(self, *, graph=None):\n",
        "        super().__init__()\n",
        "        self.graph = weakref.proxy(graph) if graph is not None else None\n",
        "\n",
        "    def _create_backward(self, input_tensor, result, output_tensor):\n",
        "        \"\"\"Creates the _backward hook for result tensor\"\"\"\n",
        "        input_ref = weakref.proxy(input_tensor)\n",
        "        result_ref = weakref.proxy(result)\n",
        "\n",
        "        def _backward():\n",
        "            if input_ref.tensor.grad is None:\n",
        "                input_ref._zero_grad()\n",
        "            grad_output = result_ref.tensor.grad\n",
        "            s_input_tensor = output_tensor / input_ref.tensor\n",
        "            grad_input = grad_output * (s_input_tensor + output_tensor * (1 - s_input_tensor))\n",
        "            input_ref.tensor.grad.add_(grad_input)\n",
        "\n",
        "        return _backward\n",
        "\n",
        "    def forward(self, input_tensor):\n",
        "        output_tensor = F.silu(input_tensor.tensor)\n",
        "        if not self.training:\n",
        "            return CustomTensor(output_tensor, due_to_operation=True)\n",
        "\n",
        "        result = CustomTensor(output_tensor, _custom_requires_grad=True, graph=self.graph,due_to_operation=True, is_leaf=False)\n",
        "        self.graph.add_edge(input_tensor._node_id, result._node_id)\n",
        "        result._backward = self._create_backward(input_tensor, result, output_tensor)\n",
        "        return result\n",
        "\n",
        "class Swish(Module):\n",
        "    # TODO: implement in future\n",
        "    def __new__(cls, *, B_initial=1.0, graph=None):\n",
        "        assert B_initial > 0\n",
        "        return super().__new__(cls)\n",
        "\n",
        "    def __init__(self, *, B_initial=1.0, graph=None):\n",
        "        super().__init__()\n",
        "        self.graph = weakref.proxy(graph) if graph is not None else None\n",
        "        self.B = CustomTensor([B_initial], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "        self.B_initial = B_initial\n",
        "\n",
        "    def _create_backward(self, input_tensor, result, output_tensor):\n",
        "        \"\"\"Creates the _backward hook for result tensor\"\"\"\n",
        "        input_ref = weakref.proxy(input_tensor)\n",
        "        result_ref = weakref.proxy(result)\n",
        "        B_ref = weakref.proxy(self.B)\n",
        "\n",
        "        def _backward():\n",
        "            if input_ref.tensor.grad is None:\n",
        "                input_ref._zero_grad()\n",
        "            if B_ref.tensor.grad is None:\n",
        "                B_ref._zero_grad()\n",
        "            grad_input, grad_B = self._calculate_gradients(input_ref.tensor, result_ref.tensor, output_tensor, B_ref.tensor)\n",
        "            # grad_output = result_ref.tensor.grad\n",
        "            # sig_B_x = output_tensor / input_ref.tensor\n",
        "            # common = sig_B_x * (1 - sig_B_x) * grad_output\n",
        "\n",
        "            # grad_input = sig_B_x * grad_output + input_ref.tensor * B_ref.tensor * common\n",
        "            # grad_B = input_ref.tensor.square() * common\n",
        "            input_ref.tensor.grad.add_(grad_input)\n",
        "            B_ref.tensor.grad.add_(grad_B)\n",
        "\n",
        "        return _backward\n",
        "\n",
        "    def forward(self, input_tensor):\n",
        "        scale = self.B.tensor.item()\n",
        "        output_tensor = F.silu(scale * input_tensor.tensor) / scale\n",
        "        if not self.training:\n",
        "            return CustomTensor(output_tensor, due_to_operation=True)\n",
        "\n",
        "        result = CustomTensor(output_tensor, _custom_requires_grad=True, graph=self.graph,due_to_operation=True, is_leaf=False)\n",
        "        self.graph.add_edge(input_tensor._node_id, result._node_id)\n",
        "        self.graph.add_edge(self.B._node_id, result._node_id)\n",
        "        result._backward = self._create_backward(input_tensor, result, output_tensor)\n",
        "        return result\n",
        "    \n",
        "    @torch.compile\n",
        "    def _calculate_gradients(self, input_tensor, result, output_tensor, B_tensor):\n",
        "        grad_output =result.grad\n",
        "        sig_B_x = output_tensor / input_tensor\n",
        "        common = sig_B_x * (1 - sig_B_x) * grad_output\n",
        "        grad_input = sig_B_x * grad_output + input_tensor * B_tensor * common\n",
        "        grad_B = input_tensor.square() * common\n",
        "        grad_B = grad_B.sum()\n",
        "        return grad_input, grad_B\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svC-VQGoLCLG"
      },
      "source": [
        "# tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "EUXa_YC8LD3X"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import numbers\n",
        "import weakref\n",
        "import rustworkx as rx\n",
        "from typing import Optional, Any\n",
        "import sys\n",
        "import gc\n",
        "import pytest\n",
        "\n",
        "\n",
        "class AutogradTester:\n",
        "    def __init__(self):\n",
        "        self.passed_tests = 0\n",
        "        self.failed_tests = 0\n",
        "        self.tolerance = 1e-6 #1e-7  # Increased tolerance slightly for complex ops\n",
        "\n",
        "    def assert_tensors_close(self, custom_tensor, pytorch_tensor, test_name, check_grad=True):\n",
        "        \"\"\"Compare custom tensor with PyTorch tensor values and optionally gradients.\"\"\"\n",
        "        try:\n",
        "            # Check values\n",
        "            np.testing.assert_allclose(\n",
        "                custom_tensor.tensor.detach().cpu().numpy(),  # Ensure on CPU for numpy\n",
        "                pytorch_tensor.detach().cpu().numpy(),\n",
        "                rtol=self.tolerance,\n",
        "                atol=self.tolerance,\n",
        "                err_msg=f\"Mismatch in tensor values for {test_name}\"\n",
        "            )\n",
        "\n",
        "            # Check gradients if requested and they exist for PyTorch tensor\n",
        "            if check_grad and pytorch_tensor.grad is not None:\n",
        "                if custom_tensor.tensor.grad is None:\n",
        "                    raise AssertionError(f\"Custom tensor has no gradient for {test_name}, but PyTorch does.\")\n",
        "\n",
        "                np.testing.assert_allclose(\n",
        "                    custom_tensor.tensor.grad.detach().cpu().numpy(),  # Ensure on CPU for numpy\n",
        "                    pytorch_tensor.grad.detach().cpu().numpy(),\n",
        "                    rtol=self.tolerance,\n",
        "                    atol=self.tolerance,\n",
        "                    err_msg=f\"Mismatch in gradients for {test_name}\"\n",
        "                )\n",
        "            elif check_grad and pytorch_tensor.grad is None and custom_tensor.tensor.grad is not None:\n",
        "                raise AssertionError(f\"Custom tensor has gradient for {test_name}, but PyTorch does not (should be no_grad).\")\n",
        "\n",
        "            print(f\"✓ {test_name}\")\n",
        "            self.passed_tests += 1\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"✗ {test_name}: {str(e)}\")\n",
        "            self.failed_tests += 1\n",
        "\n",
        "    def test_basic_operations(self):\n",
        "        \"\"\"Test basic arithmetic operations\"\"\"\n",
        "        print(\"\\n=== Testing Basic Operations ===\")\n",
        "\n",
        "        # Test scalar addition\n",
        "        with AutogradGraph() as graph:\n",
        "            x_custom = CustomTensor([2.0, 3.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = x_custom + 5.0\n",
        "            y_custom.backward(torch.ones_like(y_custom.tensor))\n",
        "\n",
        "            x_pytorch = torch.tensor([2.0, 3.0], requires_grad=True)\n",
        "            y_pytorch = x_pytorch + 5.0\n",
        "            y_pytorch.backward(torch.ones_like(y_pytorch))\n",
        "\n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Scalar Addition - x\")\n",
        "            self.assert_tensors_close(y_custom, y_pytorch, \"Scalar Addition - y (result)\")\n",
        "\n",
        "        # Test tensor addition\n",
        "        with AutogradGraph() as graph:\n",
        "            x_custom = CustomTensor([1.0, 2.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = CustomTensor([3.0, 4.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            z_custom = x_custom + y_custom\n",
        "            z_custom.backward(torch.ones_like(z_custom.tensor))\n",
        "\n",
        "            x_pytorch = torch.tensor([1.0, 2.0], requires_grad=True)\n",
        "            y_pytorch = torch.tensor([3.0, 4.0], requires_grad=True)\n",
        "            z_pytorch = x_pytorch + y_pytorch\n",
        "            z_pytorch.backward(torch.ones_like(z_pytorch))\n",
        "\n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Tensor Addition - x\")\n",
        "            self.assert_tensors_close(y_custom, y_pytorch, \"Tensor Addition - y\")\n",
        "            self.assert_tensors_close(z_custom, z_pytorch, \"Tensor Addition - z (result)\")\n",
        "\n",
        "    def test_multiplication(self):\n",
        "        \"\"\"Test multiplication operations\"\"\"\n",
        "        print(\"\\n=== Testing Multiplication ===\")\n",
        "\n",
        "        # Test scalar multiplication\n",
        "        with AutogradGraph() as graph:\n",
        "            x_custom = CustomTensor([2.0, 3.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = x_custom * 4.0\n",
        "            y_custom.backward(torch.ones_like(y_custom.tensor))\n",
        "\n",
        "            x_pytorch = torch.tensor([2.0, 3.0], requires_grad=True)\n",
        "            y_pytorch = x_pytorch * 4.0\n",
        "            y_pytorch.backward(torch.ones_like(y_pytorch))\n",
        "\n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Scalar Multiplication - x\")\n",
        "            self.assert_tensors_close(y_custom, y_pytorch, \"Scalar Multiplication - y (result)\")\n",
        "\n",
        "        # Test tensor multiplication\n",
        "        with AutogradGraph() as graph:\n",
        "            x_custom = CustomTensor([2.0, 3.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = CustomTensor([4.0, 5.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            z_custom = x_custom * y_custom\n",
        "            z_custom.backward(torch.ones_like(z_custom.tensor))\n",
        "\n",
        "            x_pytorch = torch.tensor([2.0, 3.0], requires_grad=True)\n",
        "            y_pytorch = torch.tensor([4.0, 5.0], requires_grad=True)\n",
        "            z_pytorch = x_pytorch * y_pytorch\n",
        "            z_pytorch.backward(torch.ones_like(z_pytorch))\n",
        "\n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Tensor Multiplication - x\")\n",
        "            self.assert_tensors_close(y_custom, y_pytorch, \"Tensor Multiplication - y\")\n",
        "            self.assert_tensors_close(z_custom, z_pytorch, \"Tensor Multiplication - z (result)\")\n",
        "\n",
        "    def test_subtraction_division(self):\n",
        "        \"\"\"Test subtraction and division\"\"\"\n",
        "        print(\"\\n=== Testing Subtraction and Division ===\")\n",
        "\n",
        "        # Test scalar subtraction (x - C)\n",
        "        with AutogradGraph() as graph:\n",
        "            x_custom = CustomTensor([5.0, 6.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = x_custom - 2.0\n",
        "            y_custom.backward(torch.ones_like(y_custom.tensor))\n",
        "\n",
        "            x_pytorch = torch.tensor([5.0, 6.0], requires_grad=True)\n",
        "            y_pytorch = x_pytorch - 2.0\n",
        "            y_pytorch.backward(torch.ones_like(y_pytorch))\n",
        "\n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Scalar Subtraction (x - C) - x\")\n",
        "            self.assert_tensors_close(y_custom, y_pytorch, \"Scalar Subtraction (x - C) - y (result)\")\n",
        "\n",
        "        # Test scalar reverse subtraction (C - x)\n",
        "        with AutogradGraph() as graph:\n",
        "            x_custom = CustomTensor([5.0, 6.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = 10.0 - x_custom  # Uses __rsub__\n",
        "            y_custom.backward(torch.ones_like(y_custom.tensor))\n",
        "\n",
        "            x_pytorch = torch.tensor([5.0, 6.0], requires_grad=True)\n",
        "            y_pytorch = 10.0 - x_pytorch\n",
        "            y_pytorch.backward(torch.ones_like(y_pytorch))\n",
        "\n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Scalar Reverse Subtraction (C - x) - x\")\n",
        "            self.assert_tensors_close(y_custom, y_pytorch, \"Scalar Reverse Subtraction (C - x) - y (result)\")\n",
        "\n",
        "        # Test tensor subtraction\n",
        "        with AutogradGraph() as graph:\n",
        "            x_custom = CustomTensor([7.0, 8.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = CustomTensor([2.0, 1.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            z_custom = x_custom - y_custom\n",
        "            z_custom.backward(torch.ones_like(z_custom.tensor))\n",
        "\n",
        "            x_pytorch = torch.tensor([7.0, 8.0], requires_grad=True)\n",
        "            y_pytorch = torch.tensor([2.0, 1.0], requires_grad=True)\n",
        "            z_pytorch = x_pytorch - y_pytorch\n",
        "            z_pytorch.backward(torch.ones_like(z_pytorch))\n",
        "\n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Tensor Subtraction - x\")\n",
        "            self.assert_tensors_close(y_custom, y_pytorch, \"Tensor Subtraction - y\")\n",
        "            self.assert_tensors_close(z_custom, z_pytorch, \"Tensor Subtraction - z (result)\")\n",
        "\n",
        "        # Test scalar division\n",
        "        with AutogradGraph() as graph:\n",
        "            x_custom = CustomTensor([8.0, 12.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = x_custom / 4.0\n",
        "            y_custom.backward(torch.ones_like(y_custom.tensor))\n",
        "\n",
        "            x_pytorch = torch.tensor([8.0, 12.0], requires_grad=True)\n",
        "            y_pytorch = x_pytorch / 4.0\n",
        "            y_pytorch.backward(torch.ones_like(y_pytorch))\n",
        "\n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Scalar Division - x\")\n",
        "            self.assert_tensors_close(y_custom, y_pytorch, \"Scalar Division - y (result)\")\n",
        "        # Test tensor division\n",
        "        with AutogradGraph() as graph:\n",
        "            x_custom = CustomTensor([8.0, 12.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = CustomTensor([5.0, 10.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            z_custom = x_custom / y_custom\n",
        "            z_custom.backward(torch.ones_like(z_custom.tensor))\n",
        "\n",
        "            x_pytorch = torch.tensor([8.0, 12.0], requires_grad=True)\n",
        "            y_pytorch = torch.tensor([5.0, 10.0], requires_grad=True)\n",
        "            z_pytorch = x_pytorch / y_pytorch\n",
        "            z_pytorch.backward(torch.ones_like(z_pytorch))\n",
        "\n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Tensor Division - x\")\n",
        "            self.assert_tensors_close(y_custom, y_pytorch, \"Tensir Division - y\")\n",
        "            self.assert_tensors_close(z_custom, z_pytorch, \"Tensor Division - z (result)\", )\n",
        "\n",
        "\n",
        "    def test_power_function(self):\n",
        "        \"\"\"Test power operation\"\"\"\n",
        "        print(\"\\n=== Testing Power Function ===\")\n",
        "\n",
        "        with AutogradGraph() as graph:\n",
        "            x_custom = CustomTensor([2.0, 3.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = x_custom.pow(3.0)\n",
        "            y_custom.backward(torch.ones_like(y_custom.tensor))\n",
        "\n",
        "            x_pytorch = torch.tensor([2.0, 3.0], requires_grad=True)\n",
        "            y_pytorch = torch.pow(x_pytorch, 3.0)\n",
        "            y_pytorch.backward(torch.ones_like(y_pytorch))\n",
        "\n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Power Function - x\")\n",
        "            self.assert_tensors_close(y_custom, y_pytorch, \"Power Function - y (result)\" )\n",
        "\n",
        "        # Test power with negative exponent\n",
        "        with AutogradGraph() as graph:\n",
        "            x_custom = CustomTensor([2.0, 3.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = x_custom.pow(-2.0)\n",
        "            y_custom.backward(torch.ones_like(y_custom.tensor))\n",
        "\n",
        "            x_pytorch = torch.tensor([2.0, 3.0], requires_grad=True)\n",
        "            y_pytorch = torch.pow(x_pytorch, -2.0)\n",
        "            y_pytorch.backward(torch.ones_like(y_pytorch))\n",
        "\n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Power Function (Negative Exponent) - x\")\n",
        "            self.assert_tensors_close(y_custom, y_pytorch, \"Power Function (Negative Exponent) - y (result)\")\n",
        "\n",
        "    def test_unary_functions(self):\n",
        "        \"\"\"Test unary mathematical functions\"\"\"\n",
        "        print(\"\\n=== Testing Unary Functions ===\")\n",
        "\n",
        "        # Test exp\n",
        "        with AutogradGraph() as graph:\n",
        "            x_custom = CustomTensor([1.0, 2.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = x_custom.exp()\n",
        "            y_custom.backward(torch.ones_like(y_custom.tensor))\n",
        "\n",
        "            x_pytorch = torch.tensor([1.0, 2.0], requires_grad=True)\n",
        "            y_pytorch = torch.exp(x_pytorch)\n",
        "            y_pytorch.backward(torch.ones_like(y_pytorch))\n",
        "\n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Exponential Function - x\")\n",
        "            self.assert_tensors_close(y_custom, y_pytorch, \"Exponential Function - y (result)\")\n",
        "\n",
        "        # Test log\n",
        "        with AutogradGraph() as graph:\n",
        "            x_custom = CustomTensor([1.0, 2.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = x_custom.log()\n",
        "            y_custom.backward(torch.ones_like(y_custom.tensor))\n",
        "\n",
        "            x_pytorch = torch.tensor([1.0, 2.0], requires_grad=True)\n",
        "            y_pytorch = torch.log(x_pytorch)\n",
        "            y_pytorch.backward(torch.ones_like(y_pytorch))\n",
        "\n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Logarithm Function - x\")\n",
        "            self.assert_tensors_close(y_custom, y_pytorch, \"Logarithm Function - y (result)\")\n",
        "\n",
        "        # Test sin\n",
        "        with AutogradGraph() as graph:\n",
        "            x_custom = CustomTensor([0.5, 1.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = x_custom.sin()\n",
        "            y_custom.backward(torch.ones_like(y_custom.tensor))\n",
        "\n",
        "            x_pytorch = torch.tensor([0.5, 1.0], requires_grad=True)\n",
        "            y_pytorch = torch.sin(x_pytorch)\n",
        "            y_pytorch.backward(torch.ones_like(y_pytorch))\n",
        "\n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Sine Function - x\")\n",
        "            self.assert_tensors_close(y_custom, y_pytorch, \"Sine Function - y (result)\")\n",
        "\n",
        "        # Test cos\n",
        "        with AutogradGraph() as graph:\n",
        "            x_custom = CustomTensor([0.5, 1.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = x_custom.cos()\n",
        "            y_custom.backward(torch.ones_like(y_custom.tensor))\n",
        "\n",
        "            x_pytorch = torch.tensor([0.5, 1.0], requires_grad=True)\n",
        "            y_pytorch = torch.cos(x_pytorch)\n",
        "            y_pytorch.backward(torch.ones_like(y_pytorch))\n",
        "\n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Cosine Function - x\")\n",
        "            self.assert_tensors_close(y_custom, y_pytorch, \"Cosine Function - y (result)\")\n",
        "\n",
        "        # Test sqrt\n",
        "        with AutogradGraph() as graph:\n",
        "            x_custom = CustomTensor([4.0, 9.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = x_custom.sqrt()\n",
        "            y_custom.backward(torch.ones_like(y_custom.tensor))\n",
        "\n",
        "            x_pytorch = torch.tensor([4.0, 9.0], requires_grad=True)\n",
        "            y_pytorch = torch.sqrt(x_pytorch)\n",
        "            y_pytorch.backward(torch.ones_like(y_pytorch))\n",
        "\n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Square Root Function - x\")\n",
        "            self.assert_tensors_close(y_custom, y_pytorch, \"Square Root Function - y (result)\")\n",
        "\n",
        "    def test_matrix_operations(self):\n",
        "        \"\"\"Test matrix operations\"\"\"\n",
        "        print(\"\\n=== Testing Matrix Operations ===\")\n",
        "\n",
        "        # Test matrix multiplication (2x2 @ 2x2)\n",
        "        with AutogradGraph() as graph:\n",
        "            x_custom = CustomTensor([[1.0, 2.0], [3.0, 4.0]], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = CustomTensor([[5.0, 6.0], [7.0, 8.0]], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            z_custom = x_custom.matmul(y_custom)\n",
        "            z_custom.backward(torch.ones_like(z_custom.tensor))\n",
        "\n",
        "            x_pytorch = torch.tensor([[1.0, 2.0], [3.0, 4.0]], requires_grad=True)\n",
        "            y_pytorch = torch.tensor([[5.0, 6.0], [7.0, 8.0]], requires_grad=True)\n",
        "            z_pytorch = torch.matmul(x_pytorch, y_pytorch)\n",
        "            z_pytorch.backward(torch.ones_like(z_pytorch))\n",
        "\n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Matrix Multiplication (2x2 @ 2x2) - x\")\n",
        "            self.assert_tensors_close(y_custom, y_pytorch, \"Matrix Multiplication (2x2 @ 2x2) - y\")\n",
        "            self.assert_tensors_close(z_custom, z_pytorch, \"Matrix Multiplication (2x2 @ 2x2) - z (result)\")\n",
        "\n",
        "        # Test matrix multiplication (2x3 @ 3x2)\n",
        "        with AutogradGraph() as graph:\n",
        "            x_custom = CustomTensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = CustomTensor([[7.0, 8.0], [9.0, 10.0], [11.0, 12.0]], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            z_custom = x_custom.matmul(y_custom)\n",
        "            z_custom.backward(torch.ones_like(z_custom.tensor))\n",
        "\n",
        "            x_pytorch = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]], requires_grad=True)\n",
        "            y_pytorch = torch.tensor([[7.0, 8.0], [9.0, 10.0], [11.0, 12.0]], requires_grad=True)\n",
        "            z_pytorch = torch.matmul(x_pytorch, y_pytorch)\n",
        "            z_pytorch.backward(torch.ones_like(z_pytorch))\n",
        "\n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Matrix Multiplication (2x3 @ 3x2) - x\")\n",
        "            self.assert_tensors_close(y_custom, y_pytorch, \"Matrix Multiplication (2x3 @ 3x2) - y\")\n",
        "            self.assert_tensors_close(z_custom, z_pytorch, \"Matrix Multiplication (2x3 @ 3x2) - z (result)\")\n",
        "\n",
        "        # Test dot product (vector * vector)\n",
        "        with AutogradGraph() as graph:\n",
        "            x_custom = CustomTensor([1.0, 2.0, 3.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = CustomTensor([4.0, 5.0, 6.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            z_custom = x_custom.dot(y_custom)\n",
        "            z_custom.backward()  # Scalar output, so default backward() is fine (grad=1)\n",
        "\n",
        "            x_pytorch = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
        "            y_pytorch = torch.tensor([4.0, 5.0, 6.0], requires_grad=True)\n",
        "            z_pytorch = torch.dot(x_pytorch, y_pytorch)\n",
        "            z_pytorch.backward()\n",
        "\n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Dot Product (vector) - x\")\n",
        "            self.assert_tensors_close(y_custom, y_pytorch, \"Dot Product (vector) - y\")\n",
        "            self.assert_tensors_close(z_custom, z_pytorch, \"Dot Product (vector) - z (result)\")\n",
        "\n",
        "    def test_complex_chain(self):\n",
        "        \"\"\"Test complex computational chains\"\"\"\n",
        "        print(\"\\n=== Testing Complex Chains ===\")\n",
        "\n",
        "        # Test 1: z = (x + y) * (x - y) + x^2 - sin(y)\n",
        "        with AutogradGraph() as graph:\n",
        "            x_custom = CustomTensor([3.0, 4.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = CustomTensor([1.0, 2.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "\n",
        "            sum_custom = x_custom + y_custom\n",
        "            diff_custom = x_custom - y_custom\n",
        "            prod_custom = sum_custom * diff_custom\n",
        "            x_squared_custom = x_custom.pow(2.0)\n",
        "            sin_y_custom = y_custom.sin()\n",
        "\n",
        "            inter1_custom = prod_custom + x_squared_custom\n",
        "            z_custom = inter1_custom - sin_y_custom\n",
        "\n",
        "            z_custom.backward(torch.ones_like(z_custom.tensor))\n",
        "\n",
        "            x_pytorch = torch.tensor([3.0, 4.0], requires_grad=True)\n",
        "            y_pytorch = torch.tensor([1.0, 2.0], requires_grad=True)\n",
        "\n",
        "            sum_pytorch = x_pytorch + y_pytorch\n",
        "            diff_pytorch = x_pytorch - y_pytorch\n",
        "            prod_pytorch = sum_pytorch * diff_pytorch\n",
        "            x_squared_pytorch = torch.pow(x_pytorch, 2.0)\n",
        "            sin_y_pytorch = torch.sin(y_pytorch)\n",
        "\n",
        "            inter1_pytorch = prod_pytorch + x_squared_pytorch\n",
        "            z_pytorch = inter1_pytorch - sin_y_pytorch\n",
        "\n",
        "            z_pytorch.backward(torch.ones_like(z_pytorch))\n",
        "\n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Complex Chain 1 - x\")\n",
        "            self.assert_tensors_close(y_custom, y_pytorch, \"Complex Chain 1 - y\")\n",
        "            self.assert_tensors_close(z_custom, z_pytorch, \"Complex Chain 1 - z (result)\")\n",
        "\n",
        "        # Test 2: Multiple paths to a leaf: z = x*y + x*x + y*z_fixed\n",
        "        with AutogradGraph() as graph:\n",
        "            x_custom = CustomTensor([2.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = CustomTensor([3.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            z_fixed_custom = CustomTensor([0.5])  # No grad\n",
        "\n",
        "            term1_custom = x_custom * y_custom\n",
        "            term2_custom = x_custom * x_custom  # x appears twice\n",
        "            term3_custom = y_custom * z_fixed_custom  # y appears twice, one with no-grad\n",
        "\n",
        "            inter_custom = term1_custom + term2_custom\n",
        "            z_custom = inter_custom + term3_custom\n",
        "            z_custom.backward()\n",
        "\n",
        "            x_pytorch = torch.tensor([2.0], requires_grad=True)\n",
        "            y_pytorch = torch.tensor([3.0], requires_grad=True)\n",
        "            z_fixed_pytorch = torch.tensor([0.5])  # No grad\n",
        "\n",
        "            term1_pytorch = x_pytorch * y_pytorch\n",
        "            term2_pytorch = x_pytorch * x_pytorch\n",
        "            term3_pytorch = y_pytorch * z_fixed_pytorch\n",
        "\n",
        "            inter_pytorch = term1_pytorch + term2_pytorch\n",
        "            z_pytorch = inter_pytorch + term3_pytorch\n",
        "            z_pytorch.backward()\n",
        "\n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Complex Chain 2 (Multiple Paths) - x\")\n",
        "            self.assert_tensors_close(y_custom, y_pytorch, \"Complex Chain 2 (Multiple Paths) - y\")\n",
        "            self.assert_tensors_close(z_custom, z_pytorch, \"Complex Chain 2 (Multiple Paths) - z (result)\")\n",
        "\n",
        "        # Test 3: Deeper Chain with Mixed Ops: (exp(x) * log(y)) / sqrt(x+y)\n",
        "        with AutogradGraph() as graph:\n",
        "            x_custom = CustomTensor([1.5], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = CustomTensor([2.5], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "\n",
        "            exp_x_custom = x_custom.exp()\n",
        "            log_y_custom = y_custom.log()\n",
        "            numerator_custom = exp_x_custom * log_y_custom\n",
        "\n",
        "            sum_xy_custom = x_custom + y_custom\n",
        "            sqrt_sum_custom = sum_xy_custom.sqrt()\n",
        "\n",
        "            z_custom = numerator_custom / sqrt_sum_custom\n",
        "            z_custom.backward()\n",
        "\n",
        "            x_pytorch = torch.tensor([1.5], requires_grad=True)\n",
        "            y_pytorch = torch.tensor([2.5], requires_grad=True)\n",
        "\n",
        "            exp_x_pytorch = torch.exp(x_pytorch)\n",
        "            log_y_pytorch = torch.log(y_pytorch)\n",
        "            numerator_pytorch = exp_x_pytorch * log_y_pytorch\n",
        "\n",
        "            sum_xy_pytorch = x_pytorch + y_pytorch\n",
        "            sqrt_sum_pytorch = torch.sqrt(sum_xy_pytorch)\n",
        "\n",
        "            z_pytorch = numerator_pytorch / sqrt_sum_pytorch\n",
        "            z_pytorch.backward()\n",
        "\n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Complex Chain 3 (Deeper Mixed Ops) - x\")\n",
        "            self.assert_tensors_close(y_custom, y_pytorch, \"Complex Chain 3 (Deeper Mixed Ops) - y\")\n",
        "            self.assert_tensors_close(z_custom, z_pytorch, \"Complex Chain 3 (Deeper Mixed Ops) - z (result)\")\n",
        "\n",
        "    def test_mixed_operations(self):\n",
        "        \"\"\"Test mixing operations with and without gradients\"\"\"\n",
        "        print(\"\\n=== Testing Mixed Operations ===\")\n",
        "\n",
        "        # One tensor requires grad, other doesn't (multiplication)\n",
        "        with AutogradGraph() as graph:\n",
        "            x_custom = CustomTensor([2.0, 3.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = CustomTensor([4.0, 5.0])  # No grad\n",
        "            z_custom = x_custom * y_custom\n",
        "            z_custom.backward(torch.ones_like(z_custom.tensor))\n",
        "\n",
        "            x_pytorch = torch.tensor([2.0, 3.0], requires_grad=True)\n",
        "            y_pytorch = torch.tensor([4.0, 5.0])  # No grad\n",
        "            z_pytorch = x_pytorch * y_pytorch\n",
        "            z_pytorch.backward(torch.ones_like(z_pytorch))\n",
        "\n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Mixed Operations (X*Y, Y no grad) - x\")\n",
        "            # Check that y_custom has no grad\n",
        "            self.assert_tensors_close(y_custom, y_pytorch, \"Mixed Operations (X*Y, Y no grad) - y\")\n",
        "            self.assert_tensors_close(z_custom, z_pytorch, \"Mixed Operations (X*Y, Y no grad) - z (result)\")\n",
        "\n",
        "        # One tensor requires grad, other doesn't (addition)\n",
        "        with AutogradGraph() as graph:\n",
        "            x_custom = CustomTensor([10.0, 20.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = CustomTensor([1.0, 2.0])  # No grad\n",
        "            z_custom = x_custom + y_custom\n",
        "            z_custom.backward(torch.ones_like(z_custom.tensor))\n",
        "\n",
        "            x_pytorch = torch.tensor([10.0, 20.0], requires_grad=True)\n",
        "            y_pytorch = torch.tensor([1.0, 2.0])  # No grad\n",
        "            z_pytorch = x_pytorch + y_pytorch\n",
        "            z_pytorch.backward(torch.ones_like(z_pytorch))\n",
        "\n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Mixed Operations (X+Y, Y no grad) - x\")\n",
        "            self.assert_tensors_close(y_custom, y_pytorch, \"Mixed Operations (X+Y, Y no grad) - y\")\n",
        "            self.assert_tensors_close(z_custom, z_pytorch, \"Mixed Operations (X+Y, Y no grad) - z (result)\")\n",
        "\n",
        "    def test_broadcasting(self):\n",
        "        \"\"\"Test operations with broadcasting\"\"\"\n",
        "        print(\"\\n=== Testing Broadcasting ===\")\n",
        "\n",
        "        # Vector + scalar\n",
        "        with AutogradGraph() as graph:\n",
        "            x_custom = CustomTensor([1.0, 2.0, 3.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = x_custom + 10.0\n",
        "            y_custom.backward(torch.tensor([1.0, 1.0, 1.0]))\n",
        "\n",
        "            x_pytorch = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
        "            y_pytorch = x_pytorch + 10.0\n",
        "            y_pytorch.backward(torch.tensor([1.0, 1.0, 1.0]))\n",
        "\n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Broadcasting: Vector + Scalar - x\")\n",
        "            self.assert_tensors_close(y_custom, y_pytorch, \"Broadcasting: Vector + Scalar - y (result)\")\n",
        "\n",
        "        # Matrix + vector (row broadcasting)\n",
        "        with AutogradGraph() as graph:\n",
        "            x_custom = CustomTensor([[1.0, 2.0], [3.0, 4.0]], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = CustomTensor([10.0, 20.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            z_custom = x_custom + y_custom  # y broadcasts to rows of x\n",
        "            z_custom.backward(torch.ones_like(z_custom.tensor))\n",
        "\n",
        "            x_pytorch = torch.tensor([[1.0, 2.0], [3.0, 4.0]], requires_grad=True)\n",
        "            y_pytorch = torch.tensor([10.0, 20.0], requires_grad=True)\n",
        "            z_pytorch = x_pytorch + y_pytorch\n",
        "            z_pytorch.backward(torch.ones_like(z_pytorch))\n",
        "\n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Broadcasting: Matrix + Vector (row) - x\")\n",
        "            # For broadcasted operations, the gradient needs to be summed over the broadcasted dimensions\n",
        "            # PyTorch handles this automatically. Your custom backward for add should accumulate.\n",
        "            self.assert_tensors_close(y_custom, y_pytorch, \"Broadcasting: Matrix + Vector (row) - y\")\n",
        "            self.assert_tensors_close(z_custom, z_pytorch, \"Broadcasting: Matrix + Vector (row) - z (result)\")\n",
        "\n",
        "        # Matrix * scalar\n",
        "        with AutogradGraph() as graph:\n",
        "            x_custom = CustomTensor([[1.0, 2.0], [3.0, 4.0]], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = x_custom * 5.0\n",
        "            y_custom.backward(torch.ones_like(y_custom.tensor))\n",
        "\n",
        "            x_pytorch = torch.tensor([[1.0, 2.0], [3.0, 4.0]], requires_grad=True)\n",
        "            y_pytorch = x_pytorch * 5.0\n",
        "            y_pytorch.backward(torch.ones_like(y_pytorch))\n",
        "\n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Broadcasting: Matrix * Scalar - x\")\n",
        "            self.assert_tensors_close(y_custom, y_pytorch, \"Broadcasting: Matrix * Scalar - y (result)\")\n",
        "\n",
        "    def test_backward_with_custom_grad(self):\n",
        "        \"\"\"Test backward pass with a custom initial gradient tensor.\"\"\"\n",
        "        print(\"\\n=== Testing Backward with Custom Grad ===\")\n",
        "\n",
        "        with AutogradGraph() as graph:\n",
        "            x_custom = CustomTensor([2.0, 3.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = x_custom * 4.0 + 1.0\n",
        "\n",
        "            custom_grad_output = torch.tensor([0.5, 2.0])\n",
        "            y_custom.backward(custom_grad_output)\n",
        "\n",
        "            x_pytorch = torch.tensor([2.0, 3.0], requires_grad=True)\n",
        "            y_pytorch = x_pytorch * 4.0 + 1.0\n",
        "\n",
        "            pytorch_grad_output = torch.tensor([0.5, 2.0])\n",
        "            y_pytorch.backward(pytorch_grad_output)\n",
        "\n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Backward with Custom Grad - x\")\n",
        "            self.assert_tensors_close(y_custom, y_pytorch, \"Backward with Custom Grad - y (result)\")\n",
        "\n",
        "    def test_zero_grad_behavior(self):\n",
        "        \"\"\"Test _zero_grad and subsequent backward calls.\"\"\"\n",
        "        print(\"\\n=== Testing Zero Grad Behavior ===\")\n",
        "        with AutogradGraph() as graph:\n",
        "            x_custom = CustomTensor([1.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = x_custom * 2\n",
        "            z_custom = y_custom + 3\n",
        "            self.assert_tensors_close(x_custom, torch.tensor([1.0], requires_grad=True), \"Zero Grad Init (first backward) - x\")\n",
        "            z_custom.backward(retain_graph=True)  # First backward\n",
        "\n",
        "            z_custom._zero_grad()  # Manually zero for custom\n",
        "            y_custom._zero_grad()  # Manually zero for custom\n",
        "            x_custom._zero_grad()  # Manually zero for custom leaf\n",
        "\n",
        "            # Do another backward pass\n",
        "            z_custom.backward()  # Should accumulate again from 1.0\n",
        "\n",
        "            x_pytorch = torch.tensor([1.0], requires_grad=True)\n",
        "            y_pytorch = x_pytorch * 2\n",
        "            z_pytorch = y_pytorch + 3\n",
        "            z_pytorch.backward(retain_graph=True)\n",
        "\n",
        "            x_pytorch.grad.zero_()\n",
        "            z_pytorch.backward()  # PyTorch accumulates if not zeroed explicitly\n",
        "\n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Zero Grad Behavior - x (after 2nd backward)\")\n",
        "            self.assert_tensors_close(z_custom, z_pytorch, \"Zero Grad Behavior - z (result, after 2nd backward)\")\n",
        "\n",
        "    def test_no_grad_flow(self):\n",
        "        \"\"\"Test that gradients do not flow to tensors not requiring grad.\"\"\"\n",
        "        print(\"\\n=== Testing No Grad Flow ===\")\n",
        "        with AutogradGraph() as graph:\n",
        "            x_custom = CustomTensor([5.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            y_custom = CustomTensor([2.0], _custom_requires_grad=False)  # Does NOT require grad\n",
        "            z_custom = x_custom * y_custom\n",
        "            z_custom.backward()\n",
        "\n",
        "            x_pytorch = torch.tensor([5.0], requires_grad=True)\n",
        "            y_pytorch = torch.tensor([2.0], requires_grad=False)\n",
        "            z_pytorch = x_pytorch * y_pytorch\n",
        "            z_pytorch.backward()\n",
        "\n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"No Grad Flow - x (requires grad)\")\n",
        "            # PyTorch's .grad for non-requiring-grad tensors is None\n",
        "            # Our CustomTensor.tensor.grad for non-requiring-grad should also be None\n",
        "            try:\n",
        "                # Check that y_custom.tensor.grad is None\n",
        "                if y_custom.tensor.grad is not None:\n",
        "                    raise AssertionError(\"Custom non-grad tensor unexpectedly has a gradient.\")\n",
        "                print(f\"✓ No Grad Flow - y (no grad, custom correctly None)\")\n",
        "                self.passed_tests += 1\n",
        "            except Exception as e:\n",
        "                print(f\"✗ No Grad Flow - y (no grad): {str(e)}\")\n",
        "                self.failed_tests += 1\n",
        "\n",
        "    def test_basic_add_scalar_grad_system(self):\n",
        "        print(\"\\n=== System Test: Basic Scalar Add Grad ===\")\n",
        "        try:\n",
        "            with AutogradGraph() as graph:\n",
        "                a = CustomTensor(torch.tensor([2.0, 3.0]), _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "                b = a + 5.0  # (a + 5)\n",
        "                c = b + 10.0  # (a + 5 + 10)\n",
        "\n",
        "                # Manually run backward pass\n",
        "                c.backward(weightage_tensor=1,retain_graph=True)\n",
        "\n",
        "                # Expected gradients:\n",
        "                # dC/dA = 1.0 (for each element)\n",
        "                assert torch.allclose(a.tensor.grad, torch.tensor([1.0, 1.0]))\n",
        "                assert b.tensor.grad is not None\n",
        "                assert torch.allclose(b.tensor.grad, torch.tensor([1.0, 1.0]))  # dC/dB = 1.0\n",
        "\n",
        "                # Verify graph structure\n",
        "                assert graph.graph.num_nodes() == 3\n",
        "                assert graph.graph.num_edges() == 2\n",
        "                assert graph.graph.has_edge(a._node_id, b._node_id)\n",
        "                assert graph.graph.has_edge(b._node_id, c._node_id)\n",
        "                assert graph.check_cycle() is False\n",
        "            print(\"✓ System Test: Basic Scalar Add Grad\")\n",
        "            self.passed_tests += 1\n",
        "        except Exception as e:\n",
        "            print(f\"✗ System Test: Basic Scalar Add Grad: {str(e)}\")\n",
        "            self.failed_tests += 1\n",
        "\n",
        "    def test_basic_add_tensor_grad_system(self):\n",
        "        print(\"\\n=== System Test: Basic Tensor Add Grad ===\")\n",
        "        try:\n",
        "            with AutogradGraph() as graph:\n",
        "                a = CustomTensor(torch.tensor([2.0, 3.0]), _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "                b = CustomTensor(torch.tensor([1.0, 2.0]), _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "                c = a + b  # (a + b)\n",
        "                d = c + 5.0  # (a + b + 5)\n",
        "\n",
        "                d.backward(weightage_tensor=1,retain_graph=True)\n",
        "\n",
        "                # Expected gradients:\n",
        "                # dD/dA = 1.0\n",
        "                # dD/dB = 1.0\n",
        "                assert torch.allclose(a.tensor.grad, torch.tensor([1.0, 1.0]))\n",
        "                assert torch.allclose(b.tensor.grad, torch.tensor([1.0, 1.0]))\n",
        "\n",
        "                # Verify graph structure\n",
        "                assert graph.graph.num_nodes() == 4\n",
        "                assert graph.graph.num_edges() == 3\n",
        "                assert graph.graph.has_edge(a._node_id, c._node_id)\n",
        "                assert graph.graph.has_edge(b._node_id, c._node_id)\n",
        "                assert graph.graph.has_edge(c._node_id, d._node_id)\n",
        "                assert graph.check_cycle() is False\n",
        "            print(\"✓ System Test: Basic Tensor Add Grad\")\n",
        "            self.passed_tests += 1\n",
        "        except Exception as e:\n",
        "            print(f\"✗ System Test: Basic Tensor Add Grad: {str(e)}\")\n",
        "            self.failed_tests += 1\n",
        "\n",
        "    def test_mixed_requires_grad_tensor_add_system(self):\n",
        "        print(\"\\n=== System Test: Mixed Requires Grad Tensor Add ===\")\n",
        "        try:\n",
        "            with AutogradGraph() as graph:\n",
        "                a = CustomTensor(torch.tensor([2.0, 3.0]), _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "                b = CustomTensor(torch.tensor([1.0, 2.0]), _custom_requires_grad=False)  # Does not require grad\n",
        "                c = a + b  # c should require grad, b's grad should be None\n",
        "\n",
        "                c.backward(weightage_tensor=1,retain_graph = True)\n",
        "\n",
        "                assert torch.allclose(a.tensor.grad, torch.tensor([1.0, 1.0]))\n",
        "                assert b.tensor.grad is None  # b should not have a grad\n",
        "                assert c._custom_requires_grad is True\n",
        "\n",
        "                # Verify graph structure\n",
        "                assert graph.graph.num_nodes() == 2  # Only a and c in the graph\n",
        "                assert graph.graph.num_edges() == 1\n",
        "                assert graph.graph.has_node(a._node_id)\n",
        "                assert graph.graph.has_node(c._node_id)\n",
        "                assert graph.graph.has_edge(a._node_id, c._node_id)\n",
        "                # assert not graph.graph.has_node(b._node_id) # b should not be in graph\n",
        "            print(\"✓ System Test: Mixed Requires Grad Tensor Add\")\n",
        "            self.passed_tests += 1\n",
        "        except Exception as e:\n",
        "            print(f\"✗ System Test: Mixed Requires Grad Tensor Add: {str(e)}\")\n",
        "            self.failed_tests += 1\n",
        "\n",
        "    def test_no_requires_grad_system(self):\n",
        "        print(\"\\n=== System Test: No Requires Grad ===\")\n",
        "        try:\n",
        "            with AutogradGraph() as graph:  # Graph created, but no tensors with requires_grad=True added\n",
        "                a = CustomTensor(torch.tensor([1.0]))\n",
        "                b = CustomTensor(torch.tensor([2.0]))\n",
        "                c = a + b\n",
        "                d = c + 3.0\n",
        "\n",
        "                assert not a._custom_requires_grad\n",
        "                assert not b._custom_requires_grad\n",
        "                assert not c._custom_requires_grad\n",
        "                assert not d._custom_requires_grad\n",
        "                assert graph.graph.num_nodes() == 0  # Graph should remain empty\n",
        "                assert graph.graph.num_edges() == 0\n",
        "\n",
        "                with pytest.raises(RuntimeError, match=\"Output tensor does not require grad.\"):\n",
        "                    d.backward(weightage_tensor=1)\n",
        "            print(\"✓ System Test: No Requires Grad\")\n",
        "            self.passed_tests += 1\n",
        "        except Exception as e:\n",
        "            print(f\"✗ System Test: No Requires Grad: {str(e)}\")\n",
        "            self.failed_tests += 1\n",
        "\n",
        "    def test_autograd_graph_context_manager_system(self):\n",
        "        print(\"\\n=== System Test: Autograd Graph Context Manager ===\")\n",
        "        try:\n",
        "            graph = None\n",
        "            with AutogradGraph(check_for_cycles=True, auto_cleanup=True) as g:\n",
        "                graph = g\n",
        "                a = CustomTensor(torch.tensor([1.0]), _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "                b = a + 1.0\n",
        "                assert graph.graph.num_nodes() == 2\n",
        "                assert graph.graph.num_edges() == 1\n",
        "                assert len(graph.intermediate_tensors) == 1  # b should be in intermediate_tensors\n",
        "\n",
        "            # After exiting the context, graph should be empty\n",
        "            assert graph.graph.num_nodes() == 0\n",
        "            assert graph.graph.num_edges() == 0\n",
        "            assert len(graph.intermediate_tensors) == 0\n",
        "            print(\"✓ System Test: Autograd Graph Context Manager\")\n",
        "            self.passed_tests += 1\n",
        "        except Exception as e:\n",
        "            print(f\"✗ System Test: Autograd Graph Context Manager: {str(e)}\")\n",
        "            self.failed_tests += 1\n",
        "\n",
        "    def test_cycle_detection_system(self):\n",
        "        print(\"\\n=== System Test: Cycle Detection ===\")\n",
        "        try:\n",
        "            with pytest.raises(RuntimeError, match=\"Cycle detected in autograd graph.\"):\n",
        "                with AutogradGraph(check_for_cycles=True, auto_cleanup=False) as graph:\n",
        "                    a = CustomTensor(torch.tensor([1.0]), _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "                    b = CustomTensor(torch.tensor([2.0]), _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "\n",
        "                    # Manually create a cycle (a -> b -> a)\n",
        "                    graph.add_edge(a._node_id, b._node_id)\n",
        "                    graph.add_edge(b._node_id, a._node_id)\n",
        "                    graph.check_cycle() # Explicitly check for cycle\n",
        "            print(\"✓ System Test: Cycle Detection\")\n",
        "            self.passed_tests += 1\n",
        "        except Exception as e:\n",
        "            print(f\"✗ System Test: Cycle Detection: {str(e)}\")\n",
        "            self.failed_tests += 1\n",
        "\n",
        "    def test_no_circular_references_non_leaf_tensors_die_system(self):\n",
        "        # This test relies on the garbage collector. It's a heuristic test\n",
        "        # as Python's GC timing is not strictly deterministic.\n",
        "        # However, with weakrefs, it should work for non-leaf tensors.\n",
        "\n",
        "        print(\"\\n--- Starting System Test: No Circular References (Part 1) ---\")\n",
        "        try:\n",
        "            graph_ref = None\n",
        "            output_tensor_weak_ref = None\n",
        "            node_id_d = -1  # To store node_id before d is deleted\n",
        "\n",
        "            # BLOCK 1: Create graph and tensors\n",
        "            with AutogradGraph(auto_cleanup=False) as graph:  # Keep graph for inspection\n",
        "                graph_ref = weakref.ref(graph)\n",
        "                a = CustomTensor(torch.tensor([1.0]), _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "                b = a + 1.0  # Intermediate tensor\n",
        "                c = b + 2.0  # Intermediate tensor\n",
        "                d = c + 3.0  # Output tensor (also intermediate from graph's perspective)\n",
        "\n",
        "                # Store weak reference to 'd' BEFORE its strong reference is potentially removed\n",
        "                output_tensor_weak_ref = weakref.ref(d)\n",
        "                node_id_d = d._node_id  # Store node_id while d is alive\n",
        "\n",
        "                # The ref count for `d` object itself will be high here because it's in `graph.intermediate_tensors`,\n",
        "                # and held by variable `d`, and by the temporary ref in `getrefcount`.\n",
        "                assert len(graph.intermediate_tensors) == 3  # b, c, d should be in intermediate_tensors\n",
        "\n",
        "            # BLOCK 2: After exiting context manager (auto_cleanup=False)\n",
        "            # The 'graph' variable still holds a strong reference to the AutogradGraph instance.\n",
        "            # graph_ref() should return the graph object.\n",
        "            assert graph_ref() is not None, \"Graph object should still be alive.\"\n",
        "            assert len(graph_ref().intermediate_tensors) == 3, \"Intermediate tensors should still be referenced by the graph.\"\n",
        "\n",
        "            # BLOCK 3: Remove strong reference 'd' from local scope\n",
        "            del d  # Remove the local strong reference to the CustomTensor object.\n",
        "            gc.collect()  # Force garbage collection\n",
        "\n",
        "            # Now, output_tensor_weak_ref() *still* shouldn't be None because `graph_ref().intermediate_tensors`\n",
        "            # holds the strong reference.\n",
        "            assert output_tensor_weak_ref() is not None, \"d should still be alive due to intermediate_tensors.\"\n",
        "            current_d_refcount_after_del_d = sys.getrefcount(output_tensor_weak_ref()) if output_tensor_weak_ref() else 'N/A'\n",
        "            assert current_d_refcount_after_del_d == 2, f\"Expected refcount 2, got {current_d_refcount_after_del_d}\"\n",
        "\n",
        "            # BLOCK 4: Remove strong reference from intermediate_tensors\n",
        "            graph_ref().del_non_leaf_tensor_reference(node_id_d)  # THIS IS THE CRUCIAL STEP\n",
        "            gc.collect()  # Force garbage collection again\n",
        "\n",
        "            # Now, with the last strong reference gone, 'd' should be garbage collected.\n",
        "            assert output_tensor_weak_ref() is None, \"Output tensor (non-leaf) should be garbage collected after its strong reference is deleted from intermediate_tensors.\"\n",
        "\n",
        "            # BLOCK 5: Verify other intermediate tensors are collected when graph is cleared\n",
        "            intermediate_tensors_wrefs = []\n",
        "            # Create a new graph and new tensors to avoid interference from previous block\n",
        "            with AutogradGraph(auto_cleanup=False) as graph_new:\n",
        "                a_new = CustomTensor(torch.tensor([1.0]), _custom_requires_grad=True, graph=graph_new, is_leaf=True)\n",
        "                b_new = a_new + 1.0  # Intermediate\n",
        "                c_new = b_new + 2.0  # Intermediate\n",
        "                d_new = c_new + 3.0  # Intermediate (output of a chain)\n",
        "\n",
        "                # Store weak references to the intermediate tensors\n",
        "                intermediate_tensors_wrefs.append(weakref.ref(b_new))\n",
        "                intermediate_tensors_wrefs.append(weakref.ref(c_new))\n",
        "                intermediate_tensors_wrefs.append(weakref.ref(d_new))\n",
        "\n",
        "                # Verify they are initially alive\n",
        "                assert all(wref() is not None for wref in intermediate_tensors_wrefs)\n",
        "                assert len(graph_new.intermediate_tensors) == 3\n",
        "\n",
        "            assert graph_new is not None, \"New graph object should still be alive after 'with' block.\"\n",
        "            assert len(graph_new.intermediate_tensors) == 3, \"New graph intermediate_tensors should still hold refs.\"\n",
        "\n",
        "            # Manually clear the intermediate_tensors dictionary and remove graph reference\n",
        "            graph_new.intermediate_tensors.clear()\n",
        "            del graph_new  # Remove the strong reference to the graph itself\n",
        "            del b_new, c_new, d_new  # deleting the local variable strong references\n",
        "            gc.collect()\n",
        "\n",
        "            # Now, all non-leaf tensors should be garbage collected\n",
        "            for i, wref in enumerate(intermediate_tensors_wrefs):\n",
        "                assert wref() is None, f\"Intermediate tensor {i} should be garbage collected after graph context and intermediate_tensors are cleared.\"\n",
        "            print(\"✓ System Test: No Circular References (Non-leaf tensors die)\")\n",
        "            self.passed_tests += 1\n",
        "        except Exception as e:\n",
        "            print(f\"✗ System Test: No Circular References (Non-leaf tensors die): {str(e)}\")\n",
        "            self.failed_tests += 1\n",
        "\n",
        "    def test_topological_sort_order_system(self):\n",
        "        print(\"\\n=== System Test: Topological Sort Order ===\")\n",
        "        try:\n",
        "            with AutogradGraph() as graph:\n",
        "                t1 = CustomTensor(torch.tensor([1.0]), _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "                t2 = CustomTensor(torch.tensor([2.0]), _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "                t3 = t1 + t2\n",
        "                t4 = t3 + 5.0\n",
        "                t5 = t2 + 10.0  # Another branch\n",
        "                t6 = t4 + t5\n",
        "\n",
        "                # The topological sort should produce an order where dependencies come before their dependents.\n",
        "                # Reversed topological sort should produce an order where outputs come before their inputs.\n",
        "                # Example expected order: t6, t4, t5, t3, t2, t1 (or variations respecting dependencies)\n",
        "                sorted_tensors = graph.reverse_toposort_from_tensor(t6._node_id)\n",
        "\n",
        "\n",
        "                # Check if dependencies are respected in reverse order\n",
        "                # If A -> B, then B should appear before A in reverse topological sort.\n",
        "                # t6 depends on t4, t5. So t6 should be before t4 and t5.\n",
        "                # t4 depends on t3. So t4 should be before t3.\n",
        "                # t5 depends on t2. So t5 should be before t2.\n",
        "                # t3 depends on t1, t2. So t3 should be before t1 and t2.\n",
        "\n",
        "                # Simple check: The first element should be t6 (the ultimate output).\n",
        "                assert sorted_tensors[0].__repr__() == t6.__repr__()\n",
        "\n",
        "                # Check positions:\n",
        "                sorted_tensors=[i.__repr__.__self__ for i in sorted_tensors] #converting the weakref to strongrefs\n",
        "                pos = {t: i for i, t in enumerate(sorted_tensors)}\n",
        "\n",
        "                assert pos[t6] < pos[t4]\n",
        "                assert pos[t6] < pos[t5]\n",
        "                assert pos[t4] < pos[t3]\n",
        "                assert pos[t5] < pos[t2]\n",
        "                assert pos[t3] < pos[t1]\n",
        "                assert pos[t3] < pos[t2]  # t3 also depends on t2\n",
        "\n",
        "                # Additional check: t2 is a dependency for both t3 and t5.\n",
        "                # In reverse topo sort, t3 and t5 must appear before t2.\n",
        "                assert pos[t3] < pos[t2]\n",
        "                assert pos[t5] < pos[t2]\n",
        "\n",
        "                # t1 is only a dependency for t3.\n",
        "                assert pos[t3] < pos[t1]\n",
        "\n",
        "                # Check if all 6 tensors are in the sorted list\n",
        "                assert len(sorted_tensors) == 6\n",
        "                assert set(sorted_tensors) == {t1, t2, t3, t4, t5, t6}\n",
        "                sorted_tensors=None\n",
        "\n",
        "            print(\"✓ System Test: Topological Sort Order\")\n",
        "            self.passed_tests += 1\n",
        "        except Exception as e:\n",
        "            print(f\"✗ System Test: Topological Sort Order: {str(e)}\")\n",
        "            self.failed_tests += 1\n",
        "\n",
        "    def test_very_deep_computation_graph(self):\n",
        "        \"\"\"Test with very deep computation graphs\"\"\"\n",
        "        print(\"\\n=== Testing Very Deep Computation Graph ===\")\n",
        "\n",
        "        try:\n",
        "            depth = 50  # Moderate depth to avoid stack overflow in testing\n",
        "\n",
        "            with AutogradGraph() as graph:\n",
        "                x_custom = CustomTensor([1.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "                current_custom = x_custom\n",
        "\n",
        "                # Create deep chain: x -> x+1 -> (x+1)+1 -> ... (50 times)\n",
        "                for i in range(depth):\n",
        "                    current_custom = current_custom + 1.0\n",
        "\n",
        "                final_custom = current_custom\n",
        "                final_custom.backward()\n",
        "\n",
        "            x_pytorch = torch.tensor([1.0], requires_grad=True)\n",
        "            current_pytorch = x_pytorch\n",
        "\n",
        "            for i in range(depth):\n",
        "                current_pytorch = current_pytorch + 1.0\n",
        "\n",
        "            final_pytorch = current_pytorch\n",
        "            final_pytorch.backward()\n",
        "\n",
        "            self.assert_tensors_close(x_custom, x_pytorch, f\"Deep Graph (depth={depth}) - x\")\n",
        "            self.assert_tensors_close(final_custom, final_pytorch, f\"Deep Graph (depth={depth}) - final\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"✗ Very Deep Computation Graph: {str(e)}\")\n",
        "            self.failed_tests += 1\n",
        "\n",
        "    def test_wide_computation_graph(self):\n",
        "        \"\"\"Test with very wide computation graphs (many inputs)\"\"\"\n",
        "        print(\"\\n=== Testing Wide Computation Graph ===\")\n",
        "\n",
        "        try:\n",
        "            width = 20  # 20 input tensors\n",
        "\n",
        "            with AutogradGraph() as graph:\n",
        "                # Create many input tensors\n",
        "                inputs_custom = []\n",
        "                for i in range(width):\n",
        "                    inputs_custom.append(\n",
        "                        CustomTensor([float(i + 1)], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "                    )\n",
        "\n",
        "                # Sum all inputs\n",
        "                result_custom = inputs_custom[0]\n",
        "                for i in range(1, width):\n",
        "                    result_custom = result_custom + inputs_custom[i]\n",
        "\n",
        "                result_custom.backward()\n",
        "\n",
        "            # PyTorch equivalent\n",
        "            inputs_pytorch = []\n",
        "            for i in range(width):\n",
        "                inputs_pytorch.append(torch.tensor([float(i + 1)], requires_grad=True))\n",
        "\n",
        "            result_pytorch = inputs_pytorch[0]\n",
        "            for i in range(1, width):\n",
        "                result_pytorch = result_pytorch + inputs_pytorch[i]\n",
        "\n",
        "            result_pytorch.backward()\n",
        "\n",
        "            # Check all gradients\n",
        "            for i in range(width):\n",
        "                self.assert_tensors_close(\n",
        "                    inputs_custom[i], inputs_pytorch[i],\n",
        "                    f\"Wide Graph (width={width}) - input_{i}\"\n",
        "                )\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"✗ Wide Computation Graph: {str(e)}\")\n",
        "            self.failed_tests += 1\n",
        "\n",
        "    def test_nan_and_inf_handling(self):\n",
        "        \"\"\"Test handling of NaN and Inf values\"\"\"\n",
        "        print(\"\\n=== Testing NaN and Inf Handling ===\")\n",
        "\n",
        "        try:\n",
        "            # Test with NaN input\n",
        "            with AutogradGraph() as graph:\n",
        "                x_custom = CustomTensor([float('nan')], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "                y_custom = x_custom + 1.0\n",
        "                y_custom.backward()\n",
        "\n",
        "                # Check that gradients handle NaN appropriately\n",
        "                assert torch.isnan(x_custom.tensor.grad).any() or x_custom.tensor.grad is not None\n",
        "\n",
        "            # Test with Inf input\n",
        "            with AutogradGraph() as graph:\n",
        "                x_custom = CustomTensor([float('inf')], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "                y_custom = x_custom * 2.0\n",
        "                y_custom.backward()\n",
        "\n",
        "                # Should handle inf appropriately\n",
        "                assert torch.isinf(x_custom.tensor.grad).any() or x_custom.tensor.grad is not None\n",
        "\n",
        "            print(\"ℹ NaN/Inf Handling - Consider adding explicit handling for edge numerical cases\")\n",
        "            self.passed_tests += 1\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"✗ NaN and Inf Handling: {str(e)}\")\n",
        "            self.failed_tests += 1\n",
        "\n",
        "    def test_zero_gradients(self):\n",
        "        \"\"\"Test operations that should produce zero gradients\"\"\"\n",
        "        print(\"\\n=== Testing Zero Gradients ===\")\n",
        "\n",
        "        try:\n",
        "            with AutogradGraph() as graph:\n",
        "                x_custom = CustomTensor([2.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "\n",
        "                # x - x should have zero gradient with respect to x\n",
        "                y_custom = x_custom - x_custom\n",
        "                y_custom.backward()\n",
        "\n",
        "            x_pytorch = torch.tensor([2.0], requires_grad=True)\n",
        "            y_pytorch = x_pytorch - x_pytorch\n",
        "            y_pytorch.backward()\n",
        "\n",
        "            self.assert_tensors_close(x_custom, x_pytorch, \"Zero Gradients - x\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"✗ Zero Gradients: {str(e)}\")\n",
        "            self.failed_tests += 1\n",
        "\n",
        "\n",
        "    def test_memory_efficiency(self):\n",
        "        \"\"\"Test memory efficiency with large computations\"\"\"\n",
        "        print(\"\\n=== Testing Memory Efficiency ===\")\n",
        "\n",
        "        try:\n",
        "            # Create a computation that could potentially leak memory\n",
        "            initial_tensor_count = len(gc.get_objects())\n",
        "\n",
        "            for iteration in range(5):\n",
        "                with AutogradGraph() as graph:\n",
        "                    x_custom = CustomTensor([1.0] * 100, _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "\n",
        "                    # Chain of operations\n",
        "                    current = x_custom\n",
        "                    for i in range(10):\n",
        "                        current = current + 1.0\n",
        "                        current = current * 1.1\n",
        "\n",
        "                    current.backward(torch.ones(100))\n",
        "\n",
        "                # Force cleanup\n",
        "                del current, x_custom\n",
        "                gc.collect()\n",
        "\n",
        "            final_tensor_count = len(gc.get_objects())\n",
        "\n",
        "            # Memory should not grow excessively\n",
        "            growth = final_tensor_count - initial_tensor_count\n",
        "            print(f\"Object count growth: {growth}\")\n",
        "\n",
        "            if growth < 1000:  # Reasonable threshold\n",
        "                print(\"✓ Memory Efficiency - Reasonable memory usage\")\n",
        "                self.passed_tests += 1\n",
        "            else:\n",
        "                print(f\"⚠ Memory Efficiency - High memory growth: {growth} objects\")\n",
        "                self.passed_tests += 1  # Still pass but warn\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"✗ Memory Efficiency: {str(e)}\")\n",
        "            self.failed_tests += 1\n",
        "    def test_linear_module(self):\n",
        "      \"\"\"Test Linear module forward pass, backward pass, and parameter updates.\"\"\"\n",
        "      print(\"\\n=== Testing Linear Module ===\")\n",
        "\n",
        "      # Test basic functionality\n",
        "      with AutogradGraph() as graph:\n",
        "          # Custom implementation\n",
        "          linear_custom = Linear(3, 2, bias=True, graph=graph)\n",
        "          input_custom = CustomTensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]],\n",
        "                                    _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "          output_custom = linear_custom(input_custom)\n",
        "          loss_custom = (output_custom * output_custom).sum()\n",
        "          loss_custom.backward()\n",
        "\n",
        "          # PyTorch reference\n",
        "          linear_pytorch = torch.nn.Linear(3, 2, bias=True)\n",
        "          linear_pytorch.weight.data = linear_custom.weight.tensor.data.clone()\n",
        "          linear_pytorch.bias.data = linear_custom.bias.tensor.data.clone()\n",
        "\n",
        "          input_pytorch = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]], requires_grad=True)\n",
        "          output_pytorch = linear_pytorch(input_pytorch)\n",
        "          loss_pytorch = (output_pytorch * output_pytorch).sum()\n",
        "          loss_pytorch.backward()\n",
        "\n",
        "          self.assert_tensors_close(output_custom, output_pytorch, \"Linear Forward Pass\")\n",
        "          self.assert_tensors_close(input_custom, input_pytorch, \"Linear Input Gradient\")\n",
        "          self.assert_tensors_close(linear_custom.weight, linear_pytorch.weight, \"Linear Weight Gradient\")\n",
        "          self.assert_tensors_close(linear_custom.bias, linear_pytorch.bias, \"Linear Bias Gradient\")\n",
        "\n",
        "      # Test without bias\n",
        "      with AutogradGraph() as graph:\n",
        "          linear_custom = Linear(2, 1, bias=False, graph=graph)\n",
        "          input_custom = CustomTensor([1.0, 2.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "          output_custom = linear_custom(input_custom)\n",
        "          output_custom.backward()\n",
        "\n",
        "          linear_pytorch = torch.nn.Linear(2, 1, bias=False)\n",
        "          linear_pytorch.weight.data = linear_custom.weight.tensor.data.clone()\n",
        "          input_pytorch = torch.tensor([1.0, 2.0], requires_grad=True)\n",
        "          output_pytorch = linear_pytorch(input_pytorch)\n",
        "          output_pytorch.backward()\n",
        "\n",
        "          self.assert_tensors_close(output_custom, output_pytorch, \"Linear No Bias Forward\")\n",
        "          self.assert_tensors_close(linear_custom.weight, linear_pytorch.weight, \"Linear No Bias Weight Gradient\")\n",
        "\n",
        "      # Test training vs eval mode\n",
        "      with AutogradGraph() as graph:\n",
        "          linear_custom = Linear(2, 1, graph=graph)\n",
        "          input_custom = CustomTensor([1.0, 2.0], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "\n",
        "          # Training mode\n",
        "          linear_custom.train()\n",
        "          output_train = linear_custom(input_custom)\n",
        "\n",
        "          # Eval mode\n",
        "          linear_custom.eval()\n",
        "          output_eval = linear_custom(input_custom)\n",
        "\n",
        "          # In eval mode, should not require grad for output\n",
        "          try:\n",
        "              if hasattr(output_eval, '_custom_requires_grad') and output_eval._custom_requires_grad:\n",
        "                  raise AssertionError(\"Output in eval mode should not require grad\")\n",
        "              print(\"✓ Linear Eval Mode - No Grad\")\n",
        "              self.passed_tests += 1\n",
        "          except Exception as e:\n",
        "              print(f\"✗ Linear Eval Mode - No Grad: {str(e)}\")\n",
        "              self.failed_tests += 1\n",
        "\n",
        "    def test_conv2d_module(self):\n",
        "      \"\"\"Test Conv2d module forward pass, backward pass, and parameter updates.\"\"\"\n",
        "      print(\"\\n=== Testing Conv2d Module ===\")\n",
        "\n",
        "      # Test basic convolution\n",
        "      with AutogradGraph() as graph:\n",
        "          # Custom implementation\n",
        "          conv_custom = Conv2d(in_channels=2, out_channels=3, kernel_size=3,\n",
        "                            stride=1, padding=1, bias=True, graph=graph)\n",
        "          input_custom = CustomTensor(torch.randn(1, 2, 4, 4),\n",
        "                                    _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "          output_custom = conv_custom(input_custom)\n",
        "          loss_custom = output_custom.sum()\n",
        "          loss_custom.backward()\n",
        "\n",
        "          # PyTorch reference\n",
        "          conv_pytorch = torch.nn.Conv2d(2, 3, 3, stride=1, padding=1, bias=True)\n",
        "          conv_pytorch.weight.data = conv_custom.weight.tensor.data.clone()\n",
        "          conv_pytorch.bias.data = conv_custom.bias.tensor.data.clone()\n",
        "\n",
        "          input_pytorch = input_custom.tensor.clone().detach().requires_grad_(True)\n",
        "          output_pytorch = conv_pytorch(input_pytorch)\n",
        "          loss_pytorch = output_pytorch.sum()\n",
        "          loss_pytorch.backward()\n",
        "\n",
        "          self.assert_tensors_close(output_custom, output_pytorch, \"Conv2d Forward Pass\")\n",
        "          self.assert_tensors_close(input_custom, input_pytorch, \"Conv2d Input Gradient\")\n",
        "          self.assert_tensors_close(conv_custom.weight, conv_pytorch.weight, \"Conv2d Weight Gradient\")\n",
        "          self.assert_tensors_close(conv_custom.bias, conv_pytorch.bias, \"Conv2d Bias Gradient\")\n",
        "\n",
        "      # Test different parameters\n",
        "      with AutogradGraph() as graph:\n",
        "          conv_custom = Conv2d(in_channels=1, out_channels=2, kernel_size=2,\n",
        "                            stride=2, padding=0, bias=False, graph=graph)\n",
        "          input_custom = CustomTensor(torch.randn(1, 1, 6, 6),\n",
        "                                    _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "          output_custom = conv_custom(input_custom)\n",
        "          output_custom.sum().backward()\n",
        "\n",
        "          conv_pytorch = torch.nn.Conv2d(1, 2, 2, stride=2, padding=0, bias=False)\n",
        "          conv_pytorch.weight.data = conv_custom.weight.tensor.data.clone()\n",
        "          input_pytorch = input_custom.tensor.clone().detach().requires_grad_(True)\n",
        "          output_pytorch = conv_pytorch(input_pytorch)\n",
        "          output_pytorch.sum().backward()\n",
        "\n",
        "          self.assert_tensors_close(output_custom, output_pytorch, \"Conv2d Different Params Forward\")\n",
        "          self.assert_tensors_close(conv_custom.weight, conv_pytorch.weight, \"Conv2d Different Params Weight Gradient\")\n",
        "\n",
        "    def test_batchnorm_module(self):\n",
        "      \"\"\"Test BatchNorm_Nd module forward pass, backward pass, and running statistics.\"\"\"\n",
        "      print(\"\\n=== Testing BatchNorm Module ===\")\n",
        "\n",
        "      # Test training mode\n",
        "      with AutogradGraph() as graph:\n",
        "          bn_custom = BatchNorm_Nd(num_features=3, graph=graph)\n",
        "          input_custom = CustomTensor(torch.randn(2, 3, 4, 4),\n",
        "                                    _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "\n",
        "          bn_custom.train()\n",
        "          output_custom = bn_custom(input_custom)\n",
        "          loss_custom = output_custom.sum()\n",
        "          loss_custom.backward()\n",
        "\n",
        "          # PyTorch reference\n",
        "          bn_pytorch = torch.nn.BatchNorm2d(3)\n",
        "          bn_pytorch.weight.data = bn_custom.weight.tensor.data.clone()\n",
        "          bn_pytorch.bias.data = bn_custom.bias.tensor.data.clone()\n",
        "          bn_pytorch.running_mean = bn_custom.running_mean.clone()\n",
        "          bn_pytorch.running_var = bn_custom.running_var.clone()\n",
        "\n",
        "          input_pytorch = input_custom.tensor.clone().detach().requires_grad_(True)\n",
        "          output_pytorch = bn_pytorch(input_pytorch)\n",
        "          loss_pytorch = output_pytorch.sum()\n",
        "          loss_pytorch.backward()\n",
        "\n",
        "          self.assert_tensors_close(output_custom, output_pytorch, \"BatchNorm Training Forward\")\n",
        "          self.assert_tensors_close(input_custom, input_pytorch, \"BatchNorm Input Gradient\")\n",
        "          self.assert_tensors_close(bn_custom.weight, bn_pytorch.weight, \"BatchNorm Weight Gradient\")\n",
        "          self.assert_tensors_close(bn_custom.bias, bn_pytorch.bias, \"BatchNorm Bias Gradient\")\n",
        "\n",
        "      # Test eval mode\n",
        "      with AutogradGraph() as graph:\n",
        "          bn_custom = BatchNorm_Nd(num_features=2, graph=graph)\n",
        "          input_custom = CustomTensor(torch.randn(1, 2, 3, 3),\n",
        "                                    _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "\n",
        "          # Set some running stats\n",
        "          bn_custom.running_mean = torch.tensor([0.5, -0.3])\n",
        "          bn_custom.running_var = torch.tensor([1.2, 0.8])\n",
        "\n",
        "          bn_custom.eval()\n",
        "          output_custom = bn_custom(input_custom)\n",
        "\n",
        "          bn_pytorch = torch.nn.BatchNorm2d(2)\n",
        "          bn_pytorch.weight.data = bn_custom.weight.tensor.data.clone()\n",
        "          bn_pytorch.bias.data = bn_custom.bias.tensor.data.clone()\n",
        "          bn_pytorch.running_mean = bn_custom.running_mean.clone()\n",
        "          bn_pytorch.running_var = bn_custom.running_var.clone()\n",
        "          bn_pytorch.eval()\n",
        "\n",
        "          input_pytorch = input_custom.tensor.clone().detach().requires_grad_(True)\n",
        "          output_pytorch = bn_pytorch(input_pytorch)\n",
        "\n",
        "          self.assert_tensors_close(output_custom, output_pytorch, \"BatchNorm Eval Forward\")\n",
        "\n",
        "    def test_maxpool2d_module(self):\n",
        "      \"\"\"Test MaxPool2d module forward pass and backward pass.\"\"\"\n",
        "      print(\"\\n=== Testing MaxPool2d Module ===\")\n",
        "\n",
        "      with AutogradGraph() as graph:\n",
        "          pool_custom = MaxPool2d(kernel_size=2, stride=2, padding=0, graph=graph)\n",
        "          input_custom = CustomTensor(torch.randn(1, 2, 4, 4),\n",
        "                                    _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "          output_custom = pool_custom(input_custom)\n",
        "          loss_custom = output_custom.sum()\n",
        "          loss_custom.backward()\n",
        "\n",
        "          # PyTorch reference\n",
        "          pool_pytorch = torch.nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "          input_pytorch = input_custom.tensor.clone().detach().requires_grad_(True)\n",
        "          output_pytorch = pool_pytorch(input_pytorch)\n",
        "          loss_pytorch = output_pytorch.sum()\n",
        "          loss_pytorch.backward()\n",
        "\n",
        "          self.assert_tensors_close(output_custom, output_pytorch, \"MaxPool2d Forward\")\n",
        "          self.assert_tensors_close(input_custom, input_pytorch, \"MaxPool2d Input Gradient\")\n",
        "\n",
        "      # Test with different parameters\n",
        "      with AutogradGraph() as graph:\n",
        "          pool_custom = MaxPool2d(kernel_size=3, stride=1, padding=1, graph=graph)\n",
        "          input_custom = CustomTensor(torch.randn(2, 1, 5, 5),\n",
        "                                    _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "          output_custom = pool_custom(input_custom)\n",
        "          output_custom=output_custom.sum()\n",
        "          output_custom.backward()\n",
        "\n",
        "          pool_pytorch = torch.nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n",
        "          input_pytorch = input_custom.tensor.clone().detach().requires_grad_(True)\n",
        "          output_pytorch = pool_pytorch(input_pytorch)\n",
        "          output_pytorch=output_pytorch.sum()\n",
        "          output_pytorch.backward()\n",
        "\n",
        "          self.assert_tensors_close(output_custom, output_pytorch, \"MaxPool2d Different Params Forward\")\n",
        "          self.assert_tensors_close(input_custom, input_pytorch, \"MaxPool2d Different Params Gradient\")\n",
        "\n",
        "    def test_avgpool2d_module(self):\n",
        "      \"\"\"Test AvgPool2d module forward pass and backward pass.\"\"\"\n",
        "      print(\"\\n=== Testing AvgPool2d Module ===\")\n",
        "\n",
        "      with AutogradGraph() as graph:\n",
        "          pool_custom = AvgPool2d(kernel_size=2, stride=2, padding=0, graph=graph)\n",
        "          input_custom = CustomTensor(torch.randn(1, 2, 4, 4),\n",
        "                                    _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "          output_custom = pool_custom(input_custom)\n",
        "          loss_custom = output_custom.sum()\n",
        "          loss_custom.backward()\n",
        "\n",
        "          # PyTorch reference\n",
        "          pool_pytorch = torch.nn.AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
        "          input_pytorch = input_custom.tensor.clone().detach().requires_grad_(True)\n",
        "          output_pytorch = pool_pytorch(input_pytorch)\n",
        "          loss_pytorch = output_pytorch.sum()\n",
        "          loss_pytorch.backward()\n",
        "\n",
        "          self.assert_tensors_close(output_custom, output_pytorch, \"AvgPool2d Forward\")\n",
        "          self.assert_tensors_close(input_custom, input_pytorch, \"AvgPool2d Input Gradient\")\n",
        "\n",
        "      # Test with padding\n",
        "      with AutogradGraph() as graph:\n",
        "          pool_custom = AvgPool2d(kernel_size=3, stride=1, padding=1, graph=graph)\n",
        "          input_custom = CustomTensor(torch.randn(1, 1, 4, 4),\n",
        "                                    _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "          output_custom = pool_custom(input_custom)\n",
        "          output_custom.sum().backward()\n",
        "\n",
        "          pool_pytorch = torch.nn.AvgPool2d(kernel_size=3, stride=1, padding=1)\n",
        "          input_pytorch = input_custom.tensor.clone().detach().requires_grad_(True)\n",
        "          output_pytorch = pool_pytorch(input_pytorch)\n",
        "          output_pytorch.sum().backward()\n",
        "\n",
        "          self.assert_tensors_close(output_custom, output_pytorch, \"AvgPool2d With Padding Forward\")\n",
        "          self.assert_tensors_close(input_custom, input_pytorch, \"AvgPool2d With Padding Gradient\")\n",
        "\n",
        "    def test_relu_module(self):\n",
        "        \"\"\"Test ReLU activation module.\"\"\"\n",
        "        print(\"\\n=== Testing ReLU Module ===\")\n",
        "\n",
        "        with AutogradGraph() as graph:\n",
        "            relu_custom = ReLu(graph=graph)\n",
        "            input_custom = CustomTensor(torch.randn(2, 3),\n",
        "                                        _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            output_custom = relu_custom(input_custom)\n",
        "            loss_custom = output_custom.sum()\n",
        "            loss_custom.backward()\n",
        "\n",
        "            # PyTorch reference\n",
        "            relu_pytorch = torch.nn.ReLU()\n",
        "            input_pytorch = input_custom.tensor.clone().detach().requires_grad_(True)\n",
        "            output_pytorch = relu_pytorch(input_pytorch)\n",
        "            loss_pytorch = output_pytorch.sum()\n",
        "            loss_pytorch.backward()\n",
        "\n",
        "            self.assert_tensors_close(output_custom, output_pytorch, \"ReLU Forward\")\n",
        "            self.assert_tensors_close(input_custom, input_pytorch, \"ReLU Input Gradient\")\n",
        "\n",
        "        # Test with negative values specifically\n",
        "        with AutogradGraph() as graph:\n",
        "            relu_custom = ReLu(graph=graph)\n",
        "            input_custom = CustomTensor(torch.tensor([-2.0, -1.0, 0.0, 1.0, 2.0]),\n",
        "                                        _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            output_custom = relu_custom(input_custom)\n",
        "            output_custom.sum().backward()\n",
        "\n",
        "            relu_pytorch = torch.nn.ReLU()\n",
        "            input_pytorch = torch.tensor([-2.0, -1.0, 0.0, 1.0, 2.0], requires_grad=True)\n",
        "            output_pytorch = relu_pytorch(input_pytorch)\n",
        "            output_pytorch.sum().backward()\n",
        "\n",
        "            self.assert_tensors_close(output_custom, output_pytorch, \"ReLU Negative Values Forward\")\n",
        "            self.assert_tensors_close(input_custom, input_pytorch, \"ReLU Negative Values Gradient\")\n",
        "\n",
        "    def test_leaky_relu_module(self):\n",
        "        \"\"\"Test Leaky ReLU activation module.\"\"\"\n",
        "        print(\"\\n=== Testing Leaky ReLU Module ===\")\n",
        "\n",
        "        with AutogradGraph() as graph:\n",
        "            leaky_relu_custom = Leaky_ReLu(negative_slope=0.01, graph=graph)\n",
        "            input_custom = CustomTensor(torch.randn(2, 3),\n",
        "                                        _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            output_custom = leaky_relu_custom(input_custom)\n",
        "            loss_custom = output_custom.sum()\n",
        "            loss_custom.backward()\n",
        "\n",
        "            # PyTorch reference\n",
        "            leaky_relu_pytorch = torch.nn.LeakyReLU(negative_slope=0.01)\n",
        "            input_pytorch = input_custom.tensor.clone().detach().requires_grad_(True)\n",
        "            output_pytorch = leaky_relu_pytorch(input_pytorch)\n",
        "            loss_pytorch = output_pytorch.sum()\n",
        "            loss_pytorch.backward()\n",
        "\n",
        "            self.assert_tensors_close(output_custom, output_pytorch, \"Leaky ReLU Forward\")\n",
        "            self.assert_tensors_close(input_custom, input_pytorch, \"Leaky ReLU Input Gradient\")\n",
        "\n",
        "        # Test with different slope\n",
        "        with AutogradGraph() as graph:\n",
        "            leaky_relu_custom = Leaky_ReLu(negative_slope=0.1, graph=graph)\n",
        "            input_custom = CustomTensor(torch.tensor([-2.0, -1.0, 0.0, 1.0, 2.0]),\n",
        "                                        _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            output_custom = leaky_relu_custom(input_custom)\n",
        "            output_custom.sum().backward()\n",
        "\n",
        "            leaky_relu_pytorch = torch.nn.LeakyReLU(negative_slope=0.1)\n",
        "            input_pytorch = torch.tensor([-2.0, -1.0, 0.0, 1.0, 2.0], requires_grad=True)\n",
        "            output_pytorch = leaky_relu_pytorch(input_pytorch)\n",
        "            output_pytorch.sum().backward()\n",
        "\n",
        "            self.assert_tensors_close(output_custom, output_pytorch, \"Leaky ReLU Different Slope Forward\")\n",
        "            self.assert_tensors_close(input_custom, input_pytorch, \"Leaky ReLU Different Slope Gradient\")\n",
        "\n",
        "    def test_gelu_module(self):\n",
        "        \"\"\"Test GELU activation module.\"\"\"\n",
        "        print(\"\\n=== Testing GELU Module ===\")\n",
        "\n",
        "        # Test exact GELU\n",
        "        with AutogradGraph() as graph:\n",
        "            gelu_custom = GeLu(approximate='none', graph=graph)\n",
        "            input_custom = CustomTensor(torch.randn(2, 3),\n",
        "                                        _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            output_custom = gelu_custom(input_custom)\n",
        "            loss_custom = output_custom.sum()\n",
        "            loss_custom.backward()\n",
        "\n",
        "            # PyTorch reference\n",
        "            gelu_pytorch = torch.nn.GELU(approximate='none')\n",
        "            input_pytorch = input_custom.tensor.clone().detach().requires_grad_(True)\n",
        "            output_pytorch = gelu_pytorch(input_pytorch)\n",
        "            loss_pytorch = output_pytorch.sum()\n",
        "            loss_pytorch.backward()\n",
        "\n",
        "            self.assert_tensors_close(output_custom, output_pytorch, \"GELU Exact Forward\")\n",
        "            self.assert_tensors_close(input_custom, input_pytorch, \"GELU Exact Input Gradient\")\n",
        "\n",
        "        # Test approximate GELU\n",
        "        with AutogradGraph() as graph:\n",
        "            gelu_custom = GeLu(approximate='tanh', graph=graph)\n",
        "            input_custom = CustomTensor(torch.randn(2, 3),\n",
        "                                        _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            output_custom = gelu_custom(input_custom)\n",
        "            output_custom.sum().backward()\n",
        "\n",
        "            gelu_pytorch = torch.nn.GELU(approximate='tanh')\n",
        "            input_pytorch = input_custom.tensor.clone().detach().requires_grad_(True)\n",
        "            output_pytorch = gelu_pytorch(input_pytorch)\n",
        "            output_pytorch.sum().backward()\n",
        "\n",
        "            self.assert_tensors_close(output_custom, output_pytorch, \"GELU Approximate Forward\")\n",
        "            self.assert_tensors_close(input_custom, input_pytorch, \"GELU Approximate Input Gradient\")\n",
        "\n",
        "    def test_elu_module(self):\n",
        "        \"\"\"Test ELU activation module.\"\"\"\n",
        "        print(\"\\n=== Testing ELU Module ===\")\n",
        "\n",
        "        with AutogradGraph() as graph:\n",
        "            elu_custom = Elu(alpha=1.0, graph=graph)\n",
        "            input_custom = CustomTensor(torch.randn(2, 3),\n",
        "                                        _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            output_custom = elu_custom(input_custom)\n",
        "            loss_custom = output_custom.sum()\n",
        "            loss_custom.backward()\n",
        "\n",
        "            # PyTorch reference\n",
        "            elu_pytorch = torch.nn.ELU(alpha=1.0)\n",
        "            input_pytorch = input_custom.tensor.clone().detach().requires_grad_(True)\n",
        "            output_pytorch = elu_pytorch(input_pytorch)\n",
        "            loss_pytorch = output_pytorch.sum()\n",
        "            loss_pytorch.backward()\n",
        "\n",
        "            self.assert_tensors_close(output_custom, output_pytorch, \"ELU Forward\")\n",
        "            self.assert_tensors_close(input_custom, input_pytorch, \"ELU Input Gradient\")\n",
        "\n",
        "        # Test with different alpha\n",
        "        with AutogradGraph() as graph:\n",
        "            elu_custom = Elu(alpha=0.5, graph=graph)\n",
        "            input_custom = CustomTensor(torch.tensor([-2.0, -1.0, 0.0, 1.0, 2.0]),\n",
        "                                        _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            output_custom = elu_custom(input_custom)\n",
        "            output_custom.sum().backward()\n",
        "\n",
        "            elu_pytorch = torch.nn.ELU(alpha=0.5)\n",
        "            input_pytorch = torch.tensor([-2.0, -1.0, 0.0, 1.0, 2.0], requires_grad=True)\n",
        "            output_pytorch = elu_pytorch(input_pytorch)\n",
        "            output_pytorch.sum().backward()\n",
        "\n",
        "            self.assert_tensors_close(output_custom, output_pytorch, \"ELU Different Alpha Forward\")\n",
        "            self.assert_tensors_close(input_custom, input_pytorch, \"ELU Different Alpha Gradient\")\n",
        "\n",
        "    def test_silu_module(self):\n",
        "        \"\"\"Test SiLU (Swish) activation module.\"\"\"\n",
        "        print(\"\\n=== Testing SiLU Module ===\")\n",
        "\n",
        "        with AutogradGraph() as graph:\n",
        "            silu_custom = Silu(graph=graph)\n",
        "            input_custom = CustomTensor(torch.randn(2, 3),\n",
        "                                        _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            output_custom = silu_custom(input_custom)\n",
        "            loss_custom = output_custom.sum()\n",
        "            loss_custom.backward()\n",
        "\n",
        "            # PyTorch reference\n",
        "            silu_pytorch = torch.nn.SiLU()\n",
        "            input_pytorch = input_custom.tensor.clone().detach().requires_grad_(True)\n",
        "            output_pytorch = silu_pytorch(input_pytorch)\n",
        "            loss_pytorch = output_pytorch.sum()\n",
        "            loss_pytorch.backward()\n",
        "\n",
        "            self.assert_tensors_close(output_custom, output_pytorch, \"SiLU Forward\")\n",
        "            self.assert_tensors_close(input_custom, input_pytorch, \"SiLU Input Gradient\")\n",
        "\n",
        "    def test_sigmoid_module(self):\n",
        "        \"\"\"Test Sigmoid activation module.\"\"\"\n",
        "        print(\"\\n=== Testing Sigmoid Module ===\")\n",
        "\n",
        "        with AutogradGraph() as graph:\n",
        "            sigmoid_custom = Sigmoid(graph=graph)\n",
        "            input_custom = CustomTensor(torch.randn(2, 3),\n",
        "                                        _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            output_custom = sigmoid_custom(input_custom)\n",
        "            loss_custom = output_custom.sum()\n",
        "            loss_custom.backward()\n",
        "\n",
        "            # PyTorch reference\n",
        "            sigmoid_pytorch = torch.nn.Sigmoid()\n",
        "            input_pytorch = input_custom.tensor.clone().detach().requires_grad_(True)\n",
        "            output_pytorch = sigmoid_pytorch(input_pytorch)\n",
        "            loss_pytorch = output_pytorch.sum()\n",
        "            loss_pytorch.backward()\n",
        "\n",
        "            self.assert_tensors_close(output_custom, output_pytorch, \"Sigmoid Forward\")\n",
        "            self.assert_tensors_close(input_custom, input_pytorch, \"Sigmoid Input Gradient\")\n",
        "\n",
        "    def test_tanh_module(self):\n",
        "        \"\"\"Test Tanh activation module.\"\"\"\n",
        "        print(\"\\n=== Testing Tanh Module ===\")\n",
        "\n",
        "        with AutogradGraph() as graph:\n",
        "            tanh_custom = Tanh(graph=graph)\n",
        "            input_custom = CustomTensor(torch.randn(2, 3),\n",
        "                                        _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            output_custom = tanh_custom(input_custom)\n",
        "            loss_custom = output_custom.sum()\n",
        "            loss_custom.backward()\n",
        "\n",
        "            # PyTorch reference\n",
        "            tanh_pytorch = torch.nn.Tanh()\n",
        "            input_pytorch = input_custom.tensor.clone().detach().requires_grad_(True)\n",
        "            output_pytorch = tanh_pytorch(input_pytorch)\n",
        "            loss_pytorch = output_pytorch.sum()\n",
        "            loss_pytorch.backward()\n",
        "\n",
        "            self.assert_tensors_close(output_custom, output_pytorch, \"Tanh Forward\")\n",
        "            self.assert_tensors_close(input_custom, input_pytorch, \"Tanh Input Gradient\")\n",
        "\n",
        "    def test_swish_module(self):\n",
        "        \"\"\"Test Swish activation module with learnable parameter.\"\"\"\n",
        "        print(\"\\n=== Testing Swish Module ===\")\n",
        "\n",
        "        with AutogradGraph() as graph:\n",
        "            swish_custom = Swish(B_initial=1.0, graph=graph)\n",
        "            input_custom = CustomTensor(torch.randn(2, 3),\n",
        "                                        _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            output_custom = swish_custom(input_custom)\n",
        "            loss_custom = output_custom.sum()\n",
        "            loss_custom.backward()\n",
        "\n",
        "            # PyTorch reference - manual implementation since there's no direct equivalent\n",
        "            class PyTorchSwish(torch.nn.Module):\n",
        "                def __init__(self, B_initial=1.0):\n",
        "                    super().__init__()\n",
        "                    self.B = torch.nn.Parameter(torch.tensor([B_initial]))\n",
        "\n",
        "                def forward(self, x):\n",
        "                    return x * torch.sigmoid(self.B * x)\n",
        "\n",
        "            swish_pytorch = PyTorchSwish(B_initial=1.0)\n",
        "            swish_pytorch.B.data = swish_custom.B.tensor.data.clone()\n",
        "\n",
        "            input_pytorch = input_custom.tensor.clone().detach().requires_grad_(True)\n",
        "            output_pytorch = swish_pytorch(input_pytorch)\n",
        "            loss_pytorch = output_pytorch.sum()\n",
        "            loss_pytorch.backward()\n",
        "\n",
        "            self.assert_tensors_close(output_custom, output_pytorch, \"Swish Forward\")\n",
        "            self.assert_tensors_close(input_custom, input_pytorch, \"Swish Input Gradient\")\n",
        "            self.assert_tensors_close(swish_custom.B, swish_pytorch.B, \"Swish B Parameter Gradient\")\n",
        "\n",
        "        # Test with different B_initial\n",
        "        with AutogradGraph() as graph:\n",
        "            swish_custom = Swish(B_initial=2.0, graph=graph)\n",
        "            input_custom = CustomTensor(torch.tensor([0.5, -0.5, 1.0, -1.0]),\n",
        "                                        _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            output_custom = swish_custom(input_custom)\n",
        "            output_custom.sum().backward()\n",
        "\n",
        "            swish_pytorch = PyTorchSwish(B_initial=2.0)\n",
        "            swish_pytorch.B.data = swish_custom.B.tensor.data.clone()\n",
        "            input_pytorch = torch.tensor([0.5, -0.5, 1.0, -1.0], requires_grad=True)\n",
        "            output_pytorch = swish_pytorch(input_pytorch)\n",
        "            output_pytorch.sum().backward()\n",
        "\n",
        "            self.assert_tensors_close(output_custom, output_pytorch, \"Swish Different B Forward\")\n",
        "            self.assert_tensors_close(swish_custom.B, swish_pytorch.B, \"Swish Different B Parameter Gradient\")\n",
        "\n",
        "    def test_module_parameter_management(self):\n",
        "        \"\"\"Test parameter collection and gradient zeroing across modules.\"\"\"\n",
        "        print(\"\\n=== Testing Module Parameter Management ===\")\n",
        "\n",
        "        with AutogradGraph() as graph:\n",
        "            # Create a small network\n",
        "            linear1 = Linear(3, 2, graph=graph)\n",
        "            linear2 = Linear(2, 1, graph=graph)\n",
        "\n",
        "            # Test parameter collection\n",
        "            params1 = linear1.parameters()\n",
        "            params2 = linear2.parameters()\n",
        "\n",
        "            try:\n",
        "                # Should have weight and bias for each layer\n",
        "                if len(params1) != 2:\n",
        "                    raise AssertionError(f\"Linear1 should have 2 parameters, got {len(params1)}\")\n",
        "                if len(params2) != 2:\n",
        "                    raise AssertionError(f\"Linear2 should have 2 parameters, got {len(params2)}\")\n",
        "                print(\"✓ Module Parameter Collection\")\n",
        "                self.passed_tests += 1\n",
        "            except Exception as e:\n",
        "                print(f\"✗ Module Parameter Collection: {str(e)}\")\n",
        "                self.failed_tests += 1\n",
        "\n",
        "            # Test forward pass\n",
        "            input_tensor = CustomTensor([[1.0, 2.0, 3.0]], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            hidden = linear1(input_tensor)\n",
        "            output = linear2(hidden)\n",
        "            loss = output.sum()\n",
        "            loss.backward()\n",
        "\n",
        "            # Check that all parameters have gradients\n",
        "            all_params = params1 + params2\n",
        "            try:\n",
        "                for i, param in enumerate(all_params):\n",
        "                    if param.tensor.grad is None:\n",
        "                        raise AssertionError(f\"Parameter {i} should have gradient\")\n",
        "                print(\"✓ Module All Parameters Have Gradients\")\n",
        "                self.passed_tests += 1\n",
        "            except Exception as e:\n",
        "                print(f\"✗ Module All Parameters Have Gradients: {str(e)}\")\n",
        "                self.failed_tests += 1\n",
        "\n",
        "            # Test zero_grad\n",
        "            linear1.zero_grad()\n",
        "            linear2.zero_grad()\n",
        "\n",
        "            try:\n",
        "                for i, param in enumerate(all_params):\n",
        "                    if param.tensor.grad is None or not torch.allclose(param.tensor.grad, torch.zeros_like(param.tensor.grad)):\n",
        "                        raise AssertionError(f\"Parameter {i} gradient should be zero after zero_grad()\")\n",
        "                print(\"✓ Module Zero Grad\")\n",
        "                self.passed_tests += 1\n",
        "            except Exception as e:\n",
        "                print(f\"✗ Module Zero Grad: {str(e)}\")\n",
        "                self.failed_tests += 1\n",
        "\n",
        "    def test_module_training_eval_modes(self):\n",
        "        \"\"\"Test training and evaluation mode switching.\"\"\"\n",
        "        print(\"\\n=== Testing Module Training/Eval Modes ===\")\n",
        "\n",
        "        with AutogradGraph() as graph:\n",
        "            # Test with modules that behave differently in train/eval\n",
        "            linear = Linear(2, 1, graph=graph)\n",
        "            bn = BatchNorm_Nd(1, graph=graph)\n",
        "            relu = ReLu(graph=graph)\n",
        "\n",
        "            # Initially should be in training mode\n",
        "            try:\n",
        "                if not linear.training or not bn.training or not relu.training:\n",
        "                    raise AssertionError(\"Modules should start in training mode\")\n",
        "                print(\"✓ Module Initial Training Mode\")\n",
        "                self.passed_tests += 1\n",
        "            except Exception as e:\n",
        "                print(f\"✗ Module Initial Training Mode: {str(e)}\")\n",
        "                self.failed_tests += 1\n",
        "\n",
        "            # Switch to eval mode\n",
        "            linear.eval()\n",
        "            bn.eval()\n",
        "            relu.eval()\n",
        "\n",
        "            try:\n",
        "                if linear.training or bn.training or relu.training:\n",
        "                    raise AssertionError(\"Modules should be in eval mode after eval()\")\n",
        "                print(\"✓ Module Eval Mode Switch\")\n",
        "                self.passed_tests += 1\n",
        "            except Exception as e:\n",
        "                print(f\"✗ Module Eval Mode Switch: {str(e)}\")\n",
        "                self.failed_tests += 1\n",
        "\n",
        "            # Switch back to training mode\n",
        "            linear.train()\n",
        "            bn.train()\n",
        "            relu.train()\n",
        "\n",
        "            try:\n",
        "                if not linear.training or not bn.training or not relu.training:\n",
        "                    raise AssertionError(\"Modules should be in training mode after train()\")\n",
        "                print(\"✓ Module Training Mode Switch\")\n",
        "                self.passed_tests += 1\n",
        "            except Exception as e:\n",
        "                print(f\"✗ Module Training Mode Switch: {str(e)}\")\n",
        "                self.failed_tests += 1\n",
        "\n",
        "    def test_module_nested_structure(self):\n",
        "        \"\"\"Test nested module structures and parameter collection.\"\"\"\n",
        "        print(\"\\n=== Testing Nested Module Structure ===\")\n",
        "\n",
        "        class SimpleNet(Module):\n",
        "            def __init__(self, graph):\n",
        "                super().__init__()\n",
        "                self.layer1 = Linear(3, 4, graph=graph)\n",
        "                self.activation = ReLu(graph=graph)\n",
        "                self.layer2 = Linear(4, 2, graph=graph)\n",
        "\n",
        "            def forward(self, x):\n",
        "                x = self.layer1(x)\n",
        "                x = self.activation(x)\n",
        "                x = self.layer2(x)\n",
        "                return x\n",
        "\n",
        "        with AutogradGraph() as graph:\n",
        "            net = SimpleNet(graph)\n",
        "\n",
        "            # Test nested parameter collection\n",
        "            params = net.parameters()\n",
        "\n",
        "            try:\n",
        "                # Should have 4 parameters: 2 weights + 2 biases\n",
        "                if len(params) != 4:\n",
        "                    raise AssertionError(f\"Network should have 4 parameters, got {len(params)}\")\n",
        "                print(\"✓ Nested Module Parameter Collection\")\n",
        "                self.passed_tests += 1\n",
        "            except Exception as e:\n",
        "                print(f\"✗ Nested Module Parameter Collection: {str(e)}\")\n",
        "                self.failed_tests += 1\n",
        "\n",
        "            # Test nested training mode switching\n",
        "            net.train()\n",
        "            try:\n",
        "                if not net.layer1.training or not net.activation.training or not net.layer2.training:\n",
        "                    raise AssertionError(\"All nested modules should be in training mode\")\n",
        "                print(\"✓ Nested Module Training Mode\")\n",
        "                self.passed_tests += 1\n",
        "            except Exception as e:\n",
        "                print(f\"✗ Nested Module Training Mode: {str(e)}\")\n",
        "                self.failed_tests += 1\n",
        "\n",
        "            net.eval()\n",
        "            try:\n",
        "                if net.layer1.training or net.activation.training or net.layer2.training:\n",
        "                    raise AssertionError(\"All nested modules should be in eval mode\")\n",
        "                print(\"✓ Nested Module Eval Mode\")\n",
        "                self.passed_tests += 1\n",
        "            except Exception as e:\n",
        "                print(f\"✗ Nested Module Eval Mode: {str(e)}\")\n",
        "                self.failed_tests += 1\n",
        "            net.train()\n",
        "            # Test forward pass through nested structure\n",
        "            input_tensor = CustomTensor([[1.0, 2.0, 3.0]], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            output = net(input_tensor)\n",
        "            loss = output.sum()\n",
        "            loss.backward()\n",
        "\n",
        "            # Check that all parameters have gradients\n",
        "            try:\n",
        "                for i, param in enumerate(params):\n",
        "                    if param.tensor.grad is None:\n",
        "                        raise AssertionError(f\"Parameter {i} should have gradient after backward\")\n",
        "                print(\"✓ Nested Module Gradient Flow\")\n",
        "                self.passed_tests += 1\n",
        "            except Exception as e:\n",
        "                print(f\"✗ Nested Module Gradient Flow: {str(e)}\")\n",
        "                self.failed_tests += 1\n",
        "\n",
        "    def test_module_edge_cases(self):\n",
        "        \"\"\"Test edge cases and error conditions for modules.\"\"\"\n",
        "        print(\"\\n=== Testing Module Edge Cases ===\")\n",
        "\n",
        "        # Test very small inputs\n",
        "        with AutogradGraph() as graph:\n",
        "            linear = Linear(1, 1, graph=graph)\n",
        "            tiny_input = CustomTensor([[1e-8]], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            output = linear(tiny_input)\n",
        "            output.backward()\n",
        "\n",
        "            try:\n",
        "                if linear.weight.tensor.grad is None or linear.bias.tensor.grad is None:\n",
        "                    raise AssertionError(\"Should handle very small inputs\")\n",
        "                print(\"✓ Module Tiny Input Handling\")\n",
        "                self.passed_tests += 1\n",
        "            except Exception as e:\n",
        "                print(f\"✗ Module Tiny Input Handling: {str(e)}\")\n",
        "                self.failed_tests += 1\n",
        "\n",
        "        # Test large inputs\n",
        "        with AutogradGraph() as graph:\n",
        "            linear = Linear(2, 2, graph=graph)\n",
        "            large_input = CustomTensor([[1e6, -1e6]], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            output = linear(large_input)\n",
        "            output.sum().backward()\n",
        "\n",
        "            try:\n",
        "                if torch.isnan(linear.weight.tensor.grad).any() or torch.isinf(linear.weight.tensor.grad).any():\n",
        "                    raise AssertionError(\"Should handle large inputs without NaN/Inf\")\n",
        "                print(\"✓ Module Large Input Handling\")\n",
        "                self.passed_tests += 1\n",
        "            except Exception as e:\n",
        "                print(f\"✗ Module Large Input Handling: {str(e)}\")\n",
        "                self.failed_tests += 1\n",
        "\n",
        "        # Test zero gradients don't break anything\n",
        "        with AutogradGraph() as graph:\n",
        "            relu = ReLu(graph=graph)\n",
        "            zero_input = CustomTensor([[-1.0, -2.0, -3.0]], _custom_requires_grad=True, graph=graph, is_leaf=True)\n",
        "            output = relu(zero_input)  # All outputs will be 0\n",
        "            output.sum().backward()    # All gradients will be 0\n",
        "\n",
        "            try:\n",
        "                if zero_input.tensor.grad is None:\n",
        "                    raise AssertionError(\"Should handle zero gradient case\")\n",
        "                if not torch.allclose(zero_input.tensor.grad, torch.zeros_like(zero_input.tensor.grad)):\n",
        "                    raise AssertionError(\"Gradients should be zero for negative ReLU inputs\")\n",
        "                print(\"✓ Module Zero Gradient Handling\")\n",
        "                self.passed_tests += 1\n",
        "            except Exception as e:\n",
        "                print(f\"✗ Module Zero Gradient Handling: {str(e)}\")\n",
        "                self.failed_tests += 1\n",
        "\n",
        "    def test_all_modules_comprehensive(self):\n",
        "      \"\"\"Comprehensive test running all module tests.\"\"\"\n",
        "      print(\"\\n=== Running All Module Tests ===\")\n",
        "\n",
        "      self.test_linear_module()\n",
        "      self.test_conv2d_module()\n",
        "      self.test_batchnorm_module()\n",
        "      self.test_maxpool2d_module()\n",
        "      self.test_avgpool2d_module()\n",
        "      self.test_relu_module()\n",
        "      self.test_leaky_relu_module()\n",
        "      self.test_gelu_module()\n",
        "      self.test_elu_module()\n",
        "      self.test_silu_module()\n",
        "      self.test_sigmoid_module()\n",
        "      self.test_tanh_module()\n",
        "      self.test_swish_module()\n",
        "      self.test_module_parameter_management()\n",
        "      self.test_module_training_eval_modes()\n",
        "      self.test_module_nested_structure()\n",
        "      self.test_module_edge_cases()\n",
        "\n",
        "\n",
        "    def run_all_tests(self):\n",
        "        \"\"\"Run all tests\"\"\"\n",
        "        print(\"Running Custom Autograd Correctness Tests\")\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "        self.test_basic_operations()\n",
        "        self.test_multiplication()\n",
        "        self.test_subtraction_division()\n",
        "        self.test_power_function()\n",
        "        self.test_unary_functions()\n",
        "        self.test_matrix_operations()\n",
        "        self.test_complex_chain()\n",
        "        self.test_mixed_operations()\n",
        "        self.test_broadcasting()\n",
        "        self.test_backward_with_custom_grad()\n",
        "        self.test_zero_grad_behavior()\n",
        "        self.test_no_grad_flow()\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 50)\n",
        "        print(\"Running Custom Autograd System Tests\")\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "        self.test_basic_add_scalar_grad_system()\n",
        "        self.test_basic_add_tensor_grad_system()\n",
        "        self.test_mixed_requires_grad_tensor_add_system()\n",
        "        self.test_no_requires_grad_system()\n",
        "        self.test_autograd_graph_context_manager_system()\n",
        "        self.test_cycle_detection_system()\n",
        "        self.test_no_circular_references_non_leaf_tensors_die_system()\n",
        "        self.test_topological_sort_order_system()\n",
        "        self.test_very_deep_computation_graph()\n",
        "        self.test_wide_computation_graph()\n",
        "        self.test_nan_and_inf_handling()\n",
        "        self.test_zero_gradients()\n",
        "        self.test_memory_efficiency()\n",
        "        print(\"\\n\" + \"=\" * 50)\n",
        "        print(\"Running All Module Tests\")\n",
        "        print(\"=\" * 50)\n",
        "        self.test_linear_module()\n",
        "        self.test_conv2d_module()\n",
        "        self.test_batchnorm_module()\n",
        "        self.test_maxpool2d_module()\n",
        "        self.test_avgpool2d_module()\n",
        "        self.test_relu_module()\n",
        "        self.test_leaky_relu_module()\n",
        "        self.test_gelu_module()\n",
        "        self.test_elu_module()\n",
        "        self.test_silu_module()\n",
        "        self.test_sigmoid_module()\n",
        "        self.test_tanh_module()\n",
        "        self.test_swish_module()\n",
        "        self.test_module_parameter_management()\n",
        "        self.test_module_training_eval_modes()\n",
        "        self.test_module_nested_structure()\n",
        "        self.test_module_edge_cases()\n",
        "\n",
        "\n",
        "\n",
        "        print(f\"\\n\" + \"=\" * 50)\n",
        "        print(f\"Test Results: {self.passed_tests} passed, {self.failed_tests} failed\")\n",
        "\n",
        "        if self.failed_tests == 0:\n",
        "            print(\"🎉 All tests passed! Your autograd implementation is correct.\")\n",
        "        else:\n",
        "            print(\"❌ Some tests failed. Check the implementation.\")\n",
        "\n",
        "        return self.failed_tests == 0\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "123emFEobM-O"
      },
      "source": [
        "# Running the tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UmuOWFoIbWFK",
        "outputId": "316ae0b8-d10f-49d2-f42e-ae0fbc4e5863"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running Custom Autograd Correctness Tests\n",
            "==================================================\n",
            "\n",
            "=== Testing Basic Operations ===\n",
            "✓ Scalar Addition - x\n",
            "✓ Scalar Addition - y (result)\n",
            "✓ Tensor Addition - x\n",
            "✓ Tensor Addition - y\n",
            "✓ Tensor Addition - z (result)\n",
            "\n",
            "=== Testing Multiplication ===\n",
            "✓ Scalar Multiplication - x\n",
            "✓ Scalar Multiplication - y (result)\n",
            "✓ Tensor Multiplication - x\n",
            "✓ Tensor Multiplication - y\n",
            "✓ Tensor Multiplication - z (result)\n",
            "\n",
            "=== Testing Subtraction and Division ===\n",
            "✓ Scalar Subtraction (x - C) - x\n",
            "✓ Scalar Subtraction (x - C) - y (result)\n",
            "✓ Scalar Reverse Subtraction (C - x) - x\n",
            "✓ Scalar Reverse Subtraction (C - x) - y (result)\n",
            "✓ Tensor Subtraction - x\n",
            "✓ Tensor Subtraction - y\n",
            "✓ Tensor Subtraction - z (result)\n",
            "✓ Scalar Division - x\n",
            "✓ Scalar Division - y (result)\n",
            "✓ Tensor Division - x\n",
            "✓ Tensir Division - y\n",
            "✓ Tensor Division - z (result)\n",
            "\n",
            "=== Testing Power Function ===\n",
            "✓ Power Function - x\n",
            "✓ Power Function - y (result)\n",
            "✓ Power Function (Negative Exponent) - x\n",
            "✓ Power Function (Negative Exponent) - y (result)\n",
            "\n",
            "=== Testing Unary Functions ===\n",
            "✓ Exponential Function - x\n",
            "✓ Exponential Function - y (result)\n",
            "✓ Logarithm Function - x\n",
            "✓ Logarithm Function - y (result)\n",
            "✓ Sine Function - x\n",
            "✓ Sine Function - y (result)\n",
            "✓ Cosine Function - x\n",
            "✓ Cosine Function - y (result)\n",
            "✓ Square Root Function - x\n",
            "✓ Square Root Function - y (result)\n",
            "\n",
            "=== Testing Matrix Operations ===\n",
            "✓ Matrix Multiplication (2x2 @ 2x2) - x\n",
            "✓ Matrix Multiplication (2x2 @ 2x2) - y\n",
            "✓ Matrix Multiplication (2x2 @ 2x2) - z (result)\n",
            "✓ Matrix Multiplication (2x3 @ 3x2) - x\n",
            "✓ Matrix Multiplication (2x3 @ 3x2) - y\n",
            "✓ Matrix Multiplication (2x3 @ 3x2) - z (result)\n",
            "✓ Dot Product (vector) - x\n",
            "✓ Dot Product (vector) - y\n",
            "✓ Dot Product (vector) - z (result)\n",
            "\n",
            "=== Testing Complex Chains ===\n",
            "✓ Complex Chain 1 - x\n",
            "✓ Complex Chain 1 - y\n",
            "✓ Complex Chain 1 - z (result)\n",
            "✓ Complex Chain 2 (Multiple Paths) - x\n",
            "✓ Complex Chain 2 (Multiple Paths) - y\n",
            "✓ Complex Chain 2 (Multiple Paths) - z (result)\n",
            "✓ Complex Chain 3 (Deeper Mixed Ops) - x\n",
            "✓ Complex Chain 3 (Deeper Mixed Ops) - y\n",
            "✓ Complex Chain 3 (Deeper Mixed Ops) - z (result)\n",
            "\n",
            "=== Testing Mixed Operations ===\n",
            "✓ Mixed Operations (X*Y, Y no grad) - x\n",
            "✓ Mixed Operations (X*Y, Y no grad) - y\n",
            "✓ Mixed Operations (X*Y, Y no grad) - z (result)\n",
            "✓ Mixed Operations (X+Y, Y no grad) - x\n",
            "✓ Mixed Operations (X+Y, Y no grad) - y\n",
            "✓ Mixed Operations (X+Y, Y no grad) - z (result)\n",
            "\n",
            "=== Testing Broadcasting ===\n",
            "✓ Broadcasting: Vector + Scalar - x\n",
            "✓ Broadcasting: Vector + Scalar - y (result)\n",
            "✓ Broadcasting: Matrix + Vector (row) - x\n",
            "✓ Broadcasting: Matrix + Vector (row) - y\n",
            "✓ Broadcasting: Matrix + Vector (row) - z (result)\n",
            "✓ Broadcasting: Matrix * Scalar - x\n",
            "✓ Broadcasting: Matrix * Scalar - y (result)\n",
            "\n",
            "=== Testing Backward with Custom Grad ===\n",
            "✓ Backward with Custom Grad - x\n",
            "✓ Backward with Custom Grad - y (result)\n",
            "\n",
            "=== Testing Zero Grad Behavior ===\n",
            "✓ Zero Grad Init (first backward) - x\n",
            "✓ Zero Grad Behavior - x (after 2nd backward)\n",
            "✓ Zero Grad Behavior - z (result, after 2nd backward)\n",
            "\n",
            "=== Testing No Grad Flow ===\n",
            "✓ No Grad Flow - x (requires grad)\n",
            "✓ No Grad Flow - y (no grad, custom correctly None)\n",
            "\n",
            "==================================================\n",
            "Running Custom Autograd System Tests\n",
            "==================================================\n",
            "\n",
            "=== System Test: Basic Scalar Add Grad ===\n",
            "✓ System Test: Basic Scalar Add Grad\n",
            "\n",
            "=== System Test: Basic Tensor Add Grad ===\n",
            "✓ System Test: Basic Tensor Add Grad\n",
            "\n",
            "=== System Test: Mixed Requires Grad Tensor Add ===\n",
            "✓ System Test: Mixed Requires Grad Tensor Add\n",
            "\n",
            "=== System Test: No Requires Grad ===\n",
            "✓ System Test: No Requires Grad\n",
            "\n",
            "=== System Test: Autograd Graph Context Manager ===\n",
            "✓ System Test: Autograd Graph Context Manager\n",
            "\n",
            "=== System Test: Cycle Detection ===\n",
            "✓ System Test: Cycle Detection\n",
            "\n",
            "--- Starting System Test: No Circular References (Part 1) ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-9-1090534377.py:31: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /pytorch/build/aten/src/ATen/core/TensorBody.h:489.)\n",
            "  if check_grad and pytorch_tensor.grad is not None:\n",
            "/tmp/ipython-input-9-1090534377.py:42: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /pytorch/build/aten/src/ATen/core/TensorBody.h:489.)\n",
            "  elif check_grad and pytorch_tensor.grad is None and custom_tensor.tensor.grad is not None:\n",
            "/tmp/ipython-input-9-1090534377.py:31: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /pytorch/build/aten/src/ATen/core/TensorBody.h:489.)\n",
            "  if check_grad and pytorch_tensor.grad is not None:\n",
            "/tmp/ipython-input-9-1090534377.py:42: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /pytorch/build/aten/src/ATen/core/TensorBody.h:489.)\n",
            "  elif check_grad and pytorch_tensor.grad is None and custom_tensor.tensor.grad is not None:\n",
            "/tmp/ipython-input-9-1090534377.py:31: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /pytorch/build/aten/src/ATen/core/TensorBody.h:489.)\n",
            "  if check_grad and pytorch_tensor.grad is not None:\n",
            "/tmp/ipython-input-9-1090534377.py:42: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /pytorch/build/aten/src/ATen/core/TensorBody.h:489.)\n",
            "  elif check_grad and pytorch_tensor.grad is None and custom_tensor.tensor.grad is not None:\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ System Test: No Circular References (Non-leaf tensors die)\n",
            "\n",
            "=== System Test: Topological Sort Order ===\n",
            "✓ System Test: Topological Sort Order\n",
            "\n",
            "=== Testing Very Deep Computation Graph ===\n",
            "✓ Deep Graph (depth=50) - x\n",
            "✓ Deep Graph (depth=50) - final\n",
            "\n",
            "=== Testing Wide Computation Graph ===\n",
            "✓ Wide Graph (width=20) - input_0\n",
            "✓ Wide Graph (width=20) - input_1\n",
            "✓ Wide Graph (width=20) - input_2\n",
            "✓ Wide Graph (width=20) - input_3\n",
            "✓ Wide Graph (width=20) - input_4\n",
            "✓ Wide Graph (width=20) - input_5\n",
            "✓ Wide Graph (width=20) - input_6\n",
            "✓ Wide Graph (width=20) - input_7\n",
            "✓ Wide Graph (width=20) - input_8\n",
            "✓ Wide Graph (width=20) - input_9\n",
            "✓ Wide Graph (width=20) - input_10\n",
            "✓ Wide Graph (width=20) - input_11\n",
            "✓ Wide Graph (width=20) - input_12\n",
            "✓ Wide Graph (width=20) - input_13\n",
            "✓ Wide Graph (width=20) - input_14\n",
            "✓ Wide Graph (width=20) - input_15\n",
            "✓ Wide Graph (width=20) - input_16\n",
            "✓ Wide Graph (width=20) - input_17\n",
            "✓ Wide Graph (width=20) - input_18\n",
            "✓ Wide Graph (width=20) - input_19\n",
            "\n",
            "=== Testing NaN and Inf Handling ===\n",
            "ℹ NaN/Inf Handling - Consider adding explicit handling for edge numerical cases\n",
            "\n",
            "=== Testing Zero Gradients ===\n",
            "✓ Zero Gradients - x\n",
            "\n",
            "=== Testing Memory Efficiency ===\n",
            "Object count growth: 16\n",
            "✓ Memory Efficiency - Reasonable memory usage\n",
            "\n",
            "==================================================\n",
            "Running All Module Tests\n",
            "==================================================\n",
            "\n",
            "=== Testing Linear Module ===\n",
            "✓ Linear Forward Pass\n",
            "✓ Linear Input Gradient\n",
            "✓ Linear Weight Gradient\n",
            "✓ Linear Bias Gradient\n",
            "✓ Linear No Bias Forward\n",
            "✓ Linear No Bias Weight Gradient\n",
            "✓ Linear Eval Mode - No Grad\n",
            "\n",
            "=== Testing Conv2d Module ===\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n",
            "/tmp/ipython-input-9-1090534377.py:31: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /pytorch/build/aten/src/ATen/core/TensorBody.h:489.)\n",
            "  if check_grad and pytorch_tensor.grad is not None:\n",
            "/tmp/ipython-input-9-1090534377.py:42: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /pytorch/build/aten/src/ATen/core/TensorBody.h:489.)\n",
            "  elif check_grad and pytorch_tensor.grad is None and custom_tensor.tensor.grad is not None:\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Conv2d Forward Pass\n",
            "✓ Conv2d Input Gradient\n",
            "✓ Conv2d Weight Gradient\n",
            "✓ Conv2d Bias Gradient\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-9-1090534377.py:31: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /pytorch/build/aten/src/ATen/core/TensorBody.h:489.)\n",
            "  if check_grad and pytorch_tensor.grad is not None:\n",
            "/tmp/ipython-input-9-1090534377.py:42: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /pytorch/build/aten/src/ATen/core/TensorBody.h:489.)\n",
            "  elif check_grad and pytorch_tensor.grad is None and custom_tensor.tensor.grad is not None:\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Conv2d Different Params Forward\n",
            "✓ Conv2d Different Params Weight Gradient\n",
            "\n",
            "=== Testing BatchNorm Module ===\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-9-1090534377.py:31: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /pytorch/build/aten/src/ATen/core/TensorBody.h:489.)\n",
            "  if check_grad and pytorch_tensor.grad is not None:\n",
            "/tmp/ipython-input-9-1090534377.py:42: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /pytorch/build/aten/src/ATen/core/TensorBody.h:489.)\n",
            "  elif check_grad and pytorch_tensor.grad is None and custom_tensor.tensor.grad is not None:\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ BatchNorm Training Forward\n",
            "✓ BatchNorm Input Gradient\n",
            "✓ BatchNorm Weight Gradient\n",
            "✓ BatchNorm Bias Gradient\n",
            "✓ BatchNorm Eval Forward\n",
            "\n",
            "=== Testing MaxPool2d Module ===\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-9-1090534377.py:31: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /pytorch/build/aten/src/ATen/core/TensorBody.h:489.)\n",
            "  if check_grad and pytorch_tensor.grad is not None:\n",
            "/tmp/ipython-input-9-1090534377.py:42: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /pytorch/build/aten/src/ATen/core/TensorBody.h:489.)\n",
            "  elif check_grad and pytorch_tensor.grad is None and custom_tensor.tensor.grad is not None:\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ MaxPool2d Forward\n",
            "✓ MaxPool2d Input Gradient\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-9-1090534377.py:31: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /pytorch/build/aten/src/ATen/core/TensorBody.h:489.)\n",
            "  if check_grad and pytorch_tensor.grad is not None:\n",
            "/tmp/ipython-input-9-1090534377.py:42: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /pytorch/build/aten/src/ATen/core/TensorBody.h:489.)\n",
            "  elif check_grad and pytorch_tensor.grad is None and custom_tensor.tensor.grad is not None:\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ MaxPool2d Different Params Forward\n",
            "✓ MaxPool2d Different Params Gradient\n",
            "\n",
            "=== Testing AvgPool2d Module ===\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-9-1090534377.py:31: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /pytorch/build/aten/src/ATen/core/TensorBody.h:489.)\n",
            "  if check_grad and pytorch_tensor.grad is not None:\n",
            "/tmp/ipython-input-9-1090534377.py:42: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /pytorch/build/aten/src/ATen/core/TensorBody.h:489.)\n",
            "  elif check_grad and pytorch_tensor.grad is None and custom_tensor.tensor.grad is not None:\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ AvgPool2d Forward\n",
            "✓ AvgPool2d Input Gradient\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-9-1090534377.py:31: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /pytorch/build/aten/src/ATen/core/TensorBody.h:489.)\n",
            "  if check_grad and pytorch_tensor.grad is not None:\n",
            "/tmp/ipython-input-9-1090534377.py:42: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /pytorch/build/aten/src/ATen/core/TensorBody.h:489.)\n",
            "  elif check_grad and pytorch_tensor.grad is None and custom_tensor.tensor.grad is not None:\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ AvgPool2d With Padding Forward\n",
            "✓ AvgPool2d With Padding Gradient\n",
            "\n",
            "=== Testing ReLU Module ===\n",
            "✓ ReLU Forward\n",
            "✓ ReLU Input Gradient\n",
            "✓ ReLU Negative Values Forward\n",
            "✓ ReLU Negative Values Gradient\n",
            "\n",
            "=== Testing Leaky ReLU Module ===\n",
            "✓ Leaky ReLU Forward\n",
            "✓ Leaky ReLU Input Gradient\n",
            "✓ Leaky ReLU Different Slope Forward\n",
            "✓ Leaky ReLU Different Slope Gradient\n",
            "\n",
            "=== Testing GELU Module ===\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-9-1090534377.py:31: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /pytorch/build/aten/src/ATen/core/TensorBody.h:489.)\n",
            "  if check_grad and pytorch_tensor.grad is not None:\n",
            "/tmp/ipython-input-9-1090534377.py:42: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /pytorch/build/aten/src/ATen/core/TensorBody.h:489.)\n",
            "  elif check_grad and pytorch_tensor.grad is None and custom_tensor.tensor.grad is not None:\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ GELU Exact Forward\n",
            "✓ GELU Exact Input Gradient\n",
            "✓ GELU Approximate Forward\n",
            "✓ GELU Approximate Input Gradient\n",
            "\n",
            "=== Testing ELU Module ===\n",
            "✓ ELU Forward\n",
            "✓ ELU Input Gradient\n",
            "✓ ELU Different Alpha Forward\n",
            "✓ ELU Different Alpha Gradient\n",
            "\n",
            "=== Testing SiLU Module ===\n",
            "✓ SiLU Forward\n",
            "✓ SiLU Input Gradient\n",
            "\n",
            "=== Testing Sigmoid Module ===\n",
            "✓ Sigmoid Forward\n",
            "✓ Sigmoid Input Gradient\n",
            "\n",
            "=== Testing Tanh Module ===\n",
            "✓ Tanh Forward\n",
            "✓ Tanh Input Gradient\n",
            "\n",
            "=== Testing Swish Module ===\n",
            "✓ Swish Forward\n",
            "✓ Swish Input Gradient\n",
            "✓ Swish B Parameter Gradient\n",
            "✓ Swish Different B Forward\n",
            "✓ Swish Different B Parameter Gradient\n",
            "\n",
            "=== Testing Module Parameter Management ===\n",
            "✓ Module Parameter Collection\n",
            "✓ Module All Parameters Have Gradients\n",
            "✓ Module Zero Grad\n",
            "\n",
            "=== Testing Module Training/Eval Modes ===\n",
            "✓ Module Initial Training Mode\n",
            "✓ Module Eval Mode Switch\n",
            "✓ Module Training Mode Switch\n",
            "\n",
            "=== Testing Nested Module Structure ===\n",
            "✓ Nested Module Parameter Collection\n",
            "✓ Nested Module Training Mode\n",
            "✓ Nested Module Eval Mode\n",
            "✓ Nested Module Gradient Flow\n",
            "\n",
            "=== Testing Module Edge Cases ===\n",
            "✓ Module Tiny Input Handling\n",
            "✓ Module Large Input Handling\n",
            "✓ Module Zero Gradient Handling\n",
            "\n",
            "==================================================\n",
            "Test Results: 173 passed, 0 failed\n",
            "🎉 All tests passed! Your autograd implementation is correct.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-9-1090534377.py:31: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /pytorch/build/aten/src/ATen/core/TensorBody.h:489.)\n",
            "  if check_grad and pytorch_tensor.grad is not None:\n",
            "/tmp/ipython-input-9-1090534377.py:42: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /pytorch/build/aten/src/ATen/core/TensorBody.h:489.)\n",
            "  elif check_grad and pytorch_tensor.grad is None and custom_tensor.tensor.grad is not None:\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "t=AutogradTester()\n",
        "t.run_all_tests()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "eS6JNVGwbPSN"
      },
      "outputs": [],
      "source": [
        "# the universal object\n",
        "\n",
        "tester = AutogradTester()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QhS5tX_xboKx",
        "outputId": "1836f4e9-2ffc-45ab-dc98-af3caceb392a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Testing Linear Module ===\n",
            "✓ Linear Forward Pass\n",
            "✓ Linear Input Gradient\n",
            "✓ Linear Weight Gradient\n",
            "✓ Linear Bias Gradient\n",
            "✓ Linear No Bias Forward\n",
            "✓ Linear No Bias Weight Gradient\n",
            "✓ Linear Eval Mode - No Grad\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-30-3465963366.py:31: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /pytorch/build/aten/src/ATen/core/TensorBody.h:489.)\n",
            "  if check_grad and pytorch_tensor.grad is not None:\n",
            "/tmp/ipython-input-30-3465963366.py:42: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /pytorch/build/aten/src/ATen/core/TensorBody.h:489.)\n",
            "  elif check_grad and pytorch_tensor.grad is None and custom_tensor.tensor.grad is not None:\n"
          ]
        }
      ],
      "source": [
        "tester.test_linear_module()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2joRY3lsbq1Z",
        "outputId": "580344b3-5ed7-4797-8449-9224ae6ad12c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Testing Conv2d Module ===\n",
            "✓ Conv2d Forward Pass\n",
            "✓ Conv2d Input Gradient\n",
            "✓ Conv2d Weight Gradient\n",
            "✓ Conv2d Bias Gradient\n",
            "✓ Conv2d Different Params Forward\n",
            "✓ Conv2d Different Params Weight Gradient\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-52-1090534377.py:31: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /pytorch/build/aten/src/ATen/core/TensorBody.h:489.)\n",
            "  if check_grad and pytorch_tensor.grad is not None:\n",
            "/tmp/ipython-input-52-1090534377.py:42: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /pytorch/build/aten/src/ATen/core/TensorBody.h:489.)\n",
            "  elif check_grad and pytorch_tensor.grad is None and custom_tensor.tensor.grad is not None:\n"
          ]
        }
      ],
      "source": [
        "tester.test_conv2d_module()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YL-shRU0bq5b",
        "outputId": "5a6a4c86-146d-4d5b-d98a-89de083471d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Testing BatchNorm Module ===\n",
            "✓ BatchNorm Training Forward\n",
            "✓ BatchNorm Input Gradient\n",
            "✗ BatchNorm Weight Gradient: \n",
            "Not equal to tolerance rtol=1e-07, atol=1e-07\n",
            "Mismatch in gradients for BatchNorm Weight Gradient\n",
            "Mismatched elements: 1 / 3 (33.3%)\n",
            "Max absolute difference among violations: 1.5312494e-07\n",
            "Max relative difference among violations: 1.7952683\n",
            " ACTUAL: array([-4.768372e-07, -2.384186e-07, -1.788139e-07], dtype=float32)\n",
            " DESIRED: array([-5.179609e-07, -8.529363e-08, -1.030462e-07], dtype=float32)\n",
            "✓ BatchNorm Bias Gradient\n",
            "✓ BatchNorm Eval Forward\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-30-3465963366.py:31: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /pytorch/build/aten/src/ATen/core/TensorBody.h:489.)\n",
            "  if check_grad and pytorch_tensor.grad is not None:\n",
            "/tmp/ipython-input-30-3465963366.py:42: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /pytorch/build/aten/src/ATen/core/TensorBody.h:489.)\n",
            "  elif check_grad and pytorch_tensor.grad is None and custom_tensor.tensor.grad is not None:\n"
          ]
        }
      ],
      "source": [
        "tester.test_batchnorm_module()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aYiSenfRbq7-",
        "outputId": "76eb5bbb-fdb5-4b0d-c339-f28acf6037ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Testing MaxPool2d Module ===\n",
            "✓ MaxPool2d Forward\n",
            "✓ MaxPool2d Input Gradient\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-30-3465963366.py:31: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /pytorch/build/aten/src/ATen/core/TensorBody.h:489.)\n",
            "  if check_grad and pytorch_tensor.grad is not None:\n",
            "/tmp/ipython-input-30-3465963366.py:42: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /pytorch/build/aten/src/ATen/core/TensorBody.h:489.)\n",
            "  elif check_grad and pytorch_tensor.grad is None and custom_tensor.tensor.grad is not None:\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ MaxPool2d Different Params Forward\n",
            "✓ MaxPool2d Different Params Gradient\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-30-3465963366.py:31: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /pytorch/build/aten/src/ATen/core/TensorBody.h:489.)\n",
            "  if check_grad and pytorch_tensor.grad is not None:\n",
            "/tmp/ipython-input-30-3465963366.py:42: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /pytorch/build/aten/src/ATen/core/TensorBody.h:489.)\n",
            "  elif check_grad and pytorch_tensor.grad is None and custom_tensor.tensor.grad is not None:\n"
          ]
        }
      ],
      "source": [
        "tester.test_maxpool2d_module()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "boxaZIJZbq-Z",
        "outputId": "8e7545ae-9e2f-4884-96b9-ab77c5258f53"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Testing AvgPool2d Module ===\n",
            "✓ AvgPool2d Forward\n",
            "✓ AvgPool2d Input Gradient\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-30-3465963366.py:31: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /pytorch/build/aten/src/ATen/core/TensorBody.h:489.)\n",
            "  if check_grad and pytorch_tensor.grad is not None:\n",
            "/tmp/ipython-input-30-3465963366.py:42: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /pytorch/build/aten/src/ATen/core/TensorBody.h:489.)\n",
            "  elif check_grad and pytorch_tensor.grad is None and custom_tensor.tensor.grad is not None:\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ AvgPool2d With Padding Forward\n",
            "✓ AvgPool2d With Padding Gradient\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-30-3465963366.py:31: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /pytorch/build/aten/src/ATen/core/TensorBody.h:489.)\n",
            "  if check_grad and pytorch_tensor.grad is not None:\n",
            "/tmp/ipython-input-30-3465963366.py:42: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /pytorch/build/aten/src/ATen/core/TensorBody.h:489.)\n",
            "  elif check_grad and pytorch_tensor.grad is None and custom_tensor.tensor.grad is not None:\n"
          ]
        }
      ],
      "source": [
        "tester.test_avgpool2d_module()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Hn68-XEyMQy",
        "outputId": "d7f61bf3-967c-40a3-812a-121dae8b34c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Testing ReLU Module ===\n",
            "✓ ReLU Forward\n",
            "✓ ReLU Input Gradient\n",
            "✓ ReLU Negative Values Forward\n",
            "✓ ReLU Negative Values Gradient\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-30-3465963366.py:31: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /pytorch/build/aten/src/ATen/core/TensorBody.h:489.)\n",
            "  if check_grad and pytorch_tensor.grad is not None:\n",
            "/tmp/ipython-input-30-3465963366.py:42: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /pytorch/build/aten/src/ATen/core/TensorBody.h:489.)\n",
            "  elif check_grad and pytorch_tensor.grad is None and custom_tensor.tensor.grad is not None:\n"
          ]
        }
      ],
      "source": [
        "tester.test_relu_module()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vo0X5K6VyMVL",
        "outputId": "4d9fb028-926c-4b84-d09a-96e85d4a03e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Testing Leaky ReLU Module ===\n",
            "✓ Leaky ReLU Forward\n",
            "✓ Leaky ReLU Input Gradient\n",
            "✓ Leaky ReLU Different Slope Forward\n",
            "✓ Leaky ReLU Different Slope Gradient\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-30-3465963366.py:31: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /pytorch/build/aten/src/ATen/core/TensorBody.h:489.)\n",
            "  if check_grad and pytorch_tensor.grad is not None:\n",
            "/tmp/ipython-input-30-3465963366.py:42: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /pytorch/build/aten/src/ATen/core/TensorBody.h:489.)\n",
            "  elif check_grad and pytorch_tensor.grad is None and custom_tensor.tensor.grad is not None:\n"
          ]
        }
      ],
      "source": [
        "tester.test_leaky_relu_module()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0-bfhKrwyMgL",
        "outputId": "88b1ea86-a609-474d-ce0a-eefe51538082"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Testing GELU Module ===\n",
            "✓ GELU Exact Forward\n",
            "✓ GELU Exact Input Gradient\n",
            "✓ GELU Approximate Forward\n",
            "✓ GELU Approximate Input Gradient\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-30-3465963366.py:31: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /pytorch/build/aten/src/ATen/core/TensorBody.h:489.)\n",
            "  if check_grad and pytorch_tensor.grad is not None:\n",
            "/tmp/ipython-input-30-3465963366.py:42: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /pytorch/build/aten/src/ATen/core/TensorBody.h:489.)\n",
            "  elif check_grad and pytorch_tensor.grad is None and custom_tensor.tensor.grad is not None:\n",
            "/tmp/ipython-input-30-3465963366.py:31: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /pytorch/build/aten/src/ATen/core/TensorBody.h:489.)\n",
            "  if check_grad and pytorch_tensor.grad is not None:\n",
            "/tmp/ipython-input-30-3465963366.py:42: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /pytorch/build/aten/src/ATen/core/TensorBody.h:489.)\n",
            "  elif check_grad and pytorch_tensor.grad is None and custom_tensor.tensor.grad is not None:\n"
          ]
        }
      ],
      "source": [
        "tester.test_gelu_module()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8jg8I9UB1W47",
        "outputId": "f8a2602b-e455-4ff3-8ae0-0ede0164f127"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Testing ELU Module ===\n",
            "✓ ELU Forward\n",
            "✓ ELU Input Gradient\n",
            "✓ ELU Different Alpha Forward\n",
            "✓ ELU Different Alpha Gradient\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-30-3465963366.py:31: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /pytorch/build/aten/src/ATen/core/TensorBody.h:489.)\n",
            "  if check_grad and pytorch_tensor.grad is not None:\n",
            "/tmp/ipython-input-30-3465963366.py:42: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /pytorch/build/aten/src/ATen/core/TensorBody.h:489.)\n",
            "  elif check_grad and pytorch_tensor.grad is None and custom_tensor.tensor.grad is not None:\n"
          ]
        }
      ],
      "source": [
        "tester.test_elu_module()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UoEH7Yfc1W73",
        "outputId": "b86df37d-300b-4121-ea03-10a11ca6c860"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Testing SiLU Module ===\n",
            "✓ SiLU Forward\n",
            "✓ SiLU Input Gradient\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-30-3465963366.py:31: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /pytorch/build/aten/src/ATen/core/TensorBody.h:489.)\n",
            "  if check_grad and pytorch_tensor.grad is not None:\n",
            "/tmp/ipython-input-30-3465963366.py:42: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /pytorch/build/aten/src/ATen/core/TensorBody.h:489.)\n",
            "  elif check_grad and pytorch_tensor.grad is None and custom_tensor.tensor.grad is not None:\n"
          ]
        }
      ],
      "source": [
        "tester.test_silu_module()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "avY3m4Sw1W-G",
        "outputId": "0edf616f-15ed-497f-b769-abbc04e20697"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Testing Sigmoid Module ===\n",
            "✓ Sigmoid Forward\n",
            "✓ Sigmoid Input Gradient\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-5-57483771.py:31: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /pytorch/build/aten/src/ATen/core/TensorBody.h:489.)\n",
            "  if check_grad and pytorch_tensor.grad is not None:\n",
            "/tmp/ipython-input-5-57483771.py:42: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /pytorch/build/aten/src/ATen/core/TensorBody.h:489.)\n",
            "  elif check_grad and pytorch_tensor.grad is None and custom_tensor.tensor.grad is not None:\n"
          ]
        }
      ],
      "source": [
        "tester.test_sigmoid_module()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M0T05j3k1XAm",
        "outputId": "b8c509c5-a4f1-4612-c5cd-736f583daa5f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Testing Tanh Module ===\n",
            "✓ Tanh Forward\n",
            "✓ Tanh Input Gradient\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-30-3465963366.py:31: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /pytorch/build/aten/src/ATen/core/TensorBody.h:489.)\n",
            "  if check_grad and pytorch_tensor.grad is not None:\n",
            "/tmp/ipython-input-30-3465963366.py:42: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /pytorch/build/aten/src/ATen/core/TensorBody.h:489.)\n",
            "  elif check_grad and pytorch_tensor.grad is None and custom_tensor.tensor.grad is not None:\n"
          ]
        }
      ],
      "source": [
        "tester.test_tanh_module()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mso7jLH_104o",
        "outputId": "f8ca950c-166e-4650-cf44-2856d9182a76"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Testing Swish Module ===\n",
            "✓ Swish Forward\n",
            "✓ Swish Input Gradient\n",
            "✓ Swish B Parameter Gradient\n",
            "✓ Swish Different B Forward\n",
            "✓ Swish Different B Parameter Gradient\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-30-3465963366.py:31: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /pytorch/build/aten/src/ATen/core/TensorBody.h:489.)\n",
            "  if check_grad and pytorch_tensor.grad is not None:\n",
            "/tmp/ipython-input-30-3465963366.py:42: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /pytorch/build/aten/src/ATen/core/TensorBody.h:489.)\n",
            "  elif check_grad and pytorch_tensor.grad is None and custom_tensor.tensor.grad is not None:\n"
          ]
        }
      ],
      "source": [
        "tester.test_swish_module()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SvDmct-2107J",
        "outputId": "8f437828-a4d0-4d59-d555-a56770ba60e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Testing Module Parameter Management ===\n",
            "✓ Module Parameter Collection\n",
            "✓ Module All Parameters Have Gradients\n",
            "✓ Module Zero Grad\n"
          ]
        }
      ],
      "source": [
        "tester.test_module_parameter_management()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LGMdcOGb1091",
        "outputId": "5af0ee72-fdda-4c59-9d9a-2eda3c403590"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Testing Module Training/Eval Modes ===\n",
            "✓ Module Initial Training Mode\n",
            "✓ Module Eval Mode Switch\n",
            "✓ Module Training Mode Switch\n"
          ]
        }
      ],
      "source": [
        "tester.test_module_training_eval_modes()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xu5GQqnx2DNA",
        "outputId": "02e97ba3-a4d7-4abc-e7e0-967414ea89d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Testing Nested Module Structure ===\n",
            "✓ Nested Module Parameter Collection\n",
            "✓ Nested Module Training Mode\n",
            "✓ Nested Module Eval Mode\n",
            "✓ Nested Module Gradient Flow\n"
          ]
        }
      ],
      "source": [
        "tester.test_module_nested_structure()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9AJPG4xH2DP6",
        "outputId": "0f1a459f-d195-4ab6-d7b9-4de7f51510fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Testing Module Edge Cases ===\n",
            "✓ Module Tiny Input Handling\n",
            "✓ Module Large Input Handling\n",
            "✓ Module Zero Gradient Handling\n"
          ]
        }
      ],
      "source": [
        "tester.test_module_edge_cases()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7cYv6BqB2fWD"
      },
      "outputs": [],
      "source": [
        "#FAILED FOR nested structure, relu , elu, leaky relu, both max and avg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CSe_S-o-20VE",
        "outputId": "61c2d169-aca5-450c-ea4c-415e3ee18028"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Running All Module Tests ===\n",
            "\n",
            "=== Testing Linear Module ===\n",
            "✓ Linear Forward Pass\n",
            "✓ Linear Input Gradient\n",
            "✓ Linear Weight Gradient\n",
            "✓ Linear Bias Gradient\n",
            "✓ Linear No Bias Forward\n",
            "✓ Linear No Bias Weight Gradient\n",
            "✓ Linear Eval Mode - No Grad\n",
            "\n",
            "=== Testing Conv2d Module ===\n",
            "✓ Conv2d Forward Pass\n",
            "✓ Conv2d Input Gradient\n",
            "✓ Conv2d Weight Gradient\n",
            "✓ Conv2d Bias Gradient\n",
            "✓ Conv2d Different Params Forward\n",
            "✓ Conv2d Different Params Weight Gradient\n",
            "\n",
            "=== Testing BatchNorm Module ===\n",
            "✓ BatchNorm Training Forward\n",
            "✓ BatchNorm Input Gradient\n",
            "✗ BatchNorm Weight Gradient: \n",
            "Not equal to tolerance rtol=1e-07, atol=1e-07\n",
            "Mismatch in gradients for BatchNorm Weight Gradient\n",
            "Mismatched elements: 2 / 3 (66.7%)\n",
            "Max absolute difference among violations: 5.911851e-07\n",
            "Max relative difference among violations: 3.2207937\n",
            " ACTUAL: array([ 4.768372e-07,  2.384186e-07, -2.384186e-07], dtype=float32)\n",
            " DESIRED: array([ 4.189490e-07, -3.527665e-07, -5.648667e-08], dtype=float32)\n",
            "✓ BatchNorm Bias Gradient\n",
            "✓ BatchNorm Eval Forward\n",
            "\n",
            "=== Testing MaxPool2d Module ===\n",
            "✓ MaxPool2d Forward\n",
            "✓ MaxPool2d Input Gradient\n",
            "✓ MaxPool2d Different Params Forward\n",
            "✓ MaxPool2d Different Params Gradient\n",
            "\n",
            "=== Testing AvgPool2d Module ===\n",
            "✓ AvgPool2d Forward\n",
            "✓ AvgPool2d Input Gradient\n",
            "✓ AvgPool2d With Padding Forward\n",
            "✓ AvgPool2d With Padding Gradient\n",
            "\n",
            "=== Testing Global Pooling Modules ===\n",
            "✗ GlobalAvgPool2d Forward: \n",
            "Not equal to tolerance rtol=1e-07, atol=1e-07\n",
            "Mismatch in tensor values for GlobalAvgPool2d Forward\n",
            "(shapes (2, 3, 1, 1), (2, 3) mismatch)\n",
            " ACTUAL: array([[[[-0.020636]],\n",
            "\n",
            "        [[ 0.257164]],...\n",
            " DESIRED: array([[-0.020636,  0.257164, -0.0687  ],\n",
            "       [ 0.057134,  0.071578,  0.047932]], dtype=float32)\n",
            "✓ GlobalAvgPool2d Input Gradient\n",
            "✗ GlobalMaxPool2d Forward: \n",
            "Not equal to tolerance rtol=1e-07, atol=1e-07\n",
            "Mismatch in tensor values for GlobalMaxPool2d Forward\n",
            "(shapes (2, 3, 1, 1), (2, 3) mismatch)\n",
            " ACTUAL: array([[[[2.951685]],\n",
            "\n",
            "        [[1.389349]],...\n",
            " DESIRED: array([[2.951685, 1.389349, 2.553978],\n",
            "       [1.299225, 2.926078, 1.200527]], dtype=float32)\n",
            "✗ GlobalMaxPool2d Input Gradient: \n",
            "Not equal to tolerance rtol=1e-07, atol=1e-07\n",
            "Mismatch in gradients for GlobalMaxPool2d Input Gradient\n",
            "Mismatched elements: 10 / 120 (8.33%)\n",
            "Max absolute difference among violations: 1.\n",
            "Max relative difference among violations: 1.\n",
            " ACTUAL: array([[[[0., 1., 0., 0., 1.],\n",
            "         [0., 0., 1., 0., 0.],\n",
            "         [1., 0., 0., 0., 0.],...\n",
            " DESIRED: array([[[[0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0.],...\n",
            "\n",
            "=== Testing ReLU Module ===\n",
            "✓ ReLU Forward\n",
            "✓ ReLU Input Gradient\n",
            "✓ ReLU Negative Values Forward\n",
            "✓ ReLU Negative Values Gradient\n",
            "\n",
            "=== Testing Leaky ReLU Module ===\n",
            "✓ Leaky ReLU Forward\n",
            "✓ Leaky ReLU Input Gradient\n",
            "✓ Leaky ReLU Different Slope Forward\n",
            "✓ Leaky ReLU Different Slope Gradient\n",
            "\n",
            "=== Testing GELU Module ===\n",
            "✓ GELU Exact Forward\n",
            "✓ GELU Exact Input Gradient\n",
            "✓ GELU Approximate Forward\n",
            "✗ GELU Approximate Input Gradient: \n",
            "Not equal to tolerance rtol=1e-07, atol=1e-07\n",
            "Mismatch in gradients for GELU Approximate Input Gradient\n",
            "Mismatched elements: 1 / 6 (16.7%)\n",
            "Max absolute difference among violations: 1.2665987e-07\n",
            "Max relative difference among violations: 1.4557369e-06\n",
            " ACTUAL: array([[ 1.128574,  0.01323 , -0.087008],\n",
            "       [ 0.666816,  1.035099,  1.04571 ]], dtype=float32)\n",
            " DESIRED: array([[ 1.128574,  0.01323 , -0.087007],\n",
            "       [ 0.666816,  1.035099,  1.04571 ]], dtype=float32)\n",
            "\n",
            "=== Testing ELU Module ===\n",
            "✓ ELU Forward\n",
            "✓ ELU Input Gradient\n",
            "✓ ELU Different Alpha Forward\n",
            "✓ ELU Different Alpha Gradient\n",
            "\n",
            "=== Testing SiLU Module ===\n",
            "✓ SiLU Forward\n",
            "✓ SiLU Input Gradient\n",
            "\n",
            "=== Testing Sigmoid Module ===\n",
            "✓ Sigmoid Forward\n",
            "✓ Sigmoid Input Gradient\n",
            "\n",
            "=== Testing Tanh Module ===\n",
            "✓ Tanh Forward\n",
            "✓ Tanh Input Gradient\n",
            "\n",
            "=== Testing Swish Module ===\n",
            "✓ Swish Forward\n",
            "✓ Swish Input Gradient\n",
            "✓ Swish B Parameter Gradient\n",
            "✓ Swish Different B Forward\n",
            "✓ Swish Different B Parameter Gradient\n",
            "\n",
            "=== Testing Module Parameter Management ===\n",
            "✓ Module Parameter Collection\n",
            "✓ Module All Parameters Have Gradients\n",
            "✓ Module Zero Grad\n",
            "\n",
            "=== Testing Module Training/Eval Modes ===\n",
            "✓ Module Initial Training Mode\n",
            "✓ Module Eval Mode Switch\n",
            "✓ Module Training Mode Switch\n",
            "\n",
            "=== Testing Nested Module Structure ===\n",
            "✓ Nested Module Parameter Collection\n",
            "✓ Nested Module Training Mode\n",
            "✓ Nested Module Eval Mode\n",
            "✓ Nested Module Gradient Flow\n",
            "\n",
            "=== Testing Module Edge Cases ===\n",
            "✓ Module Tiny Input Handling\n",
            "✓ Module Large Input Handling\n",
            "✓ Module Zero Gradient Handling\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-32-1195857159.py:31: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /pytorch/build/aten/src/ATen/core/TensorBody.h:489.)\n",
            "  if check_grad and pytorch_tensor.grad is not None:\n",
            "/tmp/ipython-input-32-1195857159.py:42: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /pytorch/build/aten/src/ATen/core/TensorBody.h:489.)\n",
            "  elif check_grad and pytorch_tensor.grad is None and custom_tensor.tensor.grad is not None:\n"
          ]
        }
      ],
      "source": [
        "tester.test_all_modules_comprehensive()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YpSs3w6Kst_b"
      },
      "outputs": [],
      "source": [
        "C:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Tools\\MSVC\\<version>\\bin\\Hostx64\\x64\\cl.exe\n",
        "C:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Tools\\MSVC\\14.44.35207\\bin\\Hostx64\\x64\\cl.exe"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "learn",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
