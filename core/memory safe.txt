# Scalable, Memory-Safe, and Clean Custom Autograd Design

## 1. Goals

* **Scalability**: Support large computation graphs without incurring high memory overhead.
* **Memory Safety**: Ensure intermediate tensors are freed when no longer needed.
* **Clean Separation of Concerns**: Clearly separate graph construction, reference management, and tensor operations.
* **Extensibility**: Easy to plug in new operations and backends (e.g., GPU).

---

## 2. High-Level Architecture

```text
┌──────────────────┐      ┌───────────────────┐      ┌─────────────────┐
│  CustomTensor    │◀───┐ │  AutogradGraph    │◀──┐  │ GraphExecutor   │
│  (Data + Hooks)  │    │ │  (Node/Edge store)│    │  │ (Toposort +     │
└──────────────────┘    │ └───────────────────┘    │  │  Backward Pass) │
                        │                          │  └─────────────────┘
                        │                          │
     ▲                  │                          │
     │ adds itself      │                          │ performs
     │ to graph via     │                          │ backward
     │ callbacks        │                          │ on CustomTensor
     │                  │                          │
     │     ┌────────────────────────────────────────────────────────┐
     └─────│ ReferenceManager (WeakValueDictionary + strong refs) │
           └────────────────────────────────────────────────────────┘
```

### Components

1. **CustomTensor**: Subclass of `torch.Tensor` responsible for:

   * Registering creation and operation history.
   * Holding a weakref callback to notify the `AutogradGraph`.
   * Providing a user-friendly API for operations.

2. **AutogradGraph**:

   * Represents the graph structure using `rustworkx.PyDiGraph`.
   * Stores node IDs and edges when operations occur.
   * Manages lifecycle via context manager.
   * Provides cycle-checking and reverse topological sorting.

3. **ReferenceManager**:

   * Uses `weakref.WeakValueDictionary` for intermediate tensors.
   * Holds strong references only for leaf tensors to prevent premature collection.
   * Automatically drops tensors when they go out of scope.

4. **GraphExecutor**:

   * Runs backward pass: retrieves sorted nodes, invokes individual `.backward()` hooks.

---

## 3. Key Design Patterns

### 3.1 Dependency Injection

* Pass `AutogradGraph` and `ReferenceManager` explicitly to `CustomTensor`.
* Remove reliance on global variables.

### 3.2 Weak References & Cleanup

* Store tensor objects in the graph via `weakref.ref`.
* Use `WeakValueDictionary` to track intermediate tensors: automatically drop them when not used.

### 3.3 Functional Hooks

* Override `torch.Tensor` operations via `__torch_function__` hook:

  ```python
  def __torch_function__(cls, func, types, args=(), kwargs=None):
      # Wrap ops to record graph edges
      result = super().__torch_function__(func, types, args, kwargs)
      # Identify input CustomTensors, add edge
      graph.record_op(func.__name__, args, result)
      return result
  ```

---

## 4. Simplified Code Sketch

```python
# graph.py
def with_autograd_graph():
    graph = AutogradGraph()
    mgr = ReferenceManager()
    yield graph, mgr
    graph.execute_backward()

# tensor.py
class CustomTensor(torch.Tensor):
    def __new__(cls, data, graph=None, ref_mgr=None, requires_grad=False):
        # Create Tensor subclass
        inst = torch.Tensor._make_subclass(cls, torch.as_tensor(data))
        inst._graph = graph
        inst._ref_mgr = ref_mgr
        inst.requires_grad = requires_grad
        if requires_grad:
            graph.add_node(inst)
            if not inst.is_leaf:
                ref_mgr.add(inst)
        return inst

    def __torch_function__(...):
         # Handle op

# executor.py
class GraphExecutor:
    def backward(self, graph):
        for tensor in graph.reverse_toposort():
            tensor._backward()
```

---

## 5. Benefits of this Design

* **Memory Safety**: Intermediate tensors collected automatically via weakrefs.
* **No Globals**: Context managers and DI eliminate global state.
* **Clear API**: `with_autograd_graph()` pattern separates graph lifetimes.
* **Extendable**: New ops auto-recorded via `__torch_function__`.

---

## 6. Next Steps

1. Implement `__torch_function__` override to record all tensor ops.
2. Build `GraphExecutor` to orchestrate backward.
3. Add unit tests for graph integrity and memory usage.
4. Integrate GPU support and ensure weakrefs play well across devices.

---

*End of Design Document*

the code below is shit but tries to follow the philosphy useful for reference 
"import torch
import weakref
import rustworkx as rx
from typing import Dict, List, Optional, Any, Tuple, Set
from collections import defaultdict
import uuid
from contextlib import contextmanager

# ==============================================================================
# 1. Reference Manager - Handles memory safety with weak references
# ==============================================================================

class ReferenceManager:
    """Manages tensor references using WeakValueDictionary for memory safety."""
    
    def __init__(self):
        # WeakValueDictionary for intermediate tensors - auto-cleanup
        self._intermediate_tensors: weakref.WeakValueDictionary = weakref.WeakValueDictionary()
        # Strong references for leaf tensors to prevent premature collection
        self._leaf_tensors: Dict[str, 'CustomTensor'] = {}
        # Track tensor IDs for cleanup callbacks
        self._tensor_ids: Set[str] = set()
    
    def add_intermediate(self, tensor: 'CustomTensor') -> None:
        """Add intermediate tensor with weak reference."""
        tensor_id = tensor._tensor_id
        self._intermediate_tensors[tensor_id] = tensor
        self._tensor_ids.add(tensor_id)
    
    def add_leaf(self, tensor: 'CustomTensor') -> None:
        """Add leaf tensor with strong reference."""
        tensor_id = tensor._tensor_id
        self._leaf_tensors[tensor_id] = tensor
        self._tensor_ids.add(tensor_id)
    
    def get_tensor(self, tensor_id: str) -> Optional['CustomTensor']:
        """Retrieve tensor by ID from both weak and strong references."""
        if tensor_id in self._leaf_tensors:
            return self._leaf_tensors[tensor_id]
        return self._intermediate_tensors.get(tensor_id)
    
    def remove_tensor(self, tensor_id: str) -> None:
        """Remove tensor from tracking."""
        if tensor_id in self._leaf_tensors:
            del self._leaf_tensors[tensor_id]
        if tensor_id in self._intermediate_tensors:
            del self._intermediate_tensors[tensor_id]
        self._tensor_ids.discard(tensor_id)
    
    def get_all_active_tensors(self) -> List['CustomTensor']:
        """Get all currently active tensors."""
        tensors = []
        for tensor_id in self._tensor_ids:
            tensor = self.get_tensor(tensor_id)
            if tensor is not None:
                tensors.append(tensor)
        return tensors


# ==============================================================================
# 2. Autograd Graph - Manages computation graph structure
# ==============================================================================

class AutogradGraph:
    """Manages the computational graph using rustworkx for efficiency."""
    
    def __init__(self):
        self.graph = rx.PyDiGraph()
        # Maps tensor_id to node_index in graph
        self._tensor_to_node: Dict[str, int] = {}
        # Maps node_index to tensor_id
        self._node_to_tensor: Dict[int, str] = {}
        # Store operation metadata
        self._op_metadata: Dict[int, Dict[str, Any]] = {}
        # Reference manager for memory safety
        self._ref_manager = ReferenceManager()
    
    def add_node(self, tensor: 'CustomTensor') -> int:
        """Add tensor as node to graph."""
        tensor_id = tensor._tensor_id
        
        if tensor_id in self._tensor_to_node:
            return self._tensor_to_node[tensor_id]
        
        node_index = self.graph.add_node(tensor_id)
        self._tensor_to_node[tensor_id] = node_index
        self._node_to_tensor[node_index] = tensor_id
        
        # Add to reference manager
        if tensor._is_leaf:
            self._ref_manager.add_leaf(tensor)
        else:
            self._ref_manager.add_intermediate(tensor)
        
        return node_index
    
    def add_edge(self, parent_tensor: 'CustomTensor', child_tensor: 'CustomTensor', 
                 op_name: str, op_args: Tuple = ()) -> None:
        """Add edge representing operation dependency."""
        parent_node = self.add_node(parent_tensor)
        child_node = self.add_node(child_tensor)
        
        # Add edge from parent to child
        edge_index = self.graph.add_edge(parent_node, child_node, {
            'op_name': op_name,
            'op_args': op_args
        })
        
        # Store operation metadata
        self._op_metadata[child_node] = {
            'op_name': op_name,
            'op_args': op_args,
            'parents': [parent_tensor._tensor_id]
        }
    
    def record_op(self, op_name: str, inputs: Tuple, output: 'CustomTensor') -> None:
        """Record operation in the graph."""
        input_tensors = [arg for arg in inputs if isinstance(arg, CustomTensor)]
        
        if not input_tensors:
            return
        
        # Add edges from all input tensors to output
        for input_tensor in input_tensors:
            self.add_edge(input_tensor, output, op_name, inputs)
    
    def reverse_topological_sort(self) -> List['CustomTensor']:
        """Get tensors in reverse topological order for backward pass."""
        try:
            sorted_nodes = rx.topological_sort(self.graph)
            sorted_nodes.reverse()  # Reverse for backward pass
            
            tensors = []
            for node_index in sorted_nodes:
                tensor_id = self._node_to_tensor[node_index]
                tensor = self._ref_manager.get_tensor(tensor_id)
                if tensor is not None and tensor.requires_grad:
                    tensors.append(tensor)
            
            return tensors
        except rx.DAGHasCycle:
            raise RuntimeError("Computational graph has cycles - cannot perform backward pass")
    
    def check_cycles(self) -> bool:
        """Check if graph has cycles."""
        try:
            rx.topological_sort(self.graph)
            return False
        except rx.DAGHasCycle:
            return True
    
    def get_tensor_by_id(self, tensor_id: str) -> Optional['CustomTensor']:
        """Get tensor by ID."""
        return self._ref_manager.get_tensor(tensor_id)


# ==============================================================================
# 3. Graph Executor - Orchestrates backward pass
# ==============================================================================

class GraphExecutor:
    """Executes backward pass through the computational graph."""
    
    def __init__(self, graph: AutogradGraph):
        self.graph = graph
    
    def backward(self, root_tensor: 'CustomTensor', grad_output: Optional[torch.Tensor] = None) -> None:
        """Execute backward pass starting from root tensor."""
        if not root_tensor.requires_grad:
            return
        
        # Initialize gradient for root tensor
        if grad_output is None:
            if root_tensor.numel() == 1:
                grad_output = torch.ones_like(root_tensor)
            else:
                raise RuntimeError("grad_output must be specified for non-scalar tensors")
        
        # Set gradient for root tensor
        if root_tensor.grad is None:
            root_tensor.grad = grad_output.clone()
        else:
            root_tensor.grad.add_(grad_output)
        
        # Get tensors in reverse topological order
        tensors = self.graph.reverse_topological_sort()
        
        # Execute backward pass
        for tensor in tensors:
            if tensor.grad is not None:
                tensor._backward()


# ==============================================================================
# 4. Custom Tensor - Subclass of torch.Tensor with autograd integration
# ==============================================================================

class CustomTensor(torch.Tensor):
    """Custom tensor subclass with integrated autograd graph tracking."""
    
    def __new__(cls, graph=None, save_strong_reference=None, data=None, 
                requires_grad=False, _tensor=None, _is_leaf=False, dtype=None, device=None):
        
        if _tensor is not None:
            instance = torch.Tensor._make_subclass(cls, _tensor)
        elif data is not None:
            if isinstance(data, torch.Tensor):
                instance = torch.Tensor._make_subclass(cls, data)
            else:
                tensor_data = torch.as_tensor(data, dtype=dtype, device=device)
                instance = torch.Tensor._make_subclass(cls, tensor_data)
        else:
            raise ValueError("Either data or _tensor must be provided")
        
        instance.requires_grad_(False)  # Set requires_grad manually later
        
        # Initialize custom attributes
        instance._graph = graph
        instance._tensor_id = str(uuid.uuid4())
        instance._is_leaf = _is_leaf if _is_leaf is not None else (data is not None)
        instance._backward_fn = None
        instance._op_name = None
        instance._op_args = None
        
        # Set requires_grad and register with graph
        if requires_grad and graph is not None:
            instance.requires_grad_(True)
            graph.add_node(instance)
        
        return instance
    
    @classmethod
    def __torch_function__(cls, func, types, args=(), kwargs=None):
        """Override torch operations to record in autograd graph."""
        if kwargs is None:
            kwargs = {}
        
        # Extract CustomTensors from arguments
        input_tensors = []
        for arg in args:
            if isinstance(arg, CustomTensor):
                input_tensors.append(arg)
        
        # Call original function
        result = super().__torch_function__(func, types, args, kwargs)
        
        # If we have input tensors with gradients, create CustomTensor result
        if input_tensors and any(t.requires_grad for t in input_tensors):
            graph = input_tensors[0]._graph
            if graph is not None and isinstance(result, torch.Tensor):
                # Convert result to CustomTensor
                custom_result = CustomTensor(
                    graph=graph,
                    _tensor=result,
                    requires_grad=True,
                    _is_leaf=False
                )
                
                # Record operation in graph
                graph.record_op(func.__name__, args, custom_result)
                
                # Set up backward function
                custom_result._setup_backward(func.__name__, input_tensors, args, kwargs)
                
                return custom_result
        
        return result
    
    def _setup_backward(self, op_name: str, input_tensors: List['CustomTensor'], 
                       args: Tuple, kwargs: Dict) -> None:
        """Setup backward function for this tensor."""
        self._op_name = op_name
        self._input_tensors = input_tensors
        self._op_args = args
        self._op_kwargs = kwargs
        
        # Create backward function based on operation
        if op_name == 'torch.add':
            self._backward_fn = self._add_backward
        elif op_name == 'torch.mul':
            self._backward_fn = self._mul_backward
        elif op_name == 'torch.matmul':
            self._backward_fn = self._matmul_backward
        elif op_name == 'torch.sum':
            self._backward_fn = self._sum_backward
        elif op_name == 'torch.mean':
            self._backward_fn = self._mean_backward
        else:
            # Default backward (just pass gradient through)
            self._backward_fn = self._default_backward
    
    def _backward(self) -> None:
        """Execute backward pass for this tensor."""
        if self._backward_fn is not None and self.grad is not None:
            self._backward_fn()
    
    def _add_backward(self) -> None:
        """Backward pass for addition."""
        for tensor in self._input_tensors:
            if tensor.requires_grad:
                if tensor.grad is None:
                    tensor.grad = self.grad.clone()
                else:
                    tensor.grad.add_(self.grad)
    
    def _mul_backward(self) -> None:
        """Backward pass for multiplication."""
        if len(self._input_tensors) == 2:
            a, b = self._input_tensors
            if a.requires_grad:
                grad_a = self.grad * b
                if a.grad is None:
                    a.grad = grad_a
                else:
                    a.grad.add_(grad_a)
            
            if b.requires_grad:
                grad_b = self.grad * a
                if b.grad is None:
                    b.grad = grad_b
                else:
                    b.grad.add_(grad_b)
    
    def _matmul_backward(self) -> None:
        """Backward pass for matrix multiplication."""
        if len(self._input_tensors) == 2:
            a, b = self._input_tensors
            if a.requires_grad:
                grad_a = torch.matmul(self.grad, b.T)
                if a.grad is None:
                    a.grad = grad_a
                else:
                    a.grad.add_(grad_a)
            
            if b.requires_grad:
                grad_b = torch.matmul(a.T, self.grad)
                if b.grad is None:
                    b.grad = grad_b
                else:
                    b.grad.add_(grad_b)
    
    def _sum_backward(self) -> None:
        """Backward pass for sum operation."""
        for tensor in self._input_tensors:
            if tensor.requires_grad:
                # Expand gradient to match input shape
                grad = self.grad.expand_as(tensor)
                if tensor.grad is None:
                    tensor.grad = grad
                else:
                    tensor.grad.add_(grad)
    
    def _mean_backward(self) -> None:
        """Backward pass for mean operation."""
        for tensor in self._input_tensors:
            if tensor.requires_grad:
                # Gradient is divided by number of elements
                grad = self.grad.expand_as(tensor) / tensor.numel()
                if tensor.grad is None:
                    tensor.grad = grad
                else:
                    tensor.grad.add_(grad)
    
    def _default_backward(self) -> None:
        """Default backward pass - just propagate gradient."""
        for tensor in self._input_tensors:
            if tensor.requires_grad:
                if tensor.grad is None:
                    tensor.grad = self.grad.clone()
                else:
                    tensor.grad.add_(self.grad)
    
    def backward(self, grad_output: Optional[torch.Tensor] = None) -> None:
        """Initiate backward pass from this tensor."""
        if self._graph is not None:
            executor = GraphExecutor(self._graph)
            executor.backward(self, grad_output)


# ==============================================================================
# 5. Context Manager - Clean API for graph management
# ==============================================================================

@contextmanager
def with_autograd_graph():
    """Context manager for autograd graph lifecycle."""
    graph = AutogradGraph()
    try:
        yield graph
    finally:
        # Graph cleanup happens automatically via weak references
        pass


# ==============================================================================
# 6. Utility Functions
# ==============================================================================

def create_tensor(data, requires_grad=False, graph=None):
    """Utility function to create CustomTensor."""
    return CustomTensor(
        graph=graph,
        data=data,
        requires_grad=requires_grad,
        _is_leaf=True
    )


# ==============================================================================
# 7. Example Usage
# ==============================================================================

if __name__ == "__main__":
    # Example usage demonstrating the custom autograd system
    
    with with_autograd_graph() as graph:
        # Create leaf tensors
        x = create_tensor([[1.0, 2.0], [3.0, 4.0]], requires_grad=True, graph=graph)
        y = create_tensor([[2.0, 1.0], [1.0, 2.0]], requires_grad=True, graph=graph)
        
        # Perform operations
        z = x + y  # Addition
        w = z * x  # Multiplication
        result = w.sum()  # Sum
        
        print("Forward pass:")
        print(f"x = {x}")
        print(f"y = {y}")
        print(f"z = x + y = {z}")
        print(f"w = z * x = {w}")
        print(f"result = w.sum() = {result}")
        
        # Backward pass
        print("\nBackward pass:")
        result.backward()
        
        print(f"x.grad = {x.grad}")
        print(f"y.grad = {y.grad}")
        
        # Check graph structure
        print(f"\nGraph has cycles: {graph.check_cycles()}")
        print(f"Number of nodes in graph: {len(graph._tensor_to_node)}")"